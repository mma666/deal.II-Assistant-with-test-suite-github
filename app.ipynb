{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728f1747-b8fc-4d31-96c2-047fc83c079d",
   "metadata": {},
   "source": [
    "#  deal.II Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53826946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-community langchain-cohere langchain-chroma tiktoken gradio beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a17f111-e290-4ffc-b614-4d10e5673087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.load import dumps, loads\n",
    "import json\n",
    "import tiktoken\n",
    "#from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "#from bs4 import BeautifulSoup as Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohere api\n",
    "import os\n",
    "cohere_api = 'Your Cohere API Key'\n",
    "os.environ['COHERE_API_KEY'] = cohere_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7dafe9-7e3b-4bb5-abeb-4c99b66f623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehdi\\AppData\\Local\\Temp\\ipykernel_42832\\2180031022.py:37: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  doc = loads(json.load(fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of webpages: 86\n",
      "No. of splitted documents: 11051\n",
      "No. of tokens: 2334034\n"
     ]
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "urls = []\n",
    "for i in range(1,91):\n",
    "    if i not in [73,80,84,88]:\n",
    "        base_url = \"https://dealii.org/current/doxygen/deal.II/step_\"\n",
    "        url = base_url + str(i) + \".html\"\n",
    "        urls.append(url)\n",
    "#print(urls)\n",
    "\n",
    "\"\"\"\n",
    "docs = []\n",
    "for url in urls:\n",
    "    loader = RecursiveUrlLoader(url=url, max_depth=11, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "\n",
    "    doc = loader.load()\n",
    "\n",
    "    docs.extend(doc)\n",
    "\n",
    "    string_representation = dumps(doc)\n",
    "\n",
    "    with open(\"./save_urls/step_\" + url[48:-5] + \".json\", \"w\") as fp:\n",
    "        json.dump(string_representation, fp)\n",
    "\n",
    "print(f\"No. of webpages: {len(docs)}\")\n",
    "\"\"\"\n",
    "\n",
    "docs = []\n",
    "for i in range(1,91):\n",
    "    if i not in [73,80,84,88]:\n",
    "        with open(\"./save_urls/step_\" + str(i) + \".json\", \"r\") as fp:\n",
    "            doc = loads(json.load(fp))\n",
    "            docs.extend(doc)\n",
    "\n",
    "print(f\"No. of webpages: {len(docs)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Split the document into chunks\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=400, chunk_overlap=40)\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "print(f\"No. of splitted documents: {len(splitted_docs)}\")\n",
    "\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "print(f\"No. of tokens: {sum(counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4099f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens for all of test docs: 5830042\n",
      "No. of test files: 6310\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Directory containing test files\n",
    "directory_path = \".\\\\tests\"\n",
    "\n",
    "# Size limit in bytes (1 KB = 1 * 1024)\n",
    "size_limit = 1000000000 * 1024  # No limit for now\n",
    "\n",
    "test_docs = []\n",
    "\n",
    "# Use os.walk to go through all subdirectories and files\n",
    "for root, _, files in os.walk(directory_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".cc\"):  # Process only .cc files\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            # Check if file size is within the specified limit\n",
    "            if os.path.getsize(file_path) <= size_limit:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    content = file.readlines()\n",
    "                    content = \"\".join(content[13:])  # remove the first 13 lines related to license notice\n",
    "                    content = content.strip()\n",
    "                \n",
    "                # Create a Document object with file content and metadata\n",
    "                linux_path = os.path.normpath(file_path).replace(\"\\\\\", \"/\")\n",
    "                base_github_url = 'https://github.com/dealii/dealii/blob/master/'\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\"source\": base_github_url+linux_path}\n",
    "                )\n",
    "                \n",
    "                test_docs.append(doc)\n",
    "\n",
    "test_docs_texts = [d.page_content for d in test_docs]\n",
    "test_counts = [num_tokens_from_string(d, \"cl100k_base\") for d in test_docs_texts]\n",
    "print(f\"No. of tokens for all of test docs: {sum(test_counts)}\")\n",
    "print(f\"No. of test files: {len(test_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d05bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\mehdi\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "embedding_function = CohereEmbeddings(model=\"embed-english-v3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72866af",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tutorials = Chroma(persist_directory='./dealii_db_400_40/', embedding_function=embedding_function)\n",
    "\n",
    "#db_tutorials.delete_collection()\n",
    "\n",
    "#db_tutorials = Chroma.from_documents(splitted_docs, embedding_function, persist_directory='./dealii_db_400_40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740cee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tests = Chroma(persist_directory='./dealii_db_tests/', embedding_function=embedding_function)\n",
    "\n",
    "#db_tests.delete_collection()\n",
    "\n",
    "#db_tests = Chroma.from_documents(test_docs, embedding_function, persist_directory='./dealii_db_tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c84f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "llm = ChatCohere(model='command-r', temperature=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f3c4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b99d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "def create_history(history):\n",
    "    s = \"\"\n",
    "    for i in range(0, len(history), 2):\n",
    "        s += f'Question {i//2+1}: ' + history[i]['content'] + \"\\n\"\n",
    "        s += f'Answer {i//2+1}: ' + history[i+1]['content'] + \"\\n\"\n",
    "    return s\n",
    "\n",
    "def remove_empty_string(list_of_questions):\n",
    "    indices = []\n",
    "    for i, sentence in enumerate(list_of_questions):\n",
    "        if sentence == \"\":\n",
    "            indices.append(i)\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        del list_of_questions[i]\n",
    "    return list_of_questions\n",
    "\n",
    "template1 = \"\"\"You are an expert assistant for question-answering tasks for deal.II library, \\\n",
    "an open-source C++ finite element library. The library website can be accessed at https://dealii.org. \\\n",
    "Use the following pieces of retrieved context and history of the conversation to answer the question. \\\n",
    "Provide the code examples where possible. Use contexts sourced from step-* tutorial programs to explain concepts. \\\n",
    "Use the code sourced from *.cc files for code examples. If you don't know the answer, just say that you don't know. \\\n",
    "If you know the answer, cite the source of your answer at the end. REMEMBER to add the sources of your answers at the end.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "History of questions and answers between user and assistant: {history}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "\n",
    "template2 = \"\"\"You are an expert assistant for question-answering tasks for a finite element library. \\\n",
    "Using this library, one can numerically solve ordinary differential equations \\\n",
    "and partial differential equations on mathematical domains for variety of problems. \\\n",
    "Your task is to generate four different versions of the given question. \\\n",
    "JUST output each question in one line. Add the original question also. \\\n",
    "Nothing else should be mentioned in the output, just questions separated by newlines.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "\n",
    "template3 = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, \\\n",
    "reformulate a standalone question which can be understood \\\n",
    "without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is. \\\n",
    "Do not return anything else.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "History of questions and answers between user and assistant: {history}\n",
    "\"\"\"\n",
    "prompt3 = ChatPromptTemplate.from_template(template3)\n",
    "\n",
    "tut_retriever = db_tutorials.as_retriever()\n",
    "tests_retriever = db_tests.as_retriever()\n",
    "\n",
    "reformulate_chain = prompt3 | llm | StrOutputParser()\n",
    "\n",
    "multi_question_chain = prompt2 | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")) | remove_empty_string\n",
    "\n",
    "tut_retrieval_chain = tut_retriever.map() | get_unique_union\n",
    "tests_retrieval_chain = tests_retriever.map() | get_unique_union\n",
    "\n",
    "def multi_retrieval_chain(input_dict):\n",
    "    questions = input_dict['multi_question']\n",
    "    ans1 = tut_retrieval_chain.invoke(questions)\n",
    "    ans2 = tests_retrieval_chain.invoke(questions)\n",
    "    ans1.extend(ans2)\n",
    "    return ans1\n",
    "\n",
    "rag_chain = (\n",
    "    {\"question\": reformulate_chain, \"history\": itemgetter('history')}\n",
    "    | RunnableParallel({\"multi_question\": multi_question_chain, \"question\": itemgetter('question'), \"history\": itemgetter('history')})\n",
    "    | RunnableParallel({\"context\": multi_retrieval_chain, \"question\": itemgetter('question'), \"history\": itemgetter('history')})\n",
    "    | prompt1\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45171351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def response_function(message, history):\n",
    "    history_str = create_history(history)\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for s in rag_chain.stream({'question': message, 'history': history_str}):\n",
    "        partial_message += s\n",
    "        yield partial_message\n",
    "\n",
    "examples = [\"How can you help me?\", \"What is FE_Nothing?\", \"How to write a loop over all cells?\", \"How to construct Lagrange elements?\", \"What is a preconditioner?\",\n",
    "            \"How to use IncrementalFunction function?\", \"How to use to_spherical?\", \"How to add indices to an index set?\", \"How to calculate the distance between 2 points?\"]\n",
    "\n",
    "description = \"\"\"This assistant helps with questions about the functionality of the deal.II library, including its applications, use cases, and specific functions or classes. \\\n",
    "It draws information from all 90 tutorials in the deal.II documentation and the available test suite, though it does not cover the entire documentation.\"\"\"\n",
    "\n",
    "gr.ChatInterface(response_function, type=\"messages\", title=\"deal.II Assistant\", description=description, examples=examples).launch() #share=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ea183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is FE_Nothing?\"\n",
    "\n",
    "history = []\n",
    "history_str = create_history(history)\n",
    "\n",
    "result1 = rag_chain.invoke({'question': question, 'history': history_str})\n",
    "\n",
    "history.append({\"role\": \"user\", \"content\": question})\n",
    "history.append({\"role\": \"assistant\", \"content\": result1})\n",
    "\n",
    "question2 = \"Where can I use it?\"\n",
    "\n",
    "history_str = create_history(history)\n",
    "\n",
    "result2 = rag_chain.invoke({'question': question2, 'history': history_str})\n",
    "\n",
    "history.append({\"role\": \"user\", \"content\": question2})\n",
    "history.append({\"role\": \"assistant\", \"content\": result2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c9e44-c166-4724-a1a9-98dfde0721d7",
   "metadata": {},
   "source": [
    "## Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0afa06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE_Nothing is a special finite element class with exactly zero degrees of freedom per cell. The local basis on each cell for FE_Nothing is the empty set. This finite element is used when one does not need to perform any computations with shape functions, but only needs the JxW values from an FEValues object. \n",
      "\n",
      "An example of its usage can be found in the step-10 tutorial program. Here, FE_Nothing is used together with MappingQ to set up an FEValues object which only computes the JxW values upon calling the reinit function.\n",
      "```cpp\n",
      "// Create a finite element.\n",
      "const MappingQ<dim> mapping(degree);\n",
      "const FE_Nothing<dim> fe;\n",
      "\n",
      "// Set up the FEValues object.\n",
      "FEValues<dim> fe_values(mapping,\n",
      "                      fe,\n",
      "                      quadrature,\n",
      "                      update_values | update_gradients | update_JxW_values);\n",
      "```\n",
      "The FE_Nothing class is also used in the step-46 and step-69 tutorial programs.\n",
      "\n",
      "The term FE_Nothing also appears in some of the test programs for the deal.II library, e.g. fe_nothing_03.cc, fe_nothing_06.cc, fe_nothing_08.cc, fe_nothing_09.cc, fe_nothing_22.cc, fe_nothing_dominates_02.cc, fe_nothing_dominates_03.cc.\n",
      "\n",
      "Sources:\n",
      "step-10: https://dealii.org/current/doxygen/deal.II/step_10.html\n",
      "step-46: https://dealii.org/current/doxygen/deal.II/step_46.html\n",
      "step-69: https://dealii.org/current/doxygen/deal.II/step_69.html\n",
      "fe_nothing_03.cc: https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_03.cc\n",
      "fe_nothing_06.cc: https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_06.cc\n",
      "fe_nothing_08.cc: https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_08.cc\n",
      "fe_nothing_09.cc: https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_09.cc\n",
      "fe_nothing_22.cc: https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_22.cc\n",
      "fe_nothing_dominates_02.cc: https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_dominates_02.cc\n",
      "fe_nothing_dominates_03.cc: https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_dominates_03.cc\n"
     ]
    }
   ],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efb52c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE_Nothing finite elements are used when one does not need to perform any computations with shape functions, but only requires the JxW values from an FEValues object. In such cases, FE_Nothing is a handy finite element choice as it has zero degrees of freedom per cell. This means that there is no need to compute other quantities upon calling the reinit function on the FEValues object, thereby saving computational time.\n",
      "\n",
      "FE_Nothing is employed in situations where the goal is to extend functions to the entire domain, keeping them zero on a particular cell. Two use cases of FE_Nothing in the deal.II library are:\n",
      "\n",
      "1. **Extending functions by zero to the entire domain**: In step-46, FE_Nothing is used to define a finite dimensional function space of functions that are constantly zero. This technique is employed to handle multiphysics problems where each of the physics involved is defined on different subdomains. By extending the functions to the entire domain, it becomes possible to use the same finite elements and interpolation techniques everywhere, thus making the implementation uniform and simpler.\n",
      "2. **Hanging node constraints**: In tests such as fe_nothing_05.cc, fe_nothing_06.cc, and fe_nothing_03.cc, FE_Nothing is used in combination with other finite elements to create meshes with hanging nodes and to test constraint handling capabilities of the library. Hanging node constraints arise when dealing with non-matching meshes, where different elements meet at a node but have different shapes or sizes.\n",
      "\n",
      "The fe_nothing_05.cc and fe_nothing_06.cc tests create a mesh with hanging nodes and FE_Q/FE_Nothing interfaces, and then attempt to make hanging node constraints on this mesh using the DoFTools library functions. Similarly, the fe_nothing_03.cc test sets cells within a circle to be of the FE_Nothing type, and cells outside the circle to be of the FE_Q type. This allows one to test how well the library can handle constraints in these situations.\n",
      "\n",
      "Other tests such as fe_nothing_11.cc, fe_nothing_15.cc, fe_nothing_21.cc, and fe_nothing_08.cc also demonstrate various uses of FE_Nothing, including its application in the hp-finite element setting, extracting shape functions, and interpolating boundary values.\n",
      "\n",
      "Sources:\n",
      "\n",
      "- https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_03.cc\n",
      "- https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_05.cc\n",
      "- https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_06.cc\n",
      "- https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_08.cc\n",
      "- https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_11.cc\n",
      "- https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_15.cc\n",
      "- https://github.com/dealii/dealii/blob/master/tests/hp/fe_nothing_21.cc\n",
      "- https://dealii.org/current/doxygen/deal.II/step_46.html\n",
      "- https://dealii.org/current/doxygen/deal.II/step_10.html\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e2b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
