"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_69.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-69 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-69 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-69 tutorial program\\n\\n\\nThis tutorial depends on step-33, step-40.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\nEuler's equations of gas dynamics\\n\\nSolution theory\\nVariational versus collocation-type discretizations\\n\\nDescription of the scheme \\nStable boundary conditions and conservation properties.\\n\\n The commented program\\n\\nInclude files\\nClass template declarations\\n\\nThe Discretization class\\nThe OfflineData class\\nThe ProblemDescription class\\nThe InitialValues class\\nThe %TimeStepping class\\nThe SchlierenPostprocessor class\\nThe MainLoop class\\n\\nImplementation\\n\\nGrid generation, setup of data structures\\nAssembly of offline matrices\\nTranslation to local index ranges\\nEquation of state and approximate Riemann solver\\nInitial values\\nThe Forward Euler step\\nSchlieren postprocessing\\nThe main loop\\n\\nResume\\nOutput and checkpointing\\n\\n\\n\\n\\n Results\\n\\nPossibilities for extensions\\n\\n The plain program\\n   \\n This program was contributed by Matthias Maier (Texas A&M University), and Ignacio Tomas (Sandia National Laboratories \\\\(^{\\\\!\\\\dagger}\\\\)). \\n\\\\(^\\\\dagger\\\\)Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. This document describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government.\\nNoteThis tutorial step implements a first-order accurate guaranteed maximum wavespeed method based on a first-order graph viscosity for solving Euler's equations of gas dynamics [106]. As such it is presented primarily for educational purposes. For actual research computations you might want to consider exploring a corresponding high-performance implementation of a second-order accurate scheme that uses convex limiting techniques, and strong stability-preserving (SSP) time integration, see [107] (website).\\n\\nIf you use this program as a basis for your own work, please consider citing it in your list of references. The initial version of this work was contributed to the deal.II project by the authors listed in the following citation:   \\n Introduction\\nThis tutorial presents a first-order scheme for solving compressible Euler's equations that is based on three ingredients: a collocation-type discretization of Euler's equations in the context of finite elements; a graph-viscosity stabilization based on a guaranteed upper bound of the local wave speed; and explicit time-stepping. As such, the ideas and techniques presented in this tutorial step are drastically different from those used in step-33, which focuses on the use of automatic differentiation. From a programming perspective this tutorial will focus on a number of techniques found in large-scale computations: hybrid thread-MPI parallelization; efficient local numbering of degrees of freedom; concurrent post-processing and write-out of results using worker threads; as well as checkpointing and restart.\\nIt should be noted that first-order schemes in the context of hyperbolic conservation laws require prohibitively many degrees of freedom to resolve certain key features of the simulated fluid, and thus, typically only serve as elementary building blocks in higher-order schemes [107]. However, we hope that the reader still finds the tutorial step to be a good starting point (in particular with respect to the programming techniques) before jumping into full research codes such as the second-order scheme discussed in [107].\\n Euler's equations of gas dynamics\\nThe compressible Euler's equations of gas dynamics are written in conservative form as follows:   \\n\\\\begin{align}\\n\\\\mathbf{u}_t + \\\\text{div} \\\\, \\\\mathbb{f}(\\\\mathbf{u}) = \\\\boldsymbol{0} ,\\n\\\\end{align}\\n\\n where  \\\\(\\\\mathbf{u}(\\\\textbf{x},t):\\\\mathbb{R}^{d} \\\\times \\\\mathbb{R}\\n\\\\rightarrow \\\\mathbb{R}^{d+2}\\\\), and  \\\\(\\\\mathbb{f}(\\\\mathbf{u}):\\\\mathbb{R}^{d+2}\\n\\\\rightarrow \\\\mathbb{R}^{(d+2) \\\\times d}\\\\), and \\\\(d \\\\geq 1\\\\) is the space dimension. We say that \\\\(\\\\mathbf{u} \\\\in \\\\mathbb{R}^{d+2}\\\\) is the state and \\\\(\\\\mathbb{f}(\\\\mathbf{u}) \\\\in  \\\\mathbb{R}^{(d+2) \\\\times d}\\\\) is the flux of the system. In the case of Euler's equations the state is given by \\\\(\\\\textbf{u} = [\\\\rho, \\\\textbf{m}^\\\\top,E]^{\\\\top}\\\\): where \\\\(\\\\rho \\\\in \\\\mathbb{R}^+\\\\) denotes the density, \\\\(\\\\textbf{m} \\\\in \\\\mathbb{R}^d\\\\) is the momentum, and  \\\\(E\\n\\\\in \\\\mathbb{R}^+\\\\) is the total energy of the system. The flux of the system \\\\(\\\\mathbb{f}(\\\\mathbf{u})\\\\) is defined as         \\n\\\\begin{align*}\\n\\\\mathbb{f}(\\\\textbf{u})\\n=\\n\\\\begin{bmatrix}\\n  \\\\textbf{m}^\\\\top \\\\\\\\\\n  \\\\rho^{-1} \\\\textbf{m} \\\\otimes \\\\textbf{m} + \\\\mathbb{I} p\\\\\\\\\\n  \\\\tfrac{\\\\textbf{m}^\\\\top}{\\\\rho} (E + p)\\n\\\\end{bmatrix},\\n\\\\end{align*}\\n\\n where \\\\(\\\\mathbb{I} \\\\in \\\\mathbb{R}^{d \\\\times d}\\\\) is the identity matrix and \\\\(\\\\otimes\\\\) denotes the tensor product. Here, we have introduced the pressure \\\\(p\\\\) that, in general, is defined by a closed-form equation of state. In this tutorial we limit the discussion to the class of polytropic ideal gases for which the pressure is given by     \\n\\\\begin{align*}\\np = p(\\\\textbf{u}) := (\\\\gamma -1) \\\\Big(E -\\n\\\\tfrac{|\\\\textbf{m}|^2}{2\\\\,\\\\rho}\\n\\\\Big),\\n\\\\end{align*}\\n\\n where the factor \\\\(\\\\gamma \\\\in (1,5/3]\\\\) denotes the ratio of specific heats.\\nSolution theory\\nHyperbolic conservation laws, such as   \\n\\\\begin{align*}\\n\\\\mathbf{u}_t + \\\\text{div} \\\\, \\\\mathbb{f}(\\\\mathbf{u}) = \\\\boldsymbol{0},\\n\\\\end{align*}\\n\\n pose a significant challenge with respect to solution theory. An evident observation is that rewriting the equation in variational form and testing with the solution itself does not lead to an energy estimate because the pairing \\\\(\\\\langle \\\\text{div} \\\\, \\\\mathbb{f}(\\\\mathbf{u}), \\\\mathbf{u}\\\\rangle\\\\) (understood as the \\\\(L^2(\\\\Omega)\\\\) inner product or duality pairing) is not guaranteed to be non-negative. Notions such as energy-stability or \\\\(L^2(\\\\Omega)\\\\)-stability are (in general) meaningless in this context.\\nHistorically, the most fruitful step taken in order to deepen the understanding of hyperbolic conservation laws was to assume that the solution is formally defined as  \\\\(\\\\mathbf{u} := \\\\lim_{\\\\epsilon \\\\rightarrow\\n0^+} \\\\mathbf{u}^{\\\\epsilon}\\\\) where \\\\(\\\\mathbf{u}^{\\\\epsilon}\\\\) is the solution of the parabolic regularization    \\n\\\\begin{align}\\n\\\\mathbf{u}_t^{\\\\epsilon} + \\\\text{div} \\\\, \\\\mathbb{f}(\\\\mathbf{u}^{\\\\epsilon})\\n- {\\\\epsilon} \\\\Delta \\\\mathbf{u}^{\\\\epsilon} = 0.\\n\\\\end{align}\\n\\n Such solutions, which are understood as the solution recovered in the zero-viscosity limit, are often referred to as viscosity solutions. (This is, because physically \\\\(\\\\epsilon\\\\) can be understood as related to the viscosity of the fluid, i.e., a quantity that indicates the amount of friction neighboring gas particles moving at different speeds exert on each other. The Euler equations themselves are derived under the assumption of no friction, but can physically be expected to describe the limiting case of vanishing friction or viscosity.) Global existence and uniqueness of such solutions is an open issue. However, we know at least that if such viscosity solutions exists they have to satisfy the constraint \\\\(\\\\textbf{u}(\\\\mathbf{x},t) \\\\in \\\\mathcal{B}\\\\) for all \\\\(\\\\mathbf{x} \\\\in \\\\Omega\\\\) and \\\\(t \\\\geq 0\\\\) where           \\n\\\\begin{align}\\n  \\\\mathcal{B} = \\\\big\\\\{ \\\\textbf{u} =\\n  [\\\\rho, \\\\textbf{m}^\\\\top,E]^{\\\\top} \\\\in \\\\mathbb{R}^{d+2} \\\\, \\\\big |\\n  \\\\\\n  \\\\rho > 0 \\\\, ,\\n  \\\\\\n  \\\\ E - \\\\tfrac{|\\\\textbf{m}|^2}{2 \\\\rho} > 0 \\\\, ,\\n  \\\\\\n  s(\\\\mathbf{u}) \\\\geq \\\\min_{x \\\\in \\\\Omega} s(\\\\mathbf{u}_0(\\\\mathbf{x}))\\n  \\\\big\\\\}.\\n\\\\end{align}\\n\\n Here, \\\\(s(\\\\mathbf{u})\\\\) denotes the specific entropy   \\n\\\\begin{align}\\n  s(\\\\mathbf{u}) = \\\\ln \\\\Big(\\\\frac{p(\\\\mathbf{u})}{\\\\rho^{\\\\gamma}}\\\\Big).\\n\\\\end{align}\\n\\n We will refer to \\\\(\\\\mathcal{B}\\\\) as the invariant set of Euler's equations. In other words, a state \\\\(\\\\mathbf{u}(\\\\mathbf{x},t)\\\\in\\\\mathcal{B}\\\\) obeys positivity of the density, positivity of the internal energy, and a local minimum principle on the specific entropy. This condition is a simplified version of a class of pointwise stability constraints satisfied by the exact (viscosity) solution. By pointwise we mean that the constraint has to be satisfied at every point of the domain, not just in an averaged (integral, or high order moments) sense.\\nIn context of a numerical approximation, a violation of such a constraint has dire consequences: it almost surely leads to catastrophic failure of the numerical scheme, loss of hyperbolicity, and overall, loss of well-posedness of the (discrete) problem. It would also mean that we have computed something that can not be interpreted physically. (For example, what are we to make of a computed solution with a negative density?) In the following we will formulate a scheme that ensures that the discrete approximation of \\\\(\\\\mathbf{u}(\\\\mathbf{x},t)\\\\) remains in \\\\(\\\\mathcal{B}\\\\).\\nVariational versus collocation-type discretizations\\nFollowing step-9, step-12, step-33, and step-67, at this point it might look tempting to base a discretization of Euler's equations on a (semi-discrete) variational formulation:      \\n\\\\begin{align*}\\n  (\\\\partial_t\\\\mathbf{u}_{h},\\\\textbf{v}_h)_{L^2(\\\\Omega)}\\n  - ( \\\\mathbb{f}(\\\\mathbf{u}_{h}) ,\\\\text{grad} \\\\, \\\\textbf{v}_{h})_{L^2(\\\\Omega)}\\n  + s_h(\\\\mathbf{u}_{h},\\\\textbf{v}_h)_{L^2(\\\\Omega)} = \\\\boldsymbol{0}\\n  \\\\quad\\\\forall \\\\textbf{v}_h \\\\in \\\\mathbb{V}_h.\\n\\\\end{align*}\\n\\n Here, \\\\(\\\\mathbb{V}_h\\\\) is an appropriate finite element space, and \\\\(s_h(\\\\cdot,\\\\cdot)_{L^2(\\\\Omega)}\\\\) is some linear stabilization method (possibly complemented with some ad-hoc shock-capturing technique, see for instance Chapter 5 of [82] and references therein). Most time-dependent discretization approaches described in the deal.II tutorials are based on such a (semi-discrete) variational approach. Fundamentally, from an analysis perspective, variational discretizations are conceived to provide some notion of global (integral) stability, meaning an estimate of the form   \\n\\\\begin{align*}\\n  |\\\\!|\\\\!| \\\\mathbf{u}_{h}(t) |\\\\!|\\\\!| \\\\leq |\\\\!|\\\\!| \\\\mathbf{u}_{h}(0) |\\\\!|\\\\!|\\n\\\\end{align*}\\n\\n holds true, where \\\\(|\\\\!|\\\\!| \\\\cdot |\\\\!|\\\\!| \\\\) could represent the \\\\(L^2(\\\\Omega)\\\\)-norm or, more generally, some discrete (possibly mesh dependent) energy-norm. Variational discretizations of hyperbolic conservation laws have been very popular since the mid eighties, in particular combined with SUPG-type stabilization and/or upwinding techniques (see the early work of [46] and [123]). They have proven to be some of the best approaches for simulations in the subsonic shockless regime and similarly benign situations.\\nHowever, in the transonic and supersonic regimes, and shock-hydrodynamics applications the use of variational schemes might be questionable. In fact, at the time of this writing, most shock-hydrodynamics codes are still firmly grounded on finite volume methods. The main reason for failure of variational schemes in such extreme regimes is the lack of pointwise stability. This stems from the fact that a priori bounds on integrated quantities (e.g. integrals of moments) have in general no implications on pointwise properties of the solution. While some of these problems might be alleviated by the (perpetual) chase of the right shock capturing scheme, finite difference-like and finite volume schemes still have an edge in many regards.\\nIn this tutorial step we therefore depart from variational schemes. We will present a completely algebraic formulation (with the flavor of a collocation-type scheme) that preserves constraints pointwise, i.e.,    \\n\\\\begin{align*}\\n  \\\\textbf{u}_h(\\\\mathbf{x}_i,t) \\\\in \\\\mathcal{B}\\n  \\\\;\\\\text{at every node}\\\\;\\\\mathbf{x}_i\\\\;\\\\text{of the mesh}.\\n\\\\end{align*}\\n\\n Contrary to finite difference/volume schemes, the scheme implemented in this step maximizes the use of finite element software infrastructure, works on any mesh, in any space dimension, and is theoretically guaranteed to always work, all the time, no exception. This illustrates that deal.II can be used far beyond the context of variational schemes in Hilbert spaces and that a large number of classes, topics, and namespaces from deal.II can be adapted for such a purpose.\\nDescription of the scheme \\nLet \\\\(\\\\mathbb{V}_h\\\\) be scalar-valued finite dimensional space spanned by a basis \\\\(\\\\{\\\\phi_i\\\\}_{i \\\\in \\\\mathcal{V}}\\\\) where:  \\\\(\\\\phi_i:\\\\Omega \\\\rightarrow\\n\\\\mathbb{R}\\\\) and \\\\(\\\\mathcal{V}\\\\) is the set of all indices (nonnegative integers) identifying each scalar Degree of Freedom (DOF) in the mesh. Therefore a scalar finite element functional \\\\(u_h \\\\in \\\\mathbb{V}_h\\\\) can be written as \\\\(u_h = \\\\sum_{i \\\\in \\\\mathcal{V}} U_i \\\\phi_i\\\\) with  \\\\(U_i \\\\in\\n\\\\mathbb{R}\\\\). We introduce the notation for vector-valued approximation spaces \\\\(\\\\pmb{\\\\mathbb{V}}_h := \\\\{\\\\mathbb{V}_h\\\\}^{d+2}\\\\). Let  \\\\(\\\\mathbf{u}_h\\n\\\\in \\\\pmb{\\\\mathbb{V}}_h\\\\), then it can be written as  \\\\(\\\\mathbf{u}_h = \\\\sum_{i\\n\\\\in \\\\mathcal{V}} \\\\mathbf{U}_i \\\\phi_i\\\\) where  \\\\(\\\\mathbf{U}_i \\\\in\\n\\\\mathbb{R}^{d+2}\\\\) and \\\\(\\\\phi_i\\\\) is a scalar-valued shape function.\\nNoteWe purposely refrain from using vector-valued finite element spaces in our notation. Vector-valued finite element spaces are natural for variational formulations of PDE systems (e.g. Navier-Stokes). In such context, the interactions that have to be computed describe interactions between DOFs: with proper renumbering of the vector-valued DoFHandler (i.e. initialized with an FESystem) it is possible to compute the block-matrices (required in order to advance the solution) with relative ease. However, the interactions that have to be computed in the context of time-explicit collocation-type schemes (such as finite differences and/or the scheme presented in this tutorial) can be better described as interactions between nodes (not between DOFs). In addition, in our case we do not solve a linear equation in order to advance the solution. This leaves very little reason to use vector-valued finite element spaces both in theory and/or practice.\\nWe will use the usual Lagrange finite elements: let  \\\\(\\\\{\\\\mathbf{x}_i\\\\}_{i \\\\in\\n\\\\mathcal{V}}\\\\) denote the set of all support points (see this glossary entry), where \\\\(\\\\mathbf{x}_i \\\\in \\\\mathbb{R}^d\\\\). Then each index  \\\\(i \\\\in\\n\\\\mathcal{V}\\\\) uniquely identifies a support point \\\\(\\\\mathbf{x}_i\\\\), as well as a scalar-valued shape function \\\\(\\\\phi_i\\\\). With this notation at hand we can define the (explicit time stepping) scheme as:      \\n\\\\begin{align*}\\n  m_i \\\\frac{\\\\mathbf{U}_i^{n+1} - \\\\mathbf{U}_i^{n}}{\\\\tau}\\n  + \\\\sum_{j \\\\in \\\\mathcal{I}(i)} \\\\mathbb{f}(\\\\mathbf{U}_j^{n})\\\\cdot\\n  \\\\mathbf{c}_{ij} - \\\\sum_{j \\\\in \\\\mathcal{I}(i)}\\n  d_{ij} \\\\mathbf{U}_j^{n} = \\\\boldsymbol{0} \\\\, ,\\n\\\\end{align*}\\n\\n where\\n\\\\(m_i \\\\dealcoloneq \\\\int_{\\\\Omega} \\\\phi_i \\\\, \\\\mathrm{d}\\\\mathbf{x}\\\\) is the lumped mass matrix\\n\\\\(\\\\tau\\\\) is the time step size\\n\\\\(\\\\mathbf{c}_{ij} \\\\dealcoloneq \\\\int_{\\\\Omega} \\\\nabla\\\\phi_j\\\\phi_i \\\\,\\n    \\\\mathrm{d}\\\\mathbf{x}\\\\) (note that \\\\(\\\\mathbf{c}_{ij}\\\\in \\\\mathbb{R}^d\\\\)) is a vector-valued matrix that was used to approximate the divergence of the flux in a weak sense.\\n\\\\(\\\\mathcal{I}(i) \\\\dealcoloneq \\\\{j \\\\in \\\\mathcal{V} \\\\ | \\\\ \\\\mathbf{c}_{ij}\\n    \\\\not \\\\equiv \\\\boldsymbol{0}\\\\} \\\\cup \\\\{i\\\\}\\\\) is the adjacency list containing all degrees of freedom coupling to the index \\\\(i\\\\). In other words \\\\(\\\\mathcal{I}(i)\\\\) contains all nonzero column indices for row index i. \\\\(\\\\mathcal{I}(i)\\\\) will also be called a \\\"stencil\\\".\\n\\\\(\\\\mathbb{f}(\\\\mathbf{U}_j^{n})\\\\) is the flux \\\\(\\\\mathbb{f}\\\\) of the hyperbolic system evaluated for the state \\\\(\\\\mathbf{U}_j^{n}\\\\) associated with support point \\\\(\\\\mathbf{x}_j\\\\).\\n\\\\(d_{ij} \\\\dealcoloneq \\\\max \\\\{ \\\\lambda_{\\\\text{max}}\\n    (\\\\mathbf{U}_i^{n},\\\\mathbf{U}_j^{n}, \\\\textbf{n}_{ij}),\\n    \\\\lambda_{\\\\text{max}} (\\\\mathbf{U}_j^{n}, \\\\mathbf{U}_i^{n},\\n    \\\\textbf{n}_{ji}) \\\\} \\\\|\\\\mathbf{c}_{ij}\\\\|\\\\) if \\\\(i \\\\not = j\\\\) is the so called graph viscosity. The graph viscosity serves as a stabilization term, it is somewhat the discrete counterpart of \\\\(\\\\epsilon \\\\Delta \\\\mathbf{u}\\\\) that appears in the notion of viscosity solution described above. We will base our construction of \\\\(d_{ij}\\\\) on an estimate of the maximal local wavespeed \\\\(\\\\lambda_{\\\\text{max}}\\\\) that will be explained in detail in a moment.\\nthe diagonal entries of the viscosity matrix are defined as \\\\(d_{ii} = - \\\\sum_{j \\\\in \\\\mathcal{I}(i)\\\\backslash \\\\{i\\\\}} d_{ij}\\\\).\\n\\\\(\\\\textbf{n}_{ij} = \\\\frac{\\\\mathbf{c}_{ij}}{ \\\\|\\\\mathbf{c}_{ij}\\\\| }\\\\) is a normalization of the \\\\(\\\\textbf{c}_{ij}\\\\) matrix that enters the approximate Riemann solver with which we compute an the approximations \\\\(\\\\lambda_{\\\\text{max}}\\\\) on the local wavespeed. (This will be explained further down below).\\n\\nThe definition of  \\\\(\\\\lambda_{\\\\text{max}} (\\\\mathbf{U},\\\\mathbf{V},\\n\\\\textbf{n})\\\\) is far from trivial and we will postpone the precise definition in order to focus first on some algorithmic and implementation questions. We note that\\n\\\\(m_i\\\\) and \\\\(\\\\mathbf{c}_{ij}\\\\) do not evolve in time (provided we keep the discretization fixed). It thus makes sense to assemble these matrices/vectors once in a so called offline computation and reuse them in every time step. They are part of what we are going to call off-line data.\\nAt every time step we have to evaluate \\\\(\\\\mathbb{f}(\\\\mathbf{U}_j^{n})\\\\) and    \\\\(d_{ij} \\\\dealcoloneq \\\\max \\\\{ \\\\lambda_{\\\\text{max}}\\n    (\\\\mathbf{U}_i^{n},\\\\mathbf{U}_j^{n}, \\\\textbf{n}_{ij}),\\n    \\\\lambda_{\\\\text{max}} (\\\\mathbf{U}_j^{n}, \\\\mathbf{U}_i^{n},\\n    \\\\textbf{n}_{ji}) \\\\} \\\\|\\\\mathbf{c}_{ij}\\\\| \\\\), which will constitute the bulk of the computational cost.\\n\\nConsider the following pseudo-code, illustrating a possible straight forward strategy for computing the solution \\\\(\\\\textbf{U}^{n+1}\\\\) at a new time \\\\(t_{n+1} = t_n + \\\\tau_n\\\\) given a known state \\\\(\\\\textbf{U}^{n}\\\\) at time \\\\(t_n\\\\):                \\n\\\\begin{align*}\\n&\\\\textbf{for } i \\\\in \\\\mathcal{V} \\\\\\\\\\n&\\\\ \\\\ \\\\ \\\\  \\\\{\\\\mathbf{c}_{ij}\\\\}_{j \\\\in \\\\mathcal{I}(i)} \\\\leftarrow\\n\\\\mathtt{gather\\\\_cij\\\\_vectors} (\\\\textbf{c}, \\\\mathcal{I}(i)) \\\\\\\\\\n&\\\\ \\\\ \\\\ \\\\ \\\\{\\\\textbf{U}_j^n\\\\}_{j \\\\in \\\\mathcal{I}(i)} \\\\leftarrow\\n\\\\mathtt{gather\\\\_state\\\\_vectors} (\\\\textbf{U}^n, \\\\mathcal{I}(i)) \\\\\\\\\\n&\\\\ \\\\ \\\\ \\\\ \\\\ \\\\textbf{U}_i^{n+1} \\\\leftarrow \\\\mathbf{U}_i^{n} \\\\\\\\\\n&\\\\ \\\\ \\\\ \\\\ \\\\textbf{for } j \\\\in \\\\mathcal{I}(i)\\\\backslash\\\\{i\\\\} \\\\\\\\\\n&\\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\  \\\\texttt{compute } d_{ij} \\\\\\\\\\n&\\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\  \\\\texttt{compute } \\\\mathbb{f}(\\\\mathbf{U}_j^{n}) \\\\\\\\\\n&\\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\  \\\\textbf{U}_i^{n+1} \\\\leftarrow \\\\textbf{U}_i^{n+1} - \\\\frac{\\\\tau_n}{m_i}\\n \\\\mathbb{f}(\\\\mathbf{U}_j^{n})\\\\cdot \\\\mathbf{c}_{ij} + d_{ij} \\\\mathbf{U}_j^{n} \\\\\\\\\\n&\\\\ \\\\ \\\\ \\\\ \\\\textbf{end} \\\\\\\\\\n&\\\\ \\\\ \\\\ \\\\ \\\\mathtt{scatter\\\\_updated\\\\_state} (\\\\textbf{U}_i^{n+1}) \\\\\\\\\\n&\\\\textbf{end}\\n\\\\end{align*}\\n\\nWe note here that:\\nThis \\\"assembly\\\" does not require any form of quadrature or cell-loops.\\nHere \\\\(\\\\textbf{c}\\\\) and \\\\(\\\\textbf{U}^n\\\\) are a global matrix and a global vector containing all the vectors \\\\(\\\\mathbf{c}_{ij}\\\\) and all the states \\\\(\\\\mathbf{U}_j^n\\\\) respectively.\\n\\\\(\\\\mathtt{gather\\\\_cij\\\\_vectors}\\\\), \\\\(\\\\mathtt{gather\\\\_state\\\\_vectors}\\\\), and \\\\(\\\\mathtt{scatter\\\\_updated\\\\_state}\\\\) are hypothetical implementations that either collect (from) or write (into) global matrices and vectors.\\nIf we assume a Cartesian mesh in two space dimensions, first-order polynomial space \\\\(\\\\mathbb{Q}^1\\\\), and that \\\\(\\\\mathbf{x}_i\\\\) is an interior node (i.e. \\\\(\\\\mathbf{x}_i\\\\) is not on the boundary of the domain) then: \\\\(\\\\{\\\\textbf{U}_j^n\\\\}_{j \\\\in \\\\mathcal{I}(i)}\\\\) should contain nine state vector elements (i.e. all the states in the patch/macro element associated to the shape function \\\\(\\\\phi_i\\\\)). This is one of the major differences with the usual cell-based loop where the gather functionality (encoded in FEValuesBase<dim, spacedim>.get_function_values() in the case of deal.II) only collects values for the local cell (just a subset of the patch).\\n\\nThe actual implementation will deviate from above code in one key aspect: the time-step size \\\\(\\\\tau\\\\) has to be chosen subject to a CFL condition    \\n\\\\begin{align*}\\n  \\\\tau_n = c_{\\\\text{cfl}}\\\\,\\\\min_{\\n  i\\\\in\\\\mathcal{V}}\\\\left(\\\\frac{m_i}{-2\\\\,d_{ii}^{n}}\\\\right),\\n\\\\end{align*}\\n\\n where \\\\(0<c_{\\\\text{cfl}}\\\\le1\\\\) is a chosen constant. This will require to compute all \\\\(d_{ij}\\\\) in a separate step prior to actually performing above update. The core principle remains unchanged, though: we do not loop over cells but rather over all edges of the sparsity graph.\\nNoteIt is not uncommon to encounter such fully-algebraic schemes (i.e. no bilinear forms, no cell loops, and no quadrature) outside of the finite element community in the wider CFD community. There is a rich history of application of this kind of schemes, also called edge-based or graph-based finite element schemes (see for instance [147] for a historical overview). However, it is important to highlight that the algebraic structure of the scheme (presented in this tutorial) and the node-loops are not just a performance gimmick. Actually, the structure of this scheme was born out of theoretical necessity: the proof of pointwise stability of the scheme hinges on the specific algebraic structure of the scheme. In addition, it is not possible to compute the algebraic viscosities \\\\(d_{ij}\\\\) using cell-loops since they depend nonlinearly on information that spans more than one cell (superposition does not hold: adding contributions from separate cells does not lead to the right result).\\nStable boundary conditions and conservation properties.\\nIn the example considered in this tutorial step we use three different types of boundary conditions: essential-like boundary conditions (we prescribe a state at the left boundary of our domain), outflow boundary conditions (also called \\\"do-nothing\\\" boundary conditions) at the right boundary of the domain, and \\\"reflecting\\\" boundary conditions  \\\\(\\\\mathbf{m} \\\\cdot\\n\\\\boldsymbol{\\\\nu} = 0\\\\) (also called \\\"slip\\\" boundary conditions) at the top, bottom, and surface of the obstacle. We will not discuss much about essential and \\\"do-nothing\\\" boundary conditions since their implementation is relatively easy and the reader will be able to pick-up the implementation directly from the (documented) source code. In this portion of the introduction we will focus only on the \\\"reflecting\\\" boundary conditions which are somewhat more tricky.\\nNoteAt the time of this writing (early 2020) it is not unreasonable to say that both analysis and implementation of stable boundary conditions for hyperbolic systems of conservation laws is an open issue. For the case of variational formulations, stable boundary conditions are those leading to a well-posed (coercive) bilinear form. But for general hyperbolic systems of conservation laws (and for the algebraic formulation used in this tutorial) coercivity has no applicability and/or meaning as a notion of stability. In this tutorial step we will use preservation of the invariant set as our main notion of stability which (at the very least) guarantees well-posedness of the discrete problem.\\nFor the case of the reflecting boundary conditions we will proceed as follows:\\nFor every time step advance in time satisfying no boundary condition at all.\\nLet \\\\(\\\\partial\\\\Omega^r\\\\) be the portion of the boundary where we want to enforce reflecting boundary conditions. At the end of the time step we enforce reflecting boundary conditions strongly in a post-processing step where we execute the projection           \\n\\\\begin{align*}\\n    \\\\mathbf{m}_i \\\\dealcoloneq \\\\mathbf{m}_i - (\\\\widehat{\\\\boldsymbol{\\\\nu}}_i\\n    \\\\cdot \\\\mathbf{m}_i)  \\\\widehat{\\\\boldsymbol{\\\\nu}}_i \\\\ \\\\\\n    \\\\text{where} \\\\ \\\\\\n    \\\\widehat{\\\\boldsymbol{\\\\nu}}_i \\\\dealcoloneq\\n    \\\\frac{\\\\int_{\\\\partial\\\\Omega} \\\\phi_i \\\\widehat{\\\\boldsymbol{\\\\nu}} \\\\,\\n    \\\\, \\\\mathrm{d}\\\\mathbf{s}}{\\\\big|\\\\int_{\\\\partial\\\\Omega} \\\\phi_i\\n    \\\\widehat{\\\\boldsymbol{\\\\nu}} \\\\, \\\\mathrm{d}\\\\mathbf{s}\\\\big|}\\n    \\\\ \\\\ \\\\text{for all }\\\\mathbf{x}_i \\\\in \\\\partial\\\\Omega^r\\n    \\\\ \\\\ \\\\ \\\\ \\\\boldsymbol{(1)}\\n    \\\\end{align*}\\n\\n that removes the normal component of \\\\(\\\\mathbf{m}\\\\). This is a somewhat naive idea that preserves a few fundamental properties of the PDE as we explain below.\\n\\nThis is approach is usually called \\\"explicit treatment of boundary conditions\\\". The well seasoned finite element person might find this approach questionable. No doubt, when solving parabolic, or elliptic equations, we typically enforce essential (Dirichlet-like) boundary conditions by making them part of the approximation space \\\\(\\\\mathbb{V}\\\\), and treat natural (e.g. Neumann) boundary conditions as part of the variational formulation. We also know that explicit treatment of boundary conditions (in the context of parabolic PDEs) almost surely leads to catastrophic consequences. However, in the context of nonlinear hyperbolic equations we have that:\\nIt is relatively easy to prove that (for the case of reflecting boundary conditions) explicit treatment of boundary conditions is not only conservative but also guarantees preservation of the property \\\\(\\\\mathbf{U}_i \\\\in \\\\mathcal{B}\\\\) for all \\\\(i \\\\in \\\\mathcal{V}\\\\) (well-posedness). This is perhaps the most important reason to use explicit enforcement of boundary conditions.\\nTo the best of our knowledge: we are not aware of any mathematical result proving that it is possible to guarantee the property  \\\\(\\\\mathbf{U}_i \\\\in\\n\\\\mathcal{B}\\\\) for all \\\\(i \\\\in \\\\mathcal{V}\\\\) when using either direct enforcement of boundary conditions into the approximation space, or weak enforcement using the Nitsche penalty method (which is for example widely used in discontinuous Galerkin schemes). In addition, some of these traditional ideas lead to quite restrictive time step constraints.\\nThere is enough numerical evidence suggesting that explicit treatment of Dirichlet-like boundary conditions is stable under CFL conditions and does not introduce any loss in accuracy.\\n\\nIf \\\\(\\\\mathbf{u}_t + \\\\text{div} \\\\, \\\\mathbb{f}(\\\\mathbf{u}) = \\\\boldsymbol{0}\\\\) represents Euler's equation with reflecting boundary conditions on the entirety of the boundary (i.e. \\\\(\\\\partial\\\\Omega^r \\\\equiv \\\\partial\\\\Omega\\\\)) and we integrate in space and time \\\\(\\\\int_{\\\\Omega}\\\\int_{t_1}^{t_2}\\\\) we would obtain            \\n\\\\begin{align*}\\n\\\\int_{\\\\Omega} \\\\rho(\\\\mathbf{x},t_2) \\\\, \\\\mathrm{d}\\\\mathbf{x} =\\n\\\\int_{\\\\Omega} \\\\rho(\\\\mathbf{x},t_1) \\\\, \\\\mathrm{d}\\\\mathbf{x} \\\\ , \\\\ \\\\\\n\\\\int_{\\\\Omega} \\\\mathbf{m}(\\\\mathbf{x},t_2) \\\\, \\\\mathrm{d}\\\\mathbf{x}\\n+ \\\\int_{t_1}^{t_2} \\\\! \\\\int_{\\\\partial\\\\Omega} p \\\\boldsymbol{\\\\nu} \\\\,\\n\\\\mathrm{d}\\\\mathbf{s} \\\\mathrm{d}t =\\n\\\\int_{\\\\Omega} \\\\mathbf{m}(\\\\mathbf{x},t_1) \\\\,\\n\\\\mathrm{d}\\\\mathbf{x} \\\\ , \\\\ \\\\\\n\\\\int_{\\\\Omega} E(\\\\mathbf{x},t_2) \\\\, \\\\mathrm{d}\\\\mathbf{x} =\\n\\\\int_{\\\\Omega} E(\\\\mathbf{x},t_1) \\\\, \\\\mathrm{d}\\\\mathbf{x} \\\\ \\\\ \\\\ \\\\\\n\\\\boldsymbol{(2)}\\n\\\\end{align*}\\n\\n Note that momentum is NOT a conserved quantity (interaction with walls leads to momentum gain/loss): however \\\\(\\\\mathbf{m}\\\\) has to satisfy a momentum balance. Even though we will not use reflecting boundary conditions in the entirety of the domain, we would like to know that our implementation of reflecting boundary conditions is consistent with the conservation properties mentioned above. In particular, if we use the projection \\\\(\\\\boldsymbol{(1)}\\\\) in the entirety of the domain the following discrete mass-balance can be guaranteed:           \\n\\\\begin{align*}\\n\\\\sum_{i \\\\in \\\\mathcal{V}} m_i \\\\rho_i^{n+1} =\\n\\\\sum_{i \\\\in \\\\mathcal{V}} m_i \\\\rho_i^{n} \\\\ , \\\\ \\\\\\n\\\\sum_{i \\\\in \\\\mathcal{V}} m_i \\\\mathbf{m}_i^{n+1}\\n+ \\\\tau_n \\\\int_{\\\\partial\\\\Omega} \\\\Big(\\\\sum_{i \\\\in \\\\mathcal{V}} p_i^{n} \\\\phi_i\\\\Big)\\n\\\\widehat{\\\\boldsymbol{\\\\nu}} \\\\mathrm{d}\\\\mathbf{s} =\\n\\\\sum_{i \\\\in \\\\mathcal{V}} m_i \\\\mathbf{m}_i^{n} \\\\ , \\\\ \\\\\\n\\\\sum_{i \\\\in \\\\mathcal{V}} m_i E_i^{n+1} = \\\\sum_{i \\\\in \\\\mathcal{V}} m_i\\nE_i^{n} \\\\ \\\\ \\\\ \\\\\\n\\\\boldsymbol{(3)}\\n\\\\end{align*}\\n\\n where \\\\(p_i\\\\) is the pressure at the nodes that lie at the boundary. Clearly \\\\(\\\\boldsymbol{(3)}\\\\) is the discrete counterpart of \\\\(\\\\boldsymbol{(2)}\\\\). The proof of identity \\\\(\\\\boldsymbol{(3)}\\\\) is omitted, but we briefly mention that it hinges on the definition of the nodal normal \\\\(\\\\widehat{\\\\boldsymbol{\\\\nu}}_i\\\\) provided in \\\\(\\\\boldsymbol{(1)}\\\\). We also note that this enforcement of reflecting boundary conditions is different from the one originally advanced in [107].\\n The commented program\\n Include files\\nThe set of include files is quite standard. The most intriguing part is the fact that we will rely solely on deal.II data structures for MPI parallelization, in particular parallel::distributed::Triangulation and LinearAlgebra::distributed::Vector included through distributed/tria.h and lac/la_parallel_vector.h. Instead of a Trilinos, or PETSc specific matrix class, we will use a non-distributed SparseMatrix (lac/sparse_matrix.h) to store the local part of the \\\\(\\\\mathbf{c}_{ij}\\\\), \\\\(\\\\mathbf{n}_{ij}\\\\) and \\\\(d_{ij}\\\\) matrices.\\n\\u00a0 #include <deal.II/base/conditional_ostream.h>\\n\\u00a0 #include <deal.II/base/parallel.h>\\n\\u00a0 #include <deal.II/base/parameter_acceptor.h>\\n\\u00a0 #include <deal.II/base/partitioner.h>\\n\\u00a0 #include <deal.II/base/quadrature.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 #include <deal.II/base/work_stream.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/distributed/solution_transfer.h>\\n\\u00a0 #include <deal.II/distributed/tria.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/dofs/dof_renumbering.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/fe/fe.h>\\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 #include <deal.II/fe/fe_values.h>\\n\\u00a0 #include <deal.II/fe/mapping.h>\\n\\u00a0 #include <deal.II/fe/mapping_q.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/manifold_lib.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/dynamic_sparsity_pattern.h>\\n\\u00a0 #include <deal.II/lac/la_parallel_vector.h>\\n\\u00a0 #include <deal.II/lac/sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/sparse_matrix.templates.h>\\n\\u00a0 #include <deal.II/lac/vector.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/meshworker/scratch_data.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 \\nparallelDefinition distributed.h:424\\nIn addition to above deal.II specific includes, we also include four boost headers. The first two are for binary archives that we will use for implementing a check-pointing and restart mechanism.\\n\\u00a0 #include <boost/archive/binary_iarchive.hpp>\\n\\u00a0 #include <boost/archive/binary_oarchive.hpp>\\n\\u00a0 \\nboostDefinition bounding_box.h:26\\nThe last two header files are for creating custom iterator ranges over integer intervals.\\n\\u00a0 #include <deal.II/base/std_cxx20/iota_view.h>\\n\\u00a0 #include <boost/range/iterator_range.hpp>\\n\\u00a0 \\nstd_cxx20Definition c++.h:146\\nFor std::isnan, std::isinf, std::ifstream, std::async, and std::future\\n\\u00a0 #include <cmath>\\n\\u00a0 #include <fstream>\\n\\u00a0 #include <future>\\n\\u00a0 \\n\\u00a0 \\n Class template declarations\\nWe begin our actual implementation by declaring all classes with their data structures and methods upfront. In contrast to previous example steps we use a more fine-grained encapsulation of concepts, data structures, and parameters into individual classes. A single class thus usually centers around either a single data structure (such as the Triangulation) in the Discretization class, or a single method (such as the make_one_step() function of the TimeStepping class). We typically declare parameter variables and scratch data object private and make methods and data structures used by other classes public.\\nNoteA cleaner approach would be to guard access to all data structures by getter/setter functions. For the sake of brevity, we refrain from that approach, though.\\nWe also note that the vast majority of classes is derived from ParameterAcceptor. This facilitates the population of all the global parameters into a single (global) ParameterHandler. More explanations about the use of inheritance from ParameterAcceptor as a global subscription mechanism can be found in step-60.\\n\\u00a0 namespace Step69\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\nWe start with defining a number of types::boundary_id constants used throughout the tutorial step. This allows us to refer to boundary types by a mnemonic (such as do_nothing) rather than a numerical value.\\n\\u00a0   namespace Boundaries\\n\\u00a0   {\\n\\u00a0     constexpr types::boundary_id do_nothing = 0;\\n\\u00a0     constexpr types::boundary_id free_slip  = 1;\\n\\u00a0     constexpr types::boundary_id dirichlet  = 2;\\n\\u00a0   } // namespace Boundaries\\n\\u00a0 \\nunsigned int\\n The Discretization class\\nThe class Discretization contains all data structures concerning the mesh (triangulation) and discretization (mapping, finite element, quadrature) of the problem. As mentioned, we use the ParameterAcceptor class to automatically populate problem-specific parameters, such as the geometry information (length, etc.) or the refinement level (refinement) from a parameter file. This requires us to split the initialization of data structures into two functions: We initialize everything that does not depend on parameters in the constructor, and defer the creation of the mesh to the setup() method that can be called once all parameters are read in via ParameterAcceptor::initialize().\\n\\u00a0   template <int dim>\\n\\u00a0   class Discretization : public ParameterAcceptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     Discretization(const MPI_Comm     mpi_communicator,\\n\\u00a0                    TimerOutput       &computing_timer,\\n\\u00a0                    const std::string &subsection = \\\"Discretization\\\");\\n\\u00a0 \\n\\u00a0     void setup();\\n\\u00a0 \\n\\u00a0     const MPI_Comm mpi_communicator;\\n\\u00a0 \\n\\u00a0     parallel::distributed::Triangulation<dim> triangulation;\\n\\u00a0 \\n\\u00a0     const MappingQ<dim>   mapping;\\n\\u00a0     const FE_Q<dim>       finite_element;\\n\\u00a0     const QGauss<dim>     quadrature;\\n\\u00a0     const QGauss<dim - 1> face_quadrature;\\n\\u00a0 \\n\\u00a0     unsigned int refinement;\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     TimerOutput &computing_timer;\\n\\u00a0 \\n\\u00a0     double length;\\n\\u00a0     double height;\\n\\u00a0     double disk_position;\\n\\u00a0     double disk_diameter;\\n\\u00a0   };\\n\\u00a0 \\nFE_QDefinition fe_q.h:554\\nMPI_Comm\\nMappingQDefinition mapping_q.h:110\\nParameterAcceptorDefinition parameter_acceptor.h:359\\nQGaussDefinition quadrature_lib.h:40\\nTimerOutputDefinition timer.h:549\\nparallel::distributed::TriangulationDefinition tria.h:268\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\n The OfflineData class\\nThe class OfflineData contains pretty much all components of the discretization that do not evolve in time, in particular, the DoFHandler, SparsityPattern, boundary maps, the lumped mass matrix, \\\\(\\\\mathbf{c}_{ij}\\\\) and \\\\(\\\\mathbf{n}_{ij}\\\\) matrices. Here, the term offline refers to the fact that all the class members of OfflineData have well-defined values independent of the current time step. This means that they can be initialized ahead of time (at time step zero) and are not meant to be modified at any later time step. For instance, the sparsity pattern should not change as we advance in time (we are not doing any form of adaptivity in space). Similarly, the entries of the lumped mass matrix should not be modified as we advance in time either.\\nWe also compute and store a boundary_normal_map that contains a map from a global index of type types::global_dof_index of a boundary degree of freedom to a tuple consisting of a normal vector, the boundary id, and the position associated with the degree of freedom. We have to compute and store this geometric information in this class because we won't have access to geometric (or cell-based) information later on in the algebraic loops over the sparsity pattern.\\nNoteEven though this class currently does not have any parameters that could be read in from a parameter file we nevertheless derive from ParameterAcceptor and follow the same idiom of providing a setup() (and assemble()) method as for the class Discretization.\\n\\u00a0   template <int dim>\\n\\u00a0   class OfflineData : public ParameterAcceptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     using BoundaryNormalMap =\\n\\u00a0       std::map<types::global_dof_index,\\n\\u00a0                std::tuple<Tensor<1, dim>, types::boundary_id, Point<dim>>>;\\n\\u00a0 \\n\\u00a0     OfflineData(const MPI_Comm             mpi_communicator,\\n\\u00a0                 TimerOutput               &computing_timer,\\n\\u00a0                 const Discretization<dim> &discretization,\\n\\u00a0                 const std::string         &subsection = \\\"OfflineData\\\");\\n\\u00a0 \\n\\u00a0     void setup();\\n\\u00a0     void assemble();\\n\\u00a0 \\n\\u00a0     DoFHandler<dim> dof_handler;\\n\\u00a0 \\n\\u00a0     std::shared_ptr<const Utilities::MPI::Partitioner> partitioner;\\n\\u00a0 \\n\\u00a0     unsigned int n_locally_owned;\\n\\u00a0     unsigned int n_locally_relevant;\\n\\u00a0 \\n\\u00a0     SparsityPattern sparsity_pattern;\\n\\u00a0 \\n\\u00a0     BoundaryNormalMap boundary_normal_map;\\n\\u00a0 \\n\\u00a0     SparseMatrix<double>                  lumped_mass_matrix;\\n\\u00a0     std::array<SparseMatrix<double>, dim> cij_matrix;\\n\\u00a0     std::array<SparseMatrix<double>, dim> nij_matrix;\\n\\u00a0     SparseMatrix<double>                  norm_matrix;\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     const MPI_Comm mpi_communicator;\\n\\u00a0     TimerOutput   &computing_timer;\\n\\u00a0 \\n\\u00a0     SmartPointer<const Discretization<dim>> discretization;\\n\\u00a0   };\\n\\u00a0 \\nDoFHandlerDefinition dof_handler.h:317\\nPointDefinition point.h:111\\nSmartPointerDefinition smartpointer.h:93\\nSparseMatrixDefinition sparse_matrix.h:520\\nSparsityPatternDefinition sparsity_pattern.h:343\\ntypes::boundary_idunsigned int boundary_idDefinition types.h:144\\ntypes::global_dof_indexunsigned int global_dof_indexDefinition types.h:81\\n The ProblemDescription class\\nThe member functions of this class are utility functions and data structures specific to Euler's equations:\\nThe type alias state_type is used for the states \\\\(\\\\mathbf{U}_i^n\\\\)\\nThe type alias flux_type is used for the fluxes \\\\(\\\\mathbb{f}(\\\\mathbf{U}_j^n)\\\\).\\nThe momentum function extracts \\\\(\\\\textbf{m}\\\\) out of the state vector \\\\([\\\\rho,\\\\textbf{m},E]\\\\) and stores it in a Tensor<1, dim>.\\nThe internal_energy function computes  \\\\(E -\\n   \\\\frac{|\\\\textbf{m}|^2}{2\\\\rho}\\\\) from a given state vector \\\\([\\\\rho,\\\\textbf{m},E]\\\\).\\n\\nThe purpose of the class members component_names, pressure, and speed_of_sound is evident from their names. We also provide a function compute_lambda_max(), that computes the wave speed estimate mentioned above, \\\\(\\\\lambda_{max}(\\\\mathbf{U},\\\\mathbf{V},\\\\mathbf{n})\\\\), which is used in the computation of the \\\\(d_{ij}\\\\) matrix.\\nNoteThe DEAL_II_ALWAYS_INLINE macro expands to a (compiler specific) pragma that ensures that the corresponding function defined in this class is always inlined, i.e., the function body is put in place for every invocation of the function, and no call (and code indirection) is generated. This is stronger than the inline keyword, which is more or less a (mild) suggestion to the compiler that the programmer thinks it would be beneficial to inline the function. DEAL_II_ALWAYS_INLINE should only be used rarely and with caution in situations such as this one, where we actually know (due to benchmarking) that inlining the function in question improves performance.\\nFinally, we observe that this is the only class in this tutorial step that is tied to a particular \\\"physics\\\" or \\\"hyperbolic conservation\\n   law\\\" (in this case Euler's equations). All the other classes are primarily \\\"discretization\\\" classes, very much agnostic of the particular physics being solved.\\n\\u00a0   template <int dim>\\n\\u00a0   class ProblemDescription\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     static constexpr unsigned int n_solution_variables = 2 + dim;\\n\\u00a0 \\n\\u00a0     using state_type = Tensor<1, n_solution_variables>;\\n\\u00a0     using flux_type  = Tensor<1, n_solution_variables, Tensor<1, dim>>;\\n\\u00a0 \\n\\u00a0     const static std::array<std::string, n_solution_variables> component_names;\\n\\u00a0 \\n\\u00a0     static constexpr double gamma = 7. / 5.;\\n\\u00a0 \\n\\u00a0     static DEAL_II_ALWAYS_INLINE inline Tensor<1, dim>\\n\\u00a0     momentum(const state_type &U);\\n\\u00a0 \\n\\u00a0     static DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0     internal_energy(const state_type &U);\\n\\u00a0 \\n\\u00a0     static DEAL_II_ALWAYS_INLINE inline double pressure(const state_type &U);\\n\\u00a0 \\n\\u00a0     static DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0     speed_of_sound(const state_type &U);\\n\\u00a0 \\n\\u00a0     static DEAL_II_ALWAYS_INLINE inline flux_type flux(const state_type &U);\\n\\u00a0 \\n\\u00a0     static DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0     compute_lambda_max(const state_type     &U_i,\\n\\u00a0                        const state_type     &U_j,\\n\\u00a0                        const Tensor<1, dim> &n_ij);\\n\\u00a0   };\\n\\u00a0 \\nTensorDefinition tensor.h:471\\nDEAL_II_ALWAYS_INLINE#define DEAL_II_ALWAYS_INLINEDefinition config.h:109\\n The InitialValues class\\nThe class InitialValues's only public data attribute is a std::function initial_state that computes the initial state of a given point and time. This function is used for populating the initial flow field as well as setting Dirichlet boundary conditions (at inflow boundaries) explicitly in every time step.\\nFor the purpose of this example step we simply implement a homogeneous uniform flow field for which the direction and a 1d primitive state (density, velocity, pressure) are read from the parameter file.\\nIt would be desirable to initialize the class in a single shot: initialize/set the parameters and define the class members that depend on these default parameters. However, since we do not know the actual values for the parameters, this would be sort of meaningless and unsafe in general (we would like to have mechanisms to check the consistency of the input parameters). Instead of defining another setup() method to be called (by-hand) after the call to ParameterAcceptor::initialize() we provide an \\\"implementation\\\" for the class member parse_parameters_call_back() which is automatically called when invoking ParameterAcceptor::initialize() for every class that inherits from ParameterAceptor.\\n\\u00a0   template <int dim>\\n\\u00a0   class InitialValues : public ParameterAcceptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     using state_type = typename ProblemDescription<dim>::state_type;\\n\\u00a0 \\n\\u00a0     InitialValues(const std::string &subsection = \\\"InitialValues\\\");\\n\\u00a0 \\n\\u00a0     std::function<state_type(const Point<dim> &point, double t)> initial_state;\\n\\u00a0 \\n\\u00a0   private:\\nWe declare a private callback function that will be wired up to the ParameterAcceptor::parse_parameters_call_back signal.\\n\\u00a0     void parse_parameters_callback();\\n\\u00a0 \\n\\u00a0     Tensor<1, dim> initial_direction;\\n\\u00a0     Tensor<1, 3>   initial_1d_state;\\n\\u00a0   };\\n\\u00a0 \\n The TimeStepping class\\nWith the OfflineData and ProblemDescription classes at hand we can now implement the explicit time-stepping scheme that was introduced in the discussion above. The main method of the TimeStepping class is make_one_step(vector_type &U,\\n   const double t) that takes a reference to a state vector U and a time point t (as input arguments) computes the updated solution, stores it in the vector temp, swaps its contents with the vector U, and finally returns the chosen step-size \\\\(\\\\tau\\\\).\\nThe other important method is prepare() which primarily sets the proper partition and sparsity pattern for the temporary vector temp and the matrix dij_matrix respectively.\\n\\u00a0   template <int dim>\\n\\u00a0   class TimeStepping : public ParameterAcceptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     static constexpr unsigned int n_solution_variables =\\n\\u00a0       ProblemDescription<dim>::n_solution_variables;\\n\\u00a0 \\n\\u00a0     using state_type = typename ProblemDescription<dim>::state_type;\\n\\u00a0     using flux_type  = typename ProblemDescription<dim>::flux_type;\\n\\u00a0 \\n\\u00a0     using vector_type = std::array<LinearAlgebra::distributed::Vector<double>,\\n\\u00a0                                    n_solution_variables>;\\n\\u00a0 \\n\\u00a0     TimeStepping(const MPI_Comm            mpi_communicator,\\n\\u00a0                  TimerOutput              &computing_timer,\\n\\u00a0                  const OfflineData<dim>   &offline_data,\\n\\u00a0                  const InitialValues<dim> &initial_values,\\n\\u00a0                  const std::string        &subsection = \\\"TimeStepping\\\");\\n\\u00a0 \\n\\u00a0     void prepare();\\n\\u00a0 \\n\\u00a0     double make_one_step(vector_type &U, const double t);\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     const MPI_Comm mpi_communicator;\\n\\u00a0     TimerOutput   &computing_timer;\\n\\u00a0 \\n\\u00a0     SmartPointer<const OfflineData<dim>>   offline_data;\\n\\u00a0     SmartPointer<const InitialValues<dim>> initial_values;\\n\\u00a0 \\n\\u00a0     SparseMatrix<double> dij_matrix;\\n\\u00a0 \\n\\u00a0     vector_type temporary_vector;\\n\\u00a0 \\n\\u00a0     double cfl_update;\\n\\u00a0   };\\n\\u00a0 \\nTimeSteppingDefinition time_stepping.h:33\\n The SchlierenPostprocessor class\\nAt its core, the SchlierenPostprocessor class implements the class member compute_schlieren(). The main purpose of this class member is to compute an auxiliary finite element field schlieren, that is defined at each node by  \\n\\\\[ \\\\text{schlieren}[i] = e^{\\\\beta \\\\frac{ |\\\\nabla r_i|\\n   - \\\\min_j |\\\\nabla r_j| }{\\\\max_j |\\\\nabla r_j| - \\\\min_j |\\\\nabla r_j| } }, \\\\]\\n\\n where \\\\(r\\\\) can in principle be any scalar quantity. In practice though, the density is a natural candidate, viz. \\\\(r \\\\dealcoloneq \\\\rho\\\\). Schlieren postprocessing is a standard method for enhancing the contrast of a visualization inspired by actual experimental X-ray and shadowgraphy techniques of visualization. (See step-67 for another example where we create a Schlieren plot.)\\n\\u00a0   template <int dim>\\n\\u00a0   class SchlierenPostprocessor : public ParameterAcceptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     static constexpr unsigned int n_solution_variables =\\n\\u00a0       ProblemDescription<dim>::n_solution_variables;\\n\\u00a0 \\n\\u00a0     using state_type = typename ProblemDescription<dim>::state_type;\\n\\u00a0 \\n\\u00a0     using vector_type = std::array<LinearAlgebra::distributed::Vector<double>,\\n\\u00a0                                    n_solution_variables>;\\n\\u00a0 \\n\\u00a0     SchlierenPostprocessor(\\n\\u00a0       const MPI_Comm          mpi_communicator,\\n\\u00a0       TimerOutput            &computing_timer,\\n\\u00a0       const OfflineData<dim> &offline_data,\\n\\u00a0       const std::string      &subsection = \\\"SchlierenPostprocessor\\\");\\n\\u00a0 \\n\\u00a0     void prepare();\\n\\u00a0 \\n\\u00a0     void compute_schlieren(const vector_type &U);\\n\\u00a0 \\n\\u00a0     LinearAlgebra::distributed::Vector<double> schlieren;\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     const MPI_Comm mpi_communicator;\\n\\u00a0     TimerOutput   &computing_timer;\\n\\u00a0 \\n\\u00a0     SmartPointer<const OfflineData<dim>> offline_data;\\n\\u00a0 \\n\\u00a0     Vector<double> r;\\n\\u00a0 \\n\\u00a0     unsigned int schlieren_index;\\n\\u00a0     double       schlieren_beta;\\n\\u00a0   };\\n\\u00a0 \\nLinearAlgebra::distributed::VectorDefinition la_parallel_vector.h:250\\nVectorDefinition vector.h:120\\n The MainLoop class\\nNow, all that is left to do is to chain the methods implemented in the TimeStepping, InitialValues, and SchlierenPostprocessor classes together. We do this in a separate class MainLoop that contains an object of every class and again reads in a number of parameters with the help of the ParameterAcceptor class.\\n\\u00a0   template <int dim>\\n\\u00a0   class MainLoop : public ParameterAcceptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     using vector_type = typename TimeStepping<dim>::vector_type;\\n\\u00a0 \\n\\u00a0     MainLoop(const MPI_Comm mpi_communnicator);\\n\\u00a0 \\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     vector_type interpolate_initial_values(const double t = 0);\\n\\u00a0 \\n\\u00a0     void checkpoint(const vector_type &U,\\n\\u00a0                     const std::string &name,\\n\\u00a0                     double             t,\\n\\u00a0                     unsigned int       cycle);\\n\\u00a0 \\n\\u00a0     void output(const vector_type &U,\\n\\u00a0                 const std::string &name,\\n\\u00a0                 double             t,\\n\\u00a0                 unsigned int       cycle);\\n\\u00a0 \\n\\u00a0     const MPI_Comm     mpi_communicator;\\n\\u00a0     std::ostringstream timer_output;\\n\\u00a0     TimerOutput        computing_timer;\\n\\u00a0 \\n\\u00a0     ConditionalOStream pcout;\\n\\u00a0 \\n\\u00a0     std::string base_name;\\n\\u00a0     double      t_final;\\n\\u00a0     double      output_granularity;\\n\\u00a0 \\n\\u00a0     bool asynchronous_writeback;\\n\\u00a0 \\n\\u00a0     bool resume;\\n\\u00a0 \\n\\u00a0     Discretization<dim>         discretization;\\n\\u00a0     OfflineData<dim>            offline_data;\\n\\u00a0     InitialValues<dim>          initial_values;\\n\\u00a0     TimeStepping<dim>           time_stepping;\\n\\u00a0     SchlierenPostprocessor<dim> schlieren_postprocessor;\\n\\u00a0 \\n\\u00a0     vector_type output_vector;\\n\\u00a0 \\n\\u00a0     std::future<void> background_thread_state;\\n\\u00a0   };\\n\\u00a0 \\nConditionalOStreamDefinition conditional_ostream.h:80\\n Implementation\\n Grid generation, setup of data structures\\nThe first major task at hand is the typical triplet of grid generation, setup of data structures, and assembly. A notable novelty in this example step is the use of the ParameterAcceptor class that we use to populate parameter values: we first initialize the ParameterAcceptor class by calling its constructor with a string subsection denoting the correct subsection in the parameter file. Then, in the constructor body every parameter value is initialized to a sensible default value and registered with the ParameterAcceptor class with a call to ParameterAcceptor::add_parameter().\\n\\u00a0   template <int dim>\\n\\u00a0   Discretization<dim>::Discretization(const MPI_Comm     mpi_communicator,\\n\\u00a0                                       TimerOutput       &computing_timer,\\n\\u00a0                                       const std::string &subsection)\\n\\u00a0     : ParameterAcceptor(subsection)\\n\\u00a0     , mpi_communicator(mpi_communicator)\\n\\u00a0     , triangulation(mpi_communicator)\\n\\u00a0     , mapping(1)\\n\\u00a0     , finite_element(1)\\n\\u00a0     , quadrature(3)\\n\\u00a0     , face_quadrature(3)\\n\\u00a0     , computing_timer(computing_timer)\\n\\u00a0   {\\n\\u00a0     length = 4.;\\n\\u00a0     add_parameter(\\\"length\\\", length, \\\"Length of computational domain\\\");\\n\\u00a0 \\n\\u00a0     height = 2.;\\n\\u00a0     add_parameter(\\\"height\\\", height, \\\"Height of computational domain\\\");\\n\\u00a0 \\n\\u00a0     disk_position = 0.6;\\n\\u00a0     add_parameter(\\\"object position\\\",\\n\\u00a0                   disk_position,\\n\\u00a0                   \\\"x position of immersed disk center point\\\");\\n\\u00a0 \\n\\u00a0     disk_diameter = 0.5;\\n\\u00a0     add_parameter(\\\"object diameter\\\",\\n\\u00a0                   disk_diameter,\\n\\u00a0                   \\\"Diameter of immersed disk\\\");\\n\\u00a0 \\n\\u00a0     refinement = 5;\\n\\u00a0     add_parameter(\\\"refinement\\\",\\n\\u00a0                   refinement,\\n\\u00a0                   \\\"Number of refinement steps of the geometry\\\");\\n\\u00a0   }\\n\\u00a0 \\nNote that in the previous constructor we only passed the MPI communicator to the triangulation but we still have not initialized the underlying geometry/mesh. As mentioned earlier, we have to postpone this task to the setup() function that gets called after the ParameterAcceptor::initialize() function has populated all parameter variables with the final values read from the parameter file.\\nThe setup() function is the last class member that has to be implemented. It creates the actual triangulation that is a benchmark configuration consisting of a channel with a disk obstacle, see [107]. We construct the geometry by modifying the mesh generated by GridGenerator::hyper_cube_with_cylindrical_hole(). We refer to step-49, step-53, and step-54 for an overview how to create advanced meshes. We first create 4 temporary (non distributed) coarse triangulations that we stitch together with the GridGenerator::merge_triangulation() function. We center the disk at \\\\((0,0)\\\\) with a diameter of disk_diameter. The lower left corner of the channel has coordinates (-disk_position, -height/2) and the upper right corner has (length-disk_position, height/2).\\n\\u00a0   template <int dim>\\n\\u00a0   void Discretization<dim>::setup()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope scope(computing_timer, \\\"discretization - setup\\\");\\n\\u00a0 \\n\\u00a0     triangulation.clear();\\n\\u00a0 \\n\\u00a0     Triangulation<dim> tria1, tria2, tria3, tria4, tria5, tria6;\\n\\u00a0 \\n\\u00a0     GridGenerator::hyper_cube_with_cylindrical_hole(\\n\\u00a0       tria1, disk_diameter / 2., disk_diameter, 0.5, 1, false);\\n\\u00a0 \\n\\u00a0     GridGenerator::subdivided_hyper_rectangle(\\n\\u00a0       tria2,\\n\\u00a0       {2, 1},\\n\\u00a0       Point<2>(-disk_diameter, disk_diameter),\\n\\u00a0       Point<2>(disk_diameter, height / 2.));\\n\\u00a0 \\n\\u00a0     GridGenerator::subdivided_hyper_rectangle(\\n\\u00a0       tria3,\\n\\u00a0       {2, 1},\\n\\u00a0       Point<2>(-disk_diameter, -disk_diameter),\\n\\u00a0       Point<2>(disk_diameter, -height / 2.));\\n\\u00a0 \\n\\u00a0     GridGenerator::subdivided_hyper_rectangle(\\n\\u00a0       tria4,\\n\\u00a0       {6, 2},\\n\\u00a0       Point<2>(disk_diameter, -disk_diameter),\\n\\u00a0       Point<2>(length - disk_position, disk_diameter));\\n\\u00a0 \\n\\u00a0     GridGenerator::subdivided_hyper_rectangle(\\n\\u00a0       tria5,\\n\\u00a0       {6, 1},\\n\\u00a0       Point<2>(disk_diameter, disk_diameter),\\n\\u00a0       Point<2>(length - disk_position, height / 2.));\\n\\u00a0 \\n\\u00a0     GridGenerator::subdivided_hyper_rectangle(\\n\\u00a0       tria6,\\n\\u00a0       {6, 1},\\n\\u00a0       Point<2>(disk_diameter, -height / 2.),\\n\\u00a0       Point<2>(length - disk_position, -disk_diameter));\\n\\u00a0 \\n\\u00a0     GridGenerator::merge_triangulations(\\n\\u00a0       {&tria1, &tria2, &tria3, &tria4, &tria5, &tria6},\\n\\u00a0       triangulation,\\n\\u00a0       1.e-12,\\n\\u00a0       true);\\n\\u00a0 \\n\\u00a0     triangulation.set_manifold(0, PolarManifold<2>(Point<2>()));\\n\\u00a0 \\nPolarManifoldDefinition manifold_lib.h:84\\nTimerOutput::ScopeDefinition timer.h:557\\nTriangulationDefinition tria.h:1323\\nparallel::distributed::Triangulation::clearvirtual void clear() overrideDefinition tria.cc:1864\\nTriangulation::set_manifoldvoid set_manifold(const types::manifold_id number, const Manifold< dim, spacedim > &manifold_object)\\nGridGenerator::hyper_cube_with_cylindrical_holevoid hyper_cube_with_cylindrical_hole(Triangulation< dim, spacedim > &triangulation, const double inner_radius=.25, const double outer_radius=.5, const double L=.5, const unsigned int repetitions=1, const bool colorize=false)\\nGridGenerator::subdivided_hyper_rectanglevoid subdivided_hyper_rectangle(Triangulation< dim, spacedim > &tria, const std::vector< unsigned int > &repetitions, const Point< dim > &p1, const Point< dim > &p2, const bool colorize=false)\\nGridGenerator::merge_triangulationsvoid merge_triangulations(const Triangulation< dim, spacedim > &triangulation_1, const Triangulation< dim, spacedim > &triangulation_2, Triangulation< dim, spacedim > &result, const double duplicated_vertex_tolerance=1.0e-12, const bool copy_manifold_ids=false, const bool copy_boundary_ids=false)\\nWe have to fix up the left edge that is currently located at \\\\(x=-\\\\)disk_diameter and has to be shifted to \\\\(x=-\\\\)disk_position. As a last step the boundary has to be colorized with Boundaries::do_nothing on the right, dirichlet on the left and free_slip on the upper and lower outer boundaries and the obstacle.\\n\\u00a0     for (const auto &cell : triangulation.active_cell_iterators())\\n\\u00a0       {\\n\\u00a0         for (const auto v : cell->vertex_indices())\\n\\u00a0           {\\n\\u00a0             if (cell->vertex(v)[0] <= -disk_diameter + 1.e-6)\\n\\u00a0               cell->vertex(v)[0] = -disk_position;\\n\\u00a0           }\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     for (const auto &cell : triangulation.active_cell_iterators())\\n\\u00a0       {\\n\\u00a0         for (const auto f : cell->face_indices())\\n\\u00a0           {\\n\\u00a0             const auto face = cell->face(f);\\n\\u00a0 \\n\\u00a0             if (face->at_boundary())\\n\\u00a0               {\\n\\u00a0                 const auto center = face->center();\\n\\u00a0 \\n\\u00a0                 if (center[0] > length - disk_position - 1.e-6)\\n\\u00a0                   face->set_boundary_id(Boundaries::do_nothing);\\n\\u00a0                 else if (center[0] < -disk_position + 1.e-6)\\n\\u00a0                   face->set_boundary_id(Boundaries::dirichlet);\\n\\u00a0                 else\\n\\u00a0                   face->set_boundary_id(Boundaries::free_slip);\\n\\u00a0               }\\n\\u00a0           }\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\ncenterPoint< 3 > centerDefinition data_out_base.cc:267\\nvertex_indicesunsigned int vertex_indices[2]Definition grid_tools_topology.cc:947\\n Assembly of offline matrices\\nNot much is done in the constructor of OfflineData other than initializing the corresponding class members in the initialization list.\\n\\u00a0   template <int dim>\\n\\u00a0   OfflineData<dim>::OfflineData(const MPI_Comm             mpi_communicator,\\n\\u00a0                                 TimerOutput               &computing_timer,\\n\\u00a0                                 const Discretization<dim> &discretization,\\n\\u00a0                                 const std::string         &subsection)\\n\\u00a0     : ParameterAcceptor(subsection)\\n\\u00a0     , dof_handler(discretization.triangulation)\\n\\u00a0     , mpi_communicator(mpi_communicator)\\n\\u00a0     , computing_timer(computing_timer)\\n\\u00a0     , discretization(&discretization)\\n\\u00a0   {}\\n\\u00a0 \\nNow we can initialize the DoFHandler, extract the IndexSet objects for locally owned and locally relevant DOFs, and initialize a Utilities::MPI::Partitioner object that is needed for distributed vectors.\\n\\u00a0   template <int dim>\\n\\u00a0   void OfflineData<dim>::setup()\\n\\u00a0   {\\n\\u00a0     IndexSet locally_owned;\\n\\u00a0     IndexSet locally_relevant;\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope scope(computing_timer,\\n\\u00a0                                \\\"offline_data - distribute dofs\\\");\\n\\u00a0 \\n\\u00a0       dof_handler.distribute_dofs(discretization->finite_element);\\n\\u00a0 \\n\\u00a0       locally_owned   = dof_handler.locally_owned_dofs();\\n\\u00a0       n_locally_owned = locally_owned.n_elements();\\n\\u00a0 \\n\\u00a0       locally_relevant   = DoFTools::extract_locally_relevant_dofs(dof_handler);\\n\\u00a0       n_locally_relevant = locally_relevant.n_elements();\\n\\u00a0 \\n\\u00a0       partitioner =\\n\\u00a0         std::make_shared<Utilities::MPI::Partitioner>(locally_owned,\\n\\u00a0                                                       locally_relevant,\\n\\u00a0                                                       mpi_communicator);\\n\\u00a0     }\\n\\u00a0 \\nIndexSetDefinition index_set.h:70\\nDoFTools::extract_locally_relevant_dofsIndexSet extract_locally_relevant_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1164\\n Translation to local index ranges\\nWe are now in a position to create the sparsity pattern for our matrices. There are quite a few peculiarities that need a detailed explanation. We avoid using a distributed matrix class (as for example provided by Trilinos or PETSc) and instead rely on deal.II's own SparseMatrix object to store the local part of all matrices. This design decision is motivated by the fact that (a) we actually never perform a matrix-vector multiplication, and (b) we can always assemble the local part of a matrix exclusively on a given MPI rank. Instead, we will compute nonlinear updates while iterating over (the local part) of a connectivity stencil; a task for which deal.II's own SparsityPattern is specifically optimized for.\\nThis design consideration has a caveat, though. What makes the deal.II SparseMatrix class fast is the compressed row storage (CSR) used in the SparsityPattern (see Sparsity patterns). This, unfortunately, does not play nicely with a global distributed index range because a sparsity pattern with CSR cannot contain \\\"holes\\\" in the index range. The distributed matrices offered by deal.II avoid this by translating from a global index range into a contiguous local index range. But this is precisely the type of index manipulation we want to avoid in our iteration over the stencil because it creates a measurable overhead.\\nThe Utilities::MPI::Partitioner class already implements the translation from a global index range to a contiguous local (per MPI rank) index range: we don't have to reinvent the wheel. We just need to use that translation capability (once and only once) in order to create a \\\"local\\\" sparsity pattern for the contiguous index range \\\\([0,\\\\)n_locally_relevant \\\\()\\\\). That capability can be invoked by the Utilities::MPI::Partitioner::global_to_local() function. Once the sparsity pattern is created using local indices, all that is left to do is to ensure that (when implementing our scatter and gather auxiliary functions) we always access elements of a distributed vector by a call to LinearAlgebra::distributed::Vector::local_element(). This way we avoid index translations altogether and operate exclusively with local indices.\\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope scope(\\n\\u00a0         computing_timer,\\n\\u00a0         \\\"offline_data - create sparsity pattern and set up matrices\\\");\\n\\u00a0 \\nWe have to create the \\\"local\\\" sparsity pattern by hand. We therefore loop over all locally owned and ghosted cells (see GlossArtificialCell) and extract the (global) dof_indices associated with the cell DOFs and renumber them using partitioner->global_to_local(index).\\nNoteIn the case of a locally owned dof, such renumbering consist of applying a shift (i.e. we subtract an offset) such that now they will become a number in the integer interval \\\\([0,\\\\)n_locally_owned \\\\()\\\\). However, in the case of a ghosted dof (i.e. not locally owned) the situation is quite different, since the global indices associated with ghosted DOFs will not be (in general) a contiguous set of integers.\\n  \\u00a0       DynamicSparsityPattern dsp(n_locally_relevant, n_locally_relevant);\\n  \\u00a0 \\n  \\u00a0       const auto dofs_per_cell =\\n  \\u00a0         discretization->finite_element.n_dofs_per_cell();\\n  \\u00a0       std::vector<types::global_dof_index> dof_indices(dofs_per_cell);\\n  \\u00a0 \\n  \\u00a0       for (const auto &cell : dof_handler.active_cell_iterators())\\n  \\u00a0         {\\n  \\u00a0           if (cell->is_artificial())\\n  \\u00a0             continue;\\n  \\u00a0 \\n  \\u00a0           /* We transform the set of global dof indices on the cell to the\\n* \\u00a0            * corresponding \\\"local\\\" index range on the MPI process: */\\n  \\u00a0           cell->get_dof_indices(dof_indices);\\n  \\u00a0           std::transform(dof_indices.begin(),\\n  \\u00a0                          dof_indices.end(),\\n  \\u00a0                          dof_indices.begin(),\\n  \\u00a0                          [&](types::global_dof_index index) {\\n  \\u00a0                            return partitioner->global_to_local(index);\\n  \\u00a0                          });\\n  \\u00a0 \\n  \\u00a0           /* And simply add, for each dof, a coupling to all other \\\"local\\\"\\n* \\u00a0            * dofs on the cell: */\\n  \\u00a0           for (const auto dof : dof_indices)\\n  \\u00a0             dsp.add_entries(dof, dof_indices.begin(), dof_indices.end());\\n  \\u00a0         }\\n  \\u00a0 \\n  \\u00a0       sparsity_pattern.copy_from(dsp);\\n  \\u00a0 \\n  \\u00a0       lumped_mass_matrix.reinit(sparsity_pattern);\\n  \\u00a0       norm_matrix.reinit(sparsity_pattern);\\n  \\u00a0       for (auto &matrix : cij_matrix)\\n  \\u00a0         matrix.reinit(sparsity_pattern);\\n  \\u00a0       for (auto &matrix : nij_matrix)\\n  \\u00a0         matrix.reinit(sparsity_pattern);\\n  \\u00a0     }\\n  \\u00a0   }\\n  \\u00a0 \\nDynamicSparsityPatternDefinition dynamic_sparsity_pattern.h:322\\nThis concludes the setup of the DoFHandler and SparseMatrix objects. Next, we have to assemble various matrices. We define a number of helper functions and data structures in an anonymous namespace.\\n\\u00a0   namespace\\n\\u00a0   {\\nCopyData class that will be used to assemble the offline data matrices using WorkStream. It acts as a container: it is just a struct where WorkStream stores the local cell contributions. Note that it also contains a class member local_boundary_normal_map used to store the local contributions required to compute the normals at the boundary.\\n\\u00a0     template <int dim>\\n\\u00a0     struct CopyData\\n\\u00a0     {\\n\\u00a0       bool                                         is_artificial;\\n\\u00a0       std::vector<types::global_dof_index>         local_dof_indices;\\n\\u00a0       typename OfflineData<dim>::BoundaryNormalMap local_boundary_normal_map;\\n\\u00a0       FullMatrix<double>                           cell_lumped_mass_matrix;\\n\\u00a0       std::array<FullMatrix<double>, dim>          cell_cij_matrix;\\n\\u00a0     };\\n\\u00a0 \\nFullMatrixDefinition full_matrix.h:79\\nNext we introduce a number of helper functions that are all concerned about reading and writing matrix and vector entries. They are mainly motivated by providing slightly more efficient code and syntactic sugar for otherwise somewhat tedious code.\\nThe first function we introduce, get_entry(), will be used to read the value stored at the entry pointed by a SparsityPattern iterator it of matrix. The function works around a small deficiency in the SparseMatrix interface: The SparsityPattern is concerned with all index operations of the sparse matrix stored in CRS format. As such the iterator already knows the global index of the corresponding matrix entry in the low-level vector stored in the SparseMatrix object. Due to the lack of an interface in the SparseMatrix for accessing the element directly with a SparsityPattern iterator, we unfortunately have to create a temporary SparseMatrix iterator. We simply hide this in the get_entry() function.\\n\\u00a0     template <typename IteratorType>\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline SparseMatrix<double>::value_type\\n\\u00a0     get_entry(const SparseMatrix<double> &matrix, const IteratorType &it)\\n\\u00a0     {\\n\\u00a0       const SparseMatrix<double>::const_iterator matrix_iterator(\\n\\u00a0         &matrix, it->global_index());\\n\\u00a0       return matrix_iterator->value();\\n\\u00a0     }\\n\\u00a0 \\nSparseMatrixIterators::IteratorDefinition sparse_matrix.h:347\\nSparseMatrix::value_typenumber value_typeDefinition sparse_matrix.h:531\\nThe set_entry() helper is the inverse operation of get_value(): Given an iterator and a value, it sets the entry pointed to by the iterator in the matrix.\\n\\u00a0     template <typename IteratorType>\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline void\\n\\u00a0     set_entry(SparseMatrix<double>            &matrix,\\n\\u00a0               const IteratorType              &it,\\n\\u00a0               SparseMatrix<double>::value_type value)\\n\\u00a0     {\\n\\u00a0       SparseMatrix<double>::iterator matrix_iterator(&matrix,\\n\\u00a0                                                      it->global_index());\\n\\u00a0       matrix_iterator->value() = value;\\n\\u00a0     }\\n\\u00a0 \\ngather_get_entry(): we note that  \\\\(\\\\mathbf{c}_{ij} \\\\in\\n   \\\\mathbb{R}^d\\\\). If \\\\(d=2\\\\) then  \\\\(\\\\mathbf{c}_{ij} =\\n   [\\\\mathbf{c}_{ij}^1,\\\\mathbf{c}_{ij}^2]^\\\\top\\\\). Which basically implies that we need one matrix per space dimension to store the \\\\(\\\\mathbf{c}_{ij}\\\\) vectors. Similar observation follows for the matrix \\\\(\\\\mathbf{n}_{ij}\\\\). The purpose of gather_get_entry() is to retrieve those entries and store them into a Tensor<1, dim> for our convenience.\\n\\u00a0     template <std::size_t k, typename IteratorType>\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline Tensor<1, k>\\n\\u00a0     gather_get_entry(const std::array<SparseMatrix<double>, k> &c_ij,\\n\\u00a0                      const IteratorType                         it)\\n\\u00a0     {\\n\\u00a0       Tensor<1, k> result;\\n\\u00a0       for (unsigned int j = 0; j < k; ++j)\\n\\u00a0         result[j] = get_entry(c_ij[j], it);\\n\\u00a0       return result;\\n\\u00a0     }\\n\\u00a0 \\ngather() (first interface): this first function signature, having three input arguments, will be used to retrieve the individual components (i,l) of a matrix. The functionality of gather_get_entry() and gather() is very much the same, but their context is different: the function gather() does not rely on an iterator (that actually knows the value pointed to) but rather on the indices (i,l) of the entry in order to retrieve its actual value. We should expect gather() to be slightly more expensive than gather_get_entry(). The use of gather() will be limited to the task of computing the algebraic viscosity \\\\(d_{ij}\\\\) in the particular case that when both \\\\(i\\\\) and \\\\(j\\\\) lie at the boundary.\\nNoteThe reader should be aware that accessing an arbitrary (i,l) entry of a matrix (say for instance Trilinos or PETSc matrices) is in general unacceptably expensive. Here is where we might want to keep an eye on complexity: we want this operation to have constant complexity, which is the case of the current implementation using deal.II matrices.\\n\\u00a0     template <std::size_t k>\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline Tensor<1, k>\\n\\u00a0     gather(const std::array<SparseMatrix<double>, k> &n_ij,\\n\\u00a0            const unsigned int                         i,\\n\\u00a0            const unsigned int                         j)\\n\\u00a0     {\\n\\u00a0       Tensor<1, k> result;\\n\\u00a0       for (unsigned int l = 0; l < k; ++l)\\n\\u00a0         result[l] = n_ij[l](i, j);\\n\\u00a0       return result;\\n\\u00a0     }\\n\\u00a0 \\ngathervoid gather(VectorizedArray< Number, width > &out, const std::array< Number *, width > &ptrs, const unsigned int offset)Definition vectorization.h:844\\ngather() (second interface): this second function signature having two input arguments will be used to gather the state at a node i and return it as a Tensor<1,n_solution_variables> for our convenience.\\n\\u00a0     template <std::size_t k>\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline Tensor<1, k>\\n\\u00a0     gather(const std::array<LinearAlgebra::distributed::Vector<double>, k> &U,\\n\\u00a0            const unsigned int                                               i)\\n\\u00a0     {\\n\\u00a0       Tensor<1, k> result;\\n\\u00a0       for (unsigned int j = 0; j < k; ++j)\\n\\u00a0         result[j] = U[j].local_element(i);\\n\\u00a0       return result;\\n\\u00a0     }\\n\\u00a0 \\nscatter(): this function has three input arguments, the first one is meant to be a \\\"global object\\\" (say a locally owned or locally relevant vector), the second argument which could be a Tensor<1,n_solution_variables>, and the last argument which represents a index of the global object. This function will be primarily used to write the updated nodal values, stored as Tensor<1,n_solution_variables>, into the global objects.\\n\\u00a0     template <std::size_t k, int k2>\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline void\\n\\u00a0     scatter(std::array<LinearAlgebra::distributed::Vector<double>, k> &U,\\n\\u00a0             const Tensor<1, k2>                                       &tensor,\\n\\u00a0             const unsigned int                                         i)\\n\\u00a0     {\\n\\u00a0       static_assert(k == k2,\\n\\u00a0                     \\\"The dimensions of the input arguments must agree\\\");\\n\\u00a0       for (unsigned int j = 0; j < k; ++j)\\n\\u00a0         U[j].local_element(i) = tensor[j];\\n\\u00a0     }\\n\\u00a0   } // namespace\\n\\u00a0 \\nWe are now in a position to assemble all matrices stored in OfflineData: the lumped mass entries \\\\(m_i\\\\), the vector-valued matrices \\\\(\\\\mathbf{c}_{ij}\\\\) and  \\\\(\\\\mathbf{n}_{ij} =\\n   \\\\frac{\\\\mathbf{c}_{ij}}{|\\\\mathbf{c}_{ij}|}\\\\), and the boundary normals \\\\(\\\\boldsymbol{\\\\nu}_i\\\\).\\nIn order to exploit thread parallelization we use the WorkStream approach detailed in the Parallel computing with multiple processors accessing shared memory. As customary this requires definition of\\nScratch data (i.e. input info required to carry out computations): in this case it is scratch_data.\\nThe worker: in our case this is the local_assemble_system() function that actually computes the local (i.e. current cell) contributions from the scratch data.\\nA copy data: a struct that contains all the local assembly contributions, in this case CopyData<dim>().\\nA copy data routine: in this case it is copy_local_to_global() in charge of actually copying these local contributions into the global objects (matrices and/or vectors)\\n\\nMost of the following lines are spent in the definition of the worker local_assemble_system() and the copy data routine copy_local_to_global(). There is not much to say about the WorkStream framework since the vast majority of ideas are reasonably well-documented in step-9, step-13 and step-32 among others.\\nFinally, assuming that \\\\(\\\\mathbf{x}_i\\\\) is a support point at the boundary, the (nodal) normals are defined as:\\n\\n\\\\begin{align*}\\n   \\\\widehat{\\\\boldsymbol{\\\\nu}}_i \\\\dealcoloneq\\n   \\\\frac{\\\\int_{\\\\partial\\\\Omega} \\\\phi_i \\\\widehat{\\\\boldsymbol{\\\\nu}} \\\\,\\n   \\\\, \\\\mathrm{d}\\\\mathbf{s}}{\\\\big|\\\\int_{\\\\partial\\\\Omega} \\\\phi_i\\n   \\\\widehat{\\\\boldsymbol{\\\\nu}} \\\\, \\\\mathrm{d}\\\\mathbf{s}\\\\big|}\\n   \\\\end{align*}\\n\\nWe will compute the numerator of this expression first and store it in OfflineData<dim>::BoundaryNormalMap. We will normalize these vectors in a posterior loop.\\n\\u00a0   template <int dim>\\n\\u00a0   void OfflineData<dim>::assemble()\\n\\u00a0   {\\n\\u00a0     lumped_mass_matrix = 0.;\\n\\u00a0     norm_matrix        = 0.;\\n\\u00a0     for (auto &matrix : cij_matrix)\\n\\u00a0       matrix = 0.;\\n\\u00a0     for (auto &matrix : nij_matrix)\\n\\u00a0       matrix = 0.;\\n\\u00a0 \\n\\u00a0     unsigned int dofs_per_cell =\\n\\u00a0       discretization->finite_element.n_dofs_per_cell();\\n\\u00a0     unsigned int n_q_points = discretization->quadrature.size();\\n\\u00a0 \\nWhat follows is the initialization of the scratch data required by WorkStream\\n\\u00a0     MeshWorker::ScratchData<dim> scratch_data(\\n\\u00a0       discretization->mapping,\\n\\u00a0       discretization->finite_element,\\n\\u00a0       discretization->quadrature,\\n\\u00a0       update_values | update_gradients | update_quadrature_points |\\n\\u00a0         update_JxW_values,\\n\\u00a0       discretization->face_quadrature,\\n\\u00a0       update_normal_vectors | update_values | update_JxW_values);\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope scope(\\n\\u00a0         computing_timer,\\n\\u00a0         \\\"offline_data - assemble lumped mass matrix, and c_ij\\\");\\n\\u00a0 \\n\\u00a0       const auto local_assemble_system = \\n\\u00a0         [&](const typename DoFHandler<dim>::cell_iterator &cell,\\n\\u00a0             MeshWorker::ScratchData<dim>                  &scratch,\\n\\u00a0             CopyData<dim>                                 &copy) {\\n\\u00a0           copy.is_artificial = cell->is_artificial();\\n\\u00a0           if (copy.is_artificial)\\n\\u00a0             return;\\n\\u00a0 \\n\\u00a0           copy.local_boundary_normal_map.clear();\\n\\u00a0           copy.cell_lumped_mass_matrix.reinit(dofs_per_cell, dofs_per_cell);\\n\\u00a0           for (auto &matrix : copy.cell_cij_matrix)\\n\\u00a0             matrix.reinit(dofs_per_cell, dofs_per_cell);\\n\\u00a0 \\n\\u00a0           const auto &fe_values = scratch.reinit(cell);\\n\\u00a0 \\n\\u00a0           copy.local_dof_indices.resize(dofs_per_cell);\\n\\u00a0           cell->get_dof_indices(copy.local_dof_indices);\\n\\u00a0 \\n\\u00a0           std::transform(copy.local_dof_indices.begin(),\\n\\u00a0                          copy.local_dof_indices.end(),\\n\\u00a0                          copy.local_dof_indices.begin(),\\n\\u00a0                          [&](types::global_dof_index index) {\\n\\u00a0                            return partitioner->global_to_local(index);\\n\\u00a0                          });\\n\\u00a0 \\nMeshWorker::ScratchDataDefinition scratch_data.h:210\\nDoFHandler::cell_iteratortypename ActiveSelector::cell_iterator cell_iteratorDefinition dof_handler.h:468\\nupdate_values@ update_valuesShape function values.Definition fe_update_flags.h:75\\nupdate_normal_vectors@ update_normal_vectorsNormal vectors.Definition fe_update_flags.h:141\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\nWe compute the local contributions for the lumped mass matrix entries \\\\(m_i\\\\) and and vectors \\\\(c_{ij}\\\\) in the usual fashion:\\n\\u00a0           for (unsigned int q_point = 0; q_point < n_q_points; ++q_point)\\n\\u00a0             {\\n\\u00a0               const auto JxW = fe_values.JxW(q_point);\\n\\u00a0 \\n\\u00a0               for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0                 {\\n\\u00a0                   const auto value_JxW =\\n\\u00a0                     fe_values.shape_value(j, q_point) * JxW;\\n\\u00a0                   const auto grad_JxW = fe_values.shape_grad(j, q_point) * JxW;\\n\\u00a0 \\n\\u00a0                   copy.cell_lumped_mass_matrix(j, j) += value_JxW;\\n\\u00a0 \\n\\u00a0                   for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0                     {\\n\\u00a0                       const auto value = fe_values.shape_value(i, q_point);\\n\\u00a0                       for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0                         copy.cell_cij_matrix[d](i, j) += value * grad_JxW[d];\\n\\u00a0 \\n\\u00a0                     } /* i */\\n\\u00a0                 }     /* j */\\n\\u00a0             }         /* q */\\n\\u00a0 \\nNow we have to compute the boundary normals. Note that the following loop does not do much unless the element has faces on the boundary of the domain.\\n\\u00a0           for (const auto f : cell->face_indices())\\n\\u00a0             {\\n\\u00a0               const auto face = cell->face(f);\\n\\u00a0               const auto id   = face->boundary_id();\\n\\u00a0 \\n\\u00a0               if (!face->at_boundary())\\n\\u00a0                 continue;\\n\\u00a0 \\n\\u00a0               const auto &fe_face_values = scratch.reinit(cell, f);\\n\\u00a0 \\n\\u00a0               const unsigned int n_face_q_points =\\n\\u00a0                 fe_face_values.get_quadrature().size();\\n\\u00a0 \\n\\u00a0               for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0                 {\\n\\u00a0                   if (!discretization->finite_element.has_support_on_face(j, f))\\n\\u00a0                     continue;\\n\\u00a0 \\nNote that \\\"normal\\\" will only represent the contributions from one of the faces in the support of the shape function phi_j. So we cannot normalize this local contribution right here, we have to take it \\\"as is\\\", store it and pass it to the copy data routine. The proper normalization requires an additional loop on nodes. This is done in the copy function below.\\n\\u00a0                   Tensor<1, dim> normal;\\n\\u00a0                   if (id == Boundaries::free_slip)\\n\\u00a0                     {\\n\\u00a0                       for (unsigned int q = 0; q < n_face_q_points; ++q)\\n\\u00a0                         normal += fe_face_values.normal_vector(q) *\\n\\u00a0                                   fe_face_values.shape_value(j, q);\\n\\u00a0                     }\\n\\u00a0 \\n\\u00a0                   const auto index = copy.local_dof_indices[j];\\n\\u00a0 \\n\\u00a0                   Point<dim> position;\\n\\u00a0                   for (const auto v : cell->vertex_indices())\\n\\u00a0                     if (cell->vertex_dof_index(v, 0) ==\\n\\u00a0                         partitioner->local_to_global(index))\\n\\u00a0                       {\\n\\u00a0                         position = cell->vertex(v);\\n\\u00a0                         break;\\n\\u00a0                       }\\n\\u00a0 \\n\\u00a0                   const auto old_id =\\n\\u00a0                     std::get<1>(copy.local_boundary_normal_map[index]);\\n\\u00a0                   copy.local_boundary_normal_map[index] =\\n\\u00a0                     std::make_tuple(normal, std::max(old_id, id), position);\\n\\u00a0                 }\\n\\u00a0             }\\n\\u00a0         };\\n\\u00a0 \\ninternal::VectorOperations::copyvoid copy(const T *begin, const T *end, U *dest)Definition vector_operations_internal.h:65\\ninternal::VectorizationTypes::index@ index\\nstd::max::VectorizedArray< Number, width > max(const ::VectorizedArray< Number, width > &, const ::VectorizedArray< Number, width > &)Definition vectorization.h:6943\\nLast, we provide a copy_local_to_global function as required for the WorkStream\\n\\u00a0       const auto copy_local_to_global = [&](const CopyData<dim> &copy) {\\n\\u00a0         if (copy.is_artificial)\\n\\u00a0           return;\\n\\u00a0 \\n\\u00a0         for (const auto &it : copy.local_boundary_normal_map)\\n\\u00a0           {\\n\\u00a0             std::get<0>(boundary_normal_map[it.first]) +=\\n\\u00a0               std::get<0>(it.second);\\n\\u00a0             std::get<1>(boundary_normal_map[it.first]) =\\n\\u00a0               std::max(std::get<1>(boundary_normal_map[it.first]),\\n\\u00a0                        std::get<1>(it.second));\\n\\u00a0             std::get<2>(boundary_normal_map[it.first]) = std::get<2>(it.second);\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         lumped_mass_matrix.add(copy.local_dof_indices,\\n\\u00a0                                copy.cell_lumped_mass_matrix);\\n\\u00a0 \\n\\u00a0         for (int k = 0; k < dim; ++k)\\n\\u00a0           {\\n\\u00a0             cij_matrix[k].add(copy.local_dof_indices, copy.cell_cij_matrix[k]);\\n\\u00a0             nij_matrix[k].add(copy.local_dof_indices, copy.cell_cij_matrix[k]);\\n\\u00a0           }\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0       WorkStream::run(dof_handler.begin_active(),\\n\\u00a0                       dof_handler.end(),\\n\\u00a0                       local_assemble_system,\\n\\u00a0                       copy_local_to_global,\\n\\u00a0                       scratch_data,\\n\\u00a0                       CopyData<dim>());\\n\\u00a0     }\\n\\u00a0 \\nsecondPoint< 2 > secondDefinition grid_out.cc:4624\\nfirstPoint< 2 > firstDefinition grid_out.cc:4623\\nWorkStream::runvoid run(const std::vector< std::vector< Iterator > > &colored_iterators, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length=2 *MultithreadInfo::n_threads(), const unsigned int chunk_size=8)Definition work_stream.h:1272\\nstdSTL namespace.\\nAt this point in time we are done with the computation of \\\\(m_i\\\\) and \\\\(\\\\mathbf{c}_{ij}\\\\), but so far the matrix nij_matrix contains just a copy of the matrix cij_matrix. That's not what we really want: we have to normalize its entries. In addition, we have not filled the entries of the matrix norm_matrix and the vectors stored in the map OfflineData<dim>::BoundaryNormalMap are not normalized.\\nIn principle, this is just offline data, it doesn't make much sense to over-optimize their computation, since their cost will get amortized over the many time steps that we are going to use. However, computing/storing the entries of the matrix norm_matrix and the normalization of nij_matrix are perfect to illustrate thread-parallel node-loops:\\nwe want to visit every node \\\\(i\\\\) in the mesh/sparsity graph,\\nand for every such node we want to visit to every \\\\(j\\\\) such that \\\\(\\\\mathbf{c}_{ij} \\\\not \\\\equiv 0\\\\).\\n\\nFrom an algebraic point of view, this is equivalent to: visiting every row in the matrix and for each one of these rows execute a loop on the columns. Node-loops is a core theme of this tutorial step (see the pseudo-code in the introduction) that will repeat over and over again. That's why this is the right time to introduce them.\\nWe have the thread parallelization capability parallel::apply_to_subranges() that is somehow more general than the WorkStream framework. In particular, parallel::apply_to_subranges() can be used for our node-loops. This functionality requires four input arguments which we explain in detail (for the specific case of our thread-parallel node loops):\\nThe iterator indices.begin() points to a row index.\\nThe iterator indices.end() points to a numerically higher row index.\\nThe function on_subranges(index_begin, index_end) (where index_begin and index_end define a sub-range within the range spanned by the begin and end iterators defined in the two previous bullets) applies an operation to every iterator in such subrange. We may as well call on_subranges the \\\"worker\\\".\\nGrainsize: minimum number of iterators (in this case representing rows) processed by each thread. We decided for a minimum of 4096 rows.\\n\\nA minor caveat here is that the iterators indices.begin() and indices.end() supplied to parallel::apply_to_subranges() have to be random access iterators: internally, parallel::apply_to_subranges() will break the range defined by the indices.begin() and indices.end() iterators into subranges (we want to be able to read any entry in those subranges with constant complexity). In order to provide such iterators we resort to std_cxx20::ranges::iota_view.\\nThe bulk of the following piece of code is spent defining the \\\"worker\\\" on_subranges: i.e. the operation applied at each row of the sub-range. Given a fixed row_index we want to visit every column/entry in such row. In order to execute such columns-loops we use std::for_each from the standard library, where:\\nsparsity_pattern.begin(row_index) gives us an iterator starting at the first column of the row,\\nsparsity_pattern.end(row_index) is an iterator pointing at the last column of the row,\\nthe last argument required by std::for_each is the operation applied at each nonzero entry (a lambda expression in this case) of such row.\\n\\nWe note that, parallel::apply_to_subranges() will operate on disjoint sets of rows (the subranges) and our goal is to write into these rows. Because of the simple nature of the operations we want to carry out (computation and storage of normals, and normalization of the \\\\(\\\\mathbf{c}_{ij}\\\\) of entries) threads cannot conflict attempting to write the same entry (we do not need a scheduler).\\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope scope(computing_timer,\\n\\u00a0                                \\\"offline_data - compute |c_ij|, and n_ij\\\");\\n\\u00a0 \\n\\u00a0       const std_cxx20::ranges::iota_view<unsigned int, unsigned int> indices(\\n\\u00a0         0, n_locally_relevant);\\n\\u00a0 \\n\\u00a0       const auto on_subranges = \\n\\u00a0         [&](const auto index_begin, const auto index_end) {\\n\\u00a0           for (const auto row_index :\\n\\u00a0                std_cxx20::ranges::iota_view<unsigned int, unsigned int>(\\n\\u00a0                  *index_begin, *index_end))\\n\\u00a0             {\\nint\\nstd_cxx20::ranges::iota_viewboost::integer_range< IncrementableType > iota_viewDefinition iota_view.h:45\\nFirst column-loop: we compute and store the entries of the matrix norm_matrix and write normalized entries into the matrix nij_matrix:\\n\\u00a0               std::for_each(sparsity_pattern.begin(row_index),\\n\\u00a0                             sparsity_pattern.end(row_index),\\n\\u00a0                             [&](const SparsityPatternIterators::Accessor &jt) {\\n\\u00a0                               const auto c_ij =\\n\\u00a0                                 gather_get_entry(cij_matrix, &jt);\\n\\u00a0                               const double norm = c_ij.norm();\\n\\u00a0 \\n\\u00a0                               set_entry(norm_matrix, &jt, norm);\\n\\u00a0                               for (unsigned int j = 0; j < dim; ++j)\\n\\u00a0                                 set_entry(nij_matrix[j], &jt, c_ij[j] / norm);\\n\\u00a0                             });\\n\\u00a0             }\\n\\u00a0         };\\n\\u00a0 \\n\\u00a0       parallel::apply_to_subranges(indices.begin(),\\n\\u00a0                                    indices.end(),\\n\\u00a0                                    on_subranges,\\n\\u00a0                                    4096);\\n\\u00a0 \\nSparsityPatternIterators::AccessorDefinition sparsity_pattern.h:133\\nparallel::apply_to_subrangesvoid apply_to_subranges(const Iterator &begin, const std_cxx20::type_identity_t< Iterator > &end, const Function &f, const unsigned int grainsize)Definition parallel.h:452\\nFinally, we normalize the vectors stored in OfflineData<dim>::BoundaryNormalMap. This operation has not been thread parallelized as it would neither illustrate any important concept nor lead to any noticeable speed gain.\\n\\u00a0       for (auto &it : boundary_normal_map)\\n\\u00a0         {\\n\\u00a0           auto &normal = std::get<0>(it.second);\\n\\u00a0           normal /= (normal.norm() + std::numeric_limits<double>::epsilon());\\n\\u00a0         }\\n\\u00a0     }\\n\\u00a0   }\\n\\u00a0 \\nAt this point we are very much done with anything related to offline data.\\n Equation of state and approximate Riemann solver\\nIn this section we describe the implementation of the class members of the ProblemDescription class. Most of the code here is specific to the compressible Euler's equations with an ideal gas law. If we wanted to re-purpose step-69 for a different conservation law (say for: instance the shallow water equation) most of the implementation of this class would have to change. But most of the other classes (in particular those defining loop structures) would remain unchanged.\\nWe start by implementing a number of small member functions for computing momentum, internal_energy, pressure, speed_of_sound, and the flux f of the system. The functionality of each one of these functions is self-explanatory from their names.\\n\\u00a0   template <int dim>\\n\\u00a0   DEAL_II_ALWAYS_INLINE inline Tensor<1, dim>\\n\\u00a0   ProblemDescription<dim>::momentum(const state_type &U)\\n\\u00a0   {\\n\\u00a0     Tensor<1, dim> result;\\n\\u00a0     std::copy_n(&U[1], dim, &result[0]);\\n\\u00a0     return result;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0   ProblemDescription<dim>::internal_energy(const state_type &U)\\n\\u00a0   {\\n\\u00a0     const double &rho = U[0];\\n\\u00a0     const auto    m   = momentum(U);\\n\\u00a0     const double &E   = U[dim + 1];\\n\\u00a0     return E - 0.5 * m.norm_square() / rho;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0   ProblemDescription<dim>::pressure(const state_type &U)\\n\\u00a0   {\\n\\u00a0     return (gamma - 1.) * internal_energy(U);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0   ProblemDescription<dim>::speed_of_sound(const state_type &U)\\n\\u00a0   {\\n\\u00a0     const double &rho = U[0];\\n\\u00a0     const double  p   = pressure(U);\\n\\u00a0 \\n\\u00a0     return std::sqrt(gamma * p / rho);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   DEAL_II_ALWAYS_INLINE inline typename ProblemDescription<dim>::flux_type\\n\\u00a0   ProblemDescription<dim>::flux(const state_type &U)\\n\\u00a0   {\\n\\u00a0     const double &rho = U[0];\\n\\u00a0     const auto    m   = momentum(U);\\n\\u00a0     const auto    p   = pressure(U);\\n\\u00a0     const double &E   = U[dim + 1];\\n\\u00a0 \\n\\u00a0     flux_type result;\\n\\u00a0 \\n\\u00a0     result[0] = m;\\n\\u00a0     for (unsigned int i = 0; i < dim; ++i)\\n\\u00a0       {\\n\\u00a0         result[1 + i] = m * m[i] / rho;\\n\\u00a0         result[1 + i][i] += p;\\n\\u00a0       }\\n\\u00a0     result[dim + 1] = m / rho * (E + p);\\n\\u00a0 \\n\\u00a0     return result;\\n\\u00a0   }\\n\\u00a0 \\nPhysics::Elasticity::Kinematics::ESymmetricTensor< 2, dim, Number > E(const Tensor< 2, dim, Number > &F)\\nstd::sqrt::VectorizedArray< Number, width > sqrt(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6869\\nNow we discuss the computation of  \\\\(\\\\lambda_{\\\\text{max}}\\n   (\\\\mathbf{U}_i^{n},\\\\mathbf{U}_j^{n}, \\\\textbf{n}_{ij})\\\\). The analysis and derivation of sharp upper-bounds of maximum wavespeeds of Riemann problems is a very technical endeavor and we cannot include an advanced discussion about it in this tutorial. In this portion of the documentation we will limit ourselves to sketch the main functionality of our implementation functions and point to specific academic references in order to help the (interested) reader trace the source (and proper mathematical justification) of these ideas.\\nIn general, obtaining a sharp guaranteed upper-bound on the maximum wavespeed requires solving a quite expensive scalar nonlinear problem. This is typically done with an iterative solver. In order to simplify the presentation in this example step we decided not to include such an iterative scheme. Instead, we will just use an initial guess as a guess for an upper bound on the maximum wavespeed. More precisely, equations (2.11) (3.7), (3.8) and (4.3) of [105] are enough to define a guaranteed upper bound on the maximum wavespeed. This estimate is returned by a call to the function lambda_max_two_rarefaction(). At its core the construction of such an upper bound uses the so-called two-rarefaction approximation for the intermediate pressure \\\\(p^*\\\\), see for instance Equation (4.46), page 128 in [78].\\nThe estimate returned by lambda_max_two_rarefaction() is guaranteed to be an upper bound, it is in general quite sharp, and overall sufficient for our purposes. However, for some specific situations (in particular when one of states is close to vacuum conditions) such an estimate will be overly pessimistic. That's why we used a second estimate to avoid this degeneracy that will be invoked by a call to the function lambda_max_expansion(). The most important function here is compute_lambda_max() which takes the minimum between the estimates returned by lambda_max_two_rarefaction() and lambda_max_expansion().\\nWe start again by defining a couple of helper functions:\\nThe first function takes a state U and a unit vector n_ij and computes the projected 1d state in direction of the unit vector.\\n\\u00a0   namespace\\n\\u00a0   {\\n\\u00a0     template <int dim>\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline std::array<double, 4> riemann_data_from_state(\\n\\u00a0       const typename ProblemDescription<dim>::state_type U,\\n\\u00a0       const Tensor<1, dim>                              &n_ij)\\n\\u00a0     {\\n\\u00a0       Tensor<1, 3> projected_U;\\n\\u00a0       projected_U[0] = U[0];\\n\\u00a0 \\nFor this, we have to change the momentum to  \\\\(\\\\textbf{m}\\\\cdot\\n   n_{ij}\\\\) and have to subtract the kinetic energy of the perpendicular part from the total energy:\\n\\u00a0       const auto m   = ProblemDescription<dim>::momentum(U);\\n\\u00a0       projected_U[1] = n_ij * m;\\n\\u00a0 \\n\\u00a0       const auto perpendicular_m = m - projected_U[1] * n_ij;\\n\\u00a0       projected_U[2] = U[1 + dim] - 0.5 * perpendicular_m.norm_square() / U[0];\\n\\u00a0 \\nWe return the 1d state in primitive variables instead of conserved quantities. The return array consists of density \\\\(\\\\rho\\\\), velocity \\\\(u\\\\), pressure \\\\(p\\\\) and local speed of sound \\\\(a\\\\):\\n\\u00a0       return {{projected_U[0],\\n\\u00a0                projected_U[1] / projected_U[0],\\n\\u00a0                ProblemDescription<1>::pressure(projected_U),\\n\\u00a0                ProblemDescription<1>::speed_of_sound(projected_U)}};\\n\\u00a0     }\\n\\u00a0 \\nAt this point we also define two small functions that return the positive and negative part of a double.\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline double positive_part(const double number)\\n\\u00a0     {\\n\\u00a0       return std::max(number, 0.);\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     DEAL_II_ALWAYS_INLINE inline double negative_part(const double number)\\n\\u00a0     {\\n\\u00a0       return -std::min(number, 0.);\\n\\u00a0     }\\n\\u00a0 \\nstd::min::VectorizedArray< Number, width > min(const ::VectorizedArray< Number, width > &, const ::VectorizedArray< Number, width > &)Definition vectorization.h:6960\\nNext, we need two local wavenumbers that are defined in terms of a primitive state \\\\([\\\\rho, u, p, a]\\\\) and a given pressure \\\\(p^\\\\ast\\\\) [106] Eqn. (3.7):    \\n\\\\begin{align*}\\n   \\\\lambda^- = u - a\\\\,\\\\sqrt{1 + \\\\frac{\\\\gamma+1}{2\\\\gamma}\\n   \\\\left(\\\\frac{p^\\\\ast-p}{p}\\\\right)_+}\\n   \\\\end{align*}\\n\\n Here, the \\\\((\\\\cdot)_{+}\\\\) denotes the positive part of the given argument.\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0     lambda1_minus(const std::array<double, 4> &riemann_data,\\n\\u00a0                   const double                 p_star)\\n\\u00a0     {\\n\\u00a0       /* Implements formula (3.7) in Guermond-Popov-2016 */\\n\\u00a0 \\n\\u00a0       constexpr double gamma = ProblemDescription<1>::gamma;\\n\\u00a0       const auto       u     = riemann_data[1];\\n\\u00a0       const auto       p     = riemann_data[2];\\n\\u00a0       const auto       a     = riemann_data[3];\\n\\u00a0 \\n\\u00a0       const double factor = (gamma + 1.0) / 2.0 / gamma;\\n\\u00a0       const double tmp    = positive_part((p_star - p) / p);\\n\\u00a0       return u - a * std::sqrt(1.0 + factor * tmp);\\n\\u00a0     }\\n\\u00a0 \\nAnalougously [106] Eqn. (3.8):    \\n\\\\begin{align*}\\n   \\\\lambda^+ = u + a\\\\,\\\\sqrt{1 + \\\\frac{\\\\gamma+1}{2\\\\gamma}\\n   \\\\left(\\\\frac{p^\\\\ast-p}{p}\\\\right)_+}\\n   \\\\end{align*}\\n\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0     lambda3_plus(const std::array<double, 4> &riemann_data, const double p_star)\\n\\u00a0     {\\n\\u00a0       /* Implements formula (3.8) in Guermond-Popov-2016 */\\n\\u00a0 \\n\\u00a0       constexpr double gamma = ProblemDescription<1>::gamma;\\n\\u00a0       const auto       u     = riemann_data[1];\\n\\u00a0       const auto       p     = riemann_data[2];\\n\\u00a0       const auto       a     = riemann_data[3];\\n\\u00a0 \\n\\u00a0       const double factor = (gamma + 1.0) / 2.0 / gamma;\\n\\u00a0       const double tmp    = positive_part((p_star - p) / p);\\n\\u00a0       return u + a * std::sqrt(1.0 + factor * tmp);\\n\\u00a0     }\\n\\u00a0 \\nAll that is left to do is to compute the maximum of \\\\(\\\\lambda^-\\\\) and \\\\(\\\\lambda^+\\\\) computed from the left and right primitive state ([106] Eqn. (2.11)), where \\\\(p^\\\\ast\\\\) is given by [106] Eqn (4.3):\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0     lambda_max_two_rarefaction(const std::array<double, 4> &riemann_data_i,\\n\\u00a0                                const std::array<double, 4> &riemann_data_j)\\n\\u00a0     {\\n\\u00a0       constexpr double gamma = ProblemDescription<1>::gamma;\\n\\u00a0       const auto       u_i   = riemann_data_i[1];\\n\\u00a0       const auto       p_i   = riemann_data_i[2];\\n\\u00a0       const auto       a_i   = riemann_data_i[3];\\n\\u00a0       const auto       u_j   = riemann_data_j[1];\\n\\u00a0       const auto       p_j   = riemann_data_j[2];\\n\\u00a0       const auto       a_j   = riemann_data_j[3];\\n\\u00a0 \\n\\u00a0       const double numerator = a_i + a_j - (gamma - 1.) / 2. * (u_j - u_i);\\n\\u00a0 \\n\\u00a0       const double denominator =\\n\\u00a0         a_i * std::pow(p_i / p_j, -1. * (gamma - 1.) / 2. / gamma) + a_j * 1.;\\n\\u00a0 \\n\\u00a0       /* Formula (4.3) in Guermond-Popov-2016 */\\n\\u00a0 \\n\\u00a0       const double p_star =\\n\\u00a0         p_j * std::pow(numerator / denominator, 2. * gamma / (gamma - 1));\\n\\u00a0 \\n\\u00a0       const double lambda1 = lambda1_minus(riemann_data_i, p_star);\\n\\u00a0       const double lambda3 = lambda3_plus(riemann_data_j, p_star);\\n\\u00a0 \\n\\u00a0       /* Formula (2.11) in Guermond-Popov-2016 */\\n\\u00a0 \\n\\u00a0       return std::max(positive_part(lambda3), negative_part(lambda1));\\n\\u00a0     }\\n\\u00a0 \\nstd::pow::VectorizedArray< Number, width > pow(const ::VectorizedArray< Number, width > &, const Number p)Definition vectorization.h:6885\\nWe compute the second upper bound of the maximal wavespeed that is, in general, not as sharp as the two-rarefaction estimate. But it will save the day in the context of near vacuum conditions when the two-rarefaction approximation might attain extreme values:   \\n\\\\begin{align*}\\n   \\\\lambda_{\\\\text{exp}} = \\\\max(u_i,u_j) + 5. \\\\max(a_i, a_j).\\n   \\\\end{align*}\\n\\nNoteThe constant 5.0 multiplying the maximum of the sound speeds is neither an ad-hoc constant, nor a tuning parameter. It defines an upper bound for any \\\\(\\\\gamma \\\\in [0,5/3]\\\\). Do not play with it!\\n\\u00a0     DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0     lambda_max_expansion(const std::array<double, 4> &riemann_data_i,\\n\\u00a0                          const std::array<double, 4> &riemann_data_j)\\n\\u00a0     {\\n\\u00a0       const auto u_i = riemann_data_i[1];\\n\\u00a0       const auto a_i = riemann_data_i[3];\\n\\u00a0       const auto u_j = riemann_data_j[1];\\n\\u00a0       const auto a_j = riemann_data_j[3];\\n\\u00a0 \\n\\u00a0       return std::max(std::abs(u_i), std::abs(u_j)) + 5. * std::max(a_i, a_j);\\n\\u00a0     }\\n\\u00a0   } // namespace\\n\\u00a0 \\nstd::abs::VectorizedArray< Number, width > abs(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6927\\nThe following is the main function that we are going to call in order to compute  \\\\(\\\\lambda_{\\\\text{max}} (\\\\mathbf{U}_i^{n},\\\\mathbf{U}_j^{n},\\n   \\\\textbf{n}_{ij})\\\\). We simply compute both maximal wavespeed estimates and return the minimum.\\n\\u00a0   template <int dim>\\n\\u00a0   DEAL_II_ALWAYS_INLINE inline double\\n\\u00a0   ProblemDescription<dim>::compute_lambda_max(const state_type     &U_i,\\n\\u00a0                                               const state_type     &U_j,\\n\\u00a0                                               const Tensor<1, dim> &n_ij)\\n\\u00a0   {\\n\\u00a0     const auto riemann_data_i = riemann_data_from_state(U_i, n_ij);\\n\\u00a0     const auto riemann_data_j = riemann_data_from_state(U_j, n_ij);\\n\\u00a0 \\n\\u00a0     const double lambda_1 =\\n\\u00a0       lambda_max_two_rarefaction(riemann_data_i, riemann_data_j);\\n\\u00a0 \\n\\u00a0     const double lambda_2 =\\n\\u00a0       lambda_max_expansion(riemann_data_i, riemann_data_j);\\n\\u00a0 \\n\\u00a0     return std::min(lambda_1, lambda_2);\\n\\u00a0   }\\n\\u00a0 \\nWe conclude this section by defining static arrays component_names that contain strings describing the component names of our state vector. We have template specializations for dimensions one, two and three, that are used later in DataOut for naming the corresponding components:\\n\\u00a0   template <>\\n\\u00a0   const std::array<std::string, 3> ProblemDescription<1>::component_names{\\n\\u00a0     {\\\"rho\\\", \\\"m\\\", \\\"E\\\"}};\\n\\u00a0 \\n\\u00a0   template <>\\n\\u00a0   const std::array<std::string, 4> ProblemDescription<2>::component_names{\\n\\u00a0     {\\\"rho\\\", \\\"m_1\\\", \\\"m_2\\\", \\\"E\\\"}};\\n\\u00a0 \\n\\u00a0   template <>\\n\\u00a0   const std::array<std::string, 5> ProblemDescription<3>::component_names{\\n\\u00a0     {\\\"rho\\\", \\\"m_1\\\", \\\"m_2\\\", \\\"m_3\\\", \\\"E\\\"}};\\n\\u00a0 \\n Initial values\\nThe last preparatory step, before we discuss the implementation of the forward Euler scheme, is to briefly implement the InitialValues class.\\nIn the constructor we initialize all parameters with default values, declare all parameters for the ParameterAcceptor class and connect the parse_parameters_call_back slot to the respective signal.\\nThe parse_parameters_call_back slot will be invoked from ParameterAceptor after the call to ParameterAcceptor::initialize(). In that regard, its use is appropriate for situations where the parameters have to be postprocessed (in some sense) or some consistency condition between the parameters has to be checked.\\n  \\u00a0   template <int dim>\\n  \\u00a0   InitialValues<dim>::InitialValues(const std::string &subsection)\\n  \\u00a0     : ParameterAcceptor(subsection)\\n  \\u00a0   {\\n  \\u00a0     /* We wire up the slot InitialValues<dim>::parse_parameters_callback to\\n* \\u00a0        the ParameterAcceptor::parse_parameters_call_back signal: */\\n  \\u00a0     ParameterAcceptor::parse_parameters_call_back.connect(\\n  \\u00a0       [&]() { this->parse_parameters_callback(); });\\n  \\u00a0 \\n  \\u00a0     initial_direction[0] = 1.;\\n  \\u00a0     add_parameter(\\\"initial direction\\\",\\n  \\u00a0                   initial_direction,\\n  \\u00a0                   \\\"Initial direction of the uniform flow field\\\");\\n  \\u00a0 \\n  \\u00a0     initial_1d_state[0] = ProblemDescription<dim>::gamma;\\n  \\u00a0     initial_1d_state[1] = 3.;\\n  \\u00a0     initial_1d_state[2] = 1.;\\n  \\u00a0     add_parameter(\\\"initial 1d state\\\",\\n  \\u00a0                   initial_1d_state,\\n  \\u00a0                   \\\"Initial 1d state (rho, u, p) of the uniform flow field\\\");\\n  \\u00a0   }\\n  \\u00a0 \\nSo far the constructor of InitialValues has defined default values for the two private members initial_direction and initial_1d_state and added them to the parameter list. But we have not defined an implementation of the only public member that we really care about, which is initial_state() (the function that we are going to call to actually evaluate the initial solution at the mesh nodes). At the top of the function, we have to ensure that the provided initial direction is not the zero vector.\\nNoteAs commented, we could have avoided using the method parse_parameters_call_back  and defined a class member setup() in order to define the implementation of initial_state(). But for illustrative purposes we want to document a different way here and use the call back signal from ParameterAcceptor.\\n\\u00a0   template <int dim>\\n\\u00a0   void InitialValues<dim>::parse_parameters_callback()\\n\\u00a0   {\\n\\u00a0     AssertThrow(initial_direction.norm() != 0.,\\n\\u00a0                 ExcMessage(\\n\\u00a0                   \\\"Initial shock front direction is set to the zero vector.\\\"));\\n\\u00a0     initial_direction /= initial_direction.norm();\\n\\u00a0 \\nAssertThrow#define AssertThrow(cond, exc)Definition exceptions.h:1739\\nNext, we implement the initial_state function object with a lambda function computing a uniform flow field. For this we have to translate a given primitive 1d state (density \\\\(\\\\rho\\\\), velocity \\\\(u\\\\), and pressure \\\\(p\\\\)) into a conserved n-dimensional state (density \\\\(\\\\rho\\\\), momentum \\\\(\\\\mathbf{m}\\\\), and total energy \\\\(E\\\\)).\\n\\u00a0     initial_state = [this](const Point<dim> & /*point*/, double /*t*/) {\\n\\u00a0       const double            rho   = initial_1d_state[0];\\n\\u00a0       const double            u     = initial_1d_state[1];\\n\\u00a0       const double            p     = initial_1d_state[2];\\n\\u00a0       static constexpr double gamma = ProblemDescription<dim>::gamma;\\n\\u00a0 \\n\\u00a0       state_type state;\\n\\u00a0 \\n\\u00a0       state[0] = rho;\\n\\u00a0       for (unsigned int i = 0; i < dim; ++i)\\n\\u00a0         state[1 + i] = rho * u * initial_direction[i];\\n\\u00a0 \\n\\u00a0       state[dim + 1] = p / (gamma - 1.) + 0.5 * rho * u * u;\\n\\u00a0 \\n\\u00a0       return state;\\n\\u00a0     };\\n\\u00a0   }\\n\\u00a0 \\n The Forward Euler step\\nThe constructor of the TimeStepping class does not contain any surprising code:\\n\\u00a0   template <int dim>\\n\\u00a0   TimeStepping<dim>::TimeStepping(\\n\\u00a0     const MPI_Comm            mpi_communicator,\\n\\u00a0     TimerOutput              &computing_timer,\\n\\u00a0     const OfflineData<dim>   &offline_data,\\n\\u00a0     const InitialValues<dim> &initial_values,\\n\\u00a0     const std::string        &subsection /*= \\\"TimeStepping\\\"*/)\\n\\u00a0     : ParameterAcceptor(subsection)\\n\\u00a0     , mpi_communicator(mpi_communicator)\\n\\u00a0     , computing_timer(computing_timer)\\n\\u00a0     , offline_data(&offline_data)\\n\\u00a0     , initial_values(&initial_values)\\n\\u00a0   {\\n\\u00a0     cfl_update = 0.80;\\n\\u00a0     add_parameter(\\\"cfl update\\\",\\n\\u00a0                   cfl_update,\\n\\u00a0                   \\\"Relative CFL constant used for update\\\");\\n\\u00a0   }\\n\\u00a0 \\nTimeStepping::TimeSteppingDefinition time_stepping.h:175\\nIn the class member prepare() we initialize the temporary vector temp and the matrix dij_matrix. The vector temp will be used to store the solution update temporarily before its contents is swapped with the old vector.\\n\\u00a0   template <int dim>\\n\\u00a0   void TimeStepping<dim>::prepare()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope scope(computing_timer,\\n\\u00a0                              \\\"time_stepping - prepare scratch space\\\");\\n\\u00a0 \\n\\u00a0     for (auto &it : temporary_vector)\\n\\u00a0       it.reinit(offline_data->partitioner);\\n\\u00a0 \\n\\u00a0     dij_matrix.reinit(offline_data->sparsity_pattern);\\n\\u00a0   }\\n\\u00a0 \\nIt is now time to implement the forward Euler step. Given a (writable reference) to the old state U at time \\\\(t\\\\) we update the state U in place and return the chosen time-step size. We first declare a number of read-only references to various different variables and data structures. We do this is mainly to have shorter variable names (e.g., sparsity instead of offline_data->sparsity_pattern).\\n\\u00a0   template <int dim>\\n\\u00a0   double TimeStepping<dim>::make_one_step(vector_type &U, const double t)\\n\\u00a0   {\\n\\u00a0     const auto &n_locally_owned    = offline_data->n_locally_owned;\\n\\u00a0     const auto &n_locally_relevant = offline_data->n_locally_relevant;\\n\\u00a0 \\n\\u00a0     const std_cxx20::ranges::iota_view<unsigned int, unsigned int>\\n\\u00a0       indices_owned(0, n_locally_owned);\\n\\u00a0     const std_cxx20::ranges::iota_view<unsigned int, unsigned int>\\n\\u00a0       indices_relevant(0, n_locally_relevant);\\n\\u00a0 \\n\\u00a0     const auto &sparsity = offline_data->sparsity_pattern;\\n\\u00a0 \\n\\u00a0     const auto &lumped_mass_matrix = offline_data->lumped_mass_matrix;\\n\\u00a0     const auto &norm_matrix        = offline_data->norm_matrix;\\n\\u00a0     const auto &nij_matrix         = offline_data->nij_matrix;\\n\\u00a0     const auto &cij_matrix         = offline_data->cij_matrix;\\n\\u00a0 \\n\\u00a0     const auto &boundary_normal_map = offline_data->boundary_normal_map;\\n\\u00a0 \\nStep 1: Computing the \\\\(d_{ij}\\\\) graph viscosity matrix.\\nIt is important to highlight that the viscosity matrix has to be symmetric, i.e., \\\\(d_{ij} = d_{ji}\\\\). In this regard we note here that  \\\\(\\\\int_{\\\\Omega} \\\\nabla \\\\phi_j \\\\phi_i \\\\, \\\\mathrm{d}\\\\mathbf{x}= -\\n   \\\\int_{\\\\Omega} \\\\nabla \\\\phi_i \\\\phi_j \\\\, \\\\mathrm{d}\\\\mathbf{x}\\\\) (or equivalently \\\\(\\\\mathbf{c}_{ij} = - \\\\mathbf{c}_{ji}\\\\)) provided either \\\\(\\\\mathbf{x}_i\\\\) or \\\\(\\\\mathbf{x}_j\\\\) is a support point located away from the boundary. In this case we can check that   \\\\(\\\\lambda_{\\\\text{max}} (\\\\mathbf{U}_i^{n}, \\\\mathbf{U}_j^{n},\\n   \\\\textbf{n}_{ij}) = \\\\lambda_{\\\\text{max}} (\\\\mathbf{U}_j^{n},\\n   \\\\mathbf{U}_i^{n},\\\\textbf{n}_{ji})\\\\) by construction, which guarantees the property \\\\(d_{ij} = d_{ji}\\\\).\\nHowever, if both support points \\\\(\\\\mathbf{x}_i\\\\) or \\\\(\\\\mathbf{x}_j\\\\) happen to lie on the boundary, then, the equalities  \\\\(\\\\mathbf{c}_{ij} =\\n   - \\\\mathbf{c}_{ji}\\\\) and   \\\\(\\\\lambda_{\\\\text{max}} (\\\\mathbf{U}_i^{n},\\n   \\\\mathbf{U}_j^{n}, \\\\textbf{n}_{ij}) = \\\\lambda_{\\\\text{max}}\\n   (\\\\mathbf{U}_j^{n}, \\\\mathbf{U}_i^{n}, \\\\textbf{n}_{ji})\\\\) do not necessarily hold true. The only mathematically safe solution for this dilemma is to compute both of them \\\\(d_{ij}\\\\) and \\\\(d_{ji}\\\\) and take the maximum.\\nOverall, the computation of \\\\(d_{ij}\\\\) is quite expensive. In order to save some computing time we exploit the fact that the viscosity matrix has to be symmetric (as mentioned above): we only compute the upper-triangular entries of \\\\(d_{ij}\\\\) and copy the corresponding entries to the lower-triangular counterpart.\\nWe use again parallel::apply_to_subranges() for thread-parallel for loops. Pretty much all the ideas for parallel traversal that we introduced when discussing the assembly of the matrix norm_matrix and the normalization of nij_matrix above are used here again.\\nWe define again a \\\"worker\\\" function on_subranges that computes the viscosity \\\\(d_{ij}\\\\) for a subrange [*index_begin, *index_end) of column indices:\\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope scope(computing_timer,\\n\\u00a0                                \\\"time_stepping - 1 compute d_ij\\\");\\n\\u00a0 \\n\\u00a0       const auto on_subranges = \\n\\u00a0         [&](const auto index_begin, const auto index_end) {\\n\\u00a0           for (const auto i :\\n\\u00a0                std_cxx20::ranges::iota_view<unsigned int, unsigned int>(\\n\\u00a0                  *index_begin, *index_end))\\n\\u00a0             {\\n\\u00a0               const auto U_i = gather(U, i);\\n\\u00a0 \\nFor a given column index i we iterate over the columns of the sparsity pattern from sparsity.begin(i) to sparsity.end(i):\\n\\u00a0               for (auto jt = sparsity.begin(i); jt != sparsity.end(i); ++jt)\\n\\u00a0                 {\\n\\u00a0                   const auto j = jt->column();\\n\\u00a0 \\nWe only compute \\\\(d_{ij}\\\\) if \\\\(j < i\\\\) (upper triangular entries) and later copy the values over to \\\\(d_{ji}\\\\).\\n\\u00a0                   if (j >= i)\\n\\u00a0                     continue;\\n\\u00a0 \\n\\u00a0                   const auto U_j = gather(U, j);\\n\\u00a0 \\n\\u00a0                   const auto   n_ij = gather_get_entry(nij_matrix, jt);\\n\\u00a0                   const double norm = get_entry(norm_matrix, jt);\\n\\u00a0 \\n\\u00a0                   const auto lambda_max =\\n\\u00a0                     ProblemDescription<dim>::compute_lambda_max(U_i, U_j, n_ij);\\n\\u00a0 \\n\\u00a0                   double d = norm * lambda_max;\\n\\u00a0 \\nIf both support points happen to be at the boundary we have to compute \\\\(d_{ji}\\\\) as well and then take \\\\(\\\\max(d_{ij},d_{ji})\\\\). After this we can finally set the upper triangular and lower triangular entries.\\n\\u00a0                   if (boundary_normal_map.count(i) != 0 &&\\n\\u00a0                       boundary_normal_map.count(j) != 0)\\n\\u00a0                     {\\n\\u00a0                       const auto n_ji = gather(nij_matrix, j, i);\\n\\u00a0                       const auto lambda_max_2 =\\n\\u00a0                         ProblemDescription<dim>::compute_lambda_max(U_j,\\n\\u00a0                                                                     U_i,\\n\\u00a0                                                                     n_ji);\\n\\u00a0                       const double norm_2 = norm_matrix(j, i);\\n\\u00a0 \\n\\u00a0                       d = std::max(d, norm_2 * lambda_max_2);\\n\\u00a0                     }\\n\\u00a0 \\n\\u00a0                   set_entry(dij_matrix, jt, d);\\n\\u00a0                   dij_matrix(j, i) = d;\\n\\u00a0                 }\\n\\u00a0             }\\n\\u00a0         };\\n\\u00a0 \\n\\u00a0       parallel::apply_to_subranges(indices_relevant.begin(),\\n\\u00a0                                    indices_relevant.end(),\\n\\u00a0                                    on_subranges,\\n\\u00a0                                    4096);\\n\\u00a0     }\\n\\u00a0 \\nStep 2: Compute diagonal entries \\\\(d_{ii}\\\\) and \\\\(\\\\tau_{\\\\text{max}}\\\\).\\nSo far we have computed all off-diagonal entries of the matrix dij_matrix. We still have to fill its diagonal entries defined as  \\\\(d_{ii}^n = - \\\\sum_{j \\\\in \\\\mathcal{I}(i)\\\\backslash \\\\{i\\\\}}\\n   d_{ij}^n\\\\). We use again parallel::apply_to_subranges() for this purpose. While computing the \\\\(d_{ii}\\\\)s we also determine the largest admissible time-step, which is defined as    \\n\\\\[\\n   \\\\tau_n \\\\dealcoloneq c_{\\\\text{cfl}}\\\\,\\\\min_{i\\\\in\\\\mathcal{V}}\\n   \\\\left(\\\\frac{m_i}{-2\\\\,d_{ii}^{n}}\\\\right) \\\\, .\\n   \\\\]\\n\\n Note that the operation \\\\(\\\\min_{i \\\\in \\\\mathcal{V}}\\\\) is intrinsically global, it operates on all nodes: first we have to take the minimum over all threads (of a given node) and then we have to take the minimum over all MPI processes. In the current implementation:\\nWe store tau_max (per node) as std::atomic<double>. The internal implementation of std::atomic will take care of guarding any possible race condition when more than one thread attempts to read and/or write tau_max at the same time.\\nIn order to take the minimum over all MPI process we use the utility function Utilities::MPI::min.\\n\\n\\u00a0     std::atomic<double> tau_max{std::numeric_limits<double>::infinity()};\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope scope(computing_timer,\\n\\u00a0                                \\\"time_stepping - 2 compute d_ii, and tau_max\\\");\\n\\u00a0 \\non_subranges() will be executed on every thread individually. The variable tau_max_on_subrange is thus stored thread locally.\\n\\u00a0       const auto on_subranges = \\n\\u00a0         [&](const auto index_begin, const auto index_end) {\\n\\u00a0           double tau_max_on_subrange = std::numeric_limits<double>::infinity();\\n\\u00a0 \\n\\u00a0           for (const auto i :\\n\\u00a0                std_cxx20::ranges::iota_view<unsigned int, unsigned int>(\\n\\u00a0                  *index_begin, *index_end))\\n\\u00a0             {\\n\\u00a0               double d_sum = 0.;\\n\\u00a0 \\n\\u00a0               for (auto jt = sparsity.begin(i); jt != sparsity.end(i); ++jt)\\n\\u00a0                 {\\n\\u00a0                   const auto j = jt->column();\\n\\u00a0 \\n\\u00a0                   if (j == i)\\n\\u00a0                     continue;\\n\\u00a0 \\n\\u00a0                   d_sum -= get_entry(dij_matrix, jt);\\n\\u00a0                 }\\n\\u00a0 \\nWe store the negative sum of the d_ij entries at the diagonal position\\n\\u00a0               dij_matrix.diag_element(i) = d_sum;\\nand compute the maximal local time-step size tau:\\n\\u00a0               const double mass   = lumped_mass_matrix.diag_element(i);\\n\\u00a0               const double tau    = cfl_update * mass / (-2. * d_sum);\\n\\u00a0               tau_max_on_subrange = std::min(tau_max_on_subrange, tau);\\n\\u00a0             }\\n\\u00a0 \\ntau_max_on_subrange contains the largest possible time-step size computed for the (thread local) subrange. At this point we have to synchronize the value over all threads. This is were we use the std::atomic<double> compare exchange update mechanism:\\n\\u00a0           double current_tau_max = tau_max.load();\\n\\u00a0           while (current_tau_max > tau_max_on_subrange &&\\n\\u00a0                  !tau_max.compare_exchange_weak(current_tau_max,\\n\\u00a0                                                 tau_max_on_subrange))\\n\\u00a0             ;\\n\\u00a0         };\\n\\u00a0 \\n\\u00a0       parallel::apply_to_subranges(indices_relevant.begin(),\\n\\u00a0                                    indices_relevant.end(),\\n\\u00a0                                    on_subranges,\\n\\u00a0                                    4096);\\n\\u00a0 \\nAfter all threads have finished we can simply synchronize the value over all MPI processes:\\n\\u00a0       tau_max.store(Utilities::MPI::min(tau_max.load(), mpi_communicator));\\n\\u00a0 \\nUtilities::MPI::minT min(const T &t, const MPI_Comm mpi_communicator)\\nThis is a good point to verify that the computed tau_max is indeed a valid floating point number.\\n\\u00a0       AssertThrow(\\n\\u00a0         !std::isnan(tau_max.load()) && !std::isinf(tau_max.load()) &&\\n\\u00a0           tau_max.load() > 0.,\\n\\u00a0         ExcMessage(\\n\\u00a0           \\\"I'm sorry, Dave. I'm afraid I can't do that. - We crashed.\\\"));\\n\\u00a0     }\\n\\u00a0 \\nStep 3: Perform update.\\nAt this point, we have computed all viscosity coefficients \\\\(d_{ij}\\\\) and we know the maximal admissible time-step size \\\\(\\\\tau_{\\\\text{max}}\\\\). This means we can now compute the update:\\n\\n\\\\[\\n   \\\\mathbf{U}_i^{n+1} = \\\\mathbf{U}_i^{n} - \\\\frac{\\\\tau_{\\\\text{max}} }{m_i}\\n   \\\\sum_{j \\\\in \\\\mathcal{I}(i)} (\\\\mathbb{f}(\\\\mathbf{U}_j^{n}) -\\n   \\\\mathbb{f}(\\\\mathbf{U}_i^{n})) \\\\cdot \\\\mathbf{c}_{ij} - d_{ij}\\n   (\\\\mathbf{U}_j^{n} - \\\\mathbf{U}_i^{n})\\n   \\\\]\\n\\nThis update formula is slightly different from what was discussed in the introduction (in the pseudo-code). However, it can be shown that both equations are algebraically equivalent (they will produce the same numerical values). We favor this second formula since it has natural cancellation properties that might help avoid numerical artifacts.\\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope scope(computing_timer,\\n\\u00a0                                \\\"time_stepping - 3 perform update\\\");\\n\\u00a0 \\n\\u00a0       const auto on_subranges = \\n\\u00a0         [&](const auto index_begin, const auto index_end) {\\n\\u00a0           for (const auto i :\\n\\u00a0                boost::make_iterator_range(index_begin, index_end))\\n\\u00a0             {\\n\\u00a0               Assert(i < n_locally_owned, ExcInternalError());\\n\\u00a0 \\n\\u00a0               const auto U_i = gather(U, i);\\n\\u00a0 \\n\\u00a0               const auto   f_i = ProblemDescription<dim>::flux(U_i);\\n\\u00a0               const double m_i = lumped_mass_matrix.diag_element(i);\\n\\u00a0 \\n\\u00a0               auto U_i_new = U_i;\\n\\u00a0 \\n\\u00a0               for (auto jt = sparsity.begin(i); jt != sparsity.end(i); ++jt)\\n\\u00a0                 {\\n\\u00a0                   const auto j = jt->column();\\n\\u00a0 \\n\\u00a0                   const auto U_j = gather(U, j);\\n\\u00a0                   const auto f_j = ProblemDescription<dim>::flux(U_j);\\n\\u00a0 \\n\\u00a0                   const auto c_ij = gather_get_entry(cij_matrix, jt);\\n\\u00a0                   const auto d_ij = get_entry(dij_matrix, jt);\\n\\u00a0 \\n\\u00a0                   for (unsigned int k = 0; k < n_solution_variables; ++k)\\n\\u00a0                     {\\n\\u00a0                       U_i_new[k] +=\\n\\u00a0                         tau_max / m_i *\\n\\u00a0                         (-(f_j[k] - f_i[k]) * c_ij + d_ij * (U_j[k] - U_i[k]));\\n\\u00a0                     }\\n\\u00a0                 }\\n\\u00a0 \\n\\u00a0               scatter(temporary_vector, U_i_new, i);\\n\\u00a0             }\\n\\u00a0         };\\n\\u00a0 \\n\\u00a0       parallel::apply_to_subranges(indices_owned.begin(),\\n\\u00a0                                    indices_owned.end(),\\n\\u00a0                                    on_subranges,\\n\\u00a0                                    4096);\\n\\u00a0     }\\n\\u00a0 \\nAssert#define Assert(cond, exc)Definition exceptions.h:1638\\nmake_iterator_rangeIteratorRange< BaseIterator > make_iterator_range(const BaseIterator &begin, const std_cxx20::type_identity_t< BaseIterator > &end)Definition iterator_range.h:292\\nUtilities::MPI::scatterT scatter(const MPI_Comm comm, const std::vector< T > &objects_to_send, const unsigned int root_process=0)\\nStep 4: Fix up boundary states.\\nAs a last step in the Forward Euler method, we have to fix up all boundary states. As discussed in the intro we\\nadvance in time satisfying no boundary condition at all,\\nat the end of the time step enforce boundary conditions strongly in a post-processing step.\\n\\nHere, we compute the correction    \\n\\\\[\\n   \\\\mathbf{m}_i \\\\dealcoloneq \\\\mathbf{m}_i - (\\\\boldsymbol{\\\\nu}_i \\\\cdot\\n   \\\\mathbf{m}_i) \\\\boldsymbol{\\\\nu}_i,\\n   \\\\]\\n\\n which removes the normal component of \\\\(\\\\mathbf{m}\\\\).\\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope scope(computing_timer,\\n\\u00a0                                \\\"time_stepping - 4 fix boundary states\\\");\\n\\u00a0 \\n\\u00a0       for (const auto &it : boundary_normal_map)\\n\\u00a0         {\\n\\u00a0           const auto i = it.first;\\n\\u00a0 \\nWe only iterate over the locally owned subset:\\n\\u00a0           if (i >= n_locally_owned)\\n\\u00a0             continue;\\n\\u00a0 \\n\\u00a0           const auto &normal   = std::get<0>(it.second);\\n\\u00a0           const auto &id       = std::get<1>(it.second);\\n\\u00a0           const auto &position = std::get<2>(it.second);\\n\\u00a0 \\n\\u00a0           auto U_i = gather(temporary_vector, i);\\n\\u00a0 \\nOn free slip boundaries we remove the normal component of the momentum:\\n\\u00a0           if (id == Boundaries::free_slip)\\n\\u00a0             {\\n\\u00a0               auto m = ProblemDescription<dim>::momentum(U_i);\\n\\u00a0               m -= (m * normal) * normal;\\n\\u00a0               for (unsigned int k = 0; k < dim; ++k)\\n\\u00a0                 U_i[k + 1] = m[k];\\n\\u00a0             }\\n\\u00a0 \\nOn Dirichlet boundaries we enforce initial conditions strongly:\\n\\u00a0           else if (id == Boundaries::dirichlet)\\n\\u00a0             {\\n\\u00a0               U_i = initial_values->initial_state(position, t + tau_max);\\n\\u00a0             }\\n\\u00a0 \\n\\u00a0           scatter(temporary_vector, U_i, i);\\n\\u00a0         }\\n\\u00a0     }\\n\\u00a0 \\nStep 5: We now update the ghost layer over all MPI ranks, swap the temporary vector with the solution vector U (that will get returned by reference) and return the chosen time-step size \\\\(\\\\tau_{\\\\text{max}}\\\\):\\n\\u00a0     for (auto &it : temporary_vector)\\n\\u00a0       it.update_ghost_values();\\n\\u00a0 \\n\\u00a0     U.swap(temporary_vector);\\n\\u00a0 \\n\\u00a0     return tau_max;\\n\\u00a0   }\\n\\u00a0 \\n Schlieren postprocessing\\nAt various intervals we will output the current state U of the solution together with a so-called Schlieren plot. The constructor of the SchlierenPostprocessor class again contains no surprises. We simply supply default values to and register two parameters:\\nschlieren_beta: is an ad-hoc positive amplification factor in order to enhance the contrast in the visualization. Its actual value is a matter of taste.\\nschlieren_index: is an integer indicating which component of the state \\\\([\\\\rho, \\\\mathbf{m},E]\\\\) are we going to use in order to generate the visualization.\\n\\n\\u00a0   template <int dim>\\n\\u00a0   SchlierenPostprocessor<dim>::SchlierenPostprocessor(\\n\\u00a0     const MPI_Comm          mpi_communicator,\\n\\u00a0     TimerOutput            &computing_timer,\\n\\u00a0     const OfflineData<dim> &offline_data,\\n\\u00a0     const std::string      &subsection /*= \\\"SchlierenPostprocessor\\\"*/)\\n\\u00a0     : ParameterAcceptor(subsection)\\n\\u00a0     , mpi_communicator(mpi_communicator)\\n\\u00a0     , computing_timer(computing_timer)\\n\\u00a0     , offline_data(&offline_data)\\n\\u00a0   {\\n\\u00a0     schlieren_beta = 10.;\\n\\u00a0     add_parameter(\\\"schlieren beta\\\",\\n\\u00a0                   schlieren_beta,\\n\\u00a0                   \\\"Beta factor used in Schlieren-type postprocessor\\\");\\n\\u00a0 \\n\\u00a0     schlieren_index = 0;\\n\\u00a0     add_parameter(\\\"schlieren index\\\",\\n\\u00a0                   schlieren_index,\\n\\u00a0                   \\\"Use the corresponding component of the state vector for the \\\"\\n\\u00a0                   \\\"schlieren plot\\\");\\n\\u00a0   }\\n\\u00a0 \\nAgain, the prepare() function initializes two temporary the vectors (r and schlieren).\\n\\u00a0   template <int dim>\\n\\u00a0   void SchlierenPostprocessor<dim>::prepare()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope scope(computing_timer,\\n\\u00a0                              \\\"schlieren_postprocessor - prepare scratch space\\\");\\n\\u00a0 \\n\\u00a0     r.reinit(offline_data->n_locally_relevant);\\n\\u00a0     schlieren.reinit(offline_data->partitioner);\\n\\u00a0   }\\n\\u00a0 \\nWe now discuss the implementation of the class member SchlierenPostprocessor<dim>::compute_schlieren(), which basically takes a component of the state vector U and computes the Schlieren indicator for such component (the formula of the Schlieren indicator can be found just before the declaration of the class SchlierenPostprocessor). We start by noting that this formula requires the \\\"nodal gradients\\\" \\\\(\\\\nabla r_j\\\\). However, nodal values of gradients are not defined for \\\\(\\\\mathcal{C}^0\\\\) finite element functions. More generally, pointwise values of gradients are not defined for \\\\(W^{1,p}(\\\\Omega)\\\\) functions. The simplest technique we can use to recover gradients at nodes is weighted-averaging i.e.\\n\\n\\\\[ \\\\nabla r_j \\\\dealcoloneq \\\\frac{1}{\\\\int_{S_i} \\\\omega_i(\\\\mathbf{x}) \\\\,\\n   \\\\mathrm{d}\\\\mathbf{x}}\\n   \\\\int_{S_i} r_h(\\\\mathbf{x}) \\\\omega_i(\\\\mathbf{x}) \\\\, \\\\mathrm{d}\\\\mathbf{x}\\n   \\\\ \\\\ \\\\ \\\\ \\\\ \\\\mathbf{(*)} \\\\]\\n\\nwhere \\\\(S_i\\\\) is the support of the shape function \\\\(\\\\phi_i\\\\), and \\\\(\\\\omega_i(\\\\mathbf{x})\\\\) is the weight. The weight could be any positive function such as \\\\(\\\\omega_i(\\\\mathbf{x}) \\\\equiv 1\\\\) (that would allow us to recover the usual notion of mean value). But as usual, the goal is to reuse the off-line data as much as possible. In this sense, the most natural choice of weight is \\\\(\\\\omega_i = \\\\phi_i\\\\). Inserting this choice of weight and the expansion  \\\\(r_h(\\\\mathbf{x}) = \\\\sum_{j \\\\in \\\\mathcal{V}}\\n   r_j \\\\phi_j(\\\\mathbf{x})\\\\) into \\\\(\\\\mathbf{(*)}\\\\) we get :\\n\\n\\\\[ \\\\nabla r_j \\\\dealcoloneq \\\\frac{1}{m_i} \\\\sum_{j \\\\in \\\\mathcal{I}(i)} r_j\\n   \\\\mathbf{c}_{ij} \\\\ \\\\ \\\\ \\\\ \\\\ \\\\mathbf{(**)} \\\\, . \\\\]\\n\\nUsing this last formula we can recover averaged nodal gradients without resorting to any form of quadrature. This idea aligns quite well with the whole spirit of edge-based schemes (or algebraic schemes) where we want to operate on matrices and vectors as directly as it could be possible avoiding by all means assembly of bilinear forms, cell-loops, quadrature, or any other intermediate construct/operation between the input arguments (the state from the previous time-step) and the actual matrices and vectors required to compute the update.\\nThe second thing to note is that we have to compute global minimum and maximum \\\\(\\\\max_j |\\\\nabla r_j|\\\\) and \\\\(\\\\min_j |\\\\nabla r_j|\\\\). Following the same ideas used to compute the time step size in the class member TimeStepping<dim>::step() we define \\\\(\\\\max_j |\\\\nabla r_j|\\\\) and \\\\(\\\\min_j |\\\\nabla r_j|\\\\) as atomic doubles in order to resolve any conflicts between threads. As usual, we use Utilities::MPI::max() and Utilities::MPI::min() to find the global maximum/minimum among all MPI processes.\\nFinally, it is not possible to compute the Schlieren indicator in a single loop over all nodes. The entire operation requires two loops over nodes:\\n\\nThe first loop computes \\\\(|\\\\nabla r_i|\\\\) for all \\\\(i \\\\in \\\\mathcal{V}\\\\) in the mesh, and the bounds \\\\(\\\\max_j |\\\\nabla r_j|\\\\) and \\\\(\\\\min_j |\\\\nabla r_j|\\\\).\\nThe second loop finally computes the Schlieren indicator using the formula\\n\\n\\n\\\\[ \\\\text{schlieren}[i] = e^{\\\\beta \\\\frac{ |\\\\nabla r_i|\\n   - \\\\min_j |\\\\nabla r_j| }{\\\\max_j |\\\\nabla r_j| - \\\\min_j |\\\\nabla r_j| } }\\n   \\\\, . \\\\]\\n\\nThis means that we will have to define two workers on_subranges for each one of these stages.\\n\\u00a0   template <int dim>\\n\\u00a0   void SchlierenPostprocessor<dim>::compute_schlieren(const vector_type &U)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope scope(\\n\\u00a0       computing_timer, \\\"schlieren_postprocessor - compute schlieren plot\\\");\\n\\u00a0 \\n\\u00a0     const auto &sparsity            = offline_data->sparsity_pattern;\\n\\u00a0     const auto &lumped_mass_matrix  = offline_data->lumped_mass_matrix;\\n\\u00a0     const auto &cij_matrix          = offline_data->cij_matrix;\\n\\u00a0     const auto &boundary_normal_map = offline_data->boundary_normal_map;\\n\\u00a0     const auto &n_locally_owned     = offline_data->n_locally_owned;\\n\\u00a0 \\n\\u00a0     const auto indices =\\n\\u00a0       std_cxx20::ranges::iota_view<unsigned int, unsigned int>(0,\\n\\u00a0                                                                n_locally_owned);\\n\\u00a0 \\nWe define the r_i_max and r_i_min in the current MPI process as atomic doubles in order to avoid race conditions between threads:\\n\\u00a0     std::atomic<double> r_i_max{0.};\\n\\u00a0     std::atomic<double> r_i_min{std::numeric_limits<double>::infinity()};\\n\\u00a0 \\nFirst loop: compute the averaged gradient at each node and the global maxima and minima of the gradients.\\n\\u00a0     {\\n\\u00a0       const auto on_subranges = \\n\\u00a0         [&](const auto index_begin, const auto index_end) {\\n\\u00a0           double r_i_max_on_subrange = 0.;\\n\\u00a0           double r_i_min_on_subrange = std::numeric_limits<double>::infinity();\\n\\u00a0 \\n\\u00a0           for (const auto i :\\n\\u00a0                boost::make_iterator_range(index_begin, index_end))\\n\\u00a0             {\\n\\u00a0               Assert(i < n_locally_owned, ExcInternalError());\\n\\u00a0 \\n\\u00a0               Tensor<1, dim> r_i;\\n\\u00a0 \\n\\u00a0               for (auto jt = sparsity.begin(i); jt != sparsity.end(i); ++jt)\\n\\u00a0                 {\\n\\u00a0                   const auto j = jt->column();\\n\\u00a0 \\n\\u00a0                   if (i == j)\\n\\u00a0                     continue;\\n\\u00a0 \\n\\u00a0                   const auto U_js = U[schlieren_index].local_element(j);\\n\\u00a0                   const auto c_ij = gather_get_entry(cij_matrix, jt);\\n\\u00a0                   r_i += c_ij * U_js;\\n\\u00a0                 }\\n\\u00a0 \\nWe fix up the gradient r_i at free slip boundaries similarly to how we fixed up boundary states in the forward Euler step. This avoids sharp, artificial gradients in the Schlieren plot at free slip boundaries and is a purely cosmetic choice.\\n\\u00a0               const auto bnm_it = boundary_normal_map.find(i);\\n\\u00a0               if (bnm_it != boundary_normal_map.end())\\n\\u00a0                 {\\n\\u00a0                   const auto &normal = std::get<0>(bnm_it->second);\\n\\u00a0                   const auto &id     = std::get<1>(bnm_it->second);\\n\\u00a0 \\n\\u00a0                   if (id == Boundaries::free_slip)\\n\\u00a0                     r_i -= 1. * (r_i * normal) * normal;\\n\\u00a0                   else\\n\\u00a0                     r_i = 0.;\\n\\u00a0                 }\\n\\u00a0 \\nWe remind the reader that we are not interested in the nodal gradients per se. We only want their norms in order to compute the Schlieren indicator (weighted with the lumped mass matrix \\\\(m_i\\\\)):\\n\\u00a0               const double m_i    = lumped_mass_matrix.diag_element(i);\\n\\u00a0               r[i]                = r_i.norm() / m_i;\\n\\u00a0               r_i_max_on_subrange = std::max(r_i_max_on_subrange, r[i]);\\n\\u00a0               r_i_min_on_subrange = std::min(r_i_min_on_subrange, r[i]);\\n\\u00a0             }\\n\\u00a0 \\nWe compare the current_r_i_max and current_r_i_min (in the current subrange) with r_i_max and r_i_min (for the current MPI process) and update them if necessary:\\n\\u00a0           double current_r_i_max = r_i_max.load();\\n\\u00a0           while (current_r_i_max < r_i_max_on_subrange &&\\n\\u00a0                  !r_i_max.compare_exchange_weak(current_r_i_max,\\n\\u00a0                                                 r_i_max_on_subrange))\\n\\u00a0             ;\\n\\u00a0 \\n\\u00a0           double current_r_i_min = r_i_min.load();\\n\\u00a0           while (current_r_i_min > r_i_min_on_subrange &&\\n\\u00a0                  !r_i_min.compare_exchange_weak(current_r_i_min,\\n\\u00a0                                                 r_i_min_on_subrange))\\n\\u00a0             ;\\n\\u00a0         };\\n\\u00a0 \\n\\u00a0       parallel::apply_to_subranges(indices.begin(),\\n\\u00a0                                    indices.end(),\\n\\u00a0                                    on_subranges,\\n\\u00a0                                    4096);\\n\\u00a0     }\\n\\u00a0 \\nAnd synchronize r_i_max and r_i_min over all MPI processes.\\n\\u00a0     r_i_max.store(Utilities::MPI::max(r_i_max.load(), mpi_communicator));\\n\\u00a0     r_i_min.store(Utilities::MPI::min(r_i_min.load(), mpi_communicator));\\n\\u00a0 \\nUtilities::MPI::maxT max(const T &t, const MPI_Comm mpi_communicator)\\nSecond loop: we now have the vector r and the scalars r_i_max and r_i_min at our disposal. We are thus in a position to actually compute the Schlieren indicator.\\n\\u00a0     {\\n\\u00a0       const auto on_subranges = \\n\\u00a0         [&](const auto index_begin, const auto index_end) {\\n\\u00a0           for (const auto i :\\n\\u00a0                boost::make_iterator_range(index_begin, index_end))\\n\\u00a0             {\\n\\u00a0               Assert(i < n_locally_owned, ExcInternalError());\\n\\u00a0 \\n\\u00a0               schlieren.local_element(i) =\\n\\u00a0                 1. - std::exp(-schlieren_beta * (r[i] - r_i_min) /\\n\\u00a0                               (r_i_max - r_i_min));\\n\\u00a0             }\\n\\u00a0         };\\n\\u00a0 \\n\\u00a0       parallel::apply_to_subranges(indices.begin(),\\n\\u00a0                                    indices.end(),\\n\\u00a0                                    on_subranges,\\n\\u00a0                                    4096);\\n\\u00a0     }\\n\\u00a0 \\nstd::exp::VectorizedArray< Number, width > exp(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6829\\nAnd finally, exchange ghost elements.\\n\\u00a0     schlieren.update_ghost_values();\\n\\u00a0   }\\n\\u00a0 \\n The main loop\\nWith all classes implemented it is time to create an instance of Discretization<dim>, OfflineData<dim>, InitialValues<dim>, TimeStepping<dim>, and SchlierenPostprocessor<dim>, and run the forward Euler step in a loop.\\nIn the constructor of MainLoop<dim> we now initialize an instance of all classes, and declare a number of parameters controlling output. Most notable, we declare a boolean parameter resume that will control whether the program attempts to restart from an interrupted computation, or not.\\n\\u00a0   template <int dim>\\n\\u00a0   MainLoop<dim>::MainLoop(const MPI_Comm mpi_communicator)\\n\\u00a0     : ParameterAcceptor(\\\"A - MainLoop\\\")\\n\\u00a0     , mpi_communicator(mpi_communicator)\\n\\u00a0     , computing_timer(mpi_communicator,\\n\\u00a0                       timer_output,\\n\\u00a0                       TimerOutput::never,\\n\\u00a0                       TimerOutput::cpu_and_wall_times)\\n\\u00a0     , pcout(std::cout, Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n\\u00a0     , discretization(mpi_communicator, computing_timer, \\\"B - Discretization\\\")\\n\\u00a0     , offline_data(mpi_communicator,\\n\\u00a0                    computing_timer,\\n\\u00a0                    discretization,\\n\\u00a0                    \\\"C - OfflineData\\\")\\n\\u00a0     , initial_values(\\\"D - InitialValues\\\")\\n\\u00a0     , time_stepping(mpi_communicator,\\n\\u00a0                     computing_timer,\\n\\u00a0                     offline_data,\\n\\u00a0                     initial_values,\\n\\u00a0                     \\\"E - TimeStepping\\\")\\n\\u00a0     , schlieren_postprocessor(mpi_communicator,\\n\\u00a0                               computing_timer,\\n\\u00a0                               offline_data,\\n\\u00a0                               \\\"F - SchlierenPostprocessor\\\")\\n\\u00a0   {\\n\\u00a0     base_name = \\\"test\\\";\\n\\u00a0     add_parameter(\\\"basename\\\", base_name, \\\"Base name for all output files\\\");\\n\\u00a0 \\n\\u00a0     t_final = 4.;\\n\\u00a0     add_parameter(\\\"final time\\\", t_final, \\\"Final time\\\");\\n\\u00a0 \\n\\u00a0     output_granularity = 0.02;\\n\\u00a0     add_parameter(\\\"output granularity\\\",\\n\\u00a0                   output_granularity,\\n\\u00a0                   \\\"time interval for output\\\");\\n\\u00a0 \\n\\u00a0     asynchronous_writeback = true;\\n\\u00a0     add_parameter(\\\"asynchronous writeback\\\",\\n\\u00a0                   asynchronous_writeback,\\n\\u00a0                   \\\"Write out solution in a background thread performing IO\\\");\\n\\u00a0 \\n\\u00a0     resume = false;\\n\\u00a0     add_parameter(\\\"resume\\\", resume, \\\"Resume an interrupted computation.\\\");\\n\\u00a0   }\\n\\u00a0 \\nInitializeLibrary::MPI@ MPI\\nUtilitiesDefinition communication_pattern_base.h:30\\nWe start by implementing a helper function print_head() in an anonymous namespace that is used to output messages in the terminal with some nice formatting.\\n\\u00a0   namespace\\n\\u00a0   {\\n\\u00a0     void print_head(ConditionalOStream &pcout,\\n\\u00a0                     const std::string  &header,\\n\\u00a0                     const std::string  &secondary = \\\"\\\")\\n\\u00a0     {\\n\\u00a0       const auto header_size   = header.size();\\n\\u00a0       const auto padded_header = std::string((34 - header_size) / 2, ' ') +\\n\\u00a0                                  header +\\n\\u00a0                                  std::string((35 - header_size) / 2, ' ');\\n\\u00a0 \\n\\u00a0       const auto secondary_size = secondary.size();\\n\\u00a0       const auto padded_secondary =\\n\\u00a0         std::string((34 - secondary_size) / 2, ' ') + secondary +\\n\\u00a0         std::string((35 - secondary_size) / 2, ' ');\\n\\u00a0 \\n\\u00a0       /* clang-format off */\\n\\u00a0       pcout << std::endl;\\n\\u00a0       pcout << \\\"    ####################################################\\\" << std::endl;\\n\\u00a0       pcout << \\\"    #########                                  #########\\\" << std::endl;\\n\\u00a0       pcout << \\\"    #########\\\"     <<  padded_header   <<     \\\"#########\\\" << std::endl;\\n\\u00a0       pcout << \\\"    #########\\\"     << padded_secondary <<     \\\"#########\\\" << std::endl;\\n\\u00a0       pcout << \\\"    #########                                  #########\\\" << std::endl;\\n\\u00a0       pcout << \\\"    ####################################################\\\" << std::endl;\\n\\u00a0       pcout << std::endl;\\n\\u00a0       /* clang-format on */\\n\\u00a0     }\\n\\u00a0   } // namespace\\n\\u00a0 \\nWith print_head in place it is now time to implement the MainLoop<dim>::run() that contains the main loop of our program.\\n\\u00a0   template <int dim>\\n\\u00a0   void MainLoop<dim>::run()\\n\\u00a0   {\\nWe start by reading in parameters and initializing all objects. We note here that the call to ParameterAcceptor::initialize reads in all parameters from the parameter file (whose name is given as a string argument). ParameterAcceptor handles a global ParameterHandler that is initialized with subsections and parameter declarations for all class instances that are derived from ParameterAceptor. The call to initialize enters the subsection for each each derived class, and sets all variables that were added using ParameterAcceptor::add_parameter()\\n\\u00a0     pcout << \\\"Reading parameters and allocating objects... \\\" << std::flush;\\n\\u00a0 \\n\\u00a0     ParameterAcceptor::initialize(\\\"step-69.prm\\\");\\n\\u00a0     pcout << \\\"done\\\" << std::endl;\\n\\u00a0 \\nParameterAcceptor::initializestatic void initialize(const std::string &filename=\\\"\\\", const std::string &output_filename=\\\"\\\", const ParameterHandler::OutputStyle output_style_for_output_filename=ParameterHandler::Short, ParameterHandler &prm=ParameterAcceptor::prm, const ParameterHandler::OutputStyle output_style_for_filename=ParameterHandler::DefaultStyle)Definition parameter_acceptor.cc:80\\nNext we create the triangulation, assemble all matrices, set up scratch space, and initialize the DataOut<dim> object. All of these operations are pretty standard and discussed in detail in the Discretization and OfflineData classes.\\n\\u00a0     {\\n\\u00a0       print_head(pcout, \\\"create triangulation\\\");\\n\\u00a0 \\n\\u00a0       discretization.setup();\\n\\u00a0 \\n\\u00a0       if (resume)\\n\\u00a0         discretization.triangulation.load(base_name + \\\"-checkpoint.mesh\\\");\\n\\u00a0       else\\n\\u00a0         discretization.triangulation.refine_global(discretization.refinement);\\n\\u00a0 \\n\\u00a0       pcout << \\\"Number of active cells:       \\\"\\n\\u00a0             << discretization.triangulation.n_global_active_cells()\\n\\u00a0             << std::endl;\\n\\u00a0 \\n\\u00a0       print_head(pcout, \\\"compute offline data\\\");\\n\\u00a0       offline_data.setup();\\n\\u00a0       offline_data.assemble();\\n\\u00a0 \\n\\u00a0       pcout << \\\"Number of degrees of freedom: \\\"\\n\\u00a0             << offline_data.dof_handler.n_dofs() << std::endl;\\n\\u00a0 \\n\\u00a0       print_head(pcout, \\\"set up time step\\\");\\n\\u00a0       time_stepping.prepare();\\n\\u00a0       schlieren_postprocessor.prepare();\\n\\u00a0     }\\n\\u00a0 \\nWe will store the current time and state in the variable t and vector U:\\n\\u00a0     double       t            = 0.;\\n\\u00a0     unsigned int output_cycle = 0;\\n\\u00a0 \\n\\u00a0     vector_type U;\\n\\u00a0     for (auto &it : U)\\n\\u00a0       it.reinit(offline_data.partitioner);\\n\\u00a0 \\n Resume\\nBy default the boolean resume is set to false, i.e. the following code snippet is not run. However, if resume==true we indicate that we have indeed an interrupted computation and the program shall restart by reading in an old state consisting of t, output_cycle, and the state vector U from checkpoint files.\\nA this point we have already read in the stored refinement history of our parallel distributed mesh. What is missing are the actual state vector U, the time and output cycle. We use the SolutionTransfer class in combination with the distributed::Triangulation::load() / distributed::Triangulation::save() mechanism to read in the state vector. A separate boost::archive is used to retrieve t and output_cycle. The checkpoint files will be created in the output() routine discussed below.\\n\\u00a0     if (resume)\\n\\u00a0       {\\n\\u00a0         print_head(pcout, \\\"resume interrupted computation\\\");\\n\\u00a0 \\n\\u00a0         parallel::distributed::\\n\\u00a0           SolutionTransfer<dim, LinearAlgebra::distributed::Vector<double>>\\n\\u00a0             solution_transfer(offline_data.dof_handler);\\n\\u00a0 \\n\\u00a0         std::vector<LinearAlgebra::distributed::Vector<double> *> vectors;\\n\\u00a0         std::transform(U.begin(),\\n\\u00a0                        U.end(),\\n\\u00a0                        std::back_inserter(vectors),\\n\\u00a0                        [](auto &it) { return &it; });\\n\\u00a0         solution_transfer.deserialize(vectors);\\n\\u00a0 \\n\\u00a0         for (auto &it : U)\\n\\u00a0           it.update_ghost_values();\\n\\u00a0 \\n\\u00a0         std::ifstream file(base_name + \\\"-checkpoint.metadata\\\",\\n\\u00a0                            std::ios::binary);\\n\\u00a0 \\n\\u00a0         boost::archive::binary_iarchive ia(file);\\n\\u00a0         ia >> t >> output_cycle;\\n\\u00a0       }\\n\\u00a0     else\\n\\u00a0       {\\n\\u00a0         print_head(pcout, \\\"interpolate initial values\\\");\\n\\u00a0         U = interpolate_initial_values();\\n\\u00a0       }\\n\\u00a0 \\nSolutionTransferDefinition solution_transfer.h:337\\nWith either the initial state set up, or an interrupted state restored it is time to enter the main loop:\\n\\u00a0     output(U, base_name, t, output_cycle++);\\n\\u00a0 \\n\\u00a0     print_head(pcout, \\\"enter main loop\\\");\\n\\u00a0 \\n\\u00a0     unsigned int timestep_number = 1;\\n\\u00a0     while (t < t_final)\\n\\u00a0       {\\nWe first print an informative status message\\n\\u00a0         std::ostringstream head;\\n\\u00a0         std::ostringstream secondary;\\n\\u00a0 \\n\\u00a0         head << \\\"Cycle  \\\" << Utilities::int_to_string(timestep_number, 6)\\n\\u00a0              << \\\"  (\\\"                                                   \\n\\u00a0              << std::fixed << std::setprecision(1) << t / t_final * 100 \\n\\u00a0              << \\\"%)\\\";\\n\\u00a0         secondary << \\\"at time t = \\\" << std::setprecision(8) << std::fixed << t;\\n\\u00a0 \\n\\u00a0         print_head(pcout, head.str(), secondary.str());\\n\\u00a0 \\nUtilities::int_to_stringstd::string int_to_string(const unsigned int value, const unsigned int digits=numbers::invalid_unsigned_int)Definition utilities.cc:470\\nand then perform a single forward Euler step. Note that the state vector U is updated in place and that time_stepping.make_one_step() returns the chosen step size.\\n\\u00a0         t += time_stepping.make_one_step(U, t);\\n\\u00a0 \\nPost processing, generating output and writing out the current state is a CPU and IO intensive task that we cannot afford to do every time step - in particular with explicit time stepping. We thus only schedule output by calling the output() function if we are past a threshold set by output_granularity.\\n\\u00a0         if (t > output_cycle * output_granularity)\\n\\u00a0           {\\n\\u00a0             checkpoint(U, base_name, t, output_cycle);\\n\\u00a0             output(U, base_name, t, output_cycle);\\n\\u00a0             ++output_cycle;\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         ++timestep_number;\\n\\u00a0       }\\n\\u00a0 \\nWe wait for any remaining background output thread to finish before printing a summary and exiting.\\n\\u00a0     if (background_thread_state.valid())\\n\\u00a0       background_thread_state.wait();\\n\\u00a0 \\n\\u00a0     computing_timer.print_summary();\\n\\u00a0     pcout << timer_output.str() << std::endl;\\n\\u00a0   }\\n\\u00a0 \\nThe interpolate_initial_values takes an initial time \\\"t\\\" as input argument and populates a state vector U with the help of the InitialValues<dim>::initial_state object.\\n\\u00a0   template <int dim>\\n\\u00a0   typename MainLoop<dim>::vector_type\\n\\u00a0   MainLoop<dim>::interpolate_initial_values(const double t)\\n\\u00a0   {\\n\\u00a0     pcout << \\\"MainLoop<dim>::interpolate_initial_values(t = \\\" << t << ')'\\n\\u00a0           << std::endl;\\n\\u00a0     TimerOutput::Scope scope(computing_timer,\\n\\u00a0                              \\\"main_loop - setup scratch space\\\");\\n\\u00a0 \\n\\u00a0     vector_type U;\\n\\u00a0 \\n\\u00a0     for (auto &it : U)\\n\\u00a0       it.reinit(offline_data.partitioner);\\n\\u00a0 \\n\\u00a0     constexpr auto n_solution_variables =\\n\\u00a0       ProblemDescription<dim>::n_solution_variables;\\n\\u00a0 \\nThe function signature of InitialValues<dim>::initial_state is not quite right for VectorTools::interpolate(). We work around this issue by, first, creating a lambda function that for a given position x returns just the value of the ith component. This lambda in turn is converted to a Function<dim> object with the help of the ScalarFunctionFromFunctionObject wrapper.\\n\\u00a0     for (unsigned int i = 0; i < n_solution_variables; ++i)\\n\\u00a0       VectorTools::interpolate(offline_data.dof_handler,\\n\\u00a0                                ScalarFunctionFromFunctionObject<dim, double>(\\n\\u00a0                                  [&](const Point<dim> &x) {\\n\\u00a0                                    return initial_values.initial_state(x, t)[i];\\n\\u00a0                                  }),\\n\\u00a0                                U[i]);\\n\\u00a0 \\n\\u00a0     for (auto &it : U)\\n\\u00a0       it.update_ghost_values();\\n\\u00a0 \\n\\u00a0     return U;\\n\\u00a0   }\\n\\u00a0 \\nScalarFunctionFromFunctionObjectDefinition function.h:803\\nVectorTools::interpolatevoid interpolate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Function< spacedim, typename VectorType::value_type > &function, VectorType &vec, const ComponentMask &component_mask={})\\n Output and checkpointing\\nWe checkpoint the current state by doing the precise inverse operation to what we discussed for the resume logic:\\n\\u00a0   template <int dim>\\n\\u00a0   void MainLoop<dim>::checkpoint(const typename MainLoop<dim>::vector_type &U,\\n\\u00a0                                  const std::string &name,\\n\\u00a0                                  const double       t,\\n\\u00a0                                  const unsigned int cycle)\\n\\u00a0   {\\n\\u00a0     print_head(pcout, \\\"checkpoint computation\\\");\\n\\u00a0 \\n\\u00a0     parallel::distributed::\\n\\u00a0       SolutionTransfer<dim, LinearAlgebra::distributed::Vector<double>>\\n\\u00a0         solution_transfer(offline_data.dof_handler);\\n\\u00a0 \\n\\u00a0     std::vector<const LinearAlgebra::distributed::Vector<double> *> vectors;\\n\\u00a0     std::transform(U.begin(),\\n\\u00a0                    U.end(),\\n\\u00a0                    std::back_inserter(vectors),\\n\\u00a0                    [](auto &it) { return &it; });\\n\\u00a0 \\n\\u00a0     solution_transfer.prepare_for_serialization(vectors);\\n\\u00a0 \\n\\u00a0     discretization.triangulation.save(name + \\\"-checkpoint.mesh\\\");\\n\\u00a0 \\n\\u00a0     if (Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n\\u00a0       {\\n\\u00a0         std::ofstream file(name + \\\"-checkpoint.metadata\\\", std::ios::binary);\\n\\u00a0         boost::archive::binary_oarchive oa(file);\\n\\u00a0         oa << t << cycle;\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\nUtilities::MPI::this_mpi_processunsigned int this_mpi_process(const MPI_Comm mpi_communicator)Definition mpi.cc:107\\nWriting out the final vtk files is quite an IO intensive task that can stall the main loop for a while. In order to avoid this we use an asynchronous IO strategy by creating a background thread that will perform IO while the main loop is allowed to continue. In order for this to work we have to be mindful of two things:\\nBefore running the output_worker thread, we have to create a copy of the state vector U. We store it in the vector output_vector.\\nWe have to avoid any MPI communication in the background thread, otherwise the program might deadlock. This implies that we have to run the postprocessing outside of the worker thread.\\n\\n\\u00a0   template <int dim>\\n\\u00a0   void MainLoop<dim>::output(const typename MainLoop<dim>::vector_type &U,\\n\\u00a0                              const std::string                         &name,\\n\\u00a0                              const double                               t,\\n\\u00a0                              const unsigned int                         cycle)\\n\\u00a0   {\\n\\u00a0     pcout << \\\"MainLoop<dim>::output(t = \\\" << t << ')' << std::endl;\\n\\u00a0 \\nIf the asynchronous writeback option is set we launch a background thread performing all the slow IO to disc. In that case we have to make sure that the background thread actually finished running. If not, we have to wait to for it to finish. We launch said background thread with std::async() that returns a std::future object. This std::future object contains the return value of the function, which is in our case simply void.\\n\\u00a0     if (background_thread_state.valid())\\n\\u00a0       {\\n\\u00a0         TimerOutput::Scope timer(computing_timer, \\\"main_loop - stalled output\\\");\\n\\u00a0         background_thread_state.wait();\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     constexpr auto n_solution_variables =\\n\\u00a0       ProblemDescription<dim>::n_solution_variables;\\n\\u00a0 \\nAt this point we make a copy of the state vector, run the schlieren postprocessor, and run DataOut<dim>::build_patches() The actual output code is standard: We create a DataOut instance, attach all data vectors we want to output and call DataOut<dim>::build_patches(). There is one twist, however. In order to perform asynchronous IO on a background thread we create the DataOut<dim> object as a shared pointer that we pass on to the worker thread to ensure that once we exit this function and the worker thread finishes the DataOut<dim> object gets destroyed again.\\n\\u00a0     for (unsigned int i = 0; i < n_solution_variables; ++i)\\n\\u00a0       {\\n\\u00a0         output_vector[i] = U[i];\\n\\u00a0         output_vector[i].update_ghost_values();\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     schlieren_postprocessor.compute_schlieren(output_vector);\\n\\u00a0 \\n\\u00a0     std::unique_ptr<DataOut<dim>> data_out = std::make_unique<DataOut<dim>>();\\n\\u00a0     data_out->attach_dof_handler(offline_data.dof_handler);\\n\\u00a0 \\n\\u00a0     for (unsigned int i = 0; i < n_solution_variables; ++i)\\n\\u00a0       data_out->add_data_vector(output_vector[i],\\n\\u00a0                                 ProblemDescription<dim>::component_names[i]);\\n\\u00a0 \\n\\u00a0     data_out->add_data_vector(schlieren_postprocessor.schlieren,\\n\\u00a0                               \\\"schlieren_plot\\\");\\n\\u00a0 \\n\\u00a0     data_out->build_patches(discretization.mapping,\\n\\u00a0                             discretization.finite_element.degree - 1);\\n\\u00a0 \\nNext we create a lambda function for the background thread. We capture the this pointer as well as most of the arguments of the output function by value so that we have access to them inside the lambda function.\\nThe first capture argument of the lambda function, data_out_copy in essence creates a local variable inside the lambda function into which we \\\"move\\\" the data_out variable from above. The way this works is that we create a std::unique_ptr above that points to the DataOut object. But we have no use for it any more after this point: We really just want to move ownership from the current function to the lambda function implemented in the following few lines. We could have done this by using a std::shared_ptr above and giving the lambda function a copy of that shared pointer; once the current function returns (but maybe while the lambda function is still running), our local shared pointer would go out of scope and stop pointing at the actual object, at which point the lambda function simply becomes the sole owner. But using the std::unique_ptr is conceptually cleaner as it makes it clear that the current function's data_out variable isn't even pointing to the object any more.\\n\\u00a0     auto output_worker =\\n\\u00a0       [data_out_copy = std::move(data_out), this, name, t, cycle]() {\\n\\u00a0         const DataOutBase::VtkFlags flags(\\n\\u00a0           t, cycle, true, DataOutBase::CompressionLevel::best_speed);\\n\\u00a0         data_out_copy->set_flags(flags);\\n\\u00a0 \\n\\u00a0         data_out_copy->write_vtu_with_pvtu_record(\\n\\u00a0           \\\"\\\", name + \\\"-solution\\\", cycle, mpi_communicator, 6);\\n\\u00a0       };\\n\\u00a0 \\nDataOutBase::CompressionLevel::best_speed@ best_speed\\nDataOutBase::VtkFlagsDefinition data_out_base.h:1127\\nIf the asynchronous writeback option is set we launch a new background thread with the help of std::async function. The function returns a std::future object that we can use to query the status of the background thread. At this point we can return from the output() function and resume with the time stepping in the main loop - the thread will run in the background.\\n\\u00a0     if (asynchronous_writeback)\\n\\u00a0       {\\n\\u00a0         background_thread_state =\\n\\u00a0           std::async(std::launch::async, std::move(output_worker));\\n\\u00a0       }\\n\\u00a0     else\\n\\u00a0       {\\n\\u00a0         output_worker();\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 } // namespace Step69\\n\\u00a0 \\nAnd finally, the main function.\\n\\u00a0 int main(int argc, char *argv[])\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       constexpr int dim = 2;\\n\\u00a0 \\n\\u00a0       using namespace dealii;\\n\\u00a0       using namespace Step69;\\n\\u00a0 \\n\\u00a0       Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv);\\n\\u00a0 \\n\\u00a0       MPI_Comm      mpi_communicator(MPI_COMM_WORLD);\\n\\u00a0       MainLoop<dim> main_loop(mpi_communicator);\\n\\u00a0 \\n\\u00a0       main_loop.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     };\\n\\u00a0 }\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\n Results\\nRunning the program with default parameters in release mode takes about 1 minute on a 4 core machine (with hyperthreading): # mpirun -np 4 ./step-69 | tee output\\nReading parameters and allocating objects... done\\n\\n    ####################################################\\n    #########                                  #########\\n    #########       create triangulation       #########\\n    #########                                  #########\\n    ####################################################\\n\\nNumber of active cells:       36864\\n\\n    ####################################################\\n    #########                                  #########\\n    #########       compute offline data       #########\\n    #########                                  #########\\n    ####################################################\\n\\nNumber of degrees of freedom: 37376\\n\\n    ####################################################\\n    #########                                  #########\\n    #########         set up time step         #########\\n    #########                                  #########\\n    ####################################################\\n\\n    ####################################################\\n    #########                                  #########\\n    #########    interpolate initial values    #########\\n    #########                                  #########\\n    #########                                  #########\\n    ####################################################\\n\\nTimeLoop<dim>::interpolate_initial_values(t = 0)\\nTimeLoop<dim>::output(t = 0, checkpoint = 0)\\n\\n    ####################################################\\n    #########                                  #########\\n    #########         enter main loop          #########\\n    #########                                  #########\\n    #########                                  #########\\n    ####################################################\\n\\n    ####################################################\\n    #########                                  #########\\n    #########      Cycle  000001  (0.0%)       #########\\n    #########      at time t = 0.00000000      #########\\n    #########                                  #########\\n    ####################################################\\n\\n[...]\\n\\n    ####################################################\\n    #########                                  #########\\n    #########     Cycle  007553  (100.0%)      #########\\n    #########      at time t = 3.99984036      #########\\n    #########                                  #########\\n    ####################################################\\n\\nTimeLoop<dim>::output(t = 4.00038, checkpoint = 1)\\n\\n+------------------------------------------------------------------------+------------+------------+\\n| Total CPU time elapsed since start                                     |       357s |            |\\n|                                                                        |            |            |\\n| Section                                                    | no. calls |  CPU time  | % of total |\\n+------------------------------------------------------------+-----------+------------+------------+\\n| discretization - setup                                     |         1 |     0.113s |         0% |\\n| offline_data - assemble lumped mass matrix, and c_ij       |         1 |     0.167s |         0% |\\n| offline_data - compute |c_ij|, and n_ij                    |         1 |   0.00255s |         0% |\\n| offline_data - create sparsity pattern and set up matrices |         1 |    0.0224s |         0% |\\n| offline_data - distribute dofs                             |         1 |    0.0617s |         0% |\\n| offline_data - fix slip boundary c_ij                      |         1 |    0.0329s |         0% |\\n| schlieren_postprocessor - compute schlieren plot           |       201 |     0.811s |      0.23% |\\n| schlieren_postprocessor - prepare scratch space            |         1 |   7.6e-05s |         0% |\\n| time_loop - setup scratch space                            |         1 |     0.127s |         0% |\\n| time_loop - stalled output                                 |       200 |  0.000685s |         0% |\\n| time_step - 1 compute d_ij                                 |      7553 |       240s |        67% |\\n| time_step - 2 compute d_ii, and tau_max                    |      7553 |      11.5s |       3.2% |\\n| time_step - 3 perform update                               |      7553 |       101s |        28% |\\n| time_step - 4 fix boundary states                          |      7553 |     0.724s |       0.2% |\\n| time_step - prepare scratch space                          |         1 |   0.00245s |         0% |\\n+------------------------------------------------------------+-----------+------------+------------+\\nOne thing that becomes evident is the fact that the program spends two thirds of the execution time computing the graph viscosity d_ij and about a third of the execution time in performing the update, where computing the flux \\\\(f(U)\\\\) is the expensive operation. The preset default resolution is about 37k gridpoints, which amounts to about 148k spatial degrees of freedom in 2D. An animated schlieren plot of the solution looks as follows:\\n\\nIt is evident that 37k gridpoints for the first-order method is nowhere near the resolution needed to resolve any flow features. For comparison, here is a \\\"reference\\\" computation with a second-order method and about 9.5M gridpoints (github project page):\\n\\nSo, we give the first-order method a second chance and run it with about 2.4M gridpoints on a small compute server:\\n# mpirun -np 16 ./step-69 | tee output\\n\\n[...]\\n\\n    ####################################################\\n    #########                                  #########\\n    #########     Cycle  070216  (100.0%)      #########\\n    #########      at time t = 3.99999231      #########\\n    #########                                  #########\\n    ####################################################\\n\\nTimeLoop<dim>::output(t = 4.00006, checkpoint = 1)\\n\\n[...]\\n\\n+------------------------------------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start                               |  6.75e+03s |            |\\n|                                                                        |            |            |\\n| Section                                                    | no. calls |  wall time | % of total |\\n+------------------------------------------------------------+-----------+------------+------------+\\n| discretization - setup                                     |         1 |      1.97s |         0% |\\n| offline_data - assemble lumped mass matrix, and c_ij       |         1 |      1.19s |         0% |\\n| offline_data - compute |c_ij|, and n_ij                    |         1 |    0.0172s |         0% |\\n| offline_data - create sparsity pattern and set up matrices |         1 |     0.413s |         0% |\\n| offline_data - distribute dofs                             |         1 |      1.05s |         0% |\\n| offline_data - fix slip boundary c_ij                      |         1 |     0.252s |         0% |\\n| schlieren_postprocessor - compute schlieren plot           |       201 |      1.82s |         0% |\\n| schlieren_postprocessor - prepare scratch space            |         1 |  0.000497s |         0% |\\n| time_loop - setup scratch space                            |         1 |      1.45s |         0% |\\n| time_loop - stalled output                                 |       200 |   0.00342s |         0% |\\n| time_step - 1 compute d_ij                                 |     70216 |  4.38e+03s |        65% |\\n| time_step - 2 compute d_ii, and tau_max                    |     70216 |       419s |       6.2% |\\n| time_step - 3 perform update                               |     70216 |  1.87e+03s |        28% |\\n| time_step - 4 fix boundary states                          |     70216 |        24s |      0.36% |\\n| time_step - prepare scratch space                          |         1 |    0.0227s |         0% |\\n+------------------------------------------------------------+-----------+------------+------------+\\nAnd with the following result:\\n\\nThat's substantially better, although of course at the price of having run the code for roughly 2 hours on 16 cores.\\n Possibilities for extensions\\nThe program showcased here is really only first-order accurate, as discussed above. The pictures above illustrate how much diffusion that introduces and how far the solution is from one that actually resolves the features we care about.\\nThis can be fixed, but it would exceed what a tutorial is about. Nevertheless, it is worth showing what one can achieve by adding a second-order scheme. For example, here is a video computed with the following research code that shows (with a different color scheme) a 2d simulation that corresponds to the cases shown above:\\n\\n\\n\\n\\n\\nThis simulation was done with 38 million degrees of freedom (continuous \\\\(Q_1\\\\) finite elements) per component of the solution vector. The exquisite detail of the solution is remarkable for these kinds of simulations, including in the sub-sonic region behind the obstacle.\\nOne can also with relative ease further extend this to the 3d case:\\n\\n\\n\\n\\n\\nSolving this becomes expensive, however: The simulation was done with 1,817 million degrees of freedom (continuous \\\\(Q_1\\\\) finite elements) per component (for a total of 9.09 billion spatial degrees of freedom) and ran on 30,720 MPI ranks. The code achieved an average throughput of 969M grid points per second (0.04M gridpoints per second per CPU). The front and back wall show a \\\"Schlieren plot\\\": the magnitude of the gradient of the density on an exponential scale from white (low) to black (high). All other cutplanes and the surface of the obstacle show the magnitude of the vorticity on a white (low) - yellow (medium) - red (high) scale. (The scales of the individual cutplanes have been adjusted for a nicer visualization.)\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2020 - 2023 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Authors: Matthias Maier, Texas A&M University;\\n *          Ignacio Tomas, Texas A&M University, Sandia National Laboratories\\n *\\n * Sandia National Laboratories is a multimission laboratory managed and\\n * operated by National Technology & Engineering Solutions of Sandia, LLC, a\\n * wholly owned subsidiary of Honeywell International Inc., for the U.S.\\n * Department of Energy's National Nuclear Security Administration under\\n * contract DE-NA0003525. This document describes objective technical results\\n * and analysis. Any subjective views or opinions that might be expressed in\\n * the paper do not necessarily represent the views of the U.S. Department of\\n * Energy or the United States Government.\\n */\\n \\n \\n#include <deal.II/base/conditional_ostream.h>\\n#include <deal.II/base/parallel.h>\\n#include <deal.II/base/parameter_acceptor.h>\\n#include <deal.II/base/partitioner.h>\\n#include <deal.II/base/quadrature.h>\\n#include <deal.II/base/timer.h>\\n#include <deal.II/base/work_stream.h>\\n \\n#include <deal.II/distributed/solution_transfer.h>\\n#include <deal.II/distributed/tria.h>\\n \\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/dofs/dof_renumbering.h>\\n#include <deal.II/dofs/dof_tools.h>\\n \\n#include <deal.II/fe/fe.h>\\n#include <deal.II/fe/fe_q.h>\\n#include <deal.II/fe/fe_values.h>\\n#include <deal.II/fe/mapping.h>\\n#include <deal.II/fe/mapping_q.h>\\n \\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/manifold_lib.h>\\n \\n#include <deal.II/lac/dynamic_sparsity_pattern.h>\\n#include <deal.II/lac/la_parallel_vector.h>\\n#include <deal.II/lac/sparse_matrix.h>\\n#include <deal.II/lac/sparse_matrix.templates.h>\\n#include <deal.II/lac/vector.h>\\n \\n#include <deal.II/meshworker/scratch_data.h>\\n \\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/vector_tools.h>\\n \\n#include <boost/archive/binary_iarchive.hpp>\\n#include <boost/archive/binary_oarchive.hpp>\\n \\n#include <deal.II/base/std_cxx20/iota_view.h>\\n#include <boost/range/iterator_range.hpp>\\n \\n#include <cmath>\\n#include <fstream>\\n#include <future>\\n \\n \\nnamespace Step69\\n{\\n using namespace dealii;\\n \\n \\n namespace Boundaries\\n  {\\n constexpr types::boundary_id do_nothing = 0;\\n constexpr types::boundary_id free_slip  = 1;\\n constexpr types::boundary_id dirichlet  = 2;\\n  } // namespace Boundaries\\n \\n template <int dim>\\n class Discretization : public ParameterAcceptor\\n  {\\n public:\\n    Discretization(const MPI_Comm     mpi_communicator,\\n TimerOutput       &computing_timer,\\n const std::string &subsection = \\\"Discretization\\\");\\n \\n void setup();\\n \\n const MPI_Comm mpi_communicator;\\n \\n parallel::distributed::Triangulation<dim> triangulation;\\n \\n const MappingQ<dim>   mapping;\\n const FE_Q<dim>       finite_element;\\n const QGauss<dim>     quadrature;\\n const QGauss<dim - 1> face_quadrature;\\n \\n unsigned int refinement;\\n \\n private:\\n TimerOutput &computing_timer;\\n \\n double length;\\n double height;\\n double disk_position;\\n double disk_diameter;\\n  };\\n \\n template <int dim>\\n class OfflineData : public ParameterAcceptor\\n  {\\n public:\\n using BoundaryNormalMap =\\n      std::map<types::global_dof_index,\\n               std::tuple<Tensor<1, dim>, types::boundary_id, Point<dim>>>;\\n \\n    OfflineData(const MPI_Comm             mpi_communicator,\\n TimerOutput               &computing_timer,\\n const Discretization<dim> &discretization,\\n const std::string         &subsection = \\\"OfflineData\\\");\\n \\n void setup();\\n void assemble();\\n \\n DoFHandler<dim> dof_handler;\\n \\n    std::shared_ptr<const Utilities::MPI::Partitioner> partitioner;\\n \\n unsigned int n_locally_owned;\\n unsigned int n_locally_relevant;\\n \\n SparsityPattern sparsity_pattern;\\n \\n    BoundaryNormalMap boundary_normal_map;\\n \\n SparseMatrix<double>                  lumped_mass_matrix;\\n    std::array<SparseMatrix<double>, dim> cij_matrix;\\n    std::array<SparseMatrix<double>, dim> nij_matrix;\\n SparseMatrix<double>                  norm_matrix;\\n \\n private:\\n const MPI_Comm mpi_communicator;\\n TimerOutput   &computing_timer;\\n \\n SmartPointer<const Discretization<dim>> discretization;\\n  };\\n \\n template <int dim>\\n class ProblemDescription\\n  {\\n public:\\n static constexpr unsigned int n_solution_variables = 2 + dim;\\n \\n using state_type = Tensor<1, n_solution_variables>;\\n using flux_type  = Tensor<1, n_solution_variables, Tensor<1, dim>>;\\n \\n const static std::array<std::string, n_solution_variables> component_names;\\n \\n static constexpr double gamma = 7. / 5.;\\n \\n static DEAL_II_ALWAYS_INLINE inline Tensor<1, dim>\\n    momentum(const state_type &U);\\n \\n static DEAL_II_ALWAYS_INLINE inline double\\n    internal_energy(const state_type &U);\\n \\n static DEAL_II_ALWAYS_INLINE inline double pressure(const state_type &U);\\n \\n static DEAL_II_ALWAYS_INLINE inline double\\n    speed_of_sound(const state_type &U);\\n \\n static DEAL_II_ALWAYS_INLINE inline flux_type flux(const state_type &U);\\n \\n static DEAL_II_ALWAYS_INLINE inline double\\n    compute_lambda_max(const state_type     &U_i,\\n const state_type     &U_j,\\n const Tensor<1, dim> &n_ij);\\n  };\\n \\n template <int dim>\\n class InitialValues : public ParameterAcceptor\\n  {\\n public:\\n using state_type = typename ProblemDescription<dim>::state_type;\\n \\n    InitialValues(const std::string &subsection = \\\"InitialValues\\\");\\n \\n    std::function<state_type(const Point<dim> &point, double t)> initial_state;\\n \\n private:\\n void parse_parameters_callback();\\n \\n Tensor<1, dim> initial_direction;\\n Tensor<1, 3>   initial_1d_state;\\n  };\\n \\n template <int dim>\\n class TimeStepping : public ParameterAcceptor\\n  {\\n public:\\n static constexpr unsigned int n_solution_variables =\\n      ProblemDescription<dim>::n_solution_variables;\\n \\n using state_type = typename ProblemDescription<dim>::state_type;\\n using flux_type  = typename ProblemDescription<dim>::flux_type;\\n \\n using vector_type = std::array<LinearAlgebra::distributed::Vector<double>,\\n                                   n_solution_variables>;\\n \\n TimeStepping(const MPI_Comm            mpi_communicator,\\n TimerOutput              &computing_timer,\\n const OfflineData<dim>   &offline_data,\\n const InitialValues<dim> &initial_values,\\n const std::string        &subsection = \\\"TimeStepping\\\");\\n \\n void prepare();\\n \\n double make_one_step(vector_type &U, const double t);\\n \\n private:\\n const MPI_Comm mpi_communicator;\\n TimerOutput   &computing_timer;\\n \\n SmartPointer<const OfflineData<dim>>   offline_data;\\n SmartPointer<const InitialValues<dim>> initial_values;\\n \\n SparseMatrix<double> dij_matrix;\\n \\n    vector_type temporary_vector;\\n \\n double cfl_update;\\n  };\\n \\n template <int dim>\\n class SchlierenPostprocessor : public ParameterAcceptor\\n  {\\n public:\\n static constexpr unsigned int n_solution_variables =\\n      ProblemDescription<dim>::n_solution_variables;\\n \\n using state_type = typename ProblemDescription<dim>::state_type;\\n \\n using vector_type = std::array<LinearAlgebra::distributed::Vector<double>,\\n                                   n_solution_variables>;\\n \\n    SchlierenPostprocessor(\\n const MPI_Comm          mpi_communicator,\\n TimerOutput            &computing_timer,\\n const OfflineData<dim> &offline_data,\\n const std::string      &subsection = \\\"SchlierenPostprocessor\\\");\\n \\n void prepare();\\n \\n void compute_schlieren(const vector_type &U);\\n \\n LinearAlgebra::distributed::Vector<double> schlieren;\\n \\n private:\\n const MPI_Comm mpi_communicator;\\n TimerOutput   &computing_timer;\\n \\n SmartPointer<const OfflineData<dim>> offline_data;\\n \\n Vector<double> r;\\n \\n unsigned int schlieren_index;\\n double       schlieren_beta;\\n  };\\n \\n template <int dim>\\n class MainLoop : public ParameterAcceptor\\n  {\\n public:\\n using vector_type = typename TimeStepping<dim>::vector_type;\\n \\n    MainLoop(const MPI_Comm mpi_communnicator);\\n \\n void run();\\n \\n private:\\n    vector_type interpolate_initial_values(const double t = 0);\\n \\n void checkpoint(const vector_type &U,\\n const std::string &name,\\n double             t,\\n unsigned int       cycle);\\n \\n void output(const vector_type &U,\\n const std::string &name,\\n double             t,\\n unsigned int       cycle);\\n \\n const MPI_Comm     mpi_communicator;\\n    std::ostringstream timer_output;\\n TimerOutput        computing_timer;\\n \\n ConditionalOStream pcout;\\n \\n    std::string base_name;\\n double      t_final;\\n double      output_granularity;\\n \\n bool asynchronous_writeback;\\n \\n bool resume;\\n \\n    Discretization<dim>         discretization;\\n    OfflineData<dim>            offline_data;\\n    InitialValues<dim>          initial_values;\\n    TimeStepping<dim>           time_stepping;\\n    SchlierenPostprocessor<dim> schlieren_postprocessor;\\n \\n    vector_type output_vector;\\n \\n    std::future<void> background_thread_state;\\n  };\\n \\n \\n \\n template <int dim>\\n  Discretization<dim>::Discretization(const MPI_Comm     mpi_communicator,\\n TimerOutput       &computing_timer,\\n const std::string &subsection)\\n    : ParameterAcceptor(subsection)\\n    , mpi_communicator(mpi_communicator)\\n    , triangulation(mpi_communicator)\\n    , mapping(1)\\n    , finite_element(1)\\n    , quadrature(3)\\n    , face_quadrature(3)\\n    , computing_timer(computing_timer)\\n  {\\n    length = 4.;\\n    add_parameter(\\\"length\\\", length, \\\"Length of computational domain\\\");\\n \\n    height = 2.;\\n    add_parameter(\\\"height\\\", height, \\\"Height of computational domain\\\");\\n \\n    disk_position = 0.6;\\n    add_parameter(\\\"object position\\\",\\n                  disk_position,\\n \\\"x position of immersed disk center point\\\");\\n \\n    disk_diameter = 0.5;\\n    add_parameter(\\\"object diameter\\\",\\n                  disk_diameter,\\n \\\"Diameter of immersed disk\\\");\\n \\n    refinement = 5;\\n    add_parameter(\\\"refinement\\\",\\n                  refinement,\\n \\\"Number of refinement steps of the geometry\\\");\\n  }\\n \\n template <int dim>\\n void Discretization<dim>::setup()\\n  {\\n TimerOutput::Scope scope(computing_timer, \\\"discretization - setup\\\");\\n \\n triangulation.clear();\\n \\n Triangulation<dim> tria1, tria2, tria3, tria4, tria5, tria6;\\n \\n GridGenerator::hyper_cube_with_cylindrical_hole(\\n      tria1, disk_diameter / 2., disk_diameter, 0.5, 1, false);\\n \\n GridGenerator::subdivided_hyper_rectangle(\\n      tria2,\\n      {2, 1},\\n Point<2>(-disk_diameter, disk_diameter),\\n Point<2>(disk_diameter, height / 2.));\\n \\n GridGenerator::subdivided_hyper_rectangle(\\n      tria3,\\n      {2, 1},\\n Point<2>(-disk_diameter, -disk_diameter),\\n Point<2>(disk_diameter, -height / 2.));\\n \\n GridGenerator::subdivided_hyper_rectangle(\\n      tria4,\\n      {6, 2},\\n Point<2>(disk_diameter, -disk_diameter),\\n Point<2>(length - disk_position, disk_diameter));\\n \\n GridGenerator::subdivided_hyper_rectangle(\\n      tria5,\\n      {6, 1},\\n Point<2>(disk_diameter, disk_diameter),\\n Point<2>(length - disk_position, height / 2.));\\n \\n GridGenerator::subdivided_hyper_rectangle(\\n      tria6,\\n      {6, 1},\\n Point<2>(disk_diameter, -height / 2.),\\n Point<2>(length - disk_position, -disk_diameter));\\n \\n GridGenerator::merge_triangulations(\\n      {&tria1, &tria2, &tria3, &tria4, &tria5, &tria6},\\n triangulation,\\n      1.e-12,\\n true);\\n \\n triangulation.set_manifold(0, PolarManifold<2>(Point<2>()));\\n \\n \\n for (const auto &cell : triangulation.active_cell_iterators())\\n      {\\n for (const auto v : cell->vertex_indices())\\n          {\\n if (cell->vertex(v)[0] <= -disk_diameter + 1.e-6)\\n              cell->vertex(v)[0] = -disk_position;\\n          }\\n      }\\n \\n for (const auto &cell : triangulation.active_cell_iterators())\\n      {\\n for (const auto f : cell->face_indices())\\n          {\\n const auto face = cell->face(f);\\n \\n if (face->at_boundary())\\n              {\\n const auto center = face->center();\\n \\n if (center[0] > length - disk_position - 1.e-6)\\n                  face->set_boundary_id(Boundaries::do_nothing);\\n else if (center[0] < -disk_position + 1.e-6)\\n                  face->set_boundary_id(Boundaries::dirichlet);\\n else\\n                  face->set_boundary_id(Boundaries::free_slip);\\n              }\\n          }\\n      }\\n  }\\n \\n \\n template <int dim>\\n  OfflineData<dim>::OfflineData(const MPI_Comm             mpi_communicator,\\n TimerOutput               &computing_timer,\\n const Discretization<dim> &discretization,\\n const std::string         &subsection)\\n    : ParameterAcceptor(subsection)\\n    , dof_handler(discretization.triangulation)\\n    , mpi_communicator(mpi_communicator)\\n    , computing_timer(computing_timer)\\n    , discretization(&discretization)\\n  {}\\n \\n template <int dim>\\n void OfflineData<dim>::setup()\\n  {\\n IndexSet locally_owned;\\n IndexSet locally_relevant;\\n \\n    {\\n TimerOutput::Scope scope(computing_timer,\\n \\\"offline_data - distribute dofs\\\");\\n \\n      dof_handler.distribute_dofs(discretization->finite_element);\\n \\n      locally_owned   = dof_handler.locally_owned_dofs();\\n      n_locally_owned = locally_owned.n_elements();\\n \\n      locally_relevant   = DoFTools::extract_locally_relevant_dofs(dof_handler);\\n      n_locally_relevant = locally_relevant.n_elements();\\n \\n      partitioner =\\n        std::make_shared<Utilities::MPI::Partitioner>(locally_owned,\\n                                                      locally_relevant,\\n                                                      mpi_communicator);\\n    }\\n \\n \\n \\n    {\\n TimerOutput::Scope scope(\\n        computing_timer,\\n \\\"offline_data - create sparsity pattern and set up matrices\\\");\\n \\n \\n DynamicSparsityPattern dsp(n_locally_relevant, n_locally_relevant);\\n \\n const auto dofs_per_cell =\\n        discretization->finite_element.n_dofs_per_cell();\\n      std::vector<types::global_dof_index> dof_indices(dofs_per_cell);\\n \\n for (const auto &cell : dof_handler.active_cell_iterators())\\n        {\\n if (cell->is_artificial())\\n continue;\\n \\n /* We transform the set of global dof indices on the cell to the\\n           * corresponding \\\"local\\\" index range on the MPI process: */\\n          cell->get_dof_indices(dof_indices);\\n          std::transform(dof_indices.begin(),\\n                         dof_indices.end(),\\n                         dof_indices.begin(),\\n                         [&](types::global_dof_index index) {\\n                           return partitioner->global_to_local(index);\\n                         });\\n \\n /* And simply add, for each dof, a coupling to all other \\\"local\\\"\\n           * dofs on the cell: */\\n for (const auto dof : dof_indices)\\n            dsp.add_entries(dof, dof_indices.begin(), dof_indices.end());\\n        }\\n \\n      sparsity_pattern.copy_from(dsp);\\n \\n      lumped_mass_matrix.reinit(sparsity_pattern);\\n      norm_matrix.reinit(sparsity_pattern);\\n for (auto &matrix : cij_matrix)\\n matrix.reinit(sparsity_pattern);\\n for (auto &matrix : nij_matrix)\\n matrix.reinit(sparsity_pattern);\\n    }\\n  }\\n \\n \\n namespace\\n  {\\n \\n template <int dim>\\n struct CopyData\\n    {\\n bool                                         is_artificial;\\n      std::vector<types::global_dof_index>         local_dof_indices;\\n typename OfflineData<dim>::BoundaryNormalMap local_boundary_normal_map;\\n FullMatrix<double>                           cell_lumped_mass_matrix;\\n      std::array<FullMatrix<double>, dim>          cell_cij_matrix;\\n    };\\n \\n \\n \\n template <typename IteratorType>\\n DEAL_II_ALWAYS_INLINE inline SparseMatrix<double>::value_type\\n    get_entry(const SparseMatrix<double> &matrix, const IteratorType &it)\\n    {\\n const SparseMatrix<double>::const_iterator matrix_iterator(\\n        &matrix, it->global_index());\\n return matrix_iterator->value();\\n    }\\n \\n \\n template <typename IteratorType>\\n DEAL_II_ALWAYS_INLINE inline void\\n    set_entry(SparseMatrix<double>            &matrix,\\n const IteratorType              &it,\\n SparseMatrix<double>::value_type value)\\n    {\\n SparseMatrix<double>::iterator matrix_iterator(&matrix,\\n                                                     it->global_index());\\n      matrix_iterator->value() = value;\\n    }\\n \\n \\n template <std::size_t k, typename IteratorType>\\n DEAL_II_ALWAYS_INLINE inline Tensor<1, k>\\n    gather_get_entry(const std::array<SparseMatrix<double>, k> &c_ij,\\n const IteratorType                         it)\\n    {\\n Tensor<1, k> result;\\n for (unsigned int j = 0; j < k; ++j)\\n        result[j] = get_entry(c_ij[j], it);\\n return result;\\n    }\\n \\n \\n template <std::size_t k>\\n DEAL_II_ALWAYS_INLINE inline Tensor<1, k>\\n gather(const std::array<SparseMatrix<double>, k> &n_ij,\\n const unsigned int                         i,\\n const unsigned int                         j)\\n    {\\n Tensor<1, k> result;\\n for (unsigned int l = 0; l < k; ++l)\\n        result[l] = n_ij[l](i, j);\\n return result;\\n    }\\n \\n \\n template <std::size_t k>\\n DEAL_II_ALWAYS_INLINE inline Tensor<1, k>\\n gather(const std::array<LinearAlgebra::distributed::Vector<double>, k> &U,\\n const unsigned int                                               i)\\n    {\\n Tensor<1, k> result;\\n for (unsigned int j = 0; j < k; ++j)\\n        result[j] = U[j].local_element(i);\\n return result;\\n    }\\n \\n \\n template <std::size_t k, int k2>\\n DEAL_II_ALWAYS_INLINE inline void\\n scatter(std::array<LinearAlgebra::distributed::Vector<double>, k> &U,\\n const Tensor<1, k2>                                       &tensor,\\n const unsigned int                                         i)\\n    {\\n static_assert(k == k2,\\n \\\"The dimensions of the input arguments must agree\\\");\\n for (unsigned int j = 0; j < k; ++j)\\n        U[j].local_element(i) = tensor[j];\\n    }\\n  } // namespace\\n \\n \\n template <int dim>\\n void OfflineData<dim>::assemble()\\n  {\\n    lumped_mass_matrix = 0.;\\n    norm_matrix        = 0.;\\n for (auto &matrix : cij_matrix)\\n matrix = 0.;\\n for (auto &matrix : nij_matrix)\\n matrix = 0.;\\n \\n unsigned int dofs_per_cell =\\n      discretization->finite_element.n_dofs_per_cell();\\n unsigned int n_q_points = discretization->quadrature.size();\\n \\n \\n MeshWorker::ScratchData<dim> scratch_data(\\n      discretization->mapping,\\n      discretization->finite_element,\\n      discretization->quadrature,\\n update_values | update_gradients | update_quadrature_points |\\n update_JxW_values,\\n      discretization->face_quadrature,\\n update_normal_vectors | update_values | update_JxW_values);\\n \\n    {\\n TimerOutput::Scope scope(\\n        computing_timer,\\n \\\"offline_data - assemble lumped mass matrix, and c_ij\\\");\\n \\n const auto local_assemble_system = \\n        [&](const typename DoFHandler<dim>::cell_iterator &cell,\\n MeshWorker::ScratchData<dim>                  &scratch,\\n            CopyData<dim>                                 &copy) {\\n copy.is_artificial = cell->is_artificial();\\n if (copy.is_artificial)\\n return;\\n \\n copy.local_boundary_normal_map.clear();\\n copy.cell_lumped_mass_matrix.reinit(dofs_per_cell, dofs_per_cell);\\n for (auto &matrix : copy.cell_cij_matrix)\\n matrix.reinit(dofs_per_cell, dofs_per_cell);\\n \\n const auto &fe_values = scratch.reinit(cell);\\n \\n copy.local_dof_indices.resize(dofs_per_cell);\\n          cell->get_dof_indices(copy.local_dof_indices);\\n \\n          std::transform(copy.local_dof_indices.begin(),\\n copy.local_dof_indices.end(),\\n copy.local_dof_indices.begin(),\\n                         [&](types::global_dof_index index) {\\n                           return partitioner->global_to_local(index);\\n                         });\\n \\n for (unsigned int q_point = 0; q_point < n_q_points; ++q_point)\\n            {\\n const auto JxW = fe_values.JxW(q_point);\\n \\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n                {\\n const auto value_JxW =\\n                    fe_values.shape_value(j, q_point) * JxW;\\n const auto grad_JxW = fe_values.shape_grad(j, q_point) * JxW;\\n \\n copy.cell_lumped_mass_matrix(j, j) += value_JxW;\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n                    {\\n const auto value = fe_values.shape_value(i, q_point);\\n for (unsigned int d = 0; d < dim; ++d)\\n copy.cell_cij_matrix[d](i, j) += value * grad_JxW[d];\\n \\n                    } /* i */\\n                }     /* j */\\n            }         /* q */\\n \\n for (const auto f : cell->face_indices())\\n            {\\n const auto face = cell->face(f);\\n const auto id   = face->boundary_id();\\n \\n if (!face->at_boundary())\\n continue;\\n \\n const auto &fe_face_values = scratch.reinit(cell, f);\\n \\n const unsigned int n_face_q_points =\\n                fe_face_values.get_quadrature().size();\\n \\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n                {\\n if (!discretization->finite_element.has_support_on_face(j, f))\\n continue;\\n \\n Tensor<1, dim> normal;\\n if (id == Boundaries::free_slip)\\n                    {\\n for (unsigned int q = 0; q < n_face_q_points; ++q)\\n                        normal += fe_face_values.normal_vector(q) *\\n                                  fe_face_values.shape_value(j, q);\\n                    }\\n \\n const auto index = copy.local_dof_indices[j];\\n \\n Point<dim> position;\\n for (const auto v : cell->vertex_indices())\\n                    if (cell->vertex_dof_index(v, 0) ==\\n                        partitioner->local_to_global(index))\\n                      {\\n                        position = cell->vertex(v);\\n break;\\n                      }\\n \\n const auto old_id =\\n                    std::get<1>(copy.local_boundary_normal_map[index]);\\n copy.local_boundary_normal_map[index] =\\n                    std::make_tuple(normal, std::max(old_id, id), position);\\n                }\\n            }\\n        };\\n \\n const auto copy_local_to_global = [&](const CopyData<dim> &copy) {\\n if (copy.is_artificial)\\n return;\\n \\n for (const auto &it : copy.local_boundary_normal_map)\\n          {\\n            std::get<0>(boundary_normal_map[it.first]) +=\\n              std::get<0>(it.second);\\n            std::get<1>(boundary_normal_map[it.first]) =\\n std::max(std::get<1>(boundary_normal_map[it.first]),\\n                       std::get<1>(it.second));\\n            std::get<2>(boundary_normal_map[it.first]) = std::get<2>(it.second);\\n          }\\n \\n        lumped_mass_matrix.add(copy.local_dof_indices,\\n copy.cell_lumped_mass_matrix);\\n \\n for (int k = 0; k < dim; ++k)\\n          {\\n            cij_matrix[k].add(copy.local_dof_indices, copy.cell_cij_matrix[k]);\\n            nij_matrix[k].add(copy.local_dof_indices, copy.cell_cij_matrix[k]);\\n          }\\n      };\\n \\n WorkStream::run(dof_handler.begin_active(),\\n                      dof_handler.end(),\\n                      local_assemble_system,\\n                      copy_local_to_global,\\n                      scratch_data,\\n                      CopyData<dim>());\\n    }\\n \\n    {\\n TimerOutput::Scope scope(computing_timer,\\n \\\"offline_data - compute |c_ij|, and n_ij\\\");\\n \\n const std_cxx20::ranges::iota_view<unsigned int, unsigned int> indices(\\n        0, n_locally_relevant);\\n \\n const auto on_subranges = \\n        [&](const auto index_begin, const auto index_end) {\\n for (const auto row_index :\\n std_cxx20::ranges::iota_view<unsigned int, unsigned int>(\\n                 *index_begin, *index_end))\\n            {\\n              std::for_each(sparsity_pattern.begin(row_index),\\n                            sparsity_pattern.end(row_index),\\n                            [&](const SparsityPatternIterators::Accessor &jt) {\\n                              const auto c_ij =\\n                                gather_get_entry(cij_matrix, &jt);\\n                              const double norm = c_ij.norm();\\n \\n                              set_entry(norm_matrix, &jt, norm);\\n                              for (unsigned int j = 0; j < dim; ++j)\\n                                set_entry(nij_matrix[j], &jt, c_ij[j] / norm);\\n                            });\\n            }\\n        };\\n \\n parallel::apply_to_subranges(indices.begin(),\\n                                   indices.end(),\\n                                   on_subranges,\\n                                   4096);\\n \\n for (auto &it : boundary_normal_map)\\n        {\\n auto &normal = std::get<0>(it.second);\\n          normal /= (normal.norm() + std::numeric_limits<double>::epsilon());\\n        }\\n    }\\n  }\\n \\n \\n \\n \\n template <int dim>\\n DEAL_II_ALWAYS_INLINE inline Tensor<1, dim>\\n  ProblemDescription<dim>::momentum(const state_type &U)\\n  {\\n Tensor<1, dim> result;\\n    std::copy_n(&U[1], dim, &result[0]);\\n return result;\\n  }\\n \\n template <int dim>\\n DEAL_II_ALWAYS_INLINE inline double\\n  ProblemDescription<dim>::internal_energy(const state_type &U)\\n  {\\n const double &rho = U[0];\\n const auto    m   = momentum(U);\\n const double &E   = U[dim + 1];\\n return E - 0.5 * m.norm_square() / rho;\\n  }\\n \\n template <int dim>\\n DEAL_II_ALWAYS_INLINE inline double\\n  ProblemDescription<dim>::pressure(const state_type &U)\\n  {\\n return (gamma - 1.) * internal_energy(U);\\n  }\\n \\n template <int dim>\\n DEAL_II_ALWAYS_INLINE inline double\\n  ProblemDescription<dim>::speed_of_sound(const state_type &U)\\n  {\\n const double &rho = U[0];\\n const double  p   = pressure(U);\\n \\n return std::sqrt(gamma * p / rho);\\n  }\\n \\n template <int dim>\\n DEAL_II_ALWAYS_INLINE inline typename ProblemDescription<dim>::flux_type\\n  ProblemDescription<dim>::flux(const state_type &U)\\n  {\\n const double &rho = U[0];\\n const auto    m   = momentum(U);\\n const auto    p   = pressure(U);\\n const double &E   = U[dim + 1];\\n \\n    flux_type result;\\n \\n    result[0] = m;\\n for (unsigned int i = 0; i < dim; ++i)\\n      {\\n        result[1 + i] = m * m[i] / rho;\\n        result[1 + i][i] += p;\\n      }\\n    result[dim + 1] = m / rho * (E + p);\\n \\n return result;\\n  }\\n \\n namespace\\n  {\\n template <int dim>\\n DEAL_II_ALWAYS_INLINE inline std::array<double, 4> riemann_data_from_state(\\n const typename ProblemDescription<dim>::state_type U,\\n const Tensor<1, dim>                              &n_ij)\\n    {\\n Tensor<1, 3> projected_U;\\n      projected_U[0] = U[0];\\n \\n const auto m   = ProblemDescription<dim>::momentum(U);\\n      projected_U[1] = n_ij * m;\\n \\n const auto perpendicular_m = m - projected_U[1] * n_ij;\\n      projected_U[2] = U[1 + dim] - 0.5 * perpendicular_m.norm_square() / U[0];\\n \\n \\n return {{projected_U[0],\\n               projected_U[1] / projected_U[0],\\n               ProblemDescription<1>::pressure(projected_U),\\n               ProblemDescription<1>::speed_of_sound(projected_U)}};\\n    }\\n \\n \\n DEAL_II_ALWAYS_INLINE inline double positive_part(const double number)\\n    {\\n return std::max(number, 0.);\\n    }\\n \\n \\n DEAL_II_ALWAYS_INLINE inline double negative_part(const double number)\\n    {\\n return -std::min(number, 0.);\\n    }\\n \\n \\n DEAL_II_ALWAYS_INLINE inline double\\n    lambda1_minus(const std::array<double, 4> &riemann_data,\\n const double                 p_star)\\n    {\\n /* Implements formula (3.7) in Guermond-Popov-2016 */\\n \\n constexpr double gamma = ProblemDescription<1>::gamma;\\n const auto       u     = riemann_data[1];\\n const auto       p     = riemann_data[2];\\n const auto       a     = riemann_data[3];\\n \\n const double factor = (gamma + 1.0) / 2.0 / gamma;\\n const double tmp    = positive_part((p_star - p) / p);\\n return u - a * std::sqrt(1.0 + factor * tmp);\\n    }\\n \\n \\n DEAL_II_ALWAYS_INLINE inline double\\n    lambda3_plus(const std::array<double, 4> &riemann_data, const double p_star)\\n    {\\n /* Implements formula (3.8) in Guermond-Popov-2016 */\\n \\n constexpr double gamma = ProblemDescription<1>::gamma;\\n const auto       u     = riemann_data[1];\\n const auto       p     = riemann_data[2];\\n const auto       a     = riemann_data[3];\\n \\n const double factor = (gamma + 1.0) / 2.0 / gamma;\\n const double tmp    = positive_part((p_star - p) / p);\\n return u + a * std::sqrt(1.0 + factor * tmp);\\n    }\\n \\n \\n DEAL_II_ALWAYS_INLINE inline double\\n    lambda_max_two_rarefaction(const std::array<double, 4> &riemann_data_i,\\n const std::array<double, 4> &riemann_data_j)\\n    {\\n constexpr double gamma = ProblemDescription<1>::gamma;\\n const auto       u_i   = riemann_data_i[1];\\n const auto       p_i   = riemann_data_i[2];\\n const auto       a_i   = riemann_data_i[3];\\n const auto       u_j   = riemann_data_j[1];\\n const auto       p_j   = riemann_data_j[2];\\n const auto       a_j   = riemann_data_j[3];\\n \\n const double numerator = a_i + a_j - (gamma - 1.) / 2. * (u_j - u_i);\\n \\n const double denominator =\\n        a_i * std::pow(p_i / p_j, -1. * (gamma - 1.) / 2. / gamma) + a_j * 1.;\\n \\n /* Formula (4.3) in Guermond-Popov-2016 */\\n \\n const double p_star =\\n        p_j * std::pow(numerator / denominator, 2. * gamma / (gamma - 1));\\n \\n const double lambda1 = lambda1_minus(riemann_data_i, p_star);\\n const double lambda3 = lambda3_plus(riemann_data_j, p_star);\\n \\n /* Formula (2.11) in Guermond-Popov-2016 */\\n \\n return std::max(positive_part(lambda3), negative_part(lambda1));\\n    }\\n \\n \\n DEAL_II_ALWAYS_INLINE inline double\\n    lambda_max_expansion(const std::array<double, 4> &riemann_data_i,\\n const std::array<double, 4> &riemann_data_j)\\n    {\\n const auto u_i = riemann_data_i[1];\\n const auto a_i = riemann_data_i[3];\\n const auto u_j = riemann_data_j[1];\\n const auto a_j = riemann_data_j[3];\\n \\n return std::max(std::abs(u_i), std::abs(u_j)) + 5. * std::max(a_i, a_j);\\n    }\\n  } // namespace\\n \\n \\n template <int dim>\\n DEAL_II_ALWAYS_INLINE inline double\\n  ProblemDescription<dim>::compute_lambda_max(const state_type     &U_i,\\n const state_type     &U_j,\\n const Tensor<1, dim> &n_ij)\\n  {\\n const auto riemann_data_i = riemann_data_from_state(U_i, n_ij);\\n const auto riemann_data_j = riemann_data_from_state(U_j, n_ij);\\n \\n const double lambda_1 =\\n      lambda_max_two_rarefaction(riemann_data_i, riemann_data_j);\\n \\n const double lambda_2 =\\n      lambda_max_expansion(riemann_data_i, riemann_data_j);\\n \\n return std::min(lambda_1, lambda_2);\\n  }\\n \\n \\n template <>\\n const std::array<std::string, 3> ProblemDescription<1>::component_names{\\n    {\\\"rho\\\", \\\"m\\\", \\\"E\\\"}};\\n \\n template <>\\n const std::array<std::string, 4> ProblemDescription<2>::component_names{\\n    {\\\"rho\\\", \\\"m_1\\\", \\\"m_2\\\", \\\"E\\\"}};\\n \\n template <>\\n const std::array<std::string, 5> ProblemDescription<3>::component_names{\\n    {\\\"rho\\\", \\\"m_1\\\", \\\"m_2\\\", \\\"m_3\\\", \\\"E\\\"}};\\n \\n \\n \\n template <int dim>\\n  InitialValues<dim>::InitialValues(const std::string &subsection)\\n    : ParameterAcceptor(subsection)\\n  {\\n /* We wire up the slot InitialValues<dim>::parse_parameters_callback to\\n       the ParameterAcceptor::parse_parameters_call_back signal: */\\n ParameterAcceptor::parse_parameters_call_back.connect(\\n      [&]() { this->parse_parameters_callback(); });\\n \\n    initial_direction[0] = 1.;\\n    add_parameter(\\\"initial direction\\\",\\n                  initial_direction,\\n \\\"Initial direction of the uniform flow field\\\");\\n \\n    initial_1d_state[0] = ProblemDescription<dim>::gamma;\\n    initial_1d_state[1] = 3.;\\n    initial_1d_state[2] = 1.;\\n    add_parameter(\\\"initial 1d state\\\",\\n                  initial_1d_state,\\n \\\"Initial 1d state (rho, u, p) of the uniform flow field\\\");\\n  }\\n \\n \\n template <int dim>\\n void InitialValues<dim>::parse_parameters_callback()\\n  {\\n AssertThrow(initial_direction.norm() != 0.,\\n                ExcMessage(\\n \\\"Initial shock front direction is set to the zero vector.\\\"));\\n    initial_direction /= initial_direction.norm();\\n \\n \\n    initial_state = [this](const Point<dim> & /*point*/, double /*t*/) {\\n const double            rho   = initial_1d_state[0];\\n const double            u     = initial_1d_state[1];\\n const double            p     = initial_1d_state[2];\\n static constexpr double gamma = ProblemDescription<dim>::gamma;\\n \\n      state_type state;\\n \\n      state[0] = rho;\\n for (unsigned int i = 0; i < dim; ++i)\\n        state[1 + i] = rho * u * initial_direction[i];\\n \\n      state[dim + 1] = p / (gamma - 1.) + 0.5 * rho * u * u;\\n \\n return state;\\n    };\\n  }\\n \\n \\n \\n template <int dim>\\n TimeStepping<dim>::TimeStepping(\\n const MPI_Comm            mpi_communicator,\\n TimerOutput              &computing_timer,\\n const OfflineData<dim>   &offline_data,\\n const InitialValues<dim> &initial_values,\\n const std::string        &subsection /*= \\\"TimeStepping\\\"*/)\\n    : ParameterAcceptor(subsection)\\n    , mpi_communicator(mpi_communicator)\\n    , computing_timer(computing_timer)\\n    , offline_data(&offline_data)\\n    , initial_values(&initial_values)\\n  {\\n    cfl_update = 0.80;\\n    add_parameter(\\\"cfl update\\\",\\n                  cfl_update,\\n \\\"Relative CFL constant used for update\\\");\\n  }\\n \\n \\n template <int dim>\\n void TimeStepping<dim>::prepare()\\n  {\\n TimerOutput::Scope scope(computing_timer,\\n \\\"time_stepping - prepare scratch space\\\");\\n \\n for (auto &it : temporary_vector)\\n      it.reinit(offline_data->partitioner);\\n \\n    dij_matrix.reinit(offline_data->sparsity_pattern);\\n  }\\n \\n \\n template <int dim>\\n double TimeStepping<dim>::make_one_step(vector_type &U, const double t)\\n  {\\n const auto &n_locally_owned    = offline_data->n_locally_owned;\\n const auto &n_locally_relevant = offline_data->n_locally_relevant;\\n \\n const std_cxx20::ranges::iota_view<unsigned int, unsigned int>\\n      indices_owned(0, n_locally_owned);\\n const std_cxx20::ranges::iota_view<unsigned int, unsigned int>\\n      indices_relevant(0, n_locally_relevant);\\n \\n const auto &sparsity = offline_data->sparsity_pattern;\\n \\n const auto &lumped_mass_matrix = offline_data->lumped_mass_matrix;\\n const auto &norm_matrix        = offline_data->norm_matrix;\\n const auto &nij_matrix         = offline_data->nij_matrix;\\n const auto &cij_matrix         = offline_data->cij_matrix;\\n \\n const auto &boundary_normal_map = offline_data->boundary_normal_map;\\n \\n    {\\n TimerOutput::Scope scope(computing_timer,\\n \\\"time_stepping - 1 compute d_ij\\\");\\n \\n const auto on_subranges = \\n        [&](const auto index_begin, const auto index_end) {\\n for (const auto i :\\n std_cxx20::ranges::iota_view<unsigned int, unsigned int>(\\n                 *index_begin, *index_end))\\n            {\\n const auto U_i = gather(U, i);\\n \\n for (auto jt = sparsity.begin(i); jt != sparsity.end(i); ++jt)\\n                {\\n const auto j = jt->column();\\n \\n if (j >= i)\\n continue;\\n \\n const auto U_j = gather(U, j);\\n \\n const auto   n_ij = gather_get_entry(nij_matrix, jt);\\n const double norm = get_entry(norm_matrix, jt);\\n \\n const auto lambda_max =\\n                    ProblemDescription<dim>::compute_lambda_max(U_i, U_j, n_ij);\\n \\n double d = norm * lambda_max;\\n \\n if (boundary_normal_map.count(i) != 0 &&\\n                      boundary_normal_map.count(j) != 0)\\n                    {\\n const auto n_ji = gather(nij_matrix, j, i);\\n const auto lambda_max_2 =\\n                        ProblemDescription<dim>::compute_lambda_max(U_j,\\n                                                                    U_i,\\n                                                                    n_ji);\\n const double norm_2 = norm_matrix(j, i);\\n \\n d = std::max(d, norm_2 * lambda_max_2);\\n                    }\\n \\n                  set_entry(dij_matrix, jt, d);\\n                  dij_matrix(j, i) = d;\\n                }\\n            }\\n        };\\n \\n parallel::apply_to_subranges(indices_relevant.begin(),\\n                                   indices_relevant.end(),\\n                                   on_subranges,\\n                                   4096);\\n    }\\n \\n \\n \\n    std::atomic<double> tau_max{std::numeric_limits<double>::infinity()};\\n \\n    {\\n TimerOutput::Scope scope(computing_timer,\\n \\\"time_stepping - 2 compute d_ii, and tau_max\\\");\\n \\n \\n const auto on_subranges = \\n        [&](const auto index_begin, const auto index_end) {\\n double tau_max_on_subrange = std::numeric_limits<double>::infinity();\\n \\n for (const auto i :\\n std_cxx20::ranges::iota_view<unsigned int, unsigned int>(\\n                 *index_begin, *index_end))\\n            {\\n double d_sum = 0.;\\n \\n for (auto jt = sparsity.begin(i); jt != sparsity.end(i); ++jt)\\n                {\\n const auto j = jt->column();\\n \\n if (j == i)\\n continue;\\n \\n                  d_sum -= get_entry(dij_matrix, jt);\\n                }\\n \\n              dij_matrix.diag_element(i) = d_sum;\\n const double mass   = lumped_mass_matrix.diag_element(i);\\n const double tau    = cfl_update * mass / (-2. * d_sum);\\n              tau_max_on_subrange = std::min(tau_max_on_subrange, tau);\\n            }\\n \\n double current_tau_max = tau_max.load();\\n while (current_tau_max > tau_max_on_subrange &&\\n                 !tau_max.compare_exchange_weak(current_tau_max,\\n                                                tau_max_on_subrange))\\n            ;\\n        };\\n \\n parallel::apply_to_subranges(indices_relevant.begin(),\\n                                   indices_relevant.end(),\\n                                   on_subranges,\\n                                   4096);\\n \\n \\n      tau_max.store(Utilities::MPI::min(tau_max.load(), mpi_communicator));\\n \\n AssertThrow(\\n        !std::isnan(tau_max.load()) && !std::isinf(tau_max.load()) &&\\n          tau_max.load() > 0.,\\n        ExcMessage(\\n \\\"I'm sorry, Dave. I'm afraid I can't do that. - We crashed.\\\"));\\n    }\\n \\n \\n \\n    {\\n TimerOutput::Scope scope(computing_timer,\\n \\\"time_stepping - 3 perform update\\\");\\n \\n const auto on_subranges = \\n        [&](const auto index_begin, const auto index_end) {\\n for (const auto i :\\n boost::make_iterator_range(index_begin, index_end))\\n            {\\n Assert(i < n_locally_owned, ExcInternalError());\\n \\n const auto U_i = gather(U, i);\\n \\n const auto   f_i = ProblemDescription<dim>::flux(U_i);\\n const double m_i = lumped_mass_matrix.diag_element(i);\\n \\n auto U_i_new = U_i;\\n \\n for (auto jt = sparsity.begin(i); jt != sparsity.end(i); ++jt)\\n                {\\n const auto j = jt->column();\\n \\n const auto U_j = gather(U, j);\\n const auto f_j = ProblemDescription<dim>::flux(U_j);\\n \\n const auto c_ij = gather_get_entry(cij_matrix, jt);\\n const auto d_ij = get_entry(dij_matrix, jt);\\n \\n for (unsigned int k = 0; k < n_solution_variables; ++k)\\n                    {\\n                      U_i_new[k] +=\\n                        tau_max / m_i *\\n                        (-(f_j[k] - f_i[k]) * c_ij + d_ij * (U_j[k] - U_i[k]));\\n                    }\\n                }\\n \\n scatter(temporary_vector, U_i_new, i);\\n            }\\n        };\\n \\n parallel::apply_to_subranges(indices_owned.begin(),\\n                                   indices_owned.end(),\\n                                   on_subranges,\\n                                   4096);\\n    }\\n \\n \\n \\n    {\\n TimerOutput::Scope scope(computing_timer,\\n \\\"time_stepping - 4 fix boundary states\\\");\\n \\n for (const auto &it : boundary_normal_map)\\n        {\\n const auto i = it.first;\\n \\n if (i >= n_locally_owned)\\n continue;\\n \\n const auto &normal   = std::get<0>(it.second);\\n const auto &id       = std::get<1>(it.second);\\n const auto &position = std::get<2>(it.second);\\n \\n auto U_i = gather(temporary_vector, i);\\n \\n if (id == Boundaries::free_slip)\\n            {\\n auto m = ProblemDescription<dim>::momentum(U_i);\\n              m -= (m * normal) * normal;\\n for (unsigned int k = 0; k < dim; ++k)\\n                U_i[k + 1] = m[k];\\n            }\\n \\n else if (id == Boundaries::dirichlet)\\n            {\\n              U_i = initial_values->initial_state(position, t + tau_max);\\n            }\\n \\n scatter(temporary_vector, U_i, i);\\n        }\\n    }\\n \\n \\n for (auto &it : temporary_vector)\\n      it.update_ghost_values();\\n \\n    U.swap(temporary_vector);\\n \\n return tau_max;\\n  }\\n \\n \\n template <int dim>\\n  SchlierenPostprocessor<dim>::SchlierenPostprocessor(\\n const MPI_Comm          mpi_communicator,\\n TimerOutput            &computing_timer,\\n const OfflineData<dim> &offline_data,\\n const std::string      &subsection /*= \\\"SchlierenPostprocessor\\\"*/)\\n    : ParameterAcceptor(subsection)\\n    , mpi_communicator(mpi_communicator)\\n    , computing_timer(computing_timer)\\n    , offline_data(&offline_data)\\n  {\\n    schlieren_beta = 10.;\\n    add_parameter(\\\"schlieren beta\\\",\\n                  schlieren_beta,\\n \\\"Beta factor used in Schlieren-type postprocessor\\\");\\n \\n    schlieren_index = 0;\\n    add_parameter(\\\"schlieren index\\\",\\n                  schlieren_index,\\n \\\"Use the corresponding component of the state vector for the \\\"\\n \\\"schlieren plot\\\");\\n  }\\n \\n \\n template <int dim>\\n void SchlierenPostprocessor<dim>::prepare()\\n  {\\n TimerOutput::Scope scope(computing_timer,\\n \\\"schlieren_postprocessor - prepare scratch space\\\");\\n \\n    r.reinit(offline_data->n_locally_relevant);\\n    schlieren.reinit(offline_data->partitioner);\\n  }\\n \\n \\n template <int dim>\\n void SchlierenPostprocessor<dim>::compute_schlieren(const vector_type &U)\\n  {\\n TimerOutput::Scope scope(\\n      computing_timer, \\\"schlieren_postprocessor - compute schlieren plot\\\");\\n \\n const auto &sparsity            = offline_data->sparsity_pattern;\\n const auto &lumped_mass_matrix  = offline_data->lumped_mass_matrix;\\n const auto &cij_matrix          = offline_data->cij_matrix;\\n const auto &boundary_normal_map = offline_data->boundary_normal_map;\\n const auto &n_locally_owned     = offline_data->n_locally_owned;\\n \\n const auto indices =\\n std_cxx20::ranges::iota_view<unsigned int, unsigned int>(0,\\n                                                               n_locally_owned);\\n \\n    std::atomic<double> r_i_max{0.};\\n    std::atomic<double> r_i_min{std::numeric_limits<double>::infinity()};\\n \\n    {\\n const auto on_subranges = \\n        [&](const auto index_begin, const auto index_end) {\\n double r_i_max_on_subrange = 0.;\\n double r_i_min_on_subrange = std::numeric_limits<double>::infinity();\\n \\n for (const auto i :\\n boost::make_iterator_range(index_begin, index_end))\\n            {\\n Assert(i < n_locally_owned, ExcInternalError());\\n \\n Tensor<1, dim> r_i;\\n \\n for (auto jt = sparsity.begin(i); jt != sparsity.end(i); ++jt)\\n                {\\n const auto j = jt->column();\\n \\n if (i == j)\\n continue;\\n \\n const auto U_js = U[schlieren_index].local_element(j);\\n const auto c_ij = gather_get_entry(cij_matrix, jt);\\n                  r_i += c_ij * U_js;\\n                }\\n \\n \\n const auto bnm_it = boundary_normal_map.find(i);\\n if (bnm_it != boundary_normal_map.end())\\n                {\\n const auto &normal = std::get<0>(bnm_it->second);\\n const auto &id     = std::get<1>(bnm_it->second);\\n \\n if (id == Boundaries::free_slip)\\n                    r_i -= 1. * (r_i * normal) * normal;\\n else\\n                    r_i = 0.;\\n                }\\n \\n const double m_i    = lumped_mass_matrix.diag_element(i);\\n              r[i]                = r_i.norm() / m_i;\\n              r_i_max_on_subrange = std::max(r_i_max_on_subrange, r[i]);\\n              r_i_min_on_subrange = std::min(r_i_min_on_subrange, r[i]);\\n            }\\n \\n \\n double current_r_i_max = r_i_max.load();\\n while (current_r_i_max < r_i_max_on_subrange &&\\n                 !r_i_max.compare_exchange_weak(current_r_i_max,\\n                                                r_i_max_on_subrange))\\n            ;\\n \\n double current_r_i_min = r_i_min.load();\\n while (current_r_i_min > r_i_min_on_subrange &&\\n                 !r_i_min.compare_exchange_weak(current_r_i_min,\\n                                                r_i_min_on_subrange))\\n            ;\\n        };\\n \\n parallel::apply_to_subranges(indices.begin(),\\n                                   indices.end(),\\n                                   on_subranges,\\n                                   4096);\\n    }\\n \\n \\n    r_i_max.store(Utilities::MPI::max(r_i_max.load(), mpi_communicator));\\n    r_i_min.store(Utilities::MPI::min(r_i_min.load(), mpi_communicator));\\n \\n \\n    {\\n const auto on_subranges = \\n        [&](const auto index_begin, const auto index_end) {\\n for (const auto i :\\n boost::make_iterator_range(index_begin, index_end))\\n            {\\n Assert(i < n_locally_owned, ExcInternalError());\\n \\n              schlieren.local_element(i) =\\n                1. - std::exp(-schlieren_beta * (r[i] - r_i_min) /\\n                              (r_i_max - r_i_min));\\n            }\\n        };\\n \\n parallel::apply_to_subranges(indices.begin(),\\n                                   indices.end(),\\n                                   on_subranges,\\n                                   4096);\\n    }\\n \\n    schlieren.update_ghost_values();\\n  }\\n \\n \\n template <int dim>\\n  MainLoop<dim>::MainLoop(const MPI_Comm mpi_communicator)\\n    : ParameterAcceptor(\\\"A - MainLoop\\\")\\n    , mpi_communicator(mpi_communicator)\\n    , computing_timer(mpi_communicator,\\n                      timer_output,\\n TimerOutput::never,\\n TimerOutput::cpu_and_wall_times)\\n    , pcout(std::cout, Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n    , discretization(mpi_communicator, computing_timer, \\\"B - Discretization\\\")\\n    , offline_data(mpi_communicator,\\n                   computing_timer,\\n                   discretization,\\n \\\"C - OfflineData\\\")\\n    , initial_values(\\\"D - InitialValues\\\")\\n    , time_stepping(mpi_communicator,\\n                    computing_timer,\\n                    offline_data,\\n                    initial_values,\\n \\\"E - TimeStepping\\\")\\n    , schlieren_postprocessor(mpi_communicator,\\n                              computing_timer,\\n                              offline_data,\\n \\\"F - SchlierenPostprocessor\\\")\\n  {\\n    base_name = \\\"test\\\";\\n    add_parameter(\\\"basename\\\", base_name, \\\"Base name for all output files\\\");\\n \\n    t_final = 4.;\\n    add_parameter(\\\"final time\\\", t_final, \\\"Final time\\\");\\n \\n    output_granularity = 0.02;\\n    add_parameter(\\\"output granularity\\\",\\n                  output_granularity,\\n \\\"time interval for output\\\");\\n \\n    asynchronous_writeback = true;\\n    add_parameter(\\\"asynchronous writeback\\\",\\n                  asynchronous_writeback,\\n \\\"Write out solution in a background thread performing IO\\\");\\n \\n    resume = false;\\n    add_parameter(\\\"resume\\\", resume, \\\"Resume an interrupted computation.\\\");\\n  }\\n \\n \\n namespace\\n  {\\n void print_head(ConditionalOStream &pcout,\\n const std::string  &header,\\n const std::string  &secondary = \\\"\\\")\\n    {\\n const auto header_size   = header.size();\\n const auto padded_header = std::string((34 - header_size) / 2, ' ') +\\n                                 header +\\n                                 std::string((35 - header_size) / 2, ' ');\\n \\n const auto secondary_size = secondary.size();\\n const auto padded_secondary =\\n        std::string((34 - secondary_size) / 2, ' ') + secondary +\\n        std::string((35 - secondary_size) / 2, ' ');\\n \\n /* clang-format off */\\n      pcout << std::endl;\\n      pcout << \\\"    ####################################################\\\" << std::endl;\\n      pcout << \\\"    #########                                  #########\\\" << std::endl;\\n      pcout << \\\"    #########\\\"     <<  padded_header   <<     \\\"#########\\\" << std::endl;\\n      pcout << \\\"    #########\\\"     << padded_secondary <<     \\\"#########\\\" << std::endl;\\n      pcout << \\\"    #########                                  #########\\\" << std::endl;\\n      pcout << \\\"    ####################################################\\\" << std::endl;\\n      pcout << std::endl;\\n /* clang-format on */\\n    }\\n  } // namespace\\n \\n \\n template <int dim>\\n void MainLoop<dim>::run()\\n  {\\n \\n    pcout << \\\"Reading parameters and allocating objects... \\\" << std::flush;\\n \\n ParameterAcceptor::initialize(\\\"step-69.prm\\\");\\n    pcout << \\\"done\\\" << std::endl;\\n \\n \\n    {\\n      print_head(pcout, \\\"create triangulation\\\");\\n \\n      discretization.setup();\\n \\n if (resume)\\n        discretization.triangulation.load(base_name + \\\"-checkpoint.mesh\\\");\\n else\\n        discretization.triangulation.refine_global(discretization.refinement);\\n \\n      pcout << \\\"Number of active cells:       \\\"\\n            << discretization.triangulation.n_global_active_cells()\\n            << std::endl;\\n \\n      print_head(pcout, \\\"compute offline data\\\");\\n      offline_data.setup();\\n      offline_data.assemble();\\n \\n      pcout << \\\"Number of degrees of freedom: \\\"\\n            << offline_data.dof_handler.n_dofs() << std::endl;\\n \\n      print_head(pcout, \\\"set up time step\\\");\\n      time_stepping.prepare();\\n      schlieren_postprocessor.prepare();\\n    }\\n \\n \\n double       t            = 0.;\\n unsigned int output_cycle = 0;\\n \\n    vector_type U;\\n for (auto &it : U)\\n      it.reinit(offline_data.partitioner);\\n \\n \\n if (resume)\\n      {\\n        print_head(pcout, \\\"resume interrupted computation\\\");\\n \\n        parallel::distributed::\\n          SolutionTransfer<dim, LinearAlgebra::distributed::Vector<double>>\\n            solution_transfer(offline_data.dof_handler);\\n \\n        std::vector<LinearAlgebra::distributed::Vector<double> *> vectors;\\n        std::transform(U.begin(),\\n                       U.end(),\\n                       std::back_inserter(vectors),\\n                       [](auto &it) { return &it; });\\n        solution_transfer.deserialize(vectors);\\n \\n for (auto &it : U)\\n          it.update_ghost_values();\\n \\n        std::ifstream file(base_name + \\\"-checkpoint.metadata\\\",\\n                           std::ios::binary);\\n \\n        boost::archive::binary_iarchive ia(file);\\n        ia >> t >> output_cycle;\\n      }\\n else\\n      {\\n        print_head(pcout, \\\"interpolate initial values\\\");\\n        U = interpolate_initial_values();\\n      }\\n \\n \\n    output(U, base_name, t, output_cycle++);\\n \\n    print_head(pcout, \\\"enter main loop\\\");\\n \\n unsigned int timestep_number = 1;\\n while (t < t_final)\\n      {\\n \\n        std::ostringstream head;\\n        std::ostringstream secondary;\\n \\n        head << \\\"Cycle  \\\" << Utilities::int_to_string(timestep_number, 6)\\n             << \\\"  (\\\" \\n             << std::fixed << std::setprecision(1) << t / t_final * 100 \\n             << \\\"%)\\\";\\n        secondary << \\\"at time t = \\\" << std::setprecision(8) << std::fixed << t;\\n \\n        print_head(pcout, head.str(), secondary.str());\\n \\n \\n        t += time_stepping.make_one_step(U, t);\\n \\n \\n if (t > output_cycle * output_granularity)\\n          {\\n            checkpoint(U, base_name, t, output_cycle);\\n            output(U, base_name, t, output_cycle);\\n            ++output_cycle;\\n          }\\n \\n        ++timestep_number;\\n      }\\n \\n if (background_thread_state.valid())\\n      background_thread_state.wait();\\n \\n    computing_timer.print_summary();\\n    pcout << timer_output.str() << std::endl;\\n  }\\n \\n \\n template <int dim>\\n typename MainLoop<dim>::vector_type\\n  MainLoop<dim>::interpolate_initial_values(const double t)\\n  {\\n    pcout << \\\"MainLoop<dim>::interpolate_initial_values(t = \\\" << t << ')'\\n          << std::endl;\\n TimerOutput::Scope scope(computing_timer,\\n \\\"main_loop - setup scratch space\\\");\\n \\n    vector_type U;\\n \\n for (auto &it : U)\\n      it.reinit(offline_data.partitioner);\\n \\n constexpr auto n_solution_variables =\\n      ProblemDescription<dim>::n_solution_variables;\\n \\n for (unsigned int i = 0; i < n_solution_variables; ++i)\\n VectorTools::interpolate(offline_data.dof_handler,\\n ScalarFunctionFromFunctionObject<dim, double>(\\n                                 [&](const Point<dim> &x) {\\n                                   return initial_values.initial_state(x, t)[i];\\n                                 }),\\n                               U[i]);\\n \\n for (auto &it : U)\\n      it.update_ghost_values();\\n \\n return U;\\n  }\\n \\n \\n \\n template <int dim>\\n void MainLoop<dim>::checkpoint(const typename MainLoop<dim>::vector_type &U,\\n const std::string &name,\\n const double       t,\\n const unsigned int cycle)\\n  {\\n    print_head(pcout, \\\"checkpoint computation\\\");\\n \\n    parallel::distributed::\\n      SolutionTransfer<dim, LinearAlgebra::distributed::Vector<double>>\\n        solution_transfer(offline_data.dof_handler);\\n \\n    std::vector<const LinearAlgebra::distributed::Vector<double> *> vectors;\\n    std::transform(U.begin(),\\n                   U.end(),\\n                   std::back_inserter(vectors),\\n                   [](auto &it) { return &it; });\\n \\n    solution_transfer.prepare_for_serialization(vectors);\\n \\n    discretization.triangulation.save(name + \\\"-checkpoint.mesh\\\");\\n \\n if (Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n      {\\n        std::ofstream file(name + \\\"-checkpoint.metadata\\\", std::ios::binary);\\n        boost::archive::binary_oarchive oa(file);\\n        oa << t << cycle;\\n      }\\n  }\\n \\n \\n template <int dim>\\n void MainLoop<dim>::output(const typename MainLoop<dim>::vector_type &U,\\n const std::string                         &name,\\n const double                               t,\\n const unsigned int                         cycle)\\n  {\\n    pcout << \\\"MainLoop<dim>::output(t = \\\" << t << ')' << std::endl;\\n \\n \\n if (background_thread_state.valid())\\n      {\\n TimerOutput::Scope timer(computing_timer, \\\"main_loop - stalled output\\\");\\n        background_thread_state.wait();\\n      }\\n \\n constexpr auto n_solution_variables =\\n      ProblemDescription<dim>::n_solution_variables;\\n \\n \\n for (unsigned int i = 0; i < n_solution_variables; ++i)\\n      {\\n        output_vector[i] = U[i];\\n        output_vector[i].update_ghost_values();\\n      }\\n \\n    schlieren_postprocessor.compute_schlieren(output_vector);\\n \\n    std::unique_ptr<DataOut<dim>> data_out = std::make_unique<DataOut<dim>>();\\n    data_out->attach_dof_handler(offline_data.dof_handler);\\n \\n for (unsigned int i = 0; i < n_solution_variables; ++i)\\n      data_out->add_data_vector(output_vector[i],\\n                                ProblemDescription<dim>::component_names[i]);\\n \\n    data_out->add_data_vector(schlieren_postprocessor.schlieren,\\n \\\"schlieren_plot\\\");\\n \\n    data_out->build_patches(discretization.mapping,\\n                            discretization.finite_element.degree - 1);\\n \\n auto output_worker =\\n      [data_out_copy = std::move(data_out), this, name, t, cycle]() {\\n const DataOutBase::VtkFlags flags(\\n          t, cycle, true, DataOutBase::CompressionLevel::best_speed);\\n        data_out_copy->set_flags(flags);\\n \\n        data_out_copy->write_vtu_with_pvtu_record(\\n \\\"\\\", name + \\\"-solution\\\", cycle, mpi_communicator, 6);\\n      };\\n \\n if (asynchronous_writeback)\\n      {\\n        background_thread_state =\\n          std::async(std::launch::async, std::move(output_worker));\\n      }\\n else\\n      {\\n        output_worker();\\n      }\\n  }\\n \\n} // namespace Step69\\n \\n \\nint main(int argc, char *argv[])\\n{\\n try\\n    {\\n constexpr int dim = 2;\\n \\n using namespace dealii;\\n using namespace Step69;\\n \\n Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv);\\n \\n MPI_Comm      mpi_communicator(MPI_COMM_WORLD);\\n      MainLoop<dim> main_loop(mpi_communicator);\\n \\n      main_loop.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    };\\n}\\npartitioner.h\\nDoFHandler::reinitvoid reinit(const Triangulation< dim, spacedim > &tria)\\nIndexSet::n_elementssize_type n_elements() constDefinition index_set.h:1934\\nParameterAcceptor::parse_parameters_call_backboost::signals2::signal< void()> parse_parameters_call_backDefinition parameter_acceptor.h:478\\nTensor::normnumbers::NumberTraits< Number >::real_type norm() const\\nconditional_ostream.h\\nsolution_transfer.h\\ntria.h\\ndof_handler.h\\ndof_renumbering.h\\ndof_tools.h\\ndynamic_sparsity_pattern.h\\nfe_values.h\\nfe_q.h\\nmanifold_lib.h\\ngrid_generator.h\\nparallel.h\\nquadrature.h\\nfe.h\\niota_view.h\\nla_parallel_vector.h\\nmapping.h\\nmapping_q.h\\nLAPACKSupport::matrix@ matrixContents is actually a matrix.Definition lapack_support.h:57\\nLocalIntegrators::Divergence::normdouble norm(const FEValuesBase< dim > &fe, const ArrayView< const std::vector< Tensor< 1, dim > > > &Du)Definition divergence.h:471\\nPhysics::Elasticity::Kinematics::lTensor< 2, dim, Number > l(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\nPhysics::Elasticity::Kinematics::dSymmetricTensor< 2, dim, Number > d(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\nTrilinosWrappers::internal::endVectorType::value_type * end(VectorType &V)Definition trilinos_sparse_matrix.cc:64\\nTrilinosWrappers::internal::beginVectorType::value_type * begin(VectorType &V)Definition trilinos_sparse_matrix.cc:50\\nWorkStream::internal::tbb_no_coloring::runvoid run(const Iterator &begin, const std_cxx20::type_identity_t< Iterator > &end, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length, const unsigned int chunk_size)Definition work_stream.h:471\\ninternal::QGaussLobatto::gammalong double gamma(const unsigned int n)Definition quadrature_lib.cc:103\\ninternal::assemblevoid assemble(const MeshWorker::DoFInfoBox< dim, DOFINFO > &dinfo, A *assembler)Definition loop.h:70\\ninternal::reinitvoid reinit(MatrixBlock< MatrixType > &v, const BlockSparsityPattern &p)Definition matrix_block.h:617\\ninternal::EvaluatorQuantity::value@ value\\ndata_out.h\\nparameter_acceptor.h\\nscratch_data.h\\nsparse_matrix.h\\ntimer.h\\nvector.h\\nvector_tools.h\\nwork_stream.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"