"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_55.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-55 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-55 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-55 tutorial program\\n\\n\\nThis tutorial depends on step-40, step-22.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\nOptimal preconditioners\\nThe solver and preconditioner\\nThe testcase\\n\\n The commented program\\n\\nLinear solvers and preconditioners\\nProblem setup\\nThe main program\\nSystem Setup\\nAssembly\\nSolving\\nThe rest\\n\\n\\n Results\\n\\nPossibilities for extensions\\n\\nInvestigate Trilinos iterations\\nSolve the Oseen problem instead of the Stokes system\\nAdaptive refinement\\n\\n\\n The plain program\\n   \\n\\n\\nThis program was contributed by Timo Heister. Special thanks to Sander Rhebergen for the inspiration to finally write this tutorial.\\nThis material is based upon work partially supported by National Science Foundation grant DMS1522191 and the Computational Infrastructure in Geodynamics initiative (CIG), through the National Science Foundation under Award No. EAR-0949446 and The University of California-Davis.\\nThe authors would like to thank the Isaac Newton Institute for Mathematical Sciences, Cambridge, for support and hospitality during the programme Melt in the Mantle where work on this tutorial was undertaken. This work was supported by EPSRC grant no EP/K032208/1. \\nNoteAs a prerequisite of this program, you need to have PETSc or Trilinos and the p4est library installed. The installation of deal.II together with these additional libraries is described in the README file.\\n Introduction\\nBuilding on step-40, this tutorial shows how to solve linear PDEs with several components in parallel using MPI with PETSc or Trilinos for the linear algebra. For this, we return to the Stokes equations as discussed in step-22. The motivation for writing this tutorial is to provide an intermediate step (pun intended) between step-40 (parallel Laplace) and step-32 (parallel coupled Stokes with Boussinesq for a time dependent problem).\\nThe learning outcomes for this tutorial are:\\n\\nYou are able to solve PDEs with several variables in parallel and can apply this to different problems.\\nYou understand the concept of optimal preconditioners and are able to check this for a particular problem.\\nYou are able to construct manufactured solutions using the free computer algreba system SymPy (https://sympy.org).\\nYou can implement various other tasks for parallel programs: error computation, writing graphical output, etc.\\nYou can visualize vector fields, stream lines, and contours of vector quantities.\\n\\nWe are solving for a velocity \\\\(\\\\textbf{u}\\\\) and pressure \\\\(p\\\\) that satisfy the Stokes equation, which reads    \\n\\\\begin{eqnarray*}\\n  - \\\\triangle \\\\textbf{u} + \\\\nabla p &=& \\\\textbf{f}, \\\\\\\\\\n  -\\\\textrm{div}\\\\; \\\\textbf{u} &=& 0.\\n\\\\end{eqnarray*}\\n\\nOptimal preconditioners\\nMake sure that you read (even better: try) what is described in \\\"Block Schur\\ncomplement preconditioner\\\" in the \\\"Possible Extensions\\\" section in step-22. Like described there, we are going to solve the block system using a Krylov method and a block preconditioner.\\nOur goal here is to construct a very simple (maybe the simplest?) optimal preconditioner for the linear system. A preconditioner is called \\\"optimal\\\" or \\\"of optimal complexity\\\", if the number of iterations of the preconditioned system is independent of the mesh size \\\\(h\\\\). You can extend that definition to also require independence of the number of processors used (we will discuss that in the results section), the computational domain and the mesh quality, the test case itself, the polynomial degree of the finite element space, and more.\\nWhy is a constant number of iterations considered to be \\\"optimal\\\"? Assume the discretized PDE gives a linear system with N unknowns. Because the matrix coming from the FEM discretization is sparse, a matrix-vector product can be done in O(N) time. A preconditioner application can also only be O(N) at best (for example doable with multigrid methods). If the number of iterations required to solve the linear system is independent of \\\\(h\\\\) (and therefore N), the total cost of solving the system will be O(N). It is not possible to beat this complexity, because even looking at all the entries of the right-hand side already takes O(N) time. For more information see [79], Chapter 2.5 (Multigrid).\\nThe preconditioner described here is even simpler than the one described in step-22 and will typically require more iterations and consequently time to solve. When considering preconditioners, optimality is not the only important metric. But an optimal and expensive preconditioner is typically more desirable than a cheaper, non-optimal one. This is because, eventually, as the mesh size becomes smaller and smaller and linear problems become bigger and bigger, the former will eventually beat the latter.\\nThe solver and preconditioner\\nWe precondition the linear system            \\n\\\\begin{eqnarray*}\\n  \\\\left(\\\\begin{array}{cc}\\n    A & B^T \\\\\\\\ B & 0\\n  \\\\end{array}\\\\right)\\n  \\\\left(\\\\begin{array}{c}\\n    U \\\\\\\\ P\\n  \\\\end{array}\\\\right)\\n  =\\n  \\\\left(\\\\begin{array}{c}\\n    F \\\\\\\\ 0\\n  \\\\end{array}\\\\right),\\n\\\\end{eqnarray*}\\n\\nwith the block diagonal preconditioner           \\n\\\\begin{eqnarray*}\\n  P^{-1}\\n  =\\n  \\\\left(\\\\begin{array}{cc}\\n    A & 0 \\\\\\\\ 0 & S\\n  \\\\end{array}\\\\right) ^{-1},\\n  =\\n  \\\\left(\\\\begin{array}{cc}\\n    A^{-1} & 0 \\\\\\\\ 0 & S^{-1}\\n  \\\\end{array}\\\\right),\\n\\\\end{eqnarray*}\\n\\n where \\\\(S=-BA^{-1} B^T\\\\) is the Schur complement.\\nWith this choice of \\\\(P\\\\), assuming that we handle \\\\(A^{-1}\\\\) and \\\\(S^{-1}\\\\) exactly (which is an \\\"idealized\\\" situation), the preconditioned linear system has three distinct eigenvalues independent of \\\\(h\\\\) and is therefore \\\"optimal\\\". See section 6.2.1 (especially p. 292) in [79]. For comparison, using the ideal version of the upper block-triangular preconditioner in step-22 (also used in step-56) would have all eigenvalues be equal to one.\\nWe will use approximations of the inverse operations in \\\\(P^{-1}\\\\) that are (nearly) independent of \\\\(h\\\\). In this situation, one can again show, that the eigenvalues are independent of \\\\(h\\\\). For the Krylov method we choose MINRES, which is attractive for the analysis (iteration count is proven to be independent of \\\\(h\\\\), see the remainder of the chapter 6.2.1 in [79]), great from the computational standpoint (simpler and cheaper than GMRES for example), and applicable (matrix and preconditioner are symmetric).\\nFor the approximations we will use a CG solve with the mass matrix in the pressure space for approximating the action of \\\\(S^{-1}\\\\). Note that the mass matrix is spectrally equivalent to \\\\(S\\\\). We can expect the number of CG iterations to be independent of \\\\(h\\\\), even with a simple preconditioner like ILU.\\nFor the approximation of the velocity block \\\\(A\\\\) we will perform a single AMG V-cycle. In practice this choice is not exactly independent of \\\\(h\\\\), which can explain the slight increase in iteration numbers. A possible explanation is that the coarsest level will be solved exactly and the number of levels and size of the coarsest matrix is not predictable.\\nThe testcase\\nWe will construct a manufactured solution based on the classical Kovasznay problem, see [134]. Here is an image of the solution colored by the x velocity including streamlines of the velocity:\\n\\nWe have to cheat here, though, because we are not solving the non-linear Navier-Stokes equations, but the linear Stokes system without convective term. Therefore, to recreate the exact same solution, we use the method of manufactured solutions with the solution of the Kovasznay problem. This will effectively move the convective term into the right-hand side \\\\(f\\\\).\\nThe right-hand side is computed using the script \\\"reference.py\\\" and we use the exact solution for boundary conditions and error computation.\\n The commented program\\n\\u00a0 #include <deal.II/base/quadrature_lib.h>\\n\\u00a0 #include <deal.II/base/function.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 \\nThe following chunk out code is identical to step-40 and allows switching between PETSc and Trilinos:\\n\\u00a0 #include <deal.II/lac/generic_linear_algebra.h>\\n\\u00a0 \\n\\u00a0 /* #define FORCE_USE_OF_TRILINOS */\\n\\u00a0 \\n\\u00a0 namespace LA\\n\\u00a0 {\\n\\u00a0 #if defined(DEAL_II_WITH_PETSC) && !defined(DEAL_II_PETSC_WITH_COMPLEX) && \\\\\\n\\u00a0   !(defined(DEAL_II_WITH_TRILINOS) && defined(FORCE_USE_OF_TRILINOS))\\n\\u00a0   using namespace dealii::LinearAlgebraPETSc;\\n\\u00a0 #  define USE_PETSC_LA\\n\\u00a0 #elif defined(DEAL_II_WITH_TRILINOS)\\n\\u00a0   using namespace dealii::LinearAlgebraTrilinos;\\n\\u00a0 #else\\n\\u00a0 #  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required\\n\\u00a0 #endif\\n\\u00a0 } // namespace LA\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/vector.h>\\n\\u00a0 #include <deal.II/lac/full_matrix.h>\\n\\u00a0 #include <deal.II/lac/solver_cg.h>\\n\\u00a0 #include <deal.II/lac/solver_gmres.h>\\n\\u00a0 #include <deal.II/lac/solver_minres.h>\\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/lac/dynamic_sparsity_pattern.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/petsc_sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/petsc_vector.h>\\n\\u00a0 #include <deal.II/lac/petsc_solver.h>\\n\\u00a0 #include <deal.II/lac/petsc_precondition.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/manifold_lib.h>\\n\\u00a0 #include <deal.II/grid/grid_tools.h>\\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/dofs/dof_renumbering.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 #include <deal.II/fe/fe_values.h>\\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 #include <deal.II/fe/fe_system.h>\\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/error_estimator.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/base/utilities.h>\\n\\u00a0 #include <deal.II/base/conditional_ostream.h>\\n\\u00a0 #include <deal.II/base/index_set.h>\\n\\u00a0 #include <deal.II/lac/sparsity_tools.h>\\n\\u00a0 #include <deal.II/distributed/tria.h>\\n\\u00a0 #include <deal.II/distributed/grid_refinement.h>\\n\\u00a0 \\n\\u00a0 #include <cmath>\\n\\u00a0 #include <fstream>\\n\\u00a0 #include <iostream>\\n\\u00a0 \\n\\u00a0 namespace Step55\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n Linear solvers and preconditioners\\nWe need a few helper classes to represent our solver strategy described in the introduction.\\n\\u00a0   namespace LinearSolvers\\n\\u00a0   {\\nThis class exposes the action of applying the inverse of a giving matrix via the function InverseMatrix::vmult(). Internally, the inverse is not formed explicitly. Instead, a linear solver with CG is performed. This class extends the InverseMatrix class in step-22 with an option to specify a preconditioner, and to allow for different vector types in the vmult function. We use the same mechanism as in step-31 to convert a run-time exception into a failed assertion should the inner solver not converge.\\n\\u00a0     template <class Matrix, class Preconditioner>\\n\\u00a0     class InverseMatrix : public Subscriptor\\n\\u00a0     {\\n\\u00a0     public:\\n\\u00a0       InverseMatrix(const Matrix &m, const Preconditioner &preconditioner);\\n\\u00a0 \\n\\u00a0       template <typename VectorType>\\n\\u00a0       void vmult(VectorType &dst, const VectorType &src) const;\\n\\u00a0 \\n\\u00a0     private:\\n\\u00a0       const SmartPointer<const Matrix> matrix;\\n\\u00a0       const Preconditioner            &preconditioner;\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <class Matrix, class Preconditioner>\\n\\u00a0     InverseMatrix<Matrix, Preconditioner>::InverseMatrix(\\n\\u00a0       const Matrix         &m,\\n\\u00a0       const Preconditioner &preconditioner)\\n\\u00a0       : matrix(&m)\\n\\u00a0       , preconditioner(preconditioner)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <class Matrix, class Preconditioner>\\n\\u00a0     template <typename VectorType>\\n\\u00a0     void\\n\\u00a0     InverseMatrix<Matrix, Preconditioner>::vmult(VectorType       &dst,\\n\\u00a0                                                  const VectorType &src) const\\n\\u00a0     {\\n\\u00a0       SolverControl        solver_control(src.size(), 1e-8 * src.l2_norm());\\n\\u00a0       SolverCG<VectorType> cg(solver_control);\\n\\u00a0       dst = 0;\\n\\u00a0 \\n\\u00a0       try\\n\\u00a0         {\\n\\u00a0           cg.solve(*matrix, dst, src, preconditioner);\\n\\u00a0         }\\n\\u00a0       catch (std::exception &e)\\n\\u00a0         {\\n\\u00a0           Assert(false, ExcMessage(e.what()));\\n\\u00a0         }\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\nSmartPointerDefinition smartpointer.h:93\\nSolverCGDefinition solver_cg.h:179\\nSolverControlDefinition solver_control.h:67\\nSubscriptorDefinition subscriptor.h:60\\nAssert#define Assert(cond, exc)Definition exceptions.h:1638\\nPhysics::Elasticity::Kinematics::eSymmetricTensor< 2, dim, Number > e(const Tensor< 2, dim, Number > &F)\\nThe class A template class for a simple block diagonal preconditioner for 2x2 matrices.\\n\\u00a0     template <class PreconditionerA, class PreconditionerS>\\n\\u00a0     class BlockDiagonalPreconditioner : public Subscriptor\\n\\u00a0     {\\n\\u00a0     public:\\n\\u00a0       BlockDiagonalPreconditioner(const PreconditionerA &preconditioner_A,\\n\\u00a0                                   const PreconditionerS &preconditioner_S);\\n\\u00a0 \\n\\u00a0       void vmult(LA::MPI::BlockVector       &dst,\\n\\u00a0                  const LA::MPI::BlockVector &src) const;\\n\\u00a0 \\n\\u00a0     private:\\n\\u00a0       const PreconditionerA &preconditioner_A;\\n\\u00a0       const PreconditionerS &preconditioner_S;\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0     template <class PreconditionerA, class PreconditionerS>\\n\\u00a0     BlockDiagonalPreconditioner<PreconditionerA, PreconditionerS>::\\n\\u00a0       BlockDiagonalPreconditioner(const PreconditionerA &preconditioner_A,\\n\\u00a0                                   const PreconditionerS &preconditioner_S)\\n\\u00a0       : preconditioner_A(preconditioner_A)\\n\\u00a0       , preconditioner_S(preconditioner_S)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <class PreconditionerA, class PreconditionerS>\\n\\u00a0     void BlockDiagonalPreconditioner<PreconditionerA, PreconditionerS>::vmult(\\n\\u00a0       LA::MPI::BlockVector       &dst,\\n\\u00a0       const LA::MPI::BlockVector &src) const\\n\\u00a0     {\\n\\u00a0       preconditioner_A.vmult(dst.block(0), src.block(0));\\n\\u00a0       preconditioner_S.vmult(dst.block(1), src.block(1));\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   } // namespace LinearSolvers\\n\\u00a0 \\nBlockVectorDefinition block_vector.h:71\\nInitializeLibrary::MPI@ MPI\\n Problem setup\\nThe following classes represent the right hand side and the exact solution for the test problem.\\n\\u00a0   template <int dim>\\n\\u00a0   class RightHandSide : public Function<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     RightHandSide()\\n\\u00a0       : Function<dim>(dim + 1)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0     virtual void vector_value(const Point<dim> &p,\\n\\u00a0                               Vector<double>   &value) const override;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void RightHandSide<dim>::vector_value(const Point<dim> &p,\\n\\u00a0                                         Vector<double>   &values) const\\n\\u00a0   {\\n\\u00a0     const double R_x = p[0];\\n\\u00a0     const double R_y = p[1];\\n\\u00a0 \\n\\u00a0     constexpr double pi  = numbers::PI;\\n\\u00a0     constexpr double pi2 = numbers::PI * numbers::PI;\\n\\u00a0 \\nFunctionDefinition function.h:152\\nFunction::vector_valuevirtual void vector_value(const Point< dim > &p, Vector< RangeNumberType > &values) const\\nPointDefinition point.h:111\\nVectorDefinition vector.h:120\\nnumbers::PIstatic constexpr double PIDefinition numbers.h:259\\nvelocity\\n\\u00a0     values[0] = -1.0L / 2.0L * (-2 * std::sqrt(25.0 + 4 * pi2) + 10.0) *\\n\\u00a0                   std::exp(R_x * (-2 * std::sqrt(25.0 + 4 * pi2) + 10.0)) -\\n\\u00a0                 0.4 * pi2 * std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n\\u00a0                   std::cos(2 * R_y * pi) +\\n\\u00a0                 0.1 *\\n\\u00a0                   Utilities::fixed_power<2>(-std::sqrt(25.0 + 4 * pi2) + 5.0) *\\n\\u00a0                   std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n\\u00a0                   std::cos(2 * R_y * pi);\\n\\u00a0     values[1] = 0.2 * pi * (-std::sqrt(25.0 + 4 * pi2) + 5.0) *\\n\\u00a0                   std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n\\u00a0                   std::sin(2 * R_y * pi) -\\n\\u00a0                 0.05 *\\n\\u00a0                   Utilities::fixed_power<3>(-std::sqrt(25.0 + 4 * pi2) + 5.0) *\\n\\u00a0                   std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n\\u00a0                   std::sin(2 * R_y * pi) / pi;\\n\\u00a0 \\nUtilities::fixed_powerconstexpr T fixed_power(const T t)Definition utilities.h:942\\nstd::exp::VectorizedArray< Number, width > exp(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6829\\nstd::cos::VectorizedArray< Number, width > cos(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6609\\nstd::sin::VectorizedArray< Number, width > sin(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6589\\nstd::sqrt::VectorizedArray< Number, width > sqrt(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6869\\npressure\\n\\u00a0     values[dim] = 0;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   class ExactSolution : public Function<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     ExactSolution()\\n\\u00a0       : Function<dim>(dim + 1)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0     virtual void vector_value(const Point<dim> &p,\\n\\u00a0                               Vector<double>   &values) const override;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void ExactSolution<dim>::vector_value(const Point<dim> &p,\\n\\u00a0                                         Vector<double>   &values) const\\n\\u00a0   {\\n\\u00a0     const double R_x = p[0];\\n\\u00a0     const double R_y = p[1];\\n\\u00a0 \\n\\u00a0     constexpr double pi  = numbers::PI;\\n\\u00a0     constexpr double pi2 = numbers::PI * numbers::PI;\\n\\u00a0 \\nvelocity\\n\\u00a0     values[0] = -std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n\\u00a0                   std::cos(2 * R_y * pi) +\\n\\u00a0                 1;\\n\\u00a0     values[1] = (1.0L / 2.0L) * (-std::sqrt(25.0 + 4 * pi2) + 5.0) *\\n\\u00a0                 std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n\\u00a0                 std::sin(2 * R_y * pi) / pi;\\n\\u00a0 \\npressure\\n\\u00a0     values[dim] =\\n\\u00a0       -1.0L / 2.0L * std::exp(R_x * (-2 * std::sqrt(25.0 + 4 * pi2) + 10.0)) -\\n\\u00a0       2.0 *\\n\\u00a0         (-6538034.74494422 +\\n\\u00a0          0.0134758939981709 * std::exp(4 * std::sqrt(25.0 + 4 * pi2))) /\\n\\u00a0         (-80.0 * std::exp(3 * std::sqrt(25.0 + 4 * pi2)) +\\n\\u00a0          16.0 * std::sqrt(25.0 + 4 * pi2) *\\n\\u00a0            std::exp(3 * std::sqrt(25.0 + 4 * pi2))) -\\n\\u00a0       1634508.68623606 * std::exp(-3.0 * std::sqrt(25.0 + 4 * pi2)) /\\n\\u00a0         (-10.0 + 2.0 * std::sqrt(25.0 + 4 * pi2)) +\\n\\u00a0       (-0.00673794699908547 * std::exp(std::sqrt(25.0 + 4 * pi2)) +\\n\\u00a0        3269017.37247211 * std::exp(-3 * std::sqrt(25.0 + 4 * pi2))) /\\n\\u00a0         (-8 * std::sqrt(25.0 + 4 * pi2) + 40.0) +\\n\\u00a0       0.00336897349954273 * std::exp(1.0 * std::sqrt(25.0 + 4 * pi2)) /\\n\\u00a0         (-10.0 + 2.0 * std::sqrt(25.0 + 4 * pi2));\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n The main program\\nThe main class is very similar to step-40, except that matrices and vectors are now block versions, and we store a std::vector<IndexSet> for owned and relevant DoFs instead of a single IndexSet. We have exactly two IndexSets, one for all velocity unknowns and one for all pressure unknowns.\\n\\u00a0   template <int dim>\\n\\u00a0   class StokesProblem\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     StokesProblem(unsigned int velocity_degree);\\n\\u00a0 \\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void make_grid();\\n\\u00a0     void setup_system();\\n\\u00a0     void assemble_system();\\n\\u00a0     void solve();\\n\\u00a0     void refine_grid();\\n\\u00a0     void output_results(const unsigned int cycle);\\n\\u00a0 \\n\\u00a0     const unsigned int velocity_degree;\\n\\u00a0     const double       viscosity;\\n\\u00a0     MPI_Comm           mpi_communicator;\\n\\u00a0 \\n\\u00a0     const FESystem<dim>                       fe;\\n\\u00a0     parallel::distributed::Triangulation<dim> triangulation;\\n\\u00a0     DoFHandler<dim>                           dof_handler;\\n\\u00a0 \\n\\u00a0     std::vector<IndexSet> owned_partitioning;\\n\\u00a0     std::vector<IndexSet> relevant_partitioning;\\n\\u00a0 \\n\\u00a0     AffineConstraints<double> constraints;\\n\\u00a0 \\n\\u00a0     LA::MPI::BlockSparseMatrix system_matrix;\\n\\u00a0     LA::MPI::BlockSparseMatrix preconditioner_matrix;\\n\\u00a0     LA::MPI::BlockVector       locally_relevant_solution;\\n\\u00a0     LA::MPI::BlockVector       system_rhs;\\n\\u00a0 \\n\\u00a0     ConditionalOStream pcout;\\n\\u00a0     TimerOutput        computing_timer;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   StokesProblem<dim>::StokesProblem(unsigned int velocity_degree)\\n\\u00a0     : velocity_degree(velocity_degree)\\n\\u00a0     , viscosity(0.1)\\n\\u00a0     , mpi_communicator(MPI_COMM_WORLD)\\n\\u00a0     , fe(FE_Q<dim>(velocity_degree) ^ dim, FE_Q<dim>(velocity_degree - 1))\\n\\u00a0     , triangulation(mpi_communicator,\\n\\u00a0                     typename Triangulation<dim>::MeshSmoothing(\\n\\u00a0                       Triangulation<dim>::smoothing_on_refinement |\\n\\u00a0                       Triangulation<dim>::smoothing_on_coarsening))\\n\\u00a0     , dof_handler(triangulation)\\n\\u00a0     , pcout(std::cout,\\n\\u00a0             (Utilities::MPI::this_mpi_process(mpi_communicator) == 0))\\n\\u00a0     , computing_timer(mpi_communicator,\\n\\u00a0                       pcout,\\n\\u00a0                       TimerOutput::never,\\n\\u00a0                       TimerOutput::wall_times)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nConditionalOStreamDefinition conditional_ostream.h:80\\nDoFHandlerDefinition dof_handler.h:317\\nFESystemDefinition fe_system.h:208\\nFE_QDefinition fe_q.h:554\\nMPI_Comm\\nTimerOutputDefinition timer.h:549\\nTriangulationDefinition tria.h:1323\\nparallel::distributed::TriangulationDefinition tria.h:268\\nUtilitiesDefinition communication_pattern_base.h:30\\nstdSTL namespace.\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\nThe Kovasznay flow is defined on the domain [-0.5, 1.5]^2, which we create by passing the min and max values to GridGenerator::hyper_cube.\\n\\u00a0   template <int dim>\\n\\u00a0   void StokesProblem<dim>::make_grid()\\n\\u00a0   {\\n\\u00a0     GridGenerator::hyper_cube(triangulation, -0.5, 1.5);\\n\\u00a0     triangulation.refine_global(3);\\n\\u00a0   }\\n\\u00a0 \\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nGridGenerator::hyper_cubevoid hyper_cube(Triangulation< dim, spacedim > &tria, const double left=0., const double right=1., const bool colorize=false)\\n System Setup\\nThe construction of the block matrices and vectors is new compared to step-40 and is different compared to serial codes like step-22, because we need to supply the set of rows that belong to our processor.\\n\\u00a0   template <int dim>\\n\\u00a0   void StokesProblem<dim>::setup_system()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"setup\\\");\\n\\u00a0 \\n\\u00a0     dof_handler.distribute_dofs(fe);\\n\\u00a0 \\nTimerOutput::ScopeDefinition timer.h:557\\nPut all dim velocities into block 0 and the pressure into block 1, then reorder the unknowns by block. Finally count how many unknowns we have per block.\\n\\u00a0     std::vector<unsigned int> stokes_sub_blocks(dim + 1, 0);\\n\\u00a0     stokes_sub_blocks[dim] = 1;\\n\\u00a0     DoFRenumbering::component_wise(dof_handler, stokes_sub_blocks);\\n\\u00a0 \\n\\u00a0     const std::vector<types::global_dof_index> dofs_per_block =\\n\\u00a0       DoFTools::count_dofs_per_fe_block(dof_handler, stokes_sub_blocks);\\n\\u00a0 \\n\\u00a0     const unsigned int n_u = dofs_per_block[0];\\n\\u00a0     const unsigned int n_p = dofs_per_block[1];\\n\\u00a0 \\n\\u00a0     pcout << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs() << \\\" (\\\"\\n\\u00a0           << n_u << '+' << n_p << ')' << std::endl;\\n\\u00a0 \\nDoFRenumbering::component_wisevoid component_wise(DoFHandler< dim, spacedim > &dof_handler, const std::vector< unsigned int > &target_component=std::vector< unsigned int >())Definition dof_renumbering.cc:666\\nDoFTools::count_dofs_per_fe_blockstd::vector< types::global_dof_index > count_dofs_per_fe_block(const DoFHandler< dim, spacedim > &dof, const std::vector< unsigned int > &target_block=std::vector< unsigned int >())Definition dof_tools.cc:2104\\nWe split up the IndexSet for locally owned and locally relevant DoFs into two IndexSets based on how we want to create the block matrices and vectors.\\n\\u00a0     const IndexSet &locally_owned_dofs = dof_handler.locally_owned_dofs();\\n\\u00a0     owned_partitioning                 = {locally_owned_dofs.get_view(0, n_u),\\n\\u00a0                                           locally_owned_dofs.get_view(n_u, n_u + n_p)};\\n\\u00a0 \\n\\u00a0     const IndexSet locally_relevant_dofs =\\n\\u00a0       DoFTools::extract_locally_relevant_dofs(dof_handler);\\n\\u00a0     relevant_partitioning = {locally_relevant_dofs.get_view(0, n_u),\\n\\u00a0                              locally_relevant_dofs.get_view(n_u, n_u + n_p)};\\n\\u00a0 \\nIndexSetDefinition index_set.h:70\\nIndexSet::get_viewIndexSet get_view(const size_type begin, const size_type end) constDefinition index_set.cc:273\\nDoFTools::extract_locally_relevant_dofsIndexSet extract_locally_relevant_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1164\\nSetting up the constraints for boundary conditions and hanging nodes is identical to step-40. Even though we don't have any hanging nodes because we only perform global refinement, it is still a good idea to put this function call in, in case adaptive refinement gets introduced later.\\n\\u00a0     {\\n\\u00a0       constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n\\u00a0 \\n\\u00a0       const FEValuesExtractors::Vector velocities(0);\\n\\u00a0       DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n\\u00a0       VectorTools::interpolate_boundary_values(dof_handler,\\n\\u00a0                                                0,\\n\\u00a0                                                ExactSolution<dim>(),\\n\\u00a0                                                constraints,\\n\\u00a0                                                fe.component_mask(velocities));\\n\\u00a0       constraints.close();\\n\\u00a0     }\\n\\u00a0 \\nDoFTools::make_hanging_node_constraintsvoid make_hanging_node_constraints(const DoFHandler< dim, spacedim > &dof_handler, AffineConstraints< number > &constraints)Definition dof_tools_constraints.cc:3073\\nVectorTools::interpolate_boundary_valuesvoid interpolate_boundary_values(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const std::map< types::boundary_id, const Function< spacedim, number > * > &function_map, std::map< types::global_dof_index, number > &boundary_values, const ComponentMask &component_mask={})\\nFEValuesExtractors::VectorDefinition fe_values_extractors.h:150\\nNow we create the system matrix based on a BlockDynamicSparsityPattern. We know that we won't have coupling between different velocity components (because we use the laplace and not the deformation tensor) and no coupling between pressure with its test functions, so we use a Table to communicate this coupling information to DoFTools::make_sparsity_pattern.\\n\\u00a0     {\\n\\u00a0       system_matrix.clear();\\n\\u00a0 \\n\\u00a0       Table<2, DoFTools::Coupling> coupling(dim + 1, dim + 1);\\n\\u00a0       for (unsigned int c = 0; c < dim + 1; ++c)\\n\\u00a0         for (unsigned int d = 0; d < dim + 1; ++d)\\n\\u00a0           if (c == dim && d == dim)\\n\\u00a0             coupling[c][d] = DoFTools::none;\\n\\u00a0           else if (c == dim || d == dim || c == d)\\n\\u00a0             coupling[c][d] = DoFTools::always;\\n\\u00a0           else\\n\\u00a0             coupling[c][d] = DoFTools::none;\\n\\u00a0 \\n\\u00a0       BlockDynamicSparsityPattern dsp(relevant_partitioning);\\n\\u00a0 \\n\\u00a0       DoFTools::make_sparsity_pattern(\\n\\u00a0         dof_handler, coupling, dsp, constraints, false);\\n\\u00a0 \\n\\u00a0       SparsityTools::distribute_sparsity_pattern(\\n\\u00a0         dsp,\\n\\u00a0         dof_handler.locally_owned_dofs(),\\n\\u00a0         mpi_communicator,\\n\\u00a0         locally_relevant_dofs);\\n\\u00a0 \\n\\u00a0       system_matrix.reinit(owned_partitioning, dsp, mpi_communicator);\\n\\u00a0     }\\n\\u00a0 \\nBlockDynamicSparsityPatternDefinition block_sparsity_pattern.h:549\\nTableDefinition array_view.h:39\\nDoFTools::make_sparsity_patternvoid make_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, SparsityPatternBase &sparsity_pattern, const AffineConstraints< number > &constraints={}, const bool keep_constrained_dofs=true, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id)Definition dof_tools_sparsity.cc:56\\nDoFTools::none@ noneDefinition dof_tools.h:243\\nDoFTools::always@ alwaysDefinition dof_tools.h:247\\nSparsityTools::distribute_sparsity_patternvoid distribute_sparsity_pattern(DynamicSparsityPattern &dsp, const IndexSet &locally_owned_rows, const MPI_Comm mpi_comm, const IndexSet &locally_relevant_rows)Definition sparsity_tools.cc:1020\\nThe preconditioner matrix has a different coupling (we only fill in the 1,1 block with the mass matrix), otherwise this code is identical to the construction of the system_matrix above.\\n\\u00a0     {\\n\\u00a0       preconditioner_matrix.clear();\\n\\u00a0 \\n\\u00a0       Table<2, DoFTools::Coupling> coupling(dim + 1, dim + 1);\\n\\u00a0       for (unsigned int c = 0; c < dim + 1; ++c)\\n\\u00a0         for (unsigned int d = 0; d < dim + 1; ++d)\\n\\u00a0           if (c == dim && d == dim)\\n\\u00a0             coupling[c][d] = DoFTools::always;\\n\\u00a0           else\\n\\u00a0             coupling[c][d] = DoFTools::none;\\n\\u00a0 \\n\\u00a0       BlockDynamicSparsityPattern dsp(relevant_partitioning);\\n\\u00a0 \\n\\u00a0       DoFTools::make_sparsity_pattern(\\n\\u00a0         dof_handler, coupling, dsp, constraints, false);\\n\\u00a0       SparsityTools::distribute_sparsity_pattern(\\n\\u00a0         dsp,\\n\\u00a0         Utilities::MPI::all_gather(mpi_communicator,\\n\\u00a0                                    dof_handler.locally_owned_dofs()),\\n\\u00a0         mpi_communicator,\\n\\u00a0         locally_relevant_dofs);\\n\\u00a0       preconditioner_matrix.reinit(owned_partitioning, dsp, mpi_communicator);\\n\\u00a0     }\\n\\u00a0 \\nUtilities::MPI::all_gatherstd::vector< T > all_gather(const MPI_Comm comm, const T &object_to_send)\\nFinally, we construct the block vectors with the right sizes. The function call with two std::vector<IndexSet> will create a ghosted vector.\\n\\u00a0     locally_relevant_solution.reinit(owned_partitioning,\\n\\u00a0                                      relevant_partitioning,\\n\\u00a0                                      mpi_communicator);\\n\\u00a0     system_rhs.reinit(owned_partitioning, mpi_communicator);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Assembly\\nThis function assembles the system matrix, the preconditioner matrix, and the right hand side. The code is pretty standard.\\n\\u00a0   template <int dim>\\n\\u00a0   void StokesProblem<dim>::assemble_system()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"assembly\\\");\\n\\u00a0 \\n\\u00a0     system_matrix         = 0;\\n\\u00a0     preconditioner_matrix = 0;\\n\\u00a0     system_rhs            = 0;\\n\\u00a0 \\n\\u00a0     const QGauss<dim> quadrature_formula(velocity_degree + 1);\\n\\u00a0 \\n\\u00a0     FEValues<dim> fe_values(fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_values | update_gradients |\\n\\u00a0                               update_quadrature_points | update_JxW_values);\\n\\u00a0 \\n\\u00a0     const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     FullMatrix<double> system_cell_matrix(dofs_per_cell, dofs_per_cell);\\n\\u00a0     FullMatrix<double> preconditioner_cell_matrix(dofs_per_cell, dofs_per_cell);\\n\\u00a0     Vector<double>     cell_rhs(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     const RightHandSide<dim>    right_hand_side;\\n\\u00a0     std::vector<Vector<double>> rhs_values(n_q_points, Vector<double>(dim + 1));\\n\\u00a0 \\n\\u00a0     std::vector<Tensor<2, dim>> grad_phi_u(dofs_per_cell);\\n\\u00a0     std::vector<double>         div_phi_u(dofs_per_cell);\\n\\u00a0     std::vector<double>         phi_p(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0     const FEValuesExtractors::Vector     velocities(0);\\n\\u00a0     const FEValuesExtractors::Scalar     pressure(dim);\\n\\u00a0 \\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       if (cell->is_locally_owned())\\n\\u00a0         {\\n\\u00a0           system_cell_matrix         = 0;\\n\\u00a0           preconditioner_cell_matrix = 0;\\n\\u00a0           cell_rhs                   = 0;\\n\\u00a0 \\n\\u00a0           fe_values.reinit(cell);\\n\\u00a0           right_hand_side.vector_value_list(fe_values.get_quadrature_points(),\\n\\u00a0                                             rhs_values);\\n\\u00a0           for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0             {\\n\\u00a0               for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n\\u00a0                 {\\n\\u00a0                   grad_phi_u[k] = fe_values[velocities].gradient(k, q);\\n\\u00a0                   div_phi_u[k]  = fe_values[velocities].divergence(k, q);\\n\\u00a0                   phi_p[k]      = fe_values[pressure].value(k, q);\\n\\u00a0                 }\\n\\u00a0 \\n\\u00a0               for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0                 {\\n\\u00a0                   for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0                     {\\n\\u00a0                       system_cell_matrix(i, j) +=\\n\\u00a0                         (viscosity *\\n\\u00a0                            scalar_product(grad_phi_u[i], grad_phi_u[j]) -\\n\\u00a0                          div_phi_u[i] * phi_p[j] - phi_p[i] * div_phi_u[j]) *\\n\\u00a0                         fe_values.JxW(q);\\n\\u00a0 \\n\\u00a0                       preconditioner_cell_matrix(i, j) += 1.0 / viscosity *\\n\\u00a0                                                           phi_p[i] * phi_p[j] *\\n\\u00a0                                                           fe_values.JxW(q);\\n\\u00a0                     }\\n\\u00a0 \\n\\u00a0                   const unsigned int component_i =\\n\\u00a0                     fe.system_to_component_index(i).first;\\n\\u00a0                   cell_rhs(i) += fe_values.shape_value(i, q) *\\n\\u00a0                                  rhs_values[q](component_i) * fe_values.JxW(q);\\n\\u00a0                 }\\n\\u00a0             }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0           cell->get_dof_indices(local_dof_indices);\\n\\u00a0           constraints.distribute_local_to_global(system_cell_matrix,\\n\\u00a0                                                  cell_rhs,\\n\\u00a0                                                  local_dof_indices,\\n\\u00a0                                                  system_matrix,\\n\\u00a0                                                  system_rhs);\\n\\u00a0 \\n\\u00a0           constraints.distribute_local_to_global(preconditioner_cell_matrix,\\n\\u00a0                                                  local_dof_indices,\\n\\u00a0                                                  preconditioner_matrix);\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0     system_matrix.compress(VectorOperation::add);\\n\\u00a0     preconditioner_matrix.compress(VectorOperation::add);\\n\\u00a0     system_rhs.compress(VectorOperation::add);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFEValuesDefinition fe_values.h:63\\nFullMatrixDefinition full_matrix.h:79\\nQGaussDefinition quadrature_lib.h:40\\nupdate_values@ update_valuesShape function values.Definition fe_update_flags.h:75\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\nFEValuesExtractors::ScalarDefinition fe_values_extractors.h:95\\nVectorOperation::add@ addDefinition vector_operation.h:53\\n Solving\\nThis function solves the linear system with MINRES with a block diagonal preconditioner and AMG for the two diagonal blocks as described in the introduction. The preconditioner applies a v cycle to the 0,0 block and a CG with the mass matrix for the 1,1 block (the Schur complement).\\n\\u00a0   template <int dim>\\n\\u00a0   void StokesProblem<dim>::solve()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"solve\\\");\\n\\u00a0 \\n\\u00a0     LA::MPI::PreconditionAMG prec_A;\\n\\u00a0     {\\n\\u00a0       LA::MPI::PreconditionAMG::AdditionalData data;\\n\\u00a0 \\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0       data.symmetric_operator = true;\\n\\u00a0 #endif\\n\\u00a0       prec_A.initialize(system_matrix.block(0, 0), data);\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     LA::MPI::PreconditionAMG prec_S;\\n\\u00a0     {\\n\\u00a0       LA::MPI::PreconditionAMG::AdditionalData data;\\n\\u00a0 \\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0       data.symmetric_operator = true;\\n\\u00a0 #endif\\n\\u00a0       prec_S.initialize(preconditioner_matrix.block(1, 1), data);\\n\\u00a0     }\\n\\u00a0 \\nThe InverseMatrix is used to solve for the mass matrix:\\n\\u00a0     using mp_inverse_t = LinearSolvers::InverseMatrix<LA::MPI::SparseMatrix,\\n\\u00a0                                                       LA::MPI::PreconditionAMG>;\\n\\u00a0     const mp_inverse_t mp_inverse(preconditioner_matrix.block(1, 1), prec_S);\\n\\u00a0 \\nThis constructs the block preconditioner based on the preconditioners for the individual blocks defined above.\\n\\u00a0     const LinearSolvers::BlockDiagonalPreconditioner<LA::MPI::PreconditionAMG,\\n\\u00a0                                                      mp_inverse_t>\\n\\u00a0       preconditioner(prec_A, mp_inverse);\\n\\u00a0 \\nWith that, we can finally set up the linear solver and solve the system:\\n\\u00a0     SolverControl solver_control(system_matrix.m(),\\n\\u00a0                                  1e-10 * system_rhs.l2_norm());\\n\\u00a0 \\n\\u00a0     SolverMinRes<LA::MPI::BlockVector> solver(solver_control);\\n\\u00a0 \\n\\u00a0     LA::MPI::BlockVector distributed_solution(owned_partitioning,\\n\\u00a0                                               mpi_communicator);\\n\\u00a0 \\n\\u00a0     constraints.set_zero(distributed_solution);\\n\\u00a0 \\n\\u00a0     solver.solve(system_matrix,\\n\\u00a0                  distributed_solution,\\n\\u00a0                  system_rhs,\\n\\u00a0                  preconditioner);\\n\\u00a0 \\n\\u00a0     pcout << \\\"   Solved in \\\" << solver_control.last_step() << \\\" iterations.\\\"\\n\\u00a0           << std::endl;\\n\\u00a0 \\n\\u00a0     constraints.distribute(distributed_solution);\\n\\u00a0 \\nSolverMinResDefinition solver_minres.h:72\\nLike in step-56, we subtract the mean pressure to allow error computations against our reference solution, which has a mean value of zero.\\n\\u00a0     locally_relevant_solution = distributed_solution;\\n\\u00a0     const double mean_pressure =\\n\\u00a0       VectorTools::compute_mean_value(dof_handler,\\n\\u00a0                                       QGauss<dim>(velocity_degree + 2),\\n\\u00a0                                       locally_relevant_solution,\\n\\u00a0                                       dim);\\n\\u00a0     distributed_solution.block(1).add(-mean_pressure);\\n\\u00a0     locally_relevant_solution.block(1) = distributed_solution.block(1);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nVectorTools::compute_mean_valueNumber compute_mean_value(const hp::MappingCollection< dim, spacedim > &mapping_collection, const DoFHandler< dim, spacedim > &dof, const hp::QCollection< dim > &q_collection, const ReadVector< Number > &v, const unsigned int component)\\n The rest\\nThe remainder of the code that deals with mesh refinement, output, and the main loop is pretty standard.\\n\\u00a0   template <int dim>\\n\\u00a0   void StokesProblem<dim>::refine_grid()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"refine\\\");\\n\\u00a0 \\n\\u00a0     triangulation.refine_global();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void StokesProblem<dim>::output_results(const unsigned int cycle)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"output\\\");\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       const ComponentSelectFunction<dim> pressure_mask(dim, dim + 1);\\n\\u00a0       const ComponentSelectFunction<dim> velocity_mask(std::make_pair(0, dim),\\n\\u00a0                                                        dim + 1);\\n\\u00a0 \\n\\u00a0       Vector<double>    cellwise_errors(triangulation.n_active_cells());\\n\\u00a0       const QGauss<dim> quadrature(velocity_degree + 2);\\n\\u00a0 \\n\\u00a0       VectorTools::integrate_difference(dof_handler,\\n\\u00a0                                         locally_relevant_solution,\\n\\u00a0                                         ExactSolution<dim>(),\\n\\u00a0                                         cellwise_errors,\\n\\u00a0                                         quadrature,\\n\\u00a0                                         VectorTools::L2_norm,\\n\\u00a0                                         &velocity_mask);\\n\\u00a0 \\n\\u00a0       const double error_u_l2 =\\n\\u00a0         VectorTools::compute_global_error(triangulation,\\n\\u00a0                                           cellwise_errors,\\n\\u00a0                                           VectorTools::L2_norm);\\n\\u00a0 \\n\\u00a0       VectorTools::integrate_difference(dof_handler,\\n\\u00a0                                         locally_relevant_solution,\\n\\u00a0                                         ExactSolution<dim>(),\\n\\u00a0                                         cellwise_errors,\\n\\u00a0                                         quadrature,\\n\\u00a0                                         VectorTools::L2_norm,\\n\\u00a0                                         &pressure_mask);\\n\\u00a0 \\n\\u00a0       const double error_p_l2 =\\n\\u00a0         VectorTools::compute_global_error(triangulation,\\n\\u00a0                                           cellwise_errors,\\n\\u00a0                                           VectorTools::L2_norm);\\n\\u00a0 \\n\\u00a0       pcout << \\\"error: u_0: \\\" << error_u_l2 << \\\" p_0: \\\" << error_p_l2\\n\\u00a0             << std::endl;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     std::vector<std::string> solution_names(dim, \\\"velocity\\\");\\n\\u00a0     solution_names.emplace_back(\\\"pressure\\\");\\n\\u00a0     std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n\\u00a0       data_component_interpretation(\\n\\u00a0         dim, DataComponentInterpretation::component_is_part_of_vector);\\n\\u00a0     data_component_interpretation.push_back(\\n\\u00a0       DataComponentInterpretation::component_is_scalar);\\n\\u00a0 \\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0     data_out.attach_dof_handler(dof_handler);\\n\\u00a0     data_out.add_data_vector(locally_relevant_solution,\\n\\u00a0                              solution_names,\\n\\u00a0                              DataOut<dim>::type_dof_data,\\n\\u00a0                              data_component_interpretation);\\n\\u00a0 \\n\\u00a0     LA::MPI::BlockVector interpolated;\\n\\u00a0     interpolated.reinit(owned_partitioning, MPI_COMM_WORLD);\\n\\u00a0     VectorTools::interpolate(dof_handler, ExactSolution<dim>(), interpolated);\\n\\u00a0 \\n\\u00a0     LA::MPI::BlockVector interpolated_relevant(owned_partitioning,\\n\\u00a0                                                relevant_partitioning,\\n\\u00a0                                                MPI_COMM_WORLD);\\n\\u00a0     interpolated_relevant = interpolated;\\n\\u00a0     {\\n\\u00a0       std::vector<std::string> solution_names(dim, \\\"ref_u\\\");\\n\\u00a0       solution_names.emplace_back(\\\"ref_p\\\");\\n\\u00a0       data_out.add_data_vector(interpolated_relevant,\\n\\u00a0                                solution_names,\\n\\u00a0                                DataOut<dim>::type_dof_data,\\n\\u00a0                                data_component_interpretation);\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     Vector<float> subdomain(triangulation.n_active_cells());\\n\\u00a0     for (unsigned int i = 0; i < subdomain.size(); ++i)\\n\\u00a0       subdomain(i) = triangulation.locally_owned_subdomain();\\n\\u00a0     data_out.add_data_vector(subdomain, \\\"subdomain\\\");\\n\\u00a0 \\n\\u00a0     data_out.build_patches();\\n\\u00a0 \\n\\u00a0     data_out.write_vtu_with_pvtu_record(\\n\\u00a0       \\\"./\\\", \\\"solution\\\", cycle, mpi_communicator, 2);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void StokesProblem<dim>::run()\\n\\u00a0   {\\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0     pcout << \\\"Running using PETSc.\\\" << std::endl;\\n\\u00a0 #else\\n\\u00a0     pcout << \\\"Running using Trilinos.\\\" << std::endl;\\n\\u00a0 #endif\\n\\u00a0     const unsigned int n_cycles = 5;\\n\\u00a0     for (unsigned int cycle = 0; cycle < n_cycles; ++cycle)\\n\\u00a0       {\\n\\u00a0         pcout << \\\"Cycle \\\" << cycle << ':' << std::endl;\\n\\u00a0 \\n\\u00a0         if (cycle == 0)\\n\\u00a0           make_grid();\\n\\u00a0         else\\n\\u00a0           refine_grid();\\n\\u00a0 \\n\\u00a0         setup_system();\\n\\u00a0 \\n\\u00a0         assemble_system();\\n\\u00a0         solve();\\n\\u00a0 \\n\\u00a0         if (Utilities::MPI::n_mpi_processes(mpi_communicator) <= 32)\\n\\u00a0           output_results(cycle);\\n\\u00a0 \\n\\u00a0         computing_timer.print_summary();\\n\\u00a0         computing_timer.reset();\\n\\u00a0 \\n\\u00a0         pcout << std::endl;\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 } // namespace Step55\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0 int main(int argc, char *argv[])\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       using namespace dealii;\\n\\u00a0       using namespace Step55;\\n\\u00a0 \\n\\u00a0       Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n\\u00a0 \\n\\u00a0       StokesProblem<2> problem(2);\\n\\u00a0       problem.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0 \\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return 0;\\n\\u00a0 }\\nComponentSelectFunctionDefinition function.h:582\\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOutDefinition data_out.h:147\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\nparallel::TriangulationBase::locally_owned_subdomaintypes::subdomain_id locally_owned_subdomain() const overrideDefinition tria_base.cc:345\\nDataComponentInterpretation::component_is_scalar@ component_is_scalarDefinition data_component_interpretation.h:52\\nDataComponentInterpretation::component_is_part_of_vector@ component_is_part_of_vectorDefinition data_component_interpretation.h:58\\nUtilities::MPI::n_mpi_processesunsigned int n_mpi_processes(const MPI_Comm mpi_communicator)Definition mpi.cc:92\\nVectorTools::compute_global_errordouble compute_global_error(const Triangulation< dim, spacedim > &tria, const InVector &cellwise_error, const NormType &norm, const double exponent=2.)\\nVectorTools::interpolatevoid interpolate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Function< spacedim, typename VectorType::value_type > &function, VectorType &vec, const ComponentMask &component_mask={})\\nVectorTools::L2_norm@ L2_normDefinition vector_tools_common.h:112\\nVectorTools::integrate_differencevoid integrate_difference(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const ReadVector< Number > &fe_function, const Function< spacedim, Number > &exact_solution, OutVector &difference, const Quadrature< dim > &q, const NormType &norm, const Function< spacedim, double > *weight=nullptr, const double exponent=2.)\\n Results\\nAs expected from the discussion above, the number of iterations is independent of the number of processors and only very slightly dependent on \\\\(h\\\\):\\n\\n\\nPETSc number of processors  \\n\\ncycle dofs 1 2 4 8 16 32 64 128  \\n\\n0 659 49 49 49 51 51 51 49 49  \\n\\n1 2467 52 52 52 52 52 54 54 53  \\n\\n2 9539 56 56 56 54 56 56 54 56  \\n\\n3 37507 57 57 57 57 57 56 57 56  \\n\\n4 148739 58 59 57 59 57 57 57 57  \\n\\n5 592387 60 60 59 59 59 59 59 59  \\n\\n6 2364419 62 62 61 61 61 61 61 61  \\n\\n\\n\\nTrilinos number of processors  \\n\\ncycle dofs 1 2 4 8 16 32 64 128  \\n\\n0 659 37 37 37 37 37 37 37 37  \\n\\n1 2467 92 89 89 82 86 81 78 78  \\n\\n2 9539 102 99 96 95 95 88 83 95  \\n\\n3 37507 107 105 104 99 100 96 96 90  \\n\\n4 148739 112 112 111 111 127 126 115 117  \\n\\n5 592387 116 115 114 112 118 120 131 130  \\n\\n6 2364419 130 126 120 120 121 122 121 123  \\n\\nWhile the PETSc results show a constant number of iterations, the iterations increase when using Trilinos. This is likely because of the different settings used for the AMG preconditioner. For performance reasons we do not allow coarsening below a couple thousand unknowns. As the coarse solver is an exact solve (we are using LU by default), a change in number of levels will influence the quality of a V-cycle. Therefore, a V-cycle is closer to an exact solver for smaller problem sizes.\\n Possibilities for extensions\\nInvestigate Trilinos iterations\\nPlay with the smoothers, smoothing steps, and other properties for the Trilinos AMG to achieve an optimal preconditioner.\\nSolve the Oseen problem instead of the Stokes system\\nThis change requires changing the outer solver to GMRES or BiCGStab, because the system is no longer symmetric.\\nYou can prescribe the exact flow solution as \\\\(b\\\\) in the convective term  \\\\(b\\n\\\\cdot \\\\nabla u\\\\). This should give the same solution as the original problem, if you set the right hand side to zero.\\nAdaptive refinement\\nSo far, this tutorial program refines the mesh globally in each step. Replacing the code in StokesProblem::refine_grid() by something like Vector<float> estimated_error_per_cell(triangulation.n_active_cells());\\n \\nFEValuesExtractors::Vector velocities(0);\\nKellyErrorEstimator<dim>::estimate(\\n  dof_handler,\\n QGauss<dim - 1>(fe.degree + 1),\\n  std::map<types::boundary_id, const Function<dim> *>(),\\n  locally_relevant_solution,\\n  estimated_error_per_cell,\\n  fe.component_mask(velocities));\\nparallel::distributed::GridRefinement::refine_and_coarsen_fixed_number(\\n triangulation, estimated_error_per_cell, 0.3, 0.0);\\ntriangulation.execute_coarsening_and_refinement();\\nKellyErrorEstimator::estimatestatic void estimate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Quadrature< dim - 1 > &quadrature, const std::map< types::boundary_id, const Function< spacedim, Number > * > &neumann_bc, const ReadVector< Number > &solution, Vector< float > &error, const ComponentMask &component_mask={}, const Function< spacedim > *coefficients=nullptr, const unsigned int n_threads=numbers::invalid_unsigned_int, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id, const types::material_id material_id=numbers::invalid_material_id, const Strategy strategy=cell_diameter_over_24)\\nparallel::distributed::Triangulation::execute_coarsening_and_refinementvirtual void execute_coarsening_and_refinement() overrideDefinition tria.cc:3320\\nunsigned int\\nparallel::distributed::GridRefinement::refine_and_coarsen_fixed_numbervoid refine_and_coarsen_fixed_number(::Triangulation< dim, spacedim > &tria, const ::Vector< Number > &criteria, const double top_fraction_of_cells, const double bottom_fraction_of_cells, const types::global_cell_index max_n_cells=std::numeric_limits< types::global_cell_index >::max())Definition grid_refinement.cc:503\\n makes it simple to explore adaptive mesh refinement.\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2016 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Author: Timo Heister, Clemson University, 2016\\n */\\n \\n#include <deal.II/base/quadrature_lib.h>\\n#include <deal.II/base/function.h>\\n#include <deal.II/base/timer.h>\\n \\n \\n#include <deal.II/lac/generic_linear_algebra.h>\\n \\n/* #define FORCE_USE_OF_TRILINOS */\\n \\nnamespace LA\\n{\\n#if defined(DEAL_II_WITH_PETSC) && !defined(DEAL_II_PETSC_WITH_COMPLEX) && \\\\\\n  !(defined(DEAL_II_WITH_TRILINOS) && defined(FORCE_USE_OF_TRILINOS))\\n using namespace dealii::LinearAlgebraPETSc;\\n#  define USE_PETSC_LA\\n#elif defined(DEAL_II_WITH_TRILINOS)\\n using namespace dealii::LinearAlgebraTrilinos;\\n#else\\n#  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required\\n#endif\\n} // namespace LA\\n \\n#include <deal.II/lac/vector.h>\\n#include <deal.II/lac/full_matrix.h>\\n#include <deal.II/lac/solver_cg.h>\\n#include <deal.II/lac/solver_gmres.h>\\n#include <deal.II/lac/solver_minres.h>\\n#include <deal.II/lac/affine_constraints.h>\\n#include <deal.II/lac/dynamic_sparsity_pattern.h>\\n \\n#include <deal.II/lac/petsc_sparse_matrix.h>\\n#include <deal.II/lac/petsc_vector.h>\\n#include <deal.II/lac/petsc_solver.h>\\n#include <deal.II/lac/petsc_precondition.h>\\n \\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/manifold_lib.h>\\n#include <deal.II/grid/grid_tools.h>\\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/dofs/dof_renumbering.h>\\n#include <deal.II/dofs/dof_tools.h>\\n#include <deal.II/fe/fe_values.h>\\n#include <deal.II/fe/fe_q.h>\\n#include <deal.II/fe/fe_system.h>\\n#include <deal.II/numerics/vector_tools.h>\\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/error_estimator.h>\\n \\n#include <deal.II/base/utilities.h>\\n#include <deal.II/base/conditional_ostream.h>\\n#include <deal.II/base/index_set.h>\\n#include <deal.II/lac/sparsity_tools.h>\\n#include <deal.II/distributed/tria.h>\\n#include <deal.II/distributed/grid_refinement.h>\\n \\n#include <cmath>\\n#include <fstream>\\n#include <iostream>\\n \\nnamespace Step55\\n{\\n using namespace dealii;\\n \\n \\n \\n namespace LinearSolvers\\n  {\\n template <class Matrix, class Preconditioner>\\n class InverseMatrix : public Subscriptor\\n    {\\n public:\\n      InverseMatrix(const Matrix &m, const Preconditioner &preconditioner);\\n \\n template <typename VectorType>\\n void vmult(VectorType &dst, const VectorType &src) const;\\n \\n private:\\n const SmartPointer<const Matrix> matrix;\\n const Preconditioner            &preconditioner;\\n    };\\n \\n \\n template <class Matrix, class Preconditioner>\\n    InverseMatrix<Matrix, Preconditioner>::InverseMatrix(\\n const Matrix         &m,\\n const Preconditioner &preconditioner)\\n      : matrix(&m)\\n      , preconditioner(preconditioner)\\n    {}\\n \\n \\n \\n template <class Matrix, class Preconditioner>\\n template <typename VectorType>\\n void\\n    InverseMatrix<Matrix, Preconditioner>::vmult(VectorType       &dst,\\n const VectorType &src) const\\n {\\n SolverControl        solver_control(src.size(), 1e-8 * src.l2_norm());\\n SolverCG<VectorType> cg(solver_control);\\n      dst = 0;\\n \\n try\\n        {\\n          cg.solve(*matrix, dst, src, preconditioner);\\n        }\\n catch (std::exception &e)\\n        {\\n Assert(false, ExcMessage(e.what()));\\n        }\\n    }\\n \\n \\n template <class PreconditionerA, class PreconditionerS>\\n class BlockDiagonalPreconditioner : public Subscriptor\\n    {\\n public:\\n      BlockDiagonalPreconditioner(const PreconditionerA &preconditioner_A,\\n const PreconditionerS &preconditioner_S);\\n \\n void vmult(LA::MPI::BlockVector       &dst,\\n const LA::MPI::BlockVector &src) const;\\n \\n private:\\n const PreconditionerA &preconditioner_A;\\n const PreconditionerS &preconditioner_S;\\n    };\\n \\n template <class PreconditionerA, class PreconditionerS>\\n    BlockDiagonalPreconditioner<PreconditionerA, PreconditionerS>::\\n      BlockDiagonalPreconditioner(const PreconditionerA &preconditioner_A,\\n const PreconditionerS &preconditioner_S)\\n      : preconditioner_A(preconditioner_A)\\n      , preconditioner_S(preconditioner_S)\\n    {}\\n \\n \\n template <class PreconditionerA, class PreconditionerS>\\n void BlockDiagonalPreconditioner<PreconditionerA, PreconditionerS>::vmult(\\n      LA::MPI::BlockVector       &dst,\\n const LA::MPI::BlockVector &src) const\\n {\\n      preconditioner_A.vmult(dst.block(0), src.block(0));\\n      preconditioner_S.vmult(dst.block(1), src.block(1));\\n    }\\n \\n  } // namespace LinearSolvers\\n \\n \\n \\n template <int dim>\\n class RightHandSide : public Function<dim>\\n  {\\n public:\\n    RightHandSide()\\n      : Function<dim>(dim + 1)\\n    {}\\n \\n virtual void vector_value(const Point<dim> &p,\\n Vector<double>   &value) const override;\\n  };\\n \\n \\n template <int dim>\\n void RightHandSide<dim>::vector_value(const Point<dim> &p,\\n Vector<double>   &values) const\\n {\\n const double R_x = p[0];\\n const double R_y = p[1];\\n \\n constexpr double pi  = numbers::PI;\\n constexpr double pi2 = numbers::PI * numbers::PI;\\n \\n    values[0] = -1.0L / 2.0L * (-2 * std::sqrt(25.0 + 4 * pi2) + 10.0) *\\n std::exp(R_x * (-2 * std::sqrt(25.0 + 4 * pi2) + 10.0)) -\\n                0.4 * pi2 * std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n std::cos(2 * R_y * pi) +\\n                0.1 *\\n Utilities::fixed_power<2>(-std::sqrt(25.0 + 4 * pi2) + 5.0) *\\n std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n std::cos(2 * R_y * pi);\\n    values[1] = 0.2 * pi * (-std::sqrt(25.0 + 4 * pi2) + 5.0) *\\n std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n std::sin(2 * R_y * pi) -\\n                0.05 *\\n Utilities::fixed_power<3>(-std::sqrt(25.0 + 4 * pi2) + 5.0) *\\n std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n std::sin(2 * R_y * pi) / pi;\\n \\n    values[dim] = 0;\\n  }\\n \\n \\n template <int dim>\\n class ExactSolution : public Function<dim>\\n  {\\n public:\\n    ExactSolution()\\n      : Function<dim>(dim + 1)\\n    {}\\n \\n virtual void vector_value(const Point<dim> &p,\\n Vector<double>   &values) const override;\\n  };\\n \\n template <int dim>\\n void ExactSolution<dim>::vector_value(const Point<dim> &p,\\n Vector<double>   &values) const\\n {\\n const double R_x = p[0];\\n const double R_y = p[1];\\n \\n constexpr double pi  = numbers::PI;\\n constexpr double pi2 = numbers::PI * numbers::PI;\\n \\n    values[0] = -std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n std::cos(2 * R_y * pi) +\\n                1;\\n    values[1] = (1.0L / 2.0L) * (-std::sqrt(25.0 + 4 * pi2) + 5.0) *\\n std::exp(R_x * (-std::sqrt(25.0 + 4 * pi2) + 5.0)) *\\n std::sin(2 * R_y * pi) / pi;\\n \\n    values[dim] =\\n      -1.0L / 2.0L * std::exp(R_x * (-2 * std::sqrt(25.0 + 4 * pi2) + 10.0)) -\\n      2.0 *\\n        (-6538034.74494422 +\\n         0.0134758939981709 * std::exp(4 * std::sqrt(25.0 + 4 * pi2))) /\\n        (-80.0 * std::exp(3 * std::sqrt(25.0 + 4 * pi2)) +\\n         16.0 * std::sqrt(25.0 + 4 * pi2) *\\n std::exp(3 * std::sqrt(25.0 + 4 * pi2))) -\\n      1634508.68623606 * std::exp(-3.0 * std::sqrt(25.0 + 4 * pi2)) /\\n        (-10.0 + 2.0 * std::sqrt(25.0 + 4 * pi2)) +\\n      (-0.00673794699908547 * std::exp(std::sqrt(25.0 + 4 * pi2)) +\\n       3269017.37247211 * std::exp(-3 * std::sqrt(25.0 + 4 * pi2))) /\\n        (-8 * std::sqrt(25.0 + 4 * pi2) + 40.0) +\\n      0.00336897349954273 * std::exp(1.0 * std::sqrt(25.0 + 4 * pi2)) /\\n        (-10.0 + 2.0 * std::sqrt(25.0 + 4 * pi2));\\n  }\\n \\n \\n \\n template <int dim>\\n class StokesProblem\\n  {\\n public:\\n    StokesProblem(unsigned int velocity_degree);\\n \\n void run();\\n \\n private:\\n void make_grid();\\n void setup_system();\\n void assemble_system();\\n void solve();\\n void refine_grid();\\n void output_results(const unsigned int cycle);\\n \\n const unsigned int velocity_degree;\\n const double       viscosity;\\n MPI_Comm           mpi_communicator;\\n \\n const FESystem<dim>                       fe;\\n parallel::distributed::Triangulation<dim> triangulation;\\n DoFHandler<dim>                           dof_handler;\\n \\n    std::vector<IndexSet> owned_partitioning;\\n    std::vector<IndexSet> relevant_partitioning;\\n \\n AffineConstraints<double> constraints;\\n \\n    LA::MPI::BlockSparseMatrix system_matrix;\\n    LA::MPI::BlockSparseMatrix preconditioner_matrix;\\n    LA::MPI::BlockVector       locally_relevant_solution;\\n    LA::MPI::BlockVector       system_rhs;\\n \\n ConditionalOStream pcout;\\n TimerOutput        computing_timer;\\n  };\\n \\n \\n \\n template <int dim>\\n  StokesProblem<dim>::StokesProblem(unsigned int velocity_degree)\\n    : velocity_degree(velocity_degree)\\n    , viscosity(0.1)\\n    , mpi_communicator(MPI_COMM_WORLD)\\n    , fe(FE_Q<dim>(velocity_degree) ^ dim, FE_Q<dim>(velocity_degree - 1))\\n    , triangulation(mpi_communicator,\\n                    typename Triangulation<dim>::MeshSmoothing(\\n Triangulation<dim>::smoothing_on_refinement |\\n Triangulation<dim>::smoothing_on_coarsening))\\n    , dof_handler(triangulation)\\n    , pcout(std::cout,\\n            (Utilities::MPI::this_mpi_process(mpi_communicator) == 0))\\n    , computing_timer(mpi_communicator,\\n                      pcout,\\n TimerOutput::never,\\n TimerOutput::wall_times)\\n  {}\\n \\n \\n template <int dim>\\n void StokesProblem<dim>::make_grid()\\n  {\\n GridGenerator::hyper_cube(triangulation, -0.5, 1.5);\\n triangulation.refine_global(3);\\n  }\\n \\n template <int dim>\\n void StokesProblem<dim>::setup_system()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"setup\\\");\\n \\n    dof_handler.distribute_dofs(fe);\\n \\n    std::vector<unsigned int> stokes_sub_blocks(dim + 1, 0);\\n    stokes_sub_blocks[dim] = 1;\\n DoFRenumbering::component_wise(dof_handler, stokes_sub_blocks);\\n \\n const std::vector<types::global_dof_index> dofs_per_block =\\n DoFTools::count_dofs_per_fe_block(dof_handler, stokes_sub_blocks);\\n \\n const unsigned int n_u = dofs_per_block[0];\\n const unsigned int n_p = dofs_per_block[1];\\n \\n    pcout << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs() << \\\" (\\\"\\n          << n_u << '+' << n_p << ')' << std::endl;\\n \\n const IndexSet &locally_owned_dofs = dof_handler.locally_owned_dofs();\\n    owned_partitioning                 = {locally_owned_dofs.get_view(0, n_u),\\n                                          locally_owned_dofs.get_view(n_u, n_u + n_p)};\\n \\n const IndexSet locally_relevant_dofs =\\n DoFTools::extract_locally_relevant_dofs(dof_handler);\\n    relevant_partitioning = {locally_relevant_dofs.get_view(0, n_u),\\n                             locally_relevant_dofs.get_view(n_u, n_u + n_p)};\\n \\n    {\\n      constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n \\n const FEValuesExtractors::Vector velocities(0);\\n DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n VectorTools::interpolate_boundary_values(dof_handler,\\n                                               0,\\n                                               ExactSolution<dim>(),\\n                                               constraints,\\n                                               fe.component_mask(velocities));\\n      constraints.close();\\n    }\\n \\n    {\\n      system_matrix.clear();\\n \\n Table<2, DoFTools::Coupling> coupling(dim + 1, dim + 1);\\n for (unsigned int c = 0; c < dim + 1; ++c)\\n for (unsigned int d = 0; d < dim + 1; ++d)\\n if (c == dim && d == dim)\\n            coupling[c][d] = DoFTools::none;\\n else if (c == dim || d == dim || c == d)\\n            coupling[c][d] = DoFTools::always;\\n else\\n            coupling[c][d] = DoFTools::none;\\n \\n BlockDynamicSparsityPattern dsp(relevant_partitioning);\\n \\n DoFTools::make_sparsity_pattern(\\n        dof_handler, coupling, dsp, constraints, false);\\n \\n SparsityTools::distribute_sparsity_pattern(\\n        dsp,\\n        dof_handler.locally_owned_dofs(),\\n        mpi_communicator,\\n        locally_relevant_dofs);\\n \\n      system_matrix.reinit(owned_partitioning, dsp, mpi_communicator);\\n    }\\n \\n    {\\n      preconditioner_matrix.clear();\\n \\n Table<2, DoFTools::Coupling> coupling(dim + 1, dim + 1);\\n for (unsigned int c = 0; c < dim + 1; ++c)\\n for (unsigned int d = 0; d < dim + 1; ++d)\\n if (c == dim && d == dim)\\n            coupling[c][d] = DoFTools::always;\\n else\\n            coupling[c][d] = DoFTools::none;\\n \\n BlockDynamicSparsityPattern dsp(relevant_partitioning);\\n \\n DoFTools::make_sparsity_pattern(\\n        dof_handler, coupling, dsp, constraints, false);\\n SparsityTools::distribute_sparsity_pattern(\\n        dsp,\\n Utilities::MPI::all_gather(mpi_communicator,\\n                                   dof_handler.locally_owned_dofs()),\\n        mpi_communicator,\\n        locally_relevant_dofs);\\n      preconditioner_matrix.reinit(owned_partitioning, dsp, mpi_communicator);\\n    }\\n \\n    locally_relevant_solution.reinit(owned_partitioning,\\n                                     relevant_partitioning,\\n                                     mpi_communicator);\\n    system_rhs.reinit(owned_partitioning, mpi_communicator);\\n  }\\n \\n \\n \\n template <int dim>\\n void StokesProblem<dim>::assemble_system()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"assembly\\\");\\n \\n    system_matrix         = 0;\\n    preconditioner_matrix = 0;\\n    system_rhs            = 0;\\n \\n const QGauss<dim> quadrature_formula(velocity_degree + 1);\\n \\n FEValues<dim> fe_values(fe,\\n                            quadrature_formula,\\n update_values | update_gradients |\\n update_quadrature_points | update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n FullMatrix<double> system_cell_matrix(dofs_per_cell, dofs_per_cell);\\n FullMatrix<double> preconditioner_cell_matrix(dofs_per_cell, dofs_per_cell);\\n Vector<double>     cell_rhs(dofs_per_cell);\\n \\n const RightHandSide<dim>    right_hand_side;\\n    std::vector<Vector<double>> rhs_values(n_q_points, Vector<double>(dim + 1));\\n \\n    std::vector<Tensor<2, dim>> grad_phi_u(dofs_per_cell);\\n    std::vector<double>         div_phi_u(dofs_per_cell);\\n    std::vector<double>         phi_p(dofs_per_cell);\\n \\n    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n const FEValuesExtractors::Vector     velocities(0);\\n const FEValuesExtractors::Scalar     pressure(dim);\\n \\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      if (cell->is_locally_owned())\\n        {\\n          system_cell_matrix         = 0;\\n          preconditioner_cell_matrix = 0;\\n          cell_rhs                   = 0;\\n \\n          fe_values.reinit(cell);\\n          right_hand_side.vector_value_list(fe_values.get_quadrature_points(),\\n                                            rhs_values);\\n for (unsigned int q = 0; q < n_q_points; ++q)\\n            {\\n for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n                {\\n                  grad_phi_u[k] = fe_values[velocities].gradient(k, q);\\n                  div_phi_u[k]  = fe_values[velocities].divergence(k, q);\\n                  phi_p[k]      = fe_values[pressure].value(k, q);\\n                }\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n                {\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n                    {\\n                      system_cell_matrix(i, j) +=\\n                        (viscosity *\\n                           scalar_product(grad_phi_u[i], grad_phi_u[j]) -\\n                         div_phi_u[i] * phi_p[j] - phi_p[i] * div_phi_u[j]) *\\n                        fe_values.JxW(q);\\n \\n                      preconditioner_cell_matrix(i, j) += 1.0 / viscosity *\\n                                                          phi_p[i] * phi_p[j] *\\n                                                          fe_values.JxW(q);\\n                    }\\n \\n const unsigned int component_i =\\n                    fe.system_to_component_index(i).first;\\n                  cell_rhs(i) += fe_values.shape_value(i, q) *\\n                                 rhs_values[q](component_i) * fe_values.JxW(q);\\n                }\\n            }\\n \\n \\n          cell->get_dof_indices(local_dof_indices);\\n          constraints.distribute_local_to_global(system_cell_matrix,\\n                                                 cell_rhs,\\n                                                 local_dof_indices,\\n                                                 system_matrix,\\n                                                 system_rhs);\\n \\n          constraints.distribute_local_to_global(preconditioner_cell_matrix,\\n                                                 local_dof_indices,\\n                                                 preconditioner_matrix);\\n        }\\n \\n    system_matrix.compress(VectorOperation::add);\\n    preconditioner_matrix.compress(VectorOperation::add);\\n    system_rhs.compress(VectorOperation::add);\\n  }\\n \\n \\n \\n template <int dim>\\n void StokesProblem<dim>::solve()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"solve\\\");\\n \\n    LA::MPI::PreconditionAMG prec_A;\\n    {\\n      LA::MPI::PreconditionAMG::AdditionalData data;\\n \\n#ifdef USE_PETSC_LA\\n      data.symmetric_operator = true;\\n#endif\\n      prec_A.initialize(system_matrix.block(0, 0), data);\\n    }\\n \\n    LA::MPI::PreconditionAMG prec_S;\\n    {\\n      LA::MPI::PreconditionAMG::AdditionalData data;\\n \\n#ifdef USE_PETSC_LA\\n      data.symmetric_operator = true;\\n#endif\\n      prec_S.initialize(preconditioner_matrix.block(1, 1), data);\\n    }\\n \\n using mp_inverse_t = LinearSolvers::InverseMatrix<LA::MPI::SparseMatrix,\\n                                                      LA::MPI::PreconditionAMG>;\\n const mp_inverse_t mp_inverse(preconditioner_matrix.block(1, 1), prec_S);\\n \\n const LinearSolvers::BlockDiagonalPreconditioner<LA::MPI::PreconditionAMG,\\n                                                     mp_inverse_t>\\n      preconditioner(prec_A, mp_inverse);\\n \\n SolverControl solver_control(system_matrix.m(),\\n                                 1e-10 * system_rhs.l2_norm());\\n \\n SolverMinRes<LA::MPI::BlockVector> solver(solver_control);\\n \\n    LA::MPI::BlockVector distributed_solution(owned_partitioning,\\n                                              mpi_communicator);\\n \\n    constraints.set_zero(distributed_solution);\\n \\n    solver.solve(system_matrix,\\n                 distributed_solution,\\n                 system_rhs,\\n                 preconditioner);\\n \\n    pcout << \\\"   Solved in \\\" << solver_control.last_step() << \\\" iterations.\\\"\\n          << std::endl;\\n \\n    constraints.distribute(distributed_solution);\\n \\n    locally_relevant_solution = distributed_solution;\\n const double mean_pressure =\\n VectorTools::compute_mean_value(dof_handler,\\n QGauss<dim>(velocity_degree + 2),\\n                                      locally_relevant_solution,\\n                                      dim);\\n    distributed_solution.block(1).add(-mean_pressure);\\n    locally_relevant_solution.block(1) = distributed_solution.block(1);\\n  }\\n \\n \\n \\n template <int dim>\\n void StokesProblem<dim>::refine_grid()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"refine\\\");\\n \\n triangulation.refine_global();\\n  }\\n \\n \\n \\n template <int dim>\\n void StokesProblem<dim>::output_results(const unsigned int cycle)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"output\\\");\\n \\n    {\\n const ComponentSelectFunction<dim> pressure_mask(dim, dim + 1);\\n const ComponentSelectFunction<dim> velocity_mask(std::make_pair(0, dim),\\n                                                       dim + 1);\\n \\n Vector<double>    cellwise_errors(triangulation.n_active_cells());\\n const QGauss<dim> quadrature(velocity_degree + 2);\\n \\n VectorTools::integrate_difference(dof_handler,\\n                                        locally_relevant_solution,\\n                                        ExactSolution<dim>(),\\n                                        cellwise_errors,\\n                                        quadrature,\\n VectorTools::L2_norm,\\n                                        &velocity_mask);\\n \\n const double error_u_l2 =\\n VectorTools::compute_global_error(triangulation,\\n                                          cellwise_errors,\\n VectorTools::L2_norm);\\n \\n VectorTools::integrate_difference(dof_handler,\\n                                        locally_relevant_solution,\\n                                        ExactSolution<dim>(),\\n                                        cellwise_errors,\\n                                        quadrature,\\n VectorTools::L2_norm,\\n                                        &pressure_mask);\\n \\n const double error_p_l2 =\\n VectorTools::compute_global_error(triangulation,\\n                                          cellwise_errors,\\n VectorTools::L2_norm);\\n \\n      pcout << \\\"error: u_0: \\\" << error_u_l2 << \\\" p_0: \\\" << error_p_l2\\n            << std::endl;\\n    }\\n \\n \\n    std::vector<std::string> solution_names(dim, \\\"velocity\\\");\\n    solution_names.emplace_back(\\\"pressure\\\");\\n    std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n      data_component_interpretation(\\n        dim, DataComponentInterpretation::component_is_part_of_vector);\\n    data_component_interpretation.push_back(\\n DataComponentInterpretation::component_is_scalar);\\n \\n DataOut<dim> data_out;\\n    data_out.attach_dof_handler(dof_handler);\\n    data_out.add_data_vector(locally_relevant_solution,\\n                             solution_names,\\n DataOut<dim>::type_dof_data,\\n                             data_component_interpretation);\\n \\n    LA::MPI::BlockVector interpolated;\\n    interpolated.reinit(owned_partitioning, MPI_COMM_WORLD);\\n VectorTools::interpolate(dof_handler, ExactSolution<dim>(), interpolated);\\n \\n    LA::MPI::BlockVector interpolated_relevant(owned_partitioning,\\n                                               relevant_partitioning,\\n                                               MPI_COMM_WORLD);\\n    interpolated_relevant = interpolated;\\n    {\\n      std::vector<std::string> solution_names(dim, \\\"ref_u\\\");\\n      solution_names.emplace_back(\\\"ref_p\\\");\\n      data_out.add_data_vector(interpolated_relevant,\\n                               solution_names,\\n DataOut<dim>::type_dof_data,\\n                               data_component_interpretation);\\n    }\\n \\n \\n Vector<float> subdomain(triangulation.n_active_cells());\\n for (unsigned int i = 0; i < subdomain.size(); ++i)\\n      subdomain(i) = triangulation.locally_owned_subdomain();\\n    data_out.add_data_vector(subdomain, \\\"subdomain\\\");\\n \\n    data_out.build_patches();\\n \\n    data_out.write_vtu_with_pvtu_record(\\n \\\"./\\\", \\\"solution\\\", cycle, mpi_communicator, 2);\\n  }\\n \\n \\n \\n template <int dim>\\n void StokesProblem<dim>::run()\\n  {\\n#ifdef USE_PETSC_LA\\n    pcout << \\\"Running using PETSc.\\\" << std::endl;\\n#else\\n    pcout << \\\"Running using Trilinos.\\\" << std::endl;\\n#endif\\n const unsigned int n_cycles = 5;\\n for (unsigned int cycle = 0; cycle < n_cycles; ++cycle)\\n      {\\n        pcout << \\\"Cycle \\\" << cycle << ':' << std::endl;\\n \\n if (cycle == 0)\\n          make_grid();\\n else\\n          refine_grid();\\n \\n        setup_system();\\n \\n        assemble_system();\\n        solve();\\n \\n if (Utilities::MPI::n_mpi_processes(mpi_communicator) <= 32)\\n          output_results(cycle);\\n \\n        computing_timer.print_summary();\\n        computing_timer.reset();\\n \\n        pcout << std::endl;\\n      }\\n  }\\n} // namespace Step55\\n \\n \\n \\nint main(int argc, char *argv[])\\n{\\n try\\n    {\\n using namespace dealii;\\n using namespace Step55;\\n \\n Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n \\n      StokesProblem<2> problem(2);\\n      problem.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n \\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n \\n return 0;\\n}\\naffine_constraints.h\\nDataOutInterface::write_vtu_with_pvtu_recordstd::string write_vtu_with_pvtu_record(const std::string &directory, const std::string &filename_without_extension, const unsigned int counter, const MPI_Comm mpi_communicator, const unsigned int n_digits_for_counter=numbers::invalid_unsigned_int, const unsigned int n_groups=0) constDefinition data_out_base.cc:7854\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\nconditional_ostream.h\\ngrid_refinement.h\\ntria.h\\ndof_handler.h\\ndof_renumbering.h\\ndof_tools.h\\ndynamic_sparsity_pattern.h\\nerror_estimator.h\\nfe_values.h\\nfe_q.h\\nfe_system.h\\nfull_matrix.h\\nfunction.h\\ngeneric_linear_algebra.h\\nmanifold_lib.h\\ngrid_generator.h\\ngrid_tools.h\\nutilities.h\\nindex_set.h\\nLAPACKSupport::matrix@ matrixContents is actually a matrix.Definition lapack_support.h:57\\nPhysics::Elasticity::Kinematics::dSymmetricTensor< 2, dim, Number > d(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\nUtilities::MPI::this_mpi_processunsigned int this_mpi_process(const MPI_Comm mpi_communicator)Definition mpi.cc:107\\nWorkStream::internal::tbb_no_coloring::runvoid run(const Iterator &begin, const std_cxx20::type_identity_t< Iterator > &end, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length, const unsigned int chunk_size)Definition work_stream.h:471\\ndata_out.h\\npetsc_precondition.h\\npetsc_solver.h\\npetsc_sparse_matrix.h\\npetsc_vector.h\\nquadrature_lib.h\\nsolver_cg.h\\nsolver_gmres.h\\nsolver_minres.h\\nsparsity_tools.h\\ntimer.h\\nvector.h\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"