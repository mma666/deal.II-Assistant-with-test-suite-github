"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_68.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-68 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-68 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-68 tutorial program\\n\\n\\nThis tutorial depends on step-19.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\nSimulation of the motion of massless tracer particles in a vortical flow\\nParticles in deal.II\\nChallenges related to distributed particle simulations\\n\\nParallel particle generation\\nParticle exchange\\nBalancing mesh and particle load\\n\\nThe testcase\\n\\n The commented program\\n\\nInclude files\\nRun-time parameter handling\\nThe ParticleTracking class declaration\\nThe PatricleTracking class implementation\\n\\nConstructor\\nCell weight\\nParticles generation\\nBackground DOFs and interpolation\\nTime integration of the trajectories\\nData output\\nRunning the simulation\\n\\nThe main() function\\n\\n\\n Results\\n\\n Motion of the particles \\n Dynamic load balancing \\nPossibilities for extensions\\n\\n The plain program\\n   \\n\\n\\n This program was contributed by Bruno Blais (Polytechnique Montr\\u00e9al), Toni El Geitani Nehme (Polytechnique Montr\\u00e9al), Rene Gassm\\u00f6ller (University of California Davis), and Peter Munch (Technical University of Munich and Helmholtz-Zentrum Geesthacht). Bruno Blais was supported by NSERC Discovery grant RGPIN-2020-04510, by Compute Canada and Calcul Qu\\u00e9bec. \\nIntroduction\\nSimulation of the motion of massless tracer particles in a vortical flow\\nParticles play an important part in numerical models for a large number of applications. Particles are routinely used as massless tracers to visualize the dynamic of a transient flow. They can also play an intrinsic role as part of a more complex finite element model, as is the case for the Particle-In-Cell (PIC) method [93] or they can even be used to simulate the motion of granular matter, as in the Discrete Element Method (DEM) [33]. In the case of DEM, the resulting model is not related to the finite element method anymore, but just leads to a system of ordinary differential equation which describes the motion of the particles and the dynamic of their collisions. All of these models can be built using deal.II's particle handling capabilities.\\nIn the present step, we use particles as massless tracers to illustrate the dynamic of a vortical flow. Since the particles are massless tracers, the position of each particle \\\\(i\\\\) is described by the following ordinary differential equation (ODE):   \\n\\\\[\\n\\\\frac{d \\\\textbf{x}_i}{dt} =\\\\textbf{u}(\\\\textbf{x}_i)\\n\\\\]\\n\\nwhere \\\\(\\\\textbf{x}_i\\\\) is the position of particle \\\\(i\\\\) and \\\\(\\\\textbf{u}(\\\\textbf{x}_i)\\\\) the flow velocity at its position. In the present step, this ODE is solved using the explicit Euler method. The resulting scheme is:   \\n\\\\[\\n\\\\textbf{x}_{i}^{n+1} = \\\\textbf{x}_{i}^{n} + \\\\Delta t \\\\; \\\\textbf{u}(\\\\textbf{x}_{i}^{n})\\n\\\\]\\n\\nwhere \\\\(\\\\textbf{x}_{i}^{n+1}\\\\) and \\\\(\\\\textbf{x}_{i}^{n}\\\\) are the position of particle \\\\(i\\\\) at time \\\\(t+\\\\Delta t\\\\) and \\\\(t\\\\), respectively and where \\\\(\\\\Delta t\\\\) is the time step. In the present step, the velocity at the location of particles is obtained in two different fashions:\\nBy evaluating the velocity function at the location of the particles;\\nBy evaluating the velocity function on a background triangulation and, using a finite element support, interpolating at the position of the particle.\\n\\nThe first approach is not practical, since the velocity profile is generally not known analytically. The second approach, based on interpolating a solution at the position of the particles, mimics exactly what would be done in a realistic computational fluid dynamic simulation, and this follows the way we have also evaluated the finite element solution at particle locations in step-19. In this step, we illustrate both strategies.\\nWe note that much greater accuracy could be obtained by using a fourth order Runge-Kutta method or another appropriate scheme for the time integration of the motion of the particles. Implementing a more advanced time-integration scheme would be a straightforward extension of this step.\\nParticles in deal.II\\nIn deal.II, Particles::Particle are very simple and flexible entities that can be used to build PIC, DEM or any type of particle-based models. Particles have a location in real space, a location in the reference space of the element in which they are located and a unique ID. In the majority of cases, simulations that include particles require a significant number of them. Thus, it becomes interesting to handle all particles through an entity which agglomerates all particles. In deal.II, this is achieved through the use of the Particles::ParticleHandler class.\\nBy default, particles do not have a diameter, a mass or any other physical properties which we would generally expect of physical particles. However, through a ParticleHandler, particles have access to a Particles::PropertyPool. This PropertyPool is an array which can be used to store an arbitrary number of properties associated with the particles. Consequently, users can build their own particle solver and attribute the desired properties to the particles (e.g., mass, charge, diameter, temperature, etc.). In the present tutorial, this is used to store the value of the fluid velocity and the process id to which the particles belong.\\nChallenges related to distributed particle simulations\\nAlthough the present step is not computationally intensive, simulations that include many particles can be computationally demanding and require parallelization. The present step showcases the distributed parallel capabilities of deal.II for particles. In general, there are three main challenges that specifically arise in parallel distributed simulations that include particles:\\nGenerating the particles on the distributed triangulation;\\nExchanging the particles that leave local domains between the processors;\\nLoad balancing the simulation so that every processor has a similar computational load. These challenges and their solution in deal.II have been discussed in more detail in [93], but we will summarize them below.\\n\\nThere are of course also questions on simply setting up a code that uses particles. These have largely already been addressed in step-19. Some more advanced techniques will also be discussed in step-70.\\nParallel particle generation\\nGenerating distributed particles in a scalable way is not straightforward since the processor to which they belong must first be identified before the cell in which they are located is found. deal.II provides numerous capabilities to generate particles through the Particles::Generator namespace. Some of these particle generators create particles only on the locally owned subdomain. For example, Particles::Generators::regular_reference_locations() creates particles at the same reference locations within each cell of the local subdomain and Particles::Generators::probabilistic_locations() uses a globally defined probability density function to determine how many and where to generate particles locally.\\nIn other situations, such as the present step, particles must be generated at specific locations on cells that may be owned only by a subset of the processors. In most of these situations, the insertion of the particles is done for a very limited number of time-steps and, consequently, does not constitute a large portion of the computational cost. For these occasions, deal.II provides convenient Particles::Generators that can globally insert the particles even if the particle is not located in a cell owned by the parallel process on which the call to create the particle is initiated. The generators first locate on which subdomain the particles are situated, identify in which cell they are located and exchange the necessary information among the processors to ensure that the particle is generated with the right properties. Consequently, this type of particle generation can be communication intensive. The Particles::Generators::dof_support_points and the Particles::Generators::quadrature_points generate particles using a triangulation and the points of an associated DoFHandler or quadrature respectively. The triangulation that is used to generate the particles can be the same triangulation that is used for the background mesh, in which case these functions are very similar to the Particles::Generators::regular_reference_locations() function described in the previous paragraph. However, the triangulation used to generate particles can also be different (non-matching) from the triangulation of the background grid, which is useful to generate particles in particular shapes (as in this example), or to transfer information between two different computational grids (as in step-70). Furthermore, the Particles::ParticleHandler class provides the Particles::ParticleHandler::insert_global_particles() function which enables the global insertion of particles from a vector of arbitrary points and a global vector of bounding boxes. In the present step, we use the Particles::Generators::quadrature_points() function on a non-matching triangulation to insert particles located at positions in the shape of a disk.\\nParticle exchange\\nAs particles move around in parallel distributed computations they may leave the locally owned subdomain and need to be transferred to their new owner processes. This situation can arise in two very different ways: First, if the previous owning process knows the new owner of the particles that were lost (for example because the particles moved from the locally owned cell of one processor into an adjacent ghost cells of a distributed triangulation) then the transfer can be handled efficiently as a point-to-point communication between each process and the new owners. This transfer happens automatically whenever particles are sorted into their new cells. Secondly, the previous owner may not know to which process the particle has moved. In this case the particle is discarded by default, as a global search for the owner can be expensive. step-19 shows how such a discarded particle can still be collected, interpreted, and potentially reinserted by the user. In the present example we prevent the second case by imposing a CFL criterion on the timestep to ensure particles will at most move into the ghost layer of the local process and can therefore be send to neighboring processes automatically.\\nBalancing mesh and particle load\\nThe last challenge that arises in parallel distributed computations using particles is to balance the computational load between work that is done on the grid, for example solving the finite-element problem, and the work that is done on the particles, for example advecting the particles or computing the forces between particles or between particles and grid. By default, for example in step-40, deal.II distributes the background mesh as evenly as possible between the available processes, that is it balances the number of cells on each process. However, if some cells own many more particles than other cells, or if the particles of one cell are much more computationally expensive than the particles in other cells, then this problem no longer scales efficiently (for a discussion of what we consider \\\"scalable\\\" programs, see this glossary entry). Thus, we have to apply a form of \\\"load balancing\\\", which means we estimate the computational load that is associated with each cell and its particles. Repartitioning the mesh then accounts for this combined computational load instead of the simplified assumption of the number of cells [93].\\nIn this section we only discussed the particle-specific challenges in distributed computation. Parallel challenges that particles share with finite-element solutions (parallel output, data transfer during mesh refinement) can be addressed with the solutions found for finite-element problems already discussed in other examples.\\nThe testcase\\nIn the present step, we use particles as massless tracers to illustrate the dynamics of a particular vortical flow: the Rayleigh\\u2013Kothe vortex. This flow pattern is generally used as a complex test case for interface tracking methods (e.g., volume-of-fluid and level set approaches) since it leads to strong rotation and elongation of the fluid [32].\\nThe stream function \\\\(\\\\Psi\\\\) of this Rayleigh-Kothe vortex is defined as:\\n\\n\\\\[\\n\\\\Psi = \\\\frac{1}{\\\\pi} \\\\sin^2 (\\\\pi x) \\\\sin^2 (\\\\pi y) \\\\cos \\\\left( \\\\pi \\\\frac{t}{T} \\\\right)\\n\\\\]\\n\\n where \\\\(T\\\\) is half the period of the flow. The velocity profile in 2D ( \\\\(\\\\textbf{u}=[u,v]^T\\\\)) is :    \\n\\\\begin{eqnarray*}\\n   u &=&  - \\\\frac{\\\\partial\\\\Psi}{\\\\partial y} = -2 \\\\sin^2 (\\\\pi x) \\\\sin (\\\\pi y) \\\\cos (\\\\pi y)  \\\\cos \\\\left( \\\\pi \\\\frac{t}{T} \\\\right)\\\\\\\\\\n   v &=&  \\\\frac{\\\\partial\\\\Psi}{\\\\partial x} = 2 \\\\cos(\\\\pi x) \\\\sin(\\\\pi x) \\\\sin^2 (\\\\pi y) \\\\cos \\\\left( \\\\pi \\\\frac{t}{T} \\\\right)\\n\\\\end{eqnarray*}\\n\\nThe velocity profile is illustrated in the following animation:\\n\\n\\n\\n\\n\\nIt can be seen that this velocity reverses periodically due to the term \\\\(\\\\cos \\\\left( \\\\pi \\\\frac{t}{T} \\\\right)\\\\) and that material will end up at its starting position after every period of length \\\\(t=2T\\\\). We will run this tutorial program for exactly one period and compare the final particle location to the initial location to illustrate this flow property. This example uses the testcase to produce two models that handle the particles slightly differently. The first model prescribes the exact analytical velocity solution as the velocity for each particle. Therefore in this model there is no error in the assigned velocity to the particles, and any deviation of particle positions from the analytical position at a given time results from the error in solving the equation of motion for the particle inexactly, using a time stepping method. In the second model the analytical velocity field is first interpolated to a finite-element vector space (to simulate the case that the velocity was obtained from solving a finite-element problem, in the same way as the ODE for each particle in step-19 depends on a finite element solution). This finite-element \\\"solution\\\" is then evaluated at the locations of the particles to solve their equation of motion. The difference between the two cases allows to assess whether the chosen finite-element space is sufficiently accurate to advect the particles with the optimal convergence rate of the chosen particle advection scheme, a question that is important in practice to determine the accuracy of the combined algorithm (see e.g. [94]).\\n The commented program\\n Include files\\n\\u00a0 #include <deal.II/base/bounding_box.h>\\n\\u00a0 #include <deal.II/base/conditional_ostream.h>\\n\\u00a0 #include <deal.II/base/discrete_time.h>\\n\\u00a0 #include <deal.II/base/function_lib.h>\\n\\u00a0 #include <deal.II/base/mpi.h>\\n\\u00a0 #include <deal.II/base/parameter_acceptor.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/distributed/solution_transfer.h>\\n\\u00a0 #include <deal.II/distributed/tria.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 #include <deal.II/fe/fe_system.h>\\n\\u00a0 #include <deal.II/fe/mapping_q1.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/grid_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/la_parallel_vector.h>\\n\\u00a0 #include <deal.II/lac/vector.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 \\nFrom the following include file we import the ParticleHandler class that allows you to manage a collection of particles (objects of type Particles::Particle), representing a collection of points with some attached properties (e.g., an id) floating on a parallel::distributed::Triangulation. The methods and classes in the namespace Particles allows one to easily implement Particle-In-Cell methods and particle tracing on distributed triangulations:\\n\\u00a0 #include <deal.II/particles/particle_handler.h>\\n\\u00a0 \\nWe import the particles generator which allow us to insert the particles. In the present step, the particle are globally inserted using a non-matching hyper-shell triangulation:\\n\\u00a0 #include <deal.II/particles/generators.h>\\n\\u00a0 \\nSince the particles do not form a triangulation, they have their own specific DataOut class which will enable us to write them to commonly used parallel vtu format (or any number of other file formats):\\n\\u00a0 #include <deal.II/particles/data_out.h>\\n\\u00a0 \\n\\u00a0 #include <cmath>\\n\\u00a0 #include <iostream>\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0 namespace Step68\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n Run-time parameter handling\\nSimilarly to what is done in step-60, we set up a class that holds all the parameters of our problem and derive it from the ParameterAcceptor class to simplify the management and creation of parameter files.\\nThe ParameterAcceptor paradigm requires all parameters to be writable by the ParameterAcceptor methods. In order to avoid bugs that would be very difficult to track down (such as writing things like if (time = 0) instead of if(time == 0)), we declare all the parameters in an external class, which is initialized before the actual ParticleTracking class, and pass it to the main class as a const reference.\\nThe constructor of the class is responsible for the connection between the members of this class and the corresponding entries in the ParameterHandler. Thanks to the use of the ParameterHandler::add_parameter() method, this connection is trivial, but requires all members of this class to be writable.\\n\\u00a0   class ParticleTrackingParameters : public ParameterAcceptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     ParticleTrackingParameters();\\n\\u00a0 \\nParameterAcceptorDefinition parameter_acceptor.h:359\\nThis class consists largely of member variables that describe the details of the particle tracking simulation and its discretization. The following parameters are about where output should written to, the spatial discretization of the velocity (the default is \\\\(Q_1\\\\)), the time step and the output interval (how many time steps should elapse before we generate graphical output again):\\n\\u00a0     std::string output_directory = \\\"./\\\";\\n\\u00a0 \\n\\u00a0     unsigned int velocity_degree      = 1;\\n\\u00a0     double       time_step            = 0.002;\\n\\u00a0     double       final_time           = 4.0;\\n\\u00a0     unsigned int output_interval      = 10;\\n\\u00a0     unsigned int repartition_interval = 5;\\n\\u00a0 \\nWe allow every grid to be refined independently. In this tutorial, no physics is resolved on the fluid grid, and its velocity is calculated analytically.\\n\\u00a0     unsigned int fluid_refinement              = 4;\\n\\u00a0     unsigned int particle_insertion_refinement = 3;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nThere remains the task of declaring what run-time parameters we can accept in input files. Since we have a very limited number of parameters, all parameters are declared in the same section.\\n\\u00a0   ParticleTrackingParameters::ParticleTrackingParameters()\\n\\u00a0     : ParameterAcceptor(\\\"Particle Tracking Problem/\\\")\\n\\u00a0   {\\n\\u00a0     add_parameter(\\n\\u00a0       \\\"Velocity degree\\\", velocity_degree, \\\"\\\", prm, Patterns::Integer(1));\\n\\u00a0 \\n\\u00a0     add_parameter(\\\"Output interval\\\",\\n\\u00a0                   output_interval,\\n\\u00a0                   \\\"Iteration interval between which output results are written\\\",\\n\\u00a0                   prm,\\n\\u00a0                   Patterns::Integer(1));\\n\\u00a0 \\n\\u00a0     add_parameter(\\\"Repartition interval\\\",\\n\\u00a0                   repartition_interval,\\n\\u00a0                   \\\"Iteration interval at which the mesh is load balanced\\\",\\n\\u00a0                   prm,\\n\\u00a0                   Patterns::Integer(1));\\n\\u00a0 \\n\\u00a0     add_parameter(\\\"Output directory\\\", output_directory);\\n\\u00a0 \\n\\u00a0     add_parameter(\\\"Time step\\\", time_step, \\\"\\\", prm, Patterns::Double());\\n\\u00a0 \\n\\u00a0     add_parameter(\\\"Final time\\\",\\n\\u00a0                   final_time,\\n\\u00a0                   \\\"End time of the simulation\\\",\\n\\u00a0                   prm,\\n\\u00a0                   Patterns::Double());\\n\\u00a0 \\n\\u00a0     add_parameter(\\\"Fluid refinement\\\",\\n\\u00a0                   fluid_refinement,\\n\\u00a0                   \\\"Refinement level of the fluid domain\\\",\\n\\u00a0                   prm,\\n\\u00a0                   Patterns::Integer(0));\\n\\u00a0 \\n\\u00a0     add_parameter(\\n\\u00a0       \\\"Particle insertion refinement\\\",\\n\\u00a0       particle_insertion_refinement,\\n\\u00a0       \\\"Refinement of the volumetric mesh used to insert the particles\\\",\\n\\u00a0       prm,\\n\\u00a0       Patterns::Integer(0));\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nPatterns::DoubleDefinition patterns.h:291\\nPatterns::IntegerDefinition patterns.h:188\\n The ParticleTracking class declaration\\nWe are now ready to introduce the main class of our tutorial program.\\n\\u00a0   template <int dim>\\n\\u00a0   class ParticleTracking\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     ParticleTracking(const ParticleTrackingParameters &par,\\n\\u00a0                      const bool                        interpolated_velocity);\\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\nThis function is responsible for the initial generation of the particles on top of the background grid.\\n\\u00a0     void generate_particles();\\n\\u00a0 \\nWhen the velocity profile is interpolated to the position of the particles, it must first be stored using degrees of freedom. Consequently, as is the case for other parallel case (e.g. step-40) we initialize the degrees of freedom on the background grid.\\n\\u00a0     void setup_background_dofs();\\n\\u00a0 \\nIn one of the test cases, the function is mapped to the background grid and a finite element interpolation is used to calculate the velocity at the particle location. This function calculates the value of the function at the support point of the triangulation.\\n\\u00a0     void interpolate_function_to_field();\\n\\u00a0 \\nThe next two functions are responsible for carrying out step of explicit Euler time integration for the cases where the velocity field is interpolated at the positions of the particles or calculated analytically, respectively.\\n\\u00a0     void euler_step_interpolated(const double dt);\\n\\u00a0     void euler_step_analytical(const double dt);\\n\\u00a0 \\nThe cell_weight() function indicates to the triangulation how much computational work is expected to happen on this cell, and consequently how the domain needs to be partitioned so that every MPI rank receives a roughly equal amount of work (potentially not an equal number of cells). While the function is called from the outside, it is connected to the corresponding signal from inside this class, therefore it can be private.\\n\\u00a0     unsigned int cell_weight(\\n\\u00a0       const typename parallel::distributed::Triangulation<dim>::cell_iterator\\n\\u00a0                       &cell,\\n\\u00a0       const CellStatus status) const;\\n\\u00a0 \\nCellStatusCellStatusDefinition cell_status.h:31\\nparallel::distributed::Triangulation::cell_iteratortypename ::Triangulation< dim, spacedim >::cell_iterator cell_iteratorDefinition tria.h:287\\nThe following two functions are responsible for outputting the simulation results for the particles and for the velocity profile on the background mesh, respectively.\\n\\u00a0     void output_particles(const unsigned int it);\\n\\u00a0     void output_background(const unsigned int it);\\n\\u00a0 \\nThe private members of this class are similar to other parallel deal.II examples. The parameters are stored as a const member. It is important to note that we keep the Vortex class as a member since its time must be modified as the simulation proceeds.\\n\\u00a0     const ParticleTrackingParameters &par;\\n\\u00a0 \\n\\u00a0     MPI_Comm                                  mpi_communicator;\\n\\u00a0     parallel::distributed::Triangulation<dim> background_triangulation;\\n\\u00a0     Particles::ParticleHandler<dim>           particle_handler;\\n\\u00a0 \\n\\u00a0     DoFHandler<dim>                            fluid_dh;\\n\\u00a0     const FESystem<dim>                        fluid_fe;\\n\\u00a0     MappingQ1<dim>                             mapping;\\n\\u00a0     LinearAlgebra::distributed::Vector<double> velocity_field;\\n\\u00a0 \\n\\u00a0     Functions::RayleighKotheVortex<dim> velocity;\\n\\u00a0 \\n\\u00a0     ConditionalOStream pcout;\\n\\u00a0 \\n\\u00a0     bool interpolated_velocity;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nConditionalOStreamDefinition conditional_ostream.h:80\\nDoFHandlerDefinition dof_handler.h:317\\nFESystemDefinition fe_system.h:208\\nFunctions::RayleighKotheVortexDefinition function_lib.h:1813\\nLinearAlgebra::distributed::VectorDefinition la_parallel_vector.h:250\\nMPI_Comm\\nMappingQ1Definition mapping_q1.h:55\\nParticles::ParticleHandlerDefinition particle_handler.h:63\\nparallel::distributed::TriangulationDefinition tria.h:268\\n The PatricleTracking class implementation\\n Constructor\\nThe constructors and destructors are rather trivial. They are very similar to what is done in step-40. We set the processors we want to work on to all machines available (MPI_COMM_WORLD) and initialize the pcout variable to only allow processor zero to output anything to the standard output.\\n\\u00a0   template <int dim>\\n\\u00a0   ParticleTracking<dim>::ParticleTracking(const ParticleTrackingParameters &par,\\n\\u00a0                                           const bool interpolated_velocity)\\n\\u00a0     : par(par)\\n\\u00a0     , mpi_communicator(MPI_COMM_WORLD)\\n\\u00a0     , background_triangulation(mpi_communicator)\\n\\u00a0     , fluid_dh(background_triangulation)\\n\\u00a0     , fluid_fe(FE_Q<dim>(par.velocity_degree) ^ dim)\\n\\u00a0     , velocity(4.0)\\n\\u00a0     , pcout(std::cout, Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n\\u00a0     , interpolated_velocity(interpolated_velocity)\\n\\u00a0 \\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFE_QDefinition fe_q.h:554\\nInitializeLibrary::MPI@ MPI\\nUtilitiesDefinition communication_pattern_base.h:30\\nstdSTL namespace.\\n Cell weight\\nThis function is the key component that allow us to dynamically balance the computational load for this example. The function attributes a weight to every cell that represents the computational work on this cell. Here the majority of work is expected to happen on the particles, therefore the return value of this function (representing \\\"work for this cell\\\") is calculated based on the number of particles in the current cell. The function is connected to the weight signal inside the triangulation, and will be called once per cell, whenever the triangulation repartitions the domain between ranks (the connection is created inside the generate_particles() function of this class).\\n\\u00a0   template <int dim>\\n\\u00a0   unsigned int ParticleTracking<dim>::cell_weight(\\n\\u00a0     const typename parallel::distributed::Triangulation<dim>::cell_iterator\\n\\u00a0                     &cell,\\n\\u00a0     const CellStatus status) const\\n\\u00a0   {\\nFirst, we introduce a base weight that will be assigned to every cell.\\n\\u00a0     const unsigned int base_weight = 1;\\n\\u00a0 \\nThe following variable then determines how important particle work is compared to cell work. We set the weight per particle much higher to indicate that the particle load is the only one that is important to distribute the cells in this example. The optimal value of this number depends on the application and can range from 0 (cheap particle operations, expensive cell operations) to much larger than the base weight of 1 (expensive particle operations, cheap cell operations, like presumed in this example).\\n\\u00a0     const unsigned int particle_weight = 10;\\n\\u00a0 \\nThis example does not use adaptive refinement, therefore every cell should have the status CellStatus::cell_will_persist. However this function can also be used to distribute load during refinement, therefore we consider refined or coarsened cells as well.\\n\\u00a0     unsigned int n_particles_in_cell = 0;\\n\\u00a0     switch (status)\\n\\u00a0       {\\n\\u00a0         case CellStatus::cell_will_persist:\\n\\u00a0         case CellStatus::cell_will_be_refined:\\n\\u00a0           n_particles_in_cell = particle_handler.n_particles_in_cell(cell);\\n\\u00a0           break;\\n\\u00a0 \\n\\u00a0         case CellStatus::cell_invalid:\\n\\u00a0           break;\\n\\u00a0 \\n\\u00a0         case CellStatus::children_will_be_coarsened:\\n\\u00a0           for (const auto &child : cell->child_iterators())\\n\\u00a0             n_particles_in_cell += particle_handler.n_particles_in_cell(child);\\n\\u00a0           break;\\n\\u00a0 \\n\\u00a0         default:\\n\\u00a0           DEAL_II_ASSERT_UNREACHABLE();\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     return base_weight + particle_weight * n_particles_in_cell;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nCellStatus::cell_will_be_refined@ cell_will_be_refined\\nCellStatus::children_will_be_coarsened@ children_will_be_coarsened\\nCellStatus::cell_will_persist@ cell_will_persist\\nCellStatus::cell_invalid@ cell_invalid\\nDEAL_II_ASSERT_UNREACHABLE#define DEAL_II_ASSERT_UNREACHABLE()Definition exceptions.h:1897\\n Particles generation\\nThis function generates the tracer particles and the background triangulation on which these particles evolve.\\n\\u00a0   template <int dim>\\n\\u00a0   void ParticleTracking<dim>::generate_particles()\\n\\u00a0   {\\nWe create a hyper cube triangulation which we globally refine. This triangulation covers the full trajectory of the particles.\\n\\u00a0     GridGenerator::hyper_cube(background_triangulation, 0, 1);\\n\\u00a0     background_triangulation.refine_global(par.fluid_refinement);\\n\\u00a0 \\nGridGenerator::hyper_cubevoid hyper_cube(Triangulation< dim, spacedim > &tria, const double left=0., const double right=1., const bool colorize=false)\\nIn order to consider the particles when repartitioning the triangulation the algorithm needs to know how much weight to assign to each cell (how many particles are in there).\\nWe attach a weight function to the signal inside parallel::distributed::Triangulation. This signal will be called every time the repartition() function is called. This connection only needs to be created once, so we might as well have set it up in the constructor of this class, but for the purpose of this example we want to group the particle related instructions.\\n\\u00a0     background_triangulation.signals.weight.connect(\\n\\u00a0       [&](const typename parallel::distributed::Triangulation<\\n\\u00a0             dim>::cell_iterator &cell,\\n\\u00a0           const CellStatus       status) -> unsigned int {\\n\\u00a0         return this->cell_weight(cell, status);\\n\\u00a0       });\\n\\u00a0 \\nThis initializes the background triangulation where the particles are living and the number of properties of the particles.\\n\\u00a0     particle_handler.initialize(background_triangulation, mapping, 1 + dim);\\n\\u00a0 \\nWe create a particle triangulation which is solely used to generate the points which will be used to insert the particles. This triangulation is a hyper shell which is offset from the center of the simulation domain. This will be used to generate a disk filled with particles which will allow an easy monitoring of the motion due to the vortex.\\n\\u00a0     Point<dim> center;\\n\\u00a0     center[0] = 0.5;\\n\\u00a0     center[1] = 0.75;\\n\\u00a0     if (dim == 3)\\n\\u00a0       center[2] = 0.5;\\n\\u00a0 \\n\\u00a0     const double outer_radius = 0.15;\\n\\u00a0     const double inner_radius = 0.01;\\n\\u00a0 \\n\\u00a0     parallel::distributed::Triangulation<dim> particle_triangulation(\\n\\u00a0       MPI_COMM_WORLD);\\n\\u00a0 \\n\\u00a0     GridGenerator::hyper_shell(\\n\\u00a0       particle_triangulation, center, inner_radius, outer_radius, 6);\\n\\u00a0     particle_triangulation.refine_global(par.particle_insertion_refinement);\\n\\u00a0 \\nPointDefinition point.h:111\\ncenterPoint< 3 > centerDefinition data_out_base.cc:267\\nGridGenerator::hyper_shellvoid hyper_shell(Triangulation< dim, spacedim > &tria, const Point< spacedim > &center, const double inner_radius, const double outer_radius, const unsigned int n_cells=0, bool colorize=false)\\nWe generate the necessary bounding boxes for the particles generator. These bounding boxes are required to quickly identify in which process's subdomain the inserted particle lies, and which cell owns it.\\n\\u00a0     const auto my_bounding_box = GridTools::compute_mesh_predicate_bounding_box(\\n\\u00a0       background_triangulation, IteratorFilters::LocallyOwnedCell());\\n\\u00a0     const auto global_bounding_boxes =\\n\\u00a0       Utilities::MPI::all_gather(MPI_COMM_WORLD, my_bounding_box);\\n\\u00a0 \\nIteratorFilters::LocallyOwnedCellDefinition filtered_iterator.h:195\\nGridTools::compute_mesh_predicate_bounding_boxstd::vector< BoundingBox< MeshType::space_dimension > > compute_mesh_predicate_bounding_box(const MeshType &mesh, const std::function< bool(const typename MeshType::active_cell_iterator &)> &predicate, const unsigned int refinement_level=0, const bool allow_merge=false, const unsigned int max_boxes=numbers::invalid_unsigned_int)Definition grid_tools.cc:1168\\nUtilities::MPI::all_gatherstd::vector< T > all_gather(const MPI_Comm comm, const T &object_to_send)\\nWe generate an empty vector of properties. We will attribute the properties to the particles once they are generated.\\n\\u00a0     std::vector<std::vector<double>> properties(\\n\\u00a0       particle_triangulation.n_locally_owned_active_cells(),\\n\\u00a0       std::vector<double>(dim + 1, 0.));\\n\\u00a0 \\nWe generate the particles at the position of a single point quadrature. Consequently, one particle will be generated at the centroid of each cell.\\n\\u00a0     Particles::Generators::quadrature_points(particle_triangulation,\\n\\u00a0                                              QMidpoint<dim>(),\\n\\u00a0                                              global_bounding_boxes,\\n\\u00a0                                              particle_handler,\\n\\u00a0                                              mapping,\\n\\u00a0                                              properties);\\n\\u00a0 \\n\\u00a0     pcout << \\\"Number of particles inserted: \\\"\\n\\u00a0           << particle_handler.n_global_particles() << std::endl;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nQMidpointDefinition quadrature_lib.h:162\\nParticles::Generators::quadrature_pointsvoid quadrature_points(const Triangulation< dim, spacedim > &triangulation, const Quadrature< dim > &quadrature, const std::vector< std::vector< BoundingBox< spacedim > > > &global_bounding_boxes, ParticleHandler< dim, spacedim > &particle_handler, const Mapping< dim, spacedim > &mapping=(ReferenceCells::get_hypercube< dim >() .template get_default_linear_mapping< dim, spacedim >()), const std::vector< std::vector< double > > &properties={})Definition generators.cc:473\\n Background DOFs and interpolation\\nThis function sets up the background degrees of freedom used for the velocity interpolation and allocates the field vector where the entire solution of the velocity field is stored.\\n\\u00a0   template <int dim>\\n\\u00a0   void ParticleTracking<dim>::setup_background_dofs()\\n\\u00a0   {\\n\\u00a0     fluid_dh.distribute_dofs(fluid_fe);\\n\\u00a0     const IndexSet locally_owned_dofs = fluid_dh.locally_owned_dofs();\\n\\u00a0     const IndexSet locally_relevant_dofs =\\n\\u00a0       DoFTools::extract_locally_relevant_dofs(fluid_dh);\\n\\u00a0 \\n\\u00a0     velocity_field.reinit(locally_owned_dofs,\\n\\u00a0                           locally_relevant_dofs,\\n\\u00a0                           mpi_communicator);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nIndexSetDefinition index_set.h:70\\nDoFTools::extract_locally_relevant_dofsIndexSet extract_locally_relevant_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1164\\nThis function takes care of interpolating the vortex velocity field to the field vector. This is achieved rather easily by using the VectorTools::interpolate() function.\\n\\u00a0   template <int dim>\\n\\u00a0   void ParticleTracking<dim>::interpolate_function_to_field()\\n\\u00a0   {\\n\\u00a0     velocity_field.zero_out_ghost_values();\\n\\u00a0     VectorTools::interpolate(mapping, fluid_dh, velocity, velocity_field);\\n\\u00a0     velocity_field.update_ghost_values();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nVectorTools::interpolatevoid interpolate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Function< spacedim, typename VectorType::value_type > &function, VectorType &vec, const ComponentMask &component_mask={})\\n Time integration of the trajectories\\nWe integrate the particle trajectories using an analytically defined velocity field. This demonstrates a relatively trivial usage of the particles.\\n\\u00a0   template <int dim>\\n\\u00a0   void ParticleTracking<dim>::euler_step_analytical(const double dt)\\n\\u00a0   {\\n\\u00a0     const unsigned int this_mpi_rank =\\n\\u00a0       Utilities::MPI::this_mpi_process(mpi_communicator);\\n\\u00a0     Vector<double> particle_velocity(dim);\\n\\u00a0 \\nVectorDefinition vector.h:120\\nUtilities::MPI::this_mpi_processunsigned int this_mpi_process(const MPI_Comm mpi_communicator)Definition mpi.cc:107\\nLooping over all particles in the domain using a particle iterator\\n\\u00a0     for (auto &particle : particle_handler)\\n\\u00a0       {\\nWe calculate the velocity of the particles using their current location.\\n\\u00a0         Point<dim> &particle_location = particle.get_location();\\n\\u00a0         velocity.vector_value(particle_location, particle_velocity);\\n\\u00a0 \\nThis updates the position of the particles and sets the old position equal to the new position of the particle.\\n\\u00a0         for (int d = 0; d < dim; ++d)\\n\\u00a0           particle_location[d] += particle_velocity[d] * dt;\\n\\u00a0 \\nWe store the processor id (a scalar) and the particle velocity (a vector) in the particle properties. In this example, this is done purely for visualization purposes.\\n\\u00a0         ArrayView<double> properties = particle.get_properties();\\n\\u00a0         for (int d = 0; d < dim; ++d)\\n\\u00a0           properties[d] = particle_velocity[d];\\n\\u00a0         properties[dim] = this_mpi_rank;\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nArrayViewDefinition array_view.h:88\\nIn contrast to the previous function in this function we integrate the particle trajectories by interpolating the value of the velocity field at the degrees of freedom to the position of the particles. This is achieved using the FEPointEvaluation object.\\n\\u00a0   template <int dim>\\n\\u00a0   void ParticleTracking<dim>::euler_step_interpolated(const double dt)\\n\\u00a0   {\\n\\u00a0     Vector<double> local_dof_values(fluid_fe.dofs_per_cell);\\n\\u00a0 \\n\\u00a0     FEPointEvaluation<dim, dim> evaluator(mapping, fluid_fe, update_values);\\n\\u00a0     std::vector<Point<dim>>     particle_positions;\\n\\u00a0 \\nFEPointEvaluationDefinition fe_point_evaluation.h:1126\\nupdate_values@ update_valuesShape function values.Definition fe_update_flags.h:75\\nWe loop over all the local particles. Although this could be achieved directly by looping over all the cells, this would force us to loop over numerous cells which do not contain particles. Rather, we loop over all the particles, but, we get the reference of the cell in which the particle lies and then loop over all particles within that cell. This enables us to gather the values of the velocity out of the velocity_field vector once and use them for all particles that lie within the cell.\\n\\u00a0     auto particle = particle_handler.begin();\\n\\u00a0     while (particle != particle_handler.end())\\n\\u00a0       {\\n\\u00a0         const auto cell = particle->get_surrounding_cell();\\n\\u00a0         const auto dh_cell =\\n\\u00a0           typename DoFHandler<dim>::cell_iterator(*cell, &fluid_dh);\\n\\u00a0 \\n\\u00a0         dh_cell->get_dof_values(velocity_field, local_dof_values);\\n\\u00a0 \\nDoFHandler::cell_iteratortypename ActiveSelector::cell_iterator cell_iteratorDefinition dof_handler.h:468\\nNext, compute the velocity at the particle locations by evaluating the finite element solution at the position of the particles. This is achieved using FEPointEvaluation object.\\n\\u00a0         const auto pic = particle_handler.particles_in_cell(cell);\\n\\u00a0         Assert(pic.begin() == particle, ExcInternalError());\\n\\u00a0         particle_positions.clear();\\n\\u00a0         for (auto &p : pic)\\n\\u00a0           particle_positions.push_back(p.get_reference_location());\\n\\u00a0 \\n\\u00a0         evaluator.reinit(cell, particle_positions);\\n\\u00a0         evaluator.evaluate(make_array_view(local_dof_values),\\n\\u00a0                            EvaluationFlags::values);\\n\\u00a0 \\nmake_array_viewArrayView< std::remove_reference_t< typename std::iterator_traits< Iterator >::reference >, MemorySpaceType > make_array_view(const Iterator begin, const Iterator end)Definition array_view.h:949\\nAssert#define Assert(cond, exc)Definition exceptions.h:1638\\nEvaluationFlags::values@ valuesDefinition evaluation_flags.h:50\\nWe move the particles using the interpolated velocity field\\n\\u00a0         for (unsigned int particle_index = 0; particle != pic.end();\\n\\u00a0              ++particle, ++particle_index)\\n\\u00a0           {\\n\\u00a0             Point<dim>           &particle_location = particle->get_location();\\n\\u00a0             const Tensor<1, dim> &particle_velocity =\\n\\u00a0               evaluator.get_value(particle_index);\\n\\u00a0             particle_location += particle_velocity * dt;\\n\\u00a0 \\nTensorDefinition tensor.h:471\\nAgain, we store the particle velocity and the processor id in the particle properties for visualization purposes.\\n\\u00a0             ArrayView<double> properties = particle->get_properties();\\n\\u00a0             for (int d = 0; d < dim; ++d)\\n\\u00a0               properties[d] = particle_velocity[d];\\n\\u00a0 \\n\\u00a0             properties[dim] =\\n\\u00a0               Utilities::MPI::this_mpi_process(mpi_communicator);\\n\\u00a0           }\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Data output\\nThe next two functions take care of writing both the particles and the background mesh to vtu with a pvtu record. This ensures that the simulation results can be visualized when the simulation is launched in parallel.\\n\\u00a0   template <int dim>\\n\\u00a0   void ParticleTracking<dim>::output_particles(const unsigned int it)\\n\\u00a0   {\\n\\u00a0     Particles::DataOut<dim, dim> particle_output;\\n\\u00a0 \\n\\u00a0     std::vector<std::string> solution_names(dim, \\\"velocity\\\");\\n\\u00a0     solution_names.emplace_back(\\\"process_id\\\");\\n\\u00a0 \\n\\u00a0     std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n\\u00a0       data_component_interpretation(\\n\\u00a0         dim, DataComponentInterpretation::component_is_part_of_vector);\\n\\u00a0     data_component_interpretation.push_back(\\n\\u00a0       DataComponentInterpretation::component_is_scalar);\\n\\u00a0 \\n\\u00a0     particle_output.build_patches(particle_handler,\\n\\u00a0                                   solution_names,\\n\\u00a0                                   data_component_interpretation);\\n\\u00a0     const std::string output_folder(par.output_directory);\\n\\u00a0     const std::string file_name(interpolated_velocity ?\\n\\u00a0                                   \\\"interpolated-particles\\\" :\\n\\u00a0                                   \\\"analytical-particles\\\");\\n\\u00a0 \\n\\u00a0     pcout << \\\"Writing particle output file: \\\" << file_name << '-' << it\\n\\u00a0           << std::endl;\\n\\u00a0 \\n\\u00a0     particle_output.write_vtu_with_pvtu_record(\\n\\u00a0       output_folder, file_name, it, mpi_communicator, 6);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void ParticleTracking<dim>::output_background(const unsigned int it)\\n\\u00a0   {\\n\\u00a0     std::vector<std::string> solution_names(dim, \\\"velocity\\\");\\n\\u00a0     std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n\\u00a0       data_component_interpretation(\\n\\u00a0         dim, DataComponentInterpretation::component_is_part_of_vector);\\n\\u00a0 \\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0 \\nDataOutDefinition data_out.h:147\\nParticles::DataOutDefinition data_out.h:44\\nDataComponentInterpretation::component_is_scalar@ component_is_scalarDefinition data_component_interpretation.h:52\\nDataComponentInterpretation::component_is_part_of_vector@ component_is_part_of_vectorDefinition data_component_interpretation.h:58\\nAttach the solution data to data_out object\\n\\u00a0     data_out.attach_dof_handler(fluid_dh);\\n\\u00a0     data_out.add_data_vector(velocity_field,\\n\\u00a0                              solution_names,\\n\\u00a0                              DataOut<dim>::type_dof_data,\\n\\u00a0                              data_component_interpretation);\\n\\u00a0     Vector<float> subdomain(background_triangulation.n_active_cells());\\n\\u00a0     for (unsigned int i = 0; i < subdomain.size(); ++i)\\n\\u00a0       subdomain(i) = background_triangulation.locally_owned_subdomain();\\n\\u00a0     data_out.add_data_vector(subdomain, \\\"subdomain\\\");\\n\\u00a0 \\n\\u00a0     data_out.build_patches(mapping);\\n\\u00a0 \\n\\u00a0     const std::string output_folder(par.output_directory);\\n\\u00a0     const std::string file_name(\\\"background\\\");\\n\\u00a0 \\n\\u00a0     pcout << \\\"Writing background field file: \\\" << file_name << '-' << it\\n\\u00a0           << std::endl;\\n\\u00a0 \\n\\u00a0     data_out.write_vtu_with_pvtu_record(\\n\\u00a0       output_folder, file_name, it, mpi_communicator, 6);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Running the simulation\\nThis function orchestrates the entire simulation. It is very similar to the other time dependent tutorial programs \\u2013 take step-21 or step-26 as an example. Note that we use the DiscreteTime class to monitor the time, the time-step and the step-number. This function is relatively straightforward.\\n\\u00a0   template <int dim>\\n\\u00a0   void ParticleTracking<dim>::run()\\n\\u00a0   {\\n\\u00a0     DiscreteTime discrete_time(0, par.final_time, par.time_step);\\n\\u00a0 \\n\\u00a0     generate_particles();\\n\\u00a0 \\n\\u00a0     pcout << \\\"Repartitioning triangulation after particle generation\\\"\\n\\u00a0           << std::endl;\\n\\u00a0 \\n\\u00a0     particle_handler.prepare_for_coarsening_and_refinement();\\n\\u00a0     background_triangulation.repartition();\\n\\u00a0     particle_handler.unpack_after_coarsening_and_refinement();\\n\\u00a0 \\nDiscreteTimeDefinition discrete_time.h:233\\nWe set the initial property of the particles by doing an explicit Euler iteration with a time-step of 0 both in the case of the analytical and the interpolated approach.\\n\\u00a0     if (interpolated_velocity)\\n\\u00a0       {\\n\\u00a0         setup_background_dofs();\\n\\u00a0         interpolate_function_to_field();\\n\\u00a0         euler_step_interpolated(0.);\\n\\u00a0       }\\n\\u00a0     else\\n\\u00a0       euler_step_analytical(0.);\\n\\u00a0 \\n\\u00a0     output_particles(discrete_time.get_step_number());\\n\\u00a0     if (interpolated_velocity)\\n\\u00a0       output_background(discrete_time.get_step_number());\\n\\u00a0 \\nThe particles are advected by looping over time.\\n\\u00a0     while (!discrete_time.is_at_end())\\n\\u00a0       {\\n\\u00a0         discrete_time.advance_time();\\n\\u00a0         velocity.set_time(discrete_time.get_previous_time());\\n\\u00a0 \\n\\u00a0         if ((discrete_time.get_step_number() % par.repartition_interval) == 0)\\n\\u00a0           {\\n\\u00a0             particle_handler.prepare_for_coarsening_and_refinement();\\n\\u00a0             background_triangulation.repartition();\\n\\u00a0             particle_handler.unpack_after_coarsening_and_refinement();\\n\\u00a0 \\n\\u00a0             if (interpolated_velocity)\\n\\u00a0               setup_background_dofs();\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         if (interpolated_velocity)\\n\\u00a0           {\\n\\u00a0             interpolate_function_to_field();\\n\\u00a0             euler_step_interpolated(discrete_time.get_previous_step_size());\\n\\u00a0           }\\n\\u00a0         else\\n\\u00a0           euler_step_analytical(discrete_time.get_previous_step_size());\\n\\u00a0 \\nAfter the particles have been moved, it is necessary to identify in which cell they now reside. This is achieved by calling sort_particles_into_subdomains_and_cells\\n\\u00a0         particle_handler.sort_particles_into_subdomains_and_cells();\\n\\u00a0 \\n\\u00a0         if ((discrete_time.get_step_number() % par.output_interval) == 0)\\n\\u00a0           {\\n\\u00a0             output_particles(discrete_time.get_step_number());\\n\\u00a0             if (interpolated_velocity)\\n\\u00a0               output_background(discrete_time.get_step_number());\\n\\u00a0           }\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 } // namespace Step68\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n The main() function\\nThe remainder of the code, the main() function, is standard. We note that we run the particle tracking with the analytical velocity and the interpolated velocity and produce both results\\n\\u00a0 int main(int argc, char *argv[])\\n\\u00a0 {\\n\\u00a0   using namespace Step68;\\n\\u00a0   using namespace dealii;\\n\\u00a0   deallog.depth_console(1);\\n\\u00a0 \\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n\\u00a0 \\n\\u00a0       std::string prm_file;\\n\\u00a0       if (argc > 1)\\n\\u00a0         prm_file = argv[1];\\n\\u00a0       else\\n\\u00a0         prm_file = \\\"parameters.prm\\\";\\n\\u00a0 \\n\\u00a0       ParticleTrackingParameters par;\\n\\u00a0       ParameterAcceptor::initialize(prm_file);\\n\\u00a0       {\\n\\u00a0         Step68::ParticleTracking<2> particle_tracking(par, false);\\n\\u00a0         particle_tracking.run();\\n\\u00a0       }\\n\\u00a0       {\\n\\u00a0         Step68::ParticleTracking<2> particle_tracking(par, true);\\n\\u00a0         particle_tracking.run();\\n\\u00a0       }\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0 \\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return 0;\\n\\u00a0 }\\nLogStream::depth_consoleunsigned int depth_console(const unsigned int n)Definition logstream.cc:349\\nParameterAcceptor::initializestatic void initialize(const std::string &filename=\\\"\\\", const std::string &output_filename=\\\"\\\", const ParameterHandler::OutputStyle output_style_for_output_filename=ParameterHandler::Short, ParameterHandler &prm=ParameterAcceptor::prm, const ParameterHandler::OutputStyle output_style_for_filename=ParameterHandler::DefaultStyle)Definition parameter_acceptor.cc:80\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\ndeallogLogStream deallogDefinition logstream.cc:36\\n Results\\nThe directory in which this program is run contains an example parameter file by default. If you do not specify a parameter file as an argument on the command line, the program will try to read the file \\\"parameters.prm\\\" by default, and will execute the code.\\nOn any number of cores, the simulation output will look like:\\nbash@f$ mpirun -np 4 ./step-68 parameters.prm\\nNumber of particles inserted: 606\\nRepartitioning triangulation after particle generation\\nWriting particle output file: analytical-particles-0\\nWriting particle output file: analytical-particles-10\\nWriting particle output file: analytical-particles-20\\nWriting particle output file: analytical-particles-30\\n...\\nNumber of particles inserted: 606\\nRepartitioning triangulation after particle generation\\nWriting particle output file: interpolated-particles-0\\nWriting background field file: background-0\\nWriting particle output file: interpolated-particles-10\\nWriting background field file: background-10\\nWriting particle output file: interpolated-particles-20\\nWriting background field file: background-20\\nWriting particle output file: interpolated-particles-30\\nWriting background field file: background-30\\n...\\nWriting particle output file: interpolated-particles-1980\\nWriting background field file: background-1980\\nWriting particle output file: interpolated-particles-1990\\nWriting background field file: background-1990\\nWriting particle output file: interpolated-particles-2000\\nWriting background field file: background-2000\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\nWe note that, by default, the simulation runs the particle tracking with an analytical velocity for 2000 iterations, then restarts from the beginning and runs the particle tracking with velocity interpolation for the same duration. The results are written every 10th iteration.\\nMotion of the particles \\nThe following animation displays the trajectory of the particles as they are advected by the flow field. We see that after the complete duration of the flow, the particle go back to their initial configuration as is expected.\\n\\n\\n\\n\\n\\nDynamic load balancing \\nThe following animation shows the impact of dynamic load balancing. We clearly see that the subdomains adapt themselves to balance the number of particles per subdomain. However, a perfect load balancing is not reached, in part due to the coarseness of the background mesh.\\n\\n\\n\\n\\n\\nPossibilities for extensions\\nThis program highlights some of the main capabilities for handling particles in deal.II, notably their capacity to be used in distributed parallel simulations. However, this step could be extended in numerous manners:\\nHigh-order time integration (for example using a Runge-Kutta 4 method) could be used to increase the accuracy or allow for larger time-step sizes with the same accuracy.\\nThe full equation of motion (with inertia) could be solved for the particles. In this case the particles would need to have additional properties such as their mass, as in step-19, and if one wanted to also consider interactions with the fluid, their diameter.\\nCoupling to a flow solver. This step could be straightforwardly coupled to any parallel program in which the Stokes (step-32, step-70) or the Navier-Stokes equations are solved (e.g., step-57).\\nComputing the difference in final particle positions between the two models would allow to quantify the influence of the interpolation error on particle motion.\\n\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2020 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Authors: Bruno Blais, Toni El Geitani Nehme, Rene Gassmoeller, Peter Munch\\n */\\n \\n \\n#include <deal.II/base/bounding_box.h>\\n#include <deal.II/base/conditional_ostream.h>\\n#include <deal.II/base/discrete_time.h>\\n#include <deal.II/base/function_lib.h>\\n#include <deal.II/base/mpi.h>\\n#include <deal.II/base/parameter_acceptor.h>\\n#include <deal.II/base/timer.h>\\n \\n#include <deal.II/distributed/solution_transfer.h>\\n#include <deal.II/distributed/tria.h>\\n \\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/dofs/dof_tools.h>\\n \\n#include <deal.II/fe/fe_q.h>\\n#include <deal.II/fe/fe_system.h>\\n#include <deal.II/fe/mapping_q1.h>\\n \\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/grid_tools.h>\\n \\n#include <deal.II/lac/la_parallel_vector.h>\\n#include <deal.II/lac/vector.h>\\n \\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/vector_tools.h>\\n \\n#include <deal.II/particles/particle_handler.h>\\n \\n#include <deal.II/particles/generators.h>\\n \\n#include <deal.II/particles/data_out.h>\\n \\n#include <cmath>\\n#include <iostream>\\n \\n \\n \\nnamespace Step68\\n{\\n using namespace dealii;\\n \\n \\n class ParticleTrackingParameters : public ParameterAcceptor\\n  {\\n public:\\n    ParticleTrackingParameters();\\n \\n    std::string output_directory = \\\"./\\\";\\n \\n unsigned int velocity_degree      = 1;\\n double       time_step            = 0.002;\\n double       final_time           = 4.0;\\n unsigned int output_interval      = 10;\\n unsigned int repartition_interval = 5;\\n \\n unsigned int fluid_refinement              = 4;\\n unsigned int particle_insertion_refinement = 3;\\n  };\\n \\n \\n \\n  ParticleTrackingParameters::ParticleTrackingParameters()\\n    : ParameterAcceptor(\\\"Particle Tracking Problem/\\\")\\n  {\\n    add_parameter(\\n \\\"Velocity degree\\\", velocity_degree, \\\"\\\", prm, Patterns::Integer(1));\\n \\n    add_parameter(\\\"Output interval\\\",\\n                  output_interval,\\n \\\"Iteration interval between which output results are written\\\",\\n                  prm,\\n Patterns::Integer(1));\\n \\n    add_parameter(\\\"Repartition interval\\\",\\n                  repartition_interval,\\n \\\"Iteration interval at which the mesh is load balanced\\\",\\n                  prm,\\n Patterns::Integer(1));\\n \\n    add_parameter(\\\"Output directory\\\", output_directory);\\n \\n    add_parameter(\\\"Time step\\\", time_step, \\\"\\\", prm, Patterns::Double());\\n \\n    add_parameter(\\\"Final time\\\",\\n                  final_time,\\n \\\"End time of the simulation\\\",\\n                  prm,\\n Patterns::Double());\\n \\n    add_parameter(\\\"Fluid refinement\\\",\\n                  fluid_refinement,\\n \\\"Refinement level of the fluid domain\\\",\\n                  prm,\\n Patterns::Integer(0));\\n \\n    add_parameter(\\n \\\"Particle insertion refinement\\\",\\n      particle_insertion_refinement,\\n \\\"Refinement of the volumetric mesh used to insert the particles\\\",\\n      prm,\\n Patterns::Integer(0));\\n  }\\n \\n \\n \\n \\n template <int dim>\\n class ParticleTracking\\n  {\\n public:\\n    ParticleTracking(const ParticleTrackingParameters &par,\\n const bool                        interpolated_velocity);\\n void run();\\n \\n private:\\n void generate_particles();\\n \\n void setup_background_dofs();\\n \\n void interpolate_function_to_field();\\n \\n void euler_step_interpolated(const double dt);\\n void euler_step_analytical(const double dt);\\n \\n unsigned int cell_weight(\\n const typename parallel::distributed::Triangulation<dim>::cell_iterator\\n                      &cell,\\n const CellStatus status) const;\\n \\n void output_particles(const unsigned int it);\\n void output_background(const unsigned int it);\\n \\n \\n const ParticleTrackingParameters &par;\\n \\n MPI_Comm                                  mpi_communicator;\\n parallel::distributed::Triangulation<dim> background_triangulation;\\n Particles::ParticleHandler<dim>           particle_handler;\\n \\n DoFHandler<dim>                            fluid_dh;\\n const FESystem<dim>                        fluid_fe;\\n MappingQ1<dim>                             mapping;\\n LinearAlgebra::distributed::Vector<double> velocity_field;\\n \\n Functions::RayleighKotheVortex<dim> velocity;\\n \\n ConditionalOStream pcout;\\n \\n bool interpolated_velocity;\\n  };\\n \\n \\n \\n \\n \\n \\n template <int dim>\\n  ParticleTracking<dim>::ParticleTracking(const ParticleTrackingParameters &par,\\n const bool interpolated_velocity)\\n    : par(par)\\n    , mpi_communicator(MPI_COMM_WORLD)\\n    , background_triangulation(mpi_communicator)\\n    , fluid_dh(background_triangulation)\\n    , fluid_fe(FE_Q<dim>(par.velocity_degree) ^ dim)\\n    , velocity(4.0)\\n    , pcout(std::cout, Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n    , interpolated_velocity(interpolated_velocity)\\n \\n  {}\\n \\n \\n \\n \\n template <int dim>\\n unsigned int ParticleTracking<dim>::cell_weight(\\n const typename parallel::distributed::Triangulation<dim>::cell_iterator\\n                    &cell,\\n const CellStatus status) const\\n {\\n const unsigned int base_weight = 1;\\n \\n const unsigned int particle_weight = 10;\\n \\n unsigned int n_particles_in_cell = 0;\\n switch (status)\\n      {\\n case CellStatus::cell_will_persist:\\n case CellStatus::cell_will_be_refined:\\n          n_particles_in_cell = particle_handler.n_particles_in_cell(cell);\\n break;\\n \\n case CellStatus::cell_invalid:\\n break;\\n \\n case CellStatus::children_will_be_coarsened:\\n for (const auto &child : cell->child_iterators())\\n            n_particles_in_cell += particle_handler.n_particles_in_cell(child);\\n break;\\n \\n default:\\n DEAL_II_ASSERT_UNREACHABLE();\\n      }\\n \\n return base_weight + particle_weight * n_particles_in_cell;\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void ParticleTracking<dim>::generate_particles()\\n  {\\n GridGenerator::hyper_cube(background_triangulation, 0, 1);\\n    background_triangulation.refine_global(par.fluid_refinement);\\n \\n    background_triangulation.signals.weight.connect(\\n      [&](const typename parallel::distributed::Triangulation<\\n            dim>::cell_iterator &cell,\\n const CellStatus       status) -> unsigned int {\\n return this->cell_weight(cell, status);\\n      });\\n \\n    particle_handler.initialize(background_triangulation, mapping, 1 + dim);\\n \\n Point<dim> center;\\n center[0] = 0.5;\\n center[1] = 0.75;\\n if (dim == 3)\\n center[2] = 0.5;\\n \\n const double outer_radius = 0.15;\\n const double inner_radius = 0.01;\\n \\n parallel::distributed::Triangulation<dim> particle_triangulation(\\n      MPI_COMM_WORLD);\\n \\n GridGenerator::hyper_shell(\\n      particle_triangulation, center, inner_radius, outer_radius, 6);\\n    particle_triangulation.refine_global(par.particle_insertion_refinement);\\n \\n const auto my_bounding_box = GridTools::compute_mesh_predicate_bounding_box(\\n      background_triangulation, IteratorFilters::LocallyOwnedCell());\\n const auto global_bounding_boxes =\\n Utilities::MPI::all_gather(MPI_COMM_WORLD, my_bounding_box);\\n \\n    std::vector<std::vector<double>> properties(\\n      particle_triangulation.n_locally_owned_active_cells(),\\n      std::vector<double>(dim + 1, 0.));\\n \\n Particles::Generators::quadrature_points(particle_triangulation,\\n QMidpoint<dim>(),\\n                                             global_bounding_boxes,\\n                                             particle_handler,\\n                                             mapping,\\n                                             properties);\\n \\n    pcout << \\\"Number of particles inserted: \\\"\\n          << particle_handler.n_global_particles() << std::endl;\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void ParticleTracking<dim>::setup_background_dofs()\\n  {\\n    fluid_dh.distribute_dofs(fluid_fe);\\n const IndexSet locally_owned_dofs = fluid_dh.locally_owned_dofs();\\n const IndexSet locally_relevant_dofs =\\n DoFTools::extract_locally_relevant_dofs(fluid_dh);\\n \\n    velocity_field.reinit(locally_owned_dofs,\\n                          locally_relevant_dofs,\\n                          mpi_communicator);\\n  }\\n \\n \\n \\n template <int dim>\\n void ParticleTracking<dim>::interpolate_function_to_field()\\n  {\\n    velocity_field.zero_out_ghost_values();\\n VectorTools::interpolate(mapping, fluid_dh, velocity, velocity_field);\\n    velocity_field.update_ghost_values();\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void ParticleTracking<dim>::euler_step_analytical(const double dt)\\n  {\\n const unsigned int this_mpi_rank =\\n Utilities::MPI::this_mpi_process(mpi_communicator);\\n Vector<double> particle_velocity(dim);\\n \\n for (auto &particle : particle_handler)\\n      {\\n Point<dim> &particle_location = particle.get_location();\\n        velocity.vector_value(particle_location, particle_velocity);\\n \\n for (int d = 0; d < dim; ++d)\\n          particle_location[d] += particle_velocity[d] * dt;\\n \\n ArrayView<double> properties = particle.get_properties();\\n for (int d = 0; d < dim; ++d)\\n          properties[d] = particle_velocity[d];\\n        properties[dim] = this_mpi_rank;\\n      }\\n  }\\n \\n \\n \\n template <int dim>\\n void ParticleTracking<dim>::euler_step_interpolated(const double dt)\\n  {\\n Vector<double> local_dof_values(fluid_fe.dofs_per_cell);\\n \\n FEPointEvaluation<dim, dim> evaluator(mapping, fluid_fe, update_values);\\n    std::vector<Point<dim>>     particle_positions;\\n \\n auto particle = particle_handler.begin();\\n while (particle != particle_handler.end())\\n      {\\n const auto cell = particle->get_surrounding_cell();\\n const auto dh_cell =\\n typename DoFHandler<dim>::cell_iterator(*cell, &fluid_dh);\\n \\n        dh_cell->get_dof_values(velocity_field, local_dof_values);\\n \\n const auto pic = particle_handler.particles_in_cell(cell);\\n Assert(pic.begin() == particle, ExcInternalError());\\n        particle_positions.clear();\\n for (auto &p : pic)\\n          particle_positions.push_back(p.get_reference_location());\\n \\n        evaluator.reinit(cell, particle_positions);\\n        evaluator.evaluate(make_array_view(local_dof_values),\\n EvaluationFlags::values);\\n \\n for (unsigned int particle_index = 0; particle != pic.end();\\n             ++particle, ++particle_index)\\n          {\\n Point<dim>           &particle_location = particle->get_location();\\n const Tensor<1, dim> &particle_velocity =\\n              evaluator.get_value(particle_index);\\n            particle_location += particle_velocity * dt;\\n \\n ArrayView<double> properties = particle->get_properties();\\n for (int d = 0; d < dim; ++d)\\n              properties[d] = particle_velocity[d];\\n \\n            properties[dim] =\\n Utilities::MPI::this_mpi_process(mpi_communicator);\\n          }\\n      }\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void ParticleTracking<dim>::output_particles(const unsigned int it)\\n  {\\n Particles::DataOut<dim, dim> particle_output;\\n \\n    std::vector<std::string> solution_names(dim, \\\"velocity\\\");\\n    solution_names.emplace_back(\\\"process_id\\\");\\n \\n    std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n      data_component_interpretation(\\n        dim, DataComponentInterpretation::component_is_part_of_vector);\\n    data_component_interpretation.push_back(\\n DataComponentInterpretation::component_is_scalar);\\n \\n    particle_output.build_patches(particle_handler,\\n                                  solution_names,\\n                                  data_component_interpretation);\\n const std::string output_folder(par.output_directory);\\n const std::string file_name(interpolated_velocity ?\\n \\\"interpolated-particles\\\" :\\n \\\"analytical-particles\\\");\\n \\n    pcout << \\\"Writing particle output file: \\\" << file_name << '-' << it\\n          << std::endl;\\n \\n    particle_output.write_vtu_with_pvtu_record(\\n      output_folder, file_name, it, mpi_communicator, 6);\\n  }\\n \\n \\n \\n template <int dim>\\n void ParticleTracking<dim>::output_background(const unsigned int it)\\n  {\\n    std::vector<std::string> solution_names(dim, \\\"velocity\\\");\\n    std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n      data_component_interpretation(\\n        dim, DataComponentInterpretation::component_is_part_of_vector);\\n \\n DataOut<dim> data_out;\\n \\n    data_out.attach_dof_handler(fluid_dh);\\n    data_out.add_data_vector(velocity_field,\\n                             solution_names,\\n DataOut<dim>::type_dof_data,\\n                             data_component_interpretation);\\n Vector<float> subdomain(background_triangulation.n_active_cells());\\n for (unsigned int i = 0; i < subdomain.size(); ++i)\\n      subdomain(i) = background_triangulation.locally_owned_subdomain();\\n    data_out.add_data_vector(subdomain, \\\"subdomain\\\");\\n \\n    data_out.build_patches(mapping);\\n \\n const std::string output_folder(par.output_directory);\\n const std::string file_name(\\\"background\\\");\\n \\n    pcout << \\\"Writing background field file: \\\" << file_name << '-' << it\\n          << std::endl;\\n \\n    data_out.write_vtu_with_pvtu_record(\\n      output_folder, file_name, it, mpi_communicator, 6);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void ParticleTracking<dim>::run()\\n  {\\n DiscreteTime discrete_time(0, par.final_time, par.time_step);\\n \\n    generate_particles();\\n \\n    pcout << \\\"Repartitioning triangulation after particle generation\\\"\\n          << std::endl;\\n \\n    particle_handler.prepare_for_coarsening_and_refinement();\\n    background_triangulation.repartition();\\n    particle_handler.unpack_after_coarsening_and_refinement();\\n \\n if (interpolated_velocity)\\n      {\\n        setup_background_dofs();\\n        interpolate_function_to_field();\\n        euler_step_interpolated(0.);\\n      }\\n else\\n      euler_step_analytical(0.);\\n \\n    output_particles(discrete_time.get_step_number());\\n if (interpolated_velocity)\\n      output_background(discrete_time.get_step_number());\\n \\n while (!discrete_time.is_at_end())\\n      {\\n        discrete_time.advance_time();\\n        velocity.set_time(discrete_time.get_previous_time());\\n \\n if ((discrete_time.get_step_number() % par.repartition_interval) == 0)\\n          {\\n            particle_handler.prepare_for_coarsening_and_refinement();\\n            background_triangulation.repartition();\\n            particle_handler.unpack_after_coarsening_and_refinement();\\n \\n if (interpolated_velocity)\\n              setup_background_dofs();\\n          }\\n \\n if (interpolated_velocity)\\n          {\\n            interpolate_function_to_field();\\n            euler_step_interpolated(discrete_time.get_previous_step_size());\\n          }\\n else\\n          euler_step_analytical(discrete_time.get_previous_step_size());\\n \\n        particle_handler.sort_particles_into_subdomains_and_cells();\\n \\n if ((discrete_time.get_step_number() % par.output_interval) == 0)\\n          {\\n            output_particles(discrete_time.get_step_number());\\n if (interpolated_velocity)\\n              output_background(discrete_time.get_step_number());\\n          }\\n      }\\n  }\\n \\n} // namespace Step68\\n \\n \\n \\n \\nint main(int argc, char *argv[])\\n{\\n using namespace Step68;\\n using namespace dealii;\\n deallog.depth_console(1);\\n \\n try\\n    {\\n Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n \\n      std::string prm_file;\\n if (argc > 1)\\n        prm_file = argv[1];\\n else\\n        prm_file = \\\"parameters.prm\\\";\\n \\n      ParticleTrackingParameters par;\\n ParameterAcceptor::initialize(prm_file);\\n      {\\n        Step68::ParticleTracking<2> particle_tracking(par, false);\\n        particle_tracking.run();\\n      }\\n      {\\n        Step68::ParticleTracking<2> particle_tracking(par, true);\\n        particle_tracking.run();\\n      }\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n \\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n \\n return 0;\\n}\\nbounding_box.h\\nDataOutInterface::write_vtu_with_pvtu_recordstd::string write_vtu_with_pvtu_record(const std::string &directory, const std::string &filename_without_extension, const unsigned int counter, const MPI_Comm mpi_communicator, const unsigned int n_digits_for_counter=numbers::invalid_unsigned_int, const unsigned int n_groups=0) constDefinition data_out_base.cc:7854\\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\nParticles::DataOut::build_patchesvoid build_patches(const Particles::ParticleHandler< dim, spacedim > &particles, const std::vector< std::string > &data_component_names={}, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretations={})Definition data_out.cc:27\\nconditional_ostream.h\\ndiscrete_time.h\\nsolution_transfer.h\\ntria.h\\ndof_handler.h\\ndof_tools.h\\nfe_q.h\\nfe_system.h\\nfunction_lib.h\\ngenerators.h\\ngrid_generator.h\\ngrid_tools.h\\nla_parallel_vector.h\\nmapping_q1.h\\nmpi.h\\nPhysics::Elasticity::Kinematics::dSymmetricTensor< 2, dim, Number > d(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\nWorkStream::internal::tbb_no_coloring::runvoid run(const Iterator &begin, const std_cxx20::type_identity_t< Iterator > &end, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length, const unsigned int chunk_size)Definition work_stream.h:471\\ntypes::particle_indexunsigned int particle_indexDefinition property_pool.h:64\\ndata_out.h\\nparameter_acceptor.h\\nparticle_handler.h\\ndata_out.h\\ntimer.h\\nvector.h\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"