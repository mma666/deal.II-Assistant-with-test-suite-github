"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_75.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-75 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-75 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-75 tutorial program\\n\\n\\nThis tutorial depends on step-27, step-37.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\nLoad balancing\\nhp-decision indicators\\nHybrid geometric multigrid\\nThe test case\\n\\n The commented program\\n\\nInclude files\\nThe Solution class template\\nParameters\\nMatrix-free Laplace operator\\nSolver and preconditioner\\n\\nConjugate-gradient solver with multigrid preconditioner\\nHybrid polynomial/geometric-global-coarsening multigrid preconditioner\\n\\nThe LaplaceProblem class template\\nThe LaplaceProblem class implementation\\n\\nConstructor\\nLaplaceProblem::initialize_grid\\nLaplaceProblem::setup_system\\nLaplaceProblem::print_diagnostics\\nLaplaceProblem::solve_system\\nLaplaceProblem::compute_indicators\\nLaplaceProblem::adapt_resolution\\nLaplaceProblem::output_results\\nLaplaceProblem::run\\nmain()\\n\\n\\n\\n Results\\n\\nPossibilities for extensions\\n\\nDifferent hp-decision strategies\\nSolve with matrix-based methods\\nMultigrid variants\\n\\n\\n The plain program\\n   \\n\\n\\nThis program was contributed by Marc Fehling, Peter Munch and Wolfgang Bangerth. \\n This material is based upon work partly supported by the National Science Foundation under Award No. DMS-1821210, EAR-1550901, and OAC-1835673. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the National Science Foundation. \\n Peter Munch would like to thank Timo Heister, Martin Kronbichler, and Laura Prieto Saavedra for many very interesting discussions. \\nNoteIf you use this program as a basis for your own work, please consider citing it in your list of references. The initial version of this work was contributed to the deal.II project by the authors listed in the following citation:   \\n\\nAs a prerequisite of this program, you need to have the p4est library and the Trilinos library installed. The installation of deal.II together with these additional libraries is described in the README file.\\n Introduction\\nIn the finite element context, more degrees of freedom usually yield a more accurate solution but also require more computational effort.\\nThroughout previous tutorials, we found ways to effectively distribute degrees of freedom by aligning the grid resolution locally with the complexity of the solution (adaptive mesh refinement, step-6). This approach is particularly effective if we do not only adapt the grid alone, but also locally adjust the polynomial degree of the associated finite element on each cell (hp-adaptation, step-27).\\nIn addition, assigning more processes to run your program simultaneously helps to tackle the computational workload in lesser time. Depending on the hardware architecture of your machine, your program must either be prepared for the case that all processes have access to the same memory (shared memory, step-18), or that processes are hosted on several independent nodes (distributed memory, step-40).\\nIn the high-performance computing segment, memory access turns out to be the current bottleneck on supercomputers. We can avoid storing matrices altogether by computing the effect of matrix-vector products on the fly with MatrixFree methods (step-37). They can be used for geometric multigrid methods (step-50) and also for polynomial multigrid methods to speed solving the system of equation tremendously.\\nThis tutorial combines all of these particularities and presents a state-of-the-art way how to solve a simple Laplace problem: utilizing both hp-adaptation and matrix-free hybrid multigrid methods on machines with distributed memory.\\nLoad balancing\\nFor parallel applications in FEM, we partition the grid into subdomains (aka domain decomposition), which are assigned to processes. This partitioning happens on active cells in deal.II as demonstrated in step-40. There, each cell has the same finite element and the same number of degrees of freedom assigned, and approximately the same workload. To balance the workload among all processes, we have to balance the number of cells on all participating processes.\\nWith hp-adaptive methods, this is no longer the case: the finite element type may vary from cell to cell and consequently also the number of degrees of freedom. Matching the number of cells does not yield a balanced workload. In the matrix-free context, the workload can be assumed to be proportional the number of degrees of freedom of each process, since in the best case only the source and the destination vector have to be loaded.\\nOne could balance the workload by assigning weights to every cell which are proportional to the number of degrees of freedom, and balance the sum of all weights between all processes. Assigning individual weights to each cell can be realized with the class parallel::CellWeights which we will use later.\\nhp-decision indicators\\nWith hp-adaptive methods, we not only have to decide which cells we want to refine or coarsen, but we also have the choice how we want to do that: either by adjusting the grid resolution or the polynomial degree of the finite element.\\nWe will again base the decision on which cells to adapt on (a posteriori) computed error estimates of the current solution, e.g., using the KellyErrorEstimator. We will similarly decide how to adapt with (a posteriori) computed smoothness estimates: large polynomial degrees work best on smooth parts of the solution while fine grid resolutions are favorable on irregular parts. In step-27, we presented a way to calculate smoothness estimates based on the decay of Fourier coefficients. Let us take here the opportunity and present an alternative that follows the same idea, but with Legendre coefficients.\\nWe will briefly present the idea of this new technique, but limit its description to 1D for simplicity. Suppose \\\\(u_\\\\text{hp}(x)\\\\) is a finite element function defined on a cell \\\\(K\\\\) as   \\n\\\\[\\nu_\\\\text{hp}(x) = \\\\sum c_i \\\\varphi_i(x)\\n\\\\]\\n\\n where each \\\\(\\\\varphi_i(x)\\\\) is a shape function. We can equivalently represent \\\\(u_\\\\text{hp}(x)\\\\) in the basis of Legendre polynomials \\\\(P_k\\\\) as   \\n\\\\[\\nu_\\\\text{hp}(x) = \\\\sum l_k P_k(x).\\n\\\\]\\n\\n Our goal is to obtain a mapping between the finite element coefficients \\\\(c_i\\\\) and the Legendre coefficients \\\\(l_k\\\\). We will accomplish this by writing the problem as a \\\\(L^2\\\\)-projection of \\\\(u_\\\\text{hp}(x)\\\\) onto the Legendre basis. Each coefficient \\\\(l_k\\\\) can be calculated via   \\n\\\\[\\nl_k = \\\\int_K u_\\\\text{hp}(x) P_k(x) dx.\\n\\\\]\\n\\n By construction, the Legendre polynomials are orthogonal under the \\\\(L^2\\\\)-inner product on \\\\(K\\\\). Additionally, we assume that they have been normalized, so their inner products can be written as   \\n\\\\[\\n\\\\int_K P_i(x) P_j(x) dx = \\\\det(J_K) \\\\, \\\\delta_{ij}\\n\\\\]\\n\\n where \\\\(\\\\delta_{ij}\\\\) is the Kronecker delta, and \\\\(J_K\\\\) is the Jacobian of the mapping from \\\\(\\\\hat{K}\\\\) to \\\\(K\\\\), which (in this tutorial) is assumed to be constant (i.e., the mapping must be affine).\\nHence, combining all these assumptions, the projection matrix for expressing \\\\(u_\\\\text{hp}(x)\\\\) in the Legendre basis is just  \\\\(\\\\det(J_K) \\\\,\\n\\\\mathbb{I}\\\\) \\u2013 that is, \\\\(\\\\det(J_K)\\\\) times the identity matrix. Let \\\\(F_K\\\\) be the Mapping from \\\\(K\\\\) to its reference cell \\\\(\\\\hat{K}\\\\). The entries in the right-hand side in the projection system are, therefore,    \\n\\\\[\\n\\\\int_K u_\\\\text{hp}(x) P_k(x) dx\\n= \\\\det(J_K) \\\\int_{\\\\hat{K}} u_\\\\text{hp}(F_K(\\\\hat{x})) P_k(F_K(\\\\hat{x})) d\\\\hat{x}.\\n\\\\]\\n\\n Recalling the shape function representation of \\\\(u_\\\\text{hp}(x)\\\\), we can write this as \\\\(\\\\det(J_K) \\\\, \\\\mathbf{C} \\\\, \\\\mathbf{c}\\\\), where \\\\(\\\\mathbf{C}\\\\) is the change-of-basis matrix with entries      \\n\\\\[\\n\\\\int_K P_i(x) \\\\varphi_j(x) dx\\n= \\\\det(J_K) \\\\int_{\\\\hat{K}} P_i(F_K(\\\\hat{x})) \\\\varphi_j(F_K(\\\\hat{x})) d\\\\hat{x}\\n= \\\\det(J_K) \\\\int_{\\\\hat{K}} \\\\hat{P}_i(\\\\hat{x}) \\\\hat{\\\\varphi}_j(\\\\hat{x}) d\\\\hat{x}\\n\\\\dealcoloneq \\\\det(J_K) \\\\, C_{ij}\\n\\\\]\\n\\n so the values of \\\\(\\\\mathbf{C}\\\\) can be written independently of \\\\(K\\\\) by factoring \\\\(\\\\det(J_K)\\\\) out front after transforming to reference coordinates. Hence, putting it all together, the projection problem can be written as   \\n\\\\[\\n\\\\det(J_K) \\\\, \\\\mathbb{I} \\\\, \\\\mathbf{l} = \\\\det(J_K) \\\\, \\\\mathbf{C} \\\\, \\\\mathbf{c}\\n\\\\]\\n\\n which can be rewritten as simply   \\n\\\\[\\n\\\\mathbf{l} = \\\\mathbf{C} \\\\, \\\\mathbf{c}.\\n\\\\]\\n\\nAt this point, we need to emphasize that most finite element applications use unstructured meshes for which mapping is almost always non-affine. Put another way: the assumption that \\\\(J_K\\\\) is constant across the cell is not true for general meshes. Hence, a correct calculation of \\\\(l_k\\\\) requires not only that we calculate the corresponding transformation matrix \\\\(\\\\mathbf{C}\\\\) for every single cell, but that we also define a set of Legendre-like orthogonal functions on a cell \\\\(K\\\\) which may have an arbitrary and very complex geometry. The second part, in particular, is very computationally expensive. The current implementation of the FESeries transformation classes relies on the simplification resulting from having a constant Jacobian to increase performance and thus only yields correct results for affine mappings. The transformation is only used for the purpose of smoothness estimation to decide on the type of adaptation, which is not a critical component of a finite element program. Apart from that, this circumstance does not pose a problem for this tutorial as we only use square-shaped cells.\\nEibner and Melenk [77] argued that a function is analytic, i.e., representable by a power series, if and only if the absolute values of the Legendre coefficients decay exponentially with increasing index \\\\(k\\\\):    \\n\\\\[\\n\\\\exists C,\\\\sigma > 0 : \\\\quad \\\\forall k \\\\in \\\\mathbb{N}_0 : \\\\quad |l_k|\\n\\\\leq C \\\\exp\\\\left( - \\\\sigma k \\\\right) .\\n\\\\]\\n\\n The rate of decay \\\\(\\\\sigma\\\\) can be interpreted as a measure for the smoothness of that function. We can get it as the slope of a linear regression fit of the transformation coefficients:   \\n\\\\[\\n\\\\ln(|l_k|) \\\\sim \\\\ln(C) - \\\\sigma k .\\n\\\\]\\n\\nWe will perform this fit on each cell \\\\(K\\\\) to get a local estimate for the smoothness of the finite element approximation. The decay rate \\\\(\\\\sigma_K\\\\) then acts as the decision indicator for hp-adaptation. For a finite element on a cell \\\\(K\\\\) with a polynomial degree \\\\(p\\\\), calculating the coefficients for \\\\(k \\\\leq (p+1)\\\\) proved to be a reasonable choice to estimate smoothness. You can find a more detailed and dimension independent description in [84].\\nAll of the above is already implemented in the FESeries::Legendre class and the SmoothnessEstimator::Legendre namespace. With the error estimates and smoothness indicators, we are then left to flag the cells for actual refinement and coarsening. Some functions from the parallel::distributed::GridRefinement and hp::Refinement namespaces will help us with that later.\\nHybrid geometric multigrid\\nFinite element matrices are typically very sparse. Additionally, hp-adaptive methods correspond to matrices with highly variable numbers of nonzero entries per row. Some state-of-the-art preconditioners, like the algebraic multigrid (AMG) ones as used in step-40, behave poorly in these circumstances.\\nWe will thus rely on a matrix-free hybrid multigrid preconditioner. step-50 has already demonstrated the superiority of geometric multigrid methods method when combined with the MatrixFree framework. The application on hp-adaptive FEM requires some additional work though since the children of a cell might have different polynomial degrees. As a remedy, we perform a p-relaxation to linear elements first (similar to Mitchell [155]) and then perform h-relaxation in the usual manner. On the coarsest level, we apply an algebraic multigrid solver. The combination of p-multigrid, h-multigrid, and AMG makes the solver to a hybrid multigrid solver.\\nWe will create a custom hybrid multigrid preconditioner with the special level requirements as described above with the help of the existing global-coarsening infrastructure via the use of MGTransferGlobalCoarsening.\\nThe test case\\nFor elliptic equations, each reentrant corner typically invokes a singularity [41]. We can use this circumstance to put our hp-decision algorithms to a test: on all cells to be adapted, we would prefer a fine grid near the singularity, and a high polynomial degree otherwise.\\nAs the simplest elliptic problem to solve under these conditions, we chose the Laplace equation in a L-shaped domain with the reentrant corner in the origin of the coordinate system.\\nTo be able to determine the actual error, we manufacture a boundary value problem with a known solution. On the above mentioned domain, one solution to the Laplace equation is, in polar coordinates, \\\\((r, \\\\varphi)\\\\):   \\n\\\\[\\nu_\\\\text{sol} = r^{2/3} \\\\sin(2/3 \\\\varphi).\\n\\\\]\\n\\nSee also [41] or [154]. The solution looks as follows:\\n  The singularity becomes obvious by investigating the solution's gradient in the vicinity of the reentrant corner, i.e., the origin     \\n\\\\[\\n\\\\left\\\\| \\\\nabla u_\\\\text{sol} \\\\right\\\\|_{2} = 2/3 r^{-1/3} , \\\\quad\\n\\\\lim\\\\limits_{r \\\\rightarrow 0} \\\\left\\\\| \\\\nabla u_\\\\text{sol} \\\\right\\\\|_{2} =\\n\\\\infty .\\n\\\\]\\n\\nAs we know where the singularity will be located, we expect that our hp-decision algorithm decides for a fine grid resolution in this particular region, and high polynomial degree anywhere else.\\nSo let's see if that is actually the case, and how hp-adaptation performs compared to pure h-adaptation. But first let us have a detailed look at the actual code.\\n The commented program\\n Include files\\nThe following include files have been used and discussed in previous tutorial programs, especially in step-27 and step-40.\\n\\u00a0 #include <deal.II/base/conditional_ostream.h>\\n\\u00a0 #include <deal.II/base/index_set.h>\\n\\u00a0 #include <deal.II/base/mpi.h>\\n\\u00a0 #include <deal.II/base/quadrature_lib.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/distributed/grid_refinement.h>\\n\\u00a0 #include <deal.II/distributed/tria.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 #include <deal.II/fe/fe_series.h>\\n\\u00a0 #include <deal.II/fe/mapping_q1.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/hp/fe_collection.h>\\n\\u00a0 #include <deal.II/hp/refinement.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/lac/dynamic_sparsity_pattern.h>\\n\\u00a0 #include <deal.II/lac/precondition.h>\\n\\u00a0 #include <deal.II/lac/solver_cg.h>\\n\\u00a0 #include <deal.II/lac/trilinos_precondition.h>\\n\\u00a0 #include <deal.II/lac/trilinos_sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/trilinos_sparsity_pattern.h>\\n\\u00a0 #include <deal.II/lac/vector.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/error_estimator.h>\\n\\u00a0 #include <deal.II/numerics/smoothness_estimator.h>\\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 \\n\\u00a0 #include <algorithm>\\n\\u00a0 #include <fstream>\\n\\u00a0 #include <iostream>\\n\\u00a0 \\nhpDefinition hp.h:117\\nFor load balancing we will assign individual weights on cells, and for that we will use the class parallel::CellWeights.\\n\\u00a0 #include <deal.II/distributed/cell_weights.h>\\n\\u00a0 \\nThe solution function requires a transformation from Cartesian to polar coordinates. The GeometricUtilities::Coordinates namespace provides the necessary tools.\\n\\u00a0 #include <deal.II/base/function.h>\\n\\u00a0 #include <deal.II/base/geometric_utilities.h>\\n\\u00a0 \\nThe following include files will enable the MatrixFree functionality.\\n\\u00a0 #include <deal.II/matrix_free/matrix_free.h>\\n\\u00a0 #include <deal.II/matrix_free/fe_evaluation.h>\\n\\u00a0 #include <deal.II/matrix_free/tools.h>\\n\\u00a0 \\nWe will use LinearAlgebra::distributed::Vector for linear algebra operations.\\n\\u00a0 #include <deal.II/lac/la_parallel_vector.h>\\n\\u00a0 \\nWe are left to include the files needed by the multigrid solver.\\n\\u00a0 #include <deal.II/multigrid/mg_coarse.h>\\n\\u00a0 #include <deal.II/multigrid/mg_constrained_dofs.h>\\n\\u00a0 #include <deal.II/multigrid/mg_matrix.h>\\n\\u00a0 #include <deal.II/multigrid/mg_smoother.h>\\n\\u00a0 #include <deal.II/multigrid/mg_tools.h>\\n\\u00a0 #include <deal.II/multigrid/mg_transfer_global_coarsening.h>\\n\\u00a0 #include <deal.II/multigrid/multigrid.h>\\n\\u00a0 \\n\\u00a0 namespace Step75\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n The Solution class template\\nWe have an analytic solution to the scenario at our disposal. We will use this solution to impose boundary conditions for the numerical solution of the problem. The formulation of the solution requires a transformation to polar coordinates. To transform from Cartesian to spherical coordinates, we will use a helper function from the GeometricUtilities::Coordinates namespace. The first two coordinates of this transformation correspond to polar coordinates in the x-y-plane.\\n\\u00a0   template <int dim>\\n\\u00a0   class Solution : public Function<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     Solution()\\n\\u00a0       : Function<dim>()\\n\\u00a0     {\\n\\u00a0       Assert(dim > 1, ExcNotImplemented());\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     virtual double value(const Point<dim> &p,\\n\\u00a0                          const unsigned int /*component*/) const override\\n\\u00a0     {\\n\\u00a0       const std::array<double, 2> polar =\\n\\u00a0         GeometricUtilities::Coordinates::to_spherical(Point<2>(p[0], p[1]));\\n\\u00a0 \\n\\u00a0       constexpr const double alpha = 2. / 3.;\\n\\u00a0       return std::pow(polar[0], alpha) * std::sin(alpha * polar[1]);\\n\\u00a0     }\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFunctionDefinition function.h:152\\nFunction::valuevirtual RangeNumberType value(const Point< dim > &p, const unsigned int component=0) const\\nPointDefinition point.h:111\\nStandardExceptions::ExcNotImplementedstatic ::ExceptionBase & ExcNotImplemented()\\nAssert#define Assert(cond, exc)Definition exceptions.h:1638\\nGeometricUtilities::Coordinates::to_sphericalstd::array< double, dim > to_spherical(const Point< dim > &point)Definition geometric_utilities.cc:46\\nstd::sin::VectorizedArray< Number, width > sin(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6589\\nstd::pow::VectorizedArray< Number, width > pow(const ::VectorizedArray< Number, width > &, const Number p)Definition vectorization.h:6885\\n Parameters\\nFor this tutorial, we will use a simplified set of parameters. It is also possible to use a ParameterHandler class here, but to keep this tutorial short we decided on using simple structs. The actual intention of all these parameters will be described in the upcoming classes at their respective location where they are used.\\nThe following parameter set controls the coarse-grid solver, the smoothers, and the inter-grid transfer scheme of the multigrid mechanism. We populate it with default parameters.\\n\\u00a0   struct MultigridParameters\\n\\u00a0   {\\n\\u00a0     struct\\n\\u00a0     {\\n\\u00a0       std::string  type            = \\\"cg_with_amg\\\";\\n\\u00a0       unsigned int maxiter         = 10000;\\n\\u00a0       double       abstol          = 1e-20;\\n\\u00a0       double       reltol          = 1e-4;\\n\\u00a0       unsigned int smoother_sweeps = 1;\\n\\u00a0       unsigned int n_cycles        = 1;\\n\\u00a0       std::string  smoother_type   = \\\"ILU\\\";\\n\\u00a0     } coarse_solver;\\n\\u00a0 \\n\\u00a0     struct\\n\\u00a0     {\\n\\u00a0       std::string  type                = \\\"chebyshev\\\";\\n\\u00a0       double       smoothing_range     = 20;\\n\\u00a0       unsigned int degree              = 5;\\n\\u00a0       unsigned int eig_cg_n_iterations = 20;\\n\\u00a0     } smoother;\\n\\u00a0 \\n\\u00a0     struct\\n\\u00a0     {\\n\\u00a0       MGTransferGlobalCoarseningTools::PolynomialCoarseningSequenceType\\n\\u00a0         p_sequence = MGTransferGlobalCoarseningTools::\\n\\u00a0           PolynomialCoarseningSequenceType::decrease_by_one;\\n\\u00a0       bool perform_h_transfer = true;\\n\\u00a0     } transfer;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nMGTransferGlobalCoarseningTools::PolynomialCoarseningSequenceTypePolynomialCoarseningSequenceTypeDefinition mg_transfer_global_coarsening.h:119\\nThis is the general parameter struct for the problem class. You will find this struct divided into several categories, including general runtime parameters, level limits, refine and coarsen fractions, as well as parameters for cell weighting. It also contains an instance of the above struct for multigrid parameters which will be passed to the multigrid algorithm.\\n\\u00a0   struct Parameters\\n\\u00a0   {\\n\\u00a0     unsigned int n_cycles         = 8;\\n\\u00a0     double       tolerance_factor = 1e-12;\\n\\u00a0 \\n\\u00a0     MultigridParameters mg_data;\\n\\u00a0 \\n\\u00a0     unsigned int min_h_level            = 5;\\n\\u00a0     unsigned int max_h_level            = 12;\\n\\u00a0     unsigned int min_p_degree           = 2;\\n\\u00a0     unsigned int max_p_degree           = 6;\\n\\u00a0     unsigned int max_p_level_difference = 1;\\n\\u00a0 \\n\\u00a0     double refine_fraction    = 0.3;\\n\\u00a0     double coarsen_fraction   = 0.03;\\n\\u00a0     double p_refine_fraction  = 0.9;\\n\\u00a0     double p_coarsen_fraction = 0.9;\\n\\u00a0 \\n\\u00a0     double weighting_factor   = 1.;\\n\\u00a0     double weighting_exponent = 1.;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Matrix-free Laplace operator\\nThis is a matrix-free implementation of the Laplace operator that will basically take over the part of the assemble_system() function from other tutorials. The meaning of all member functions will be explained at their definition later.\\nWe will use the FEEvaluation class to evaluate the solution vector at the quadrature points and to perform the integration. In contrast to other tutorials, the template arguments degree is set to \\\\(-1\\\\) and number of quadrature in 1d to \\\\(0\\\\). In this case, FEEvaluation selects dynamically the correct polynomial degree and number of quadrature points. Here, we introduce an alias to FEEvaluation with the correct template parameters so that we do not have to worry about them later on.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   class LaplaceOperator : public Subscriptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     using VectorType = LinearAlgebra::distributed::Vector<number>;\\n\\u00a0 \\n\\u00a0     using FECellIntegrator = FEEvaluation<dim, -1, 0, 1, number>;\\n\\u00a0 \\n\\u00a0     LaplaceOperator() = default;\\n\\u00a0 \\n\\u00a0     LaplaceOperator(const hp::MappingCollection<dim> &mapping,\\n\\u00a0                     const DoFHandler<dim>            &dof_handler,\\n\\u00a0                     const hp::QCollection<dim>       &quad,\\n\\u00a0                     const AffineConstraints<number>  &constraints,\\n\\u00a0                     VectorType                       &system_rhs);\\n\\u00a0 \\n\\u00a0     void reinit(const hp::MappingCollection<dim> &mapping,\\n\\u00a0                 const DoFHandler<dim>            &dof_handler,\\n\\u00a0                 const hp::QCollection<dim>       &quad,\\n\\u00a0                 const AffineConstraints<number>  &constraints,\\n\\u00a0                 VectorType                       &system_rhs);\\n\\u00a0 \\n\\u00a0     types::global_dof_index m() const;\\n\\u00a0 \\n\\u00a0     number el(unsigned int, unsigned int) const;\\n\\u00a0 \\n\\u00a0     void initialize_dof_vector(VectorType &vec) const;\\n\\u00a0 \\n\\u00a0     void vmult(VectorType &dst, const VectorType &src) const;\\n\\u00a0 \\n\\u00a0     void Tvmult(VectorType &dst, const VectorType &src) const;\\n\\u00a0 \\n\\u00a0     const TrilinosWrappers::SparseMatrix &get_system_matrix() const;\\n\\u00a0 \\n\\u00a0     void compute_inverse_diagonal(VectorType &diagonal) const;\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void do_cell_integral_local(FECellIntegrator &integrator) const;\\n\\u00a0 \\n\\u00a0     void do_cell_integral_global(FECellIntegrator &integrator,\\n\\u00a0                                  VectorType       &dst,\\n\\u00a0                                  const VectorType &src) const;\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     void do_cell_integral_range(\\n\\u00a0       const MatrixFree<dim, number>               &matrix_free,\\n\\u00a0       VectorType                                  &dst,\\n\\u00a0       const VectorType                            &src,\\n\\u00a0       const std::pair<unsigned int, unsigned int> &range) const;\\n\\u00a0 \\n\\u00a0     MatrixFree<dim, number> matrix_free;\\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nDoFHandlerDefinition dof_handler.h:317\\nFEEvaluationDefinition fe_evaluation.h:1355\\nLinearAlgebra::distributed::VectorDefinition la_parallel_vector.h:250\\nMatrixFreeDefinition matrix_free.h:113\\nSubscriptorDefinition subscriptor.h:60\\nTrilinosWrappers::SparseMatrixDefinition trilinos_sparse_matrix.h:550\\nhp::MappingCollectionDefinition vector_tools_rhs.h:40\\nhp::QCollectionDefinition vector_tools_rhs.h:42\\nunsigned int\\nTo solve the equation system on the coarsest level with an AMG preconditioner, we need an actual system matrix on the coarsest level. For this purpose, we provide a mechanism that optionally computes a matrix from the matrix-free formulation, for which we introduce a dedicated SparseMatrix object. In the default case, this matrix stays empty. Once get_system_matrix() is called, this matrix is filled (lazy allocation). Since this is a const function, we need the \\\"mutable\\\" keyword here. We also need a the constraints object to build the matrix.\\n\\u00a0     AffineConstraints<number>              constraints;\\n\\u00a0     mutable TrilinosWrappers::SparseMatrix system_matrix;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nThe following section contains functions to initialize and reinitialize the class. In particular, these functions initialize the internal MatrixFree instance. For sake of simplicity, we also compute the system right-hand-side vector.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   LaplaceOperator<dim, number>::LaplaceOperator(\\n\\u00a0     const hp::MappingCollection<dim> &mapping,\\n\\u00a0     const DoFHandler<dim>            &dof_handler,\\n\\u00a0     const hp::QCollection<dim>       &quad,\\n\\u00a0     const AffineConstraints<number>  &constraints,\\n\\u00a0     VectorType                       &system_rhs)\\n\\u00a0   {\\n\\u00a0     this->reinit(mapping, dof_handler, quad, constraints, system_rhs);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim, typename number>\\n\\u00a0   void LaplaceOperator<dim, number>::reinit(\\n\\u00a0     const hp::MappingCollection<dim> &mapping,\\n\\u00a0     const DoFHandler<dim>            &dof_handler,\\n\\u00a0     const hp::QCollection<dim>       &quad,\\n\\u00a0     const AffineConstraints<number>  &constraints,\\n\\u00a0     VectorType                       &system_rhs)\\n\\u00a0   {\\nClear internal data structures (in the case that the operator is reused).\\n\\u00a0     this->system_matrix.clear();\\n\\u00a0 \\nCopy the constraints, since they might be needed for computation of the system matrix later on.\\n\\u00a0     this->constraints.copy_from(constraints);\\n\\u00a0 \\nSet up MatrixFree. At the quadrature points, we only need to evaluate the gradient of the solution and test with the gradient of the shape functions so that we only need to set the flag update_gradients.\\n\\u00a0     typename MatrixFree<dim, number>::AdditionalData data;\\n\\u00a0     data.mapping_update_flags = update_gradients;\\n\\u00a0 \\n\\u00a0     matrix_free.reinit(mapping, dof_handler, constraints, quad, data);\\n\\u00a0 \\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nMatrixFree::AdditionalDataDefinition matrix_free.h:184\\nMatrixFree::AdditionalData::mapping_update_flagsUpdateFlags mapping_update_flagsDefinition matrix_free.h:373\\nCompute the right-hand side vector. For this purpose, we set up a second MatrixFree instance that uses a modified AffineConstraints not containing the constraints due to Dirichlet-boundary conditions. This modified operator is applied to a vector with only the Dirichlet values set. The result is the negative right-hand-side vector.\\n\\u00a0     {\\n\\u00a0       AffineConstraints<number> constraints_without_dbc(\\n\\u00a0         dof_handler.locally_owned_dofs(),\\n\\u00a0         DoFTools::extract_locally_relevant_dofs(dof_handler));\\n\\u00a0 \\n\\u00a0       DoFTools::make_hanging_node_constraints(dof_handler,\\n\\u00a0                                               constraints_without_dbc);\\n\\u00a0       constraints_without_dbc.close();\\n\\u00a0 \\n\\u00a0       VectorType b, x;\\n\\u00a0 \\n\\u00a0       this->initialize_dof_vector(system_rhs);\\n\\u00a0 \\n\\u00a0       MatrixFree<dim, number> matrix_free;\\n\\u00a0       matrix_free.reinit(\\n\\u00a0         mapping, dof_handler, constraints_without_dbc, quad, data);\\n\\u00a0 \\n\\u00a0       matrix_free.initialize_dof_vector(b);\\n\\u00a0       matrix_free.initialize_dof_vector(x);\\n\\u00a0 \\n\\u00a0       constraints.distribute(x);\\n\\u00a0 \\n\\u00a0       matrix_free.cell_loop(&LaplaceOperator::do_cell_integral_range,\\n\\u00a0                             this,\\n\\u00a0                             b,\\n\\u00a0                             x);\\n\\u00a0 \\n\\u00a0       constraints.set_zero(b);\\n\\u00a0 \\n\\u00a0       system_rhs -= b;\\n\\u00a0     }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nMatrixFree::reinitvoid reinit(const MappingType &mapping, const DoFHandler< dim > &dof_handler, const AffineConstraints< number2 > &constraint, const QuadratureType &quad, const AdditionalData &additional_data=AdditionalData())\\nDoFTools::make_hanging_node_constraintsvoid make_hanging_node_constraints(const DoFHandler< dim, spacedim > &dof_handler, AffineConstraints< number > &constraints)Definition dof_tools_constraints.cc:3073\\nDoFTools::extract_locally_relevant_dofsIndexSet extract_locally_relevant_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1164\\nThe following functions are implicitly needed by the multigrid algorithm, including the smoothers.\\nSince we do not have a matrix, query the DoFHandler for the number of degrees of freedom.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   types::global_dof_index LaplaceOperator<dim, number>::m() const\\n\\u00a0   {\\n\\u00a0     return matrix_free.get_dof_handler().n_dofs();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nAccess a particular element in the matrix. This function is neither needed nor implemented, however, is required to compile the program.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   number LaplaceOperator<dim, number>::el(unsigned int, unsigned int) const\\n\\u00a0   {\\n\\u00a0     DEAL_II_NOT_IMPLEMENTED();\\n\\u00a0     return 0;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nDEAL_II_NOT_IMPLEMENTED#define DEAL_II_NOT_IMPLEMENTED()Definition exceptions.h:1814\\nInitialize the given vector. We simply delegate the task to the MatrixFree function with the same name.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   void\\n\\u00a0   LaplaceOperator<dim, number>::initialize_dof_vector(VectorType &vec) const\\n\\u00a0   {\\n\\u00a0     matrix_free.initialize_dof_vector(vec);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nPerform an operator evaluation by looping with the help of MatrixFree over all cells and evaluating the effect of the cell integrals (see also: do_cell_integral_local() and do_cell_integral_global()).\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   void LaplaceOperator<dim, number>::vmult(VectorType       &dst,\\n\\u00a0                                            const VectorType &src) const\\n\\u00a0   {\\n\\u00a0     this->matrix_free.cell_loop(\\n\\u00a0       &LaplaceOperator::do_cell_integral_range, this, dst, src, true);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nPerform the transposed operator evaluation. Since we are considering symmetric \\\"matrices\\\", this function can simply delegate it task to vmult().\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   void LaplaceOperator<dim, number>::Tvmult(VectorType       &dst,\\n\\u00a0                                             const VectorType &src) const\\n\\u00a0   {\\n\\u00a0     this->vmult(dst, src);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nSince we do not have a system matrix, we cannot loop over the the diagonal entries of the matrix. Instead, we compute the diagonal by performing a sequence of operator evaluations to unit basis vectors. For this purpose, an optimized function from the MatrixFreeTools namespace is used. The inversion is performed manually afterwards.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   void LaplaceOperator<dim, number>::compute_inverse_diagonal(\\n\\u00a0     VectorType &diagonal) const\\n\\u00a0   {\\n\\u00a0     this->matrix_free.initialize_dof_vector(diagonal);\\n\\u00a0     MatrixFreeTools::compute_diagonal(matrix_free,\\n\\u00a0                                       diagonal,\\n\\u00a0                                       &LaplaceOperator::do_cell_integral_local,\\n\\u00a0                                       this);\\n\\u00a0 \\n\\u00a0     for (auto &i : diagonal)\\n\\u00a0       i = (std::abs(i) > 1.0e-10) ? (1.0 / i) : 1.0;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nMatrixFreeTools::compute_diagonalvoid compute_diagonal(const MatrixFree< dim, Number, VectorizedArrayType > &matrix_free, VectorType &diagonal_global, const std::function< void(FEEvaluation< dim, fe_degree, n_q_points_1d, n_components, Number, VectorizedArrayType > &)> &cell_operation, const unsigned int dof_no=0, const unsigned int quad_no=0, const unsigned int first_selected_component=0)\\nstdSTL namespace.\\nIn the matrix-free context, no system matrix is set up during initialization of this class. As a consequence, it has to be computed here if it should be requested. Since the matrix is only computed in this tutorial for linear elements (on the coarse grid), this is acceptable. The matrix entries are obtained via sequence of operator evaluations. For this purpose, the optimized function MatrixFreeTools::compute_matrix() is used. The matrix will only be computed if it has not been set up yet (lazy allocation).\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   const TrilinosWrappers::SparseMatrix &\\n\\u00a0   LaplaceOperator<dim, number>::get_system_matrix() const\\n\\u00a0   {\\n\\u00a0     if (system_matrix.m() == 0 && system_matrix.n() == 0)\\n\\u00a0       {\\n\\u00a0         const auto &dof_handler = this->matrix_free.get_dof_handler();\\n\\u00a0 \\n\\u00a0         TrilinosWrappers::SparsityPattern dsp(\\n\\u00a0           dof_handler.locally_owned_dofs(),\\n\\u00a0           dof_handler.get_triangulation().get_communicator());\\n\\u00a0 \\n\\u00a0         DoFTools::make_sparsity_pattern(dof_handler, dsp, this->constraints);\\n\\u00a0 \\n\\u00a0         dsp.compress();\\n\\u00a0         system_matrix.reinit(dsp);\\n\\u00a0 \\n\\u00a0         MatrixFreeTools::compute_matrix(\\n\\u00a0           matrix_free,\\n\\u00a0           constraints,\\n\\u00a0           system_matrix,\\n\\u00a0           &LaplaceOperator::do_cell_integral_local,\\n\\u00a0           this);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     return this->system_matrix;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTrilinosWrappers::SparsityPatternDefinition trilinos_sparsity_pattern.h:275\\nDoFTools::make_sparsity_patternvoid make_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, SparsityPatternBase &sparsity_pattern, const AffineConstraints< number > &constraints={}, const bool keep_constrained_dofs=true, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id)Definition dof_tools_sparsity.cc:56\\nMatrixFreeTools::compute_matrixvoid compute_matrix(const MatrixFree< dim, Number, VectorizedArrayType > &matrix_free, const AffineConstraints< Number > &constraints, MatrixType &matrix, const std::function< void(FEEvaluation< dim, fe_degree, n_q_points_1d, n_components, Number, VectorizedArrayType > &)> &cell_operation, const unsigned int dof_no=0, const unsigned int quad_no=0, const unsigned int first_selected_component=0)\\nPerform cell integral on a cell batch without gathering and scattering the values. This function is needed for the MatrixFreeTools functions since these functions operate directly on the buffers of FEEvaluation.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   void LaplaceOperator<dim, number>::do_cell_integral_local(\\n\\u00a0     FECellIntegrator &integrator) const\\n\\u00a0   {\\n\\u00a0     integrator.evaluate(EvaluationFlags::gradients);\\n\\u00a0 \\n\\u00a0     for (const unsigned int q : integrator.quadrature_point_indices())\\n\\u00a0       integrator.submit_gradient(integrator.get_gradient(q), q);\\n\\u00a0 \\n\\u00a0     integrator.integrate(EvaluationFlags::gradients);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nEvaluationFlags::gradients@ gradientsDefinition evaluation_flags.h:54\\nSame as above but with access to the global vectors.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   void LaplaceOperator<dim, number>::do_cell_integral_global(\\n\\u00a0     FECellIntegrator &integrator,\\n\\u00a0     VectorType       &dst,\\n\\u00a0     const VectorType &src) const\\n\\u00a0   {\\n\\u00a0     integrator.gather_evaluate(src, EvaluationFlags::gradients);\\n\\u00a0 \\n\\u00a0     for (const unsigned int q : integrator.quadrature_point_indices())\\n\\u00a0       integrator.submit_gradient(integrator.get_gradient(q), q);\\n\\u00a0 \\n\\u00a0     integrator.integrate_scatter(EvaluationFlags::gradients, dst);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nThis function loops over all cell batches within a cell-batch range and calls the above function.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   void LaplaceOperator<dim, number>::do_cell_integral_range(\\n\\u00a0     const MatrixFree<dim, number>               &matrix_free,\\n\\u00a0     VectorType                                  &dst,\\n\\u00a0     const VectorType                            &src,\\n\\u00a0     const std::pair<unsigned int, unsigned int> &range) const\\n\\u00a0   {\\n\\u00a0     FECellIntegrator integrator(matrix_free, range);\\n\\u00a0 \\n\\u00a0     for (unsigned cell = range.first; cell < range.second; ++cell)\\n\\u00a0       {\\n\\u00a0         integrator.reinit(cell);\\n\\u00a0 \\n\\u00a0         do_cell_integral_global(integrator, dst, src);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Solver and preconditioner\\n Conjugate-gradient solver with multigrid preconditioner\\nThis function solves the equation system with a sequence of provided multigrid objects. It is meant to be treated as general as possible, hence the multitude of template parameters.\\n\\u00a0   template <typename VectorType,\\n\\u00a0             int dim,\\n\\u00a0             typename SystemMatrixType,\\n\\u00a0             typename LevelMatrixType,\\n\\u00a0             typename MGTransferType>\\n\\u00a0   static void\\n\\u00a0   mg_solve(SolverControl             &solver_control,\\n\\u00a0            VectorType                &dst,\\n\\u00a0            const VectorType          &src,\\n\\u00a0            const MultigridParameters &mg_data,\\n\\u00a0            const DoFHandler<dim>     &dof,\\n\\u00a0            const SystemMatrixType    &fine_matrix,\\n\\u00a0            const MGLevelObject<std::unique_ptr<LevelMatrixType>> &mg_matrices,\\n\\u00a0            const MGTransferType                                  &mg_transfer)\\n\\u00a0   {\\n\\u00a0     AssertThrow(mg_data.coarse_solver.type == \\\"cg_with_amg\\\",\\n\\u00a0                 ExcNotImplemented());\\n\\u00a0     AssertThrow(mg_data.smoother.type == \\\"chebyshev\\\", ExcNotImplemented());\\n\\u00a0 \\n\\u00a0     const unsigned int min_level = mg_matrices.min_level();\\n\\u00a0     const unsigned int max_level = mg_matrices.max_level();\\n\\u00a0 \\n\\u00a0     using SmootherPreconditionerType = DiagonalMatrix<VectorType>;\\n\\u00a0     using SmootherType               = PreconditionChebyshev<LevelMatrixType,\\n\\u00a0                                                VectorType,\\n\\u00a0                                                SmootherPreconditionerType>;\\n\\u00a0     using PreconditionerType = PreconditionMG<dim, VectorType, MGTransferType>;\\n\\u00a0 \\nDiagonalMatrixDefinition diagonal_matrix.h:62\\nMGLevelObjectDefinition mg_level_object.h:49\\nPreconditionChebyshevDefinition precondition.h:2105\\nPreconditionMGDefinition multigrid.h:501\\nSolverControlDefinition solver_control.h:67\\nAssertThrow#define AssertThrow(cond, exc)Definition exceptions.h:1739\\nWe initialize level operators and Chebyshev smoothers here.\\n\\u00a0     mg::Matrix<VectorType> mg_matrix(mg_matrices);\\n\\u00a0 \\n\\u00a0     MGLevelObject<typename SmootherType::AdditionalData> smoother_data(\\n\\u00a0       min_level, max_level);\\n\\u00a0 \\n\\u00a0     for (unsigned int level = min_level; level <= max_level; ++level)\\n\\u00a0       {\\n\\u00a0         smoother_data[level].preconditioner =\\n\\u00a0           std::make_shared<SmootherPreconditionerType>();\\n\\u00a0         mg_matrices[level]->compute_inverse_diagonal(\\n\\u00a0           smoother_data[level].preconditioner->get_vector());\\n\\u00a0         smoother_data[level].smoothing_range = mg_data.smoother.smoothing_range;\\n\\u00a0         smoother_data[level].degree          = mg_data.smoother.degree;\\n\\u00a0         smoother_data[level].eig_cg_n_iterations =\\n\\u00a0           mg_data.smoother.eig_cg_n_iterations;\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     MGSmootherPrecondition<LevelMatrixType, SmootherType, VectorType>\\n\\u00a0       mg_smoother;\\n\\u00a0     mg_smoother.initialize(mg_matrices, smoother_data);\\n\\u00a0 \\nMGSmootherPreconditionDefinition mg_smoother.h:446\\nMGSmootherPrecondition::initializevoid initialize(const MGLevelObject< MatrixType2 > &matrices, const typename PreconditionerType::AdditionalData &additional_data=typename PreconditionerType::AdditionalData())\\nmg::MatrixDefinition mg_matrix.h:46\\nlevelunsigned int levelDefinition grid_out.cc:4626\\nNext, we initialize the coarse-grid solver. We use conjugate-gradient method with AMG as preconditioner.\\n\\u00a0     ReductionControl coarse_grid_solver_control(mg_data.coarse_solver.maxiter,\\n\\u00a0                                                 mg_data.coarse_solver.abstol,\\n\\u00a0                                                 mg_data.coarse_solver.reltol,\\n\\u00a0                                                 false,\\n\\u00a0                                                 false);\\n\\u00a0     SolverCG<VectorType> coarse_grid_solver(coarse_grid_solver_control);\\n\\u00a0 \\n\\u00a0     std::unique_ptr<MGCoarseGridBase<VectorType>> mg_coarse;\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::PreconditionAMG                 precondition_amg;\\n\\u00a0     TrilinosWrappers::PreconditionAMG::AdditionalData amg_data;\\n\\u00a0     amg_data.smoother_sweeps = mg_data.coarse_solver.smoother_sweeps;\\n\\u00a0     amg_data.n_cycles        = mg_data.coarse_solver.n_cycles;\\n\\u00a0     amg_data.smoother_type   = mg_data.coarse_solver.smoother_type.c_str();\\n\\u00a0 \\n\\u00a0     precondition_amg.initialize(mg_matrices[min_level]->get_system_matrix(),\\n\\u00a0                                 amg_data);\\n\\u00a0 \\n\\u00a0     mg_coarse =\\n\\u00a0       std::make_unique<MGCoarseGridIterativeSolver<VectorType,\\n\\u00a0                                                    SolverCG<VectorType>,\\n\\u00a0                                                    LevelMatrixType,\\n\\u00a0                                                    decltype(precondition_amg)>>(\\n\\u00a0         coarse_grid_solver, *mg_matrices[min_level], precondition_amg);\\n\\u00a0 \\nMGCoarseGridIterativeSolverDefinition mg_coarse.h:94\\nReductionControlDefinition solver_control.h:424\\nSolverCGDefinition solver_cg.h:179\\nTrilinosWrappers::PreconditionAMGDefinition trilinos_precondition.h:1321\\nTrilinosWrappers::PreconditionAMG::AdditionalDataDefinition trilinos_precondition.h:1330\\nTrilinosWrappers::PreconditionAMG::AdditionalData::smoother_sweepsunsigned int smoother_sweepsDefinition trilinos_precondition.h:1512\\nFinally, we create the Multigrid object, convert it to a preconditioner, and use it inside of a conjugate-gradient solver to solve the linear system of equations.\\n\\u00a0     Multigrid<VectorType> mg(\\n\\u00a0       mg_matrix, *mg_coarse, mg_transfer, mg_smoother, mg_smoother);\\n\\u00a0 \\n\\u00a0     PreconditionerType preconditioner(dof, mg, mg_transfer);\\n\\u00a0 \\n\\u00a0     SolverCG<VectorType>(solver_control)\\n\\u00a0       .solve(fine_matrix, dst, src, preconditioner);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nMultigridDefinition multigrid.h:163\\nSolverCG::solvevoid solve(const MatrixType &A, VectorType &x, const VectorType &b, const PreconditionerType &preconditioner)\\nmgDefinition mg.h:81\\n Hybrid polynomial/geometric-global-coarsening multigrid preconditioner\\nThe above function deals with the actual solution for a given sequence of multigrid objects. This functions creates the actual multigrid levels, in particular the operators, and the transfer operator as a MGTransferGlobalCoarsening object.\\n\\u00a0   template <typename VectorType, typename OperatorType, int dim>\\n\\u00a0   void solve_with_gmg(SolverControl                   &solver_control,\\n\\u00a0                       const OperatorType              &system_matrix,\\n\\u00a0                       VectorType                      &dst,\\n\\u00a0                       const VectorType                &src,\\n\\u00a0                       const MultigridParameters       &mg_data,\\n\\u00a0                       const hp::MappingCollection<dim> mapping_collection,\\n\\u00a0                       const DoFHandler<dim>           &dof_handler,\\n\\u00a0                       const hp::QCollection<dim>      &quadrature_collection)\\n\\u00a0   {\\nCreate a DoFHandler and operator for each multigrid level, as well as, create transfer operators. To be able to set up the operators, we need a set of DoFHandler that we create via global coarsening of p or h. For latter, we need also a sequence of Triangulation objects that are obtained by Triangulation::coarsen_global().\\nIn case no h-transfer is requested, we provide an empty deleter for the emplace_back() function, since the Triangulation of our DoFHandler is an external field and its destructor is called somewhere else.\\n\\u00a0     MGLevelObject<DoFHandler<dim>>                     dof_handlers;\\n\\u00a0     MGLevelObject<std::unique_ptr<OperatorType>>       operators;\\n\\u00a0     MGLevelObject<MGTwoLevelTransfer<dim, VectorType>> transfers;\\n\\u00a0 \\n\\u00a0     std::vector<std::shared_ptr<const Triangulation<dim>>>\\n\\u00a0       coarse_grid_triangulations;\\n\\u00a0     if (mg_data.transfer.perform_h_transfer)\\n\\u00a0       coarse_grid_triangulations =\\n\\u00a0         MGTransferGlobalCoarseningTools::create_geometric_coarsening_sequence(\\n\\u00a0           dof_handler.get_triangulation());\\n\\u00a0     else\\n\\u00a0       coarse_grid_triangulations.emplace_back(\\n\\u00a0         &(dof_handler.get_triangulation()), [](auto *) {});\\n\\u00a0 \\nMGTransferGlobalCoarseningTools::create_geometric_coarsening_sequencestd::vector< std::shared_ptr< const Triangulation< dim, spacedim > > > create_geometric_coarsening_sequence(const Triangulation< dim, spacedim > &tria)\\nDetermine the total number of levels for the multigrid operation and allocate sufficient memory for all levels.\\n\\u00a0     const unsigned int n_h_levels = coarse_grid_triangulations.size() - 1;\\n\\u00a0 \\n\\u00a0     const auto get_max_active_fe_degree = [&](const auto &dof_handler) {\\n\\u00a0       unsigned int max = 0;\\n\\u00a0 \\n\\u00a0       for (auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0         if (cell->is_locally_owned())\\n\\u00a0           max =\\n\\u00a0             std::max(max, dof_handler.get_fe(cell->active_fe_index()).degree);\\n\\u00a0 \\n\\u00a0       return Utilities::MPI::max(max, MPI_COMM_WORLD);\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0     const unsigned int n_p_levels =\\n\\u00a0       MGTransferGlobalCoarseningTools::create_polynomial_coarsening_sequence(\\n\\u00a0         get_max_active_fe_degree(dof_handler), mg_data.transfer.p_sequence)\\n\\u00a0         .size();\\n\\u00a0 \\n\\u00a0     std::map<unsigned int, unsigned int> fe_index_for_degree;\\n\\u00a0     for (unsigned int i = 0; i < dof_handler.get_fe_collection().size(); ++i)\\n\\u00a0       {\\n\\u00a0         const unsigned int degree = dof_handler.get_fe(i).degree;\\n\\u00a0         Assert(fe_index_for_degree.find(degree) == fe_index_for_degree.end(),\\n\\u00a0                ExcMessage(\\\"FECollection does not contain unique degrees.\\\"));\\n\\u00a0         fe_index_for_degree[degree] = i;\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     unsigned int minlevel = 0;\\n\\u00a0     unsigned int maxlevel = n_h_levels + n_p_levels - 1;\\n\\u00a0 \\n\\u00a0     dof_handlers.resize(minlevel, maxlevel);\\n\\u00a0     operators.resize(minlevel, maxlevel);\\n\\u00a0     transfers.resize(minlevel, maxlevel);\\n\\u00a0 \\nMGTransferGlobalCoarseningTools::create_polynomial_coarsening_sequencestd::vector< unsigned int > create_polynomial_coarsening_sequence(const unsigned int max_degree, const PolynomialCoarseningSequenceType &p_sequence)Definition mg_transfer_global_coarsening.cc:43\\nUtilities::MPI::maxT max(const T &t, const MPI_Comm mpi_communicator)\\nLoop from the minimum (coarsest) to the maximum (finest) level and set up DoFHandler accordingly. We start with the h-levels, where we distribute on increasingly finer meshes linear elements.\\n\\u00a0     for (unsigned int l = 0; l < n_h_levels; ++l)\\n\\u00a0       {\\n\\u00a0         dof_handlers[l].reinit(*coarse_grid_triangulations[l]);\\n\\u00a0         dof_handlers[l].distribute_dofs(dof_handler.get_fe_collection());\\n\\u00a0       }\\n\\u00a0 \\nAfter we reached the finest mesh, we will adjust the polynomial degrees on each level. We reverse iterate over our data structure and start at the finest mesh that contains all information about the active FE indices. We then lower the polynomial degree of each cell level by level.\\n\\u00a0     for (unsigned int i = 0, l = maxlevel; i < n_p_levels; ++i, --l)\\n\\u00a0       {\\n\\u00a0         dof_handlers[l].reinit(dof_handler.get_triangulation());\\n\\u00a0 \\n\\u00a0         if (l == maxlevel) // finest level\\n\\u00a0           {\\n\\u00a0             auto &dof_handler_mg = dof_handlers[l];\\n\\u00a0 \\n\\u00a0             auto cell_other = dof_handler.begin_active();\\n\\u00a0             for (auto &cell : dof_handler_mg.active_cell_iterators())\\n\\u00a0               {\\n\\u00a0                 if (cell->is_locally_owned())\\n\\u00a0                   cell->set_active_fe_index(cell_other->active_fe_index());\\n\\u00a0                 ++cell_other;\\n\\u00a0               }\\n\\u00a0           }\\n\\u00a0         else // coarse level\\n\\u00a0           {\\n\\u00a0             auto &dof_handler_fine   = dof_handlers[l + 1];\\n\\u00a0             auto &dof_handler_coarse = dof_handlers[l + 0];\\n\\u00a0 \\n\\u00a0             auto cell_other = dof_handler_fine.begin_active();\\n\\u00a0             for (auto &cell : dof_handler_coarse.active_cell_iterators())\\n\\u00a0               {\\n\\u00a0                 if (cell->is_locally_owned())\\n\\u00a0                   {\\n\\u00a0                     const unsigned int next_degree =\\n\\u00a0                       MGTransferGlobalCoarseningTools::\\n\\u00a0                         create_next_polynomial_coarsening_degree(\\n\\u00a0                           cell_other->get_fe().degree,\\n\\u00a0                           mg_data.transfer.p_sequence);\\n\\u00a0                     Assert(fe_index_for_degree.find(next_degree) !=\\n\\u00a0                              fe_index_for_degree.end(),\\n\\u00a0                            ExcMessage(\\\"Next polynomial degree in sequence \\\"\\n\\u00a0                                       \\\"does not exist in FECollection.\\\"));\\n\\u00a0 \\n\\u00a0                     cell->set_active_fe_index(fe_index_for_degree[next_degree]);\\n\\u00a0                   }\\n\\u00a0                 ++cell_other;\\n\\u00a0               }\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         dof_handlers[l].distribute_dofs(dof_handler.get_fe_collection());\\n\\u00a0       }\\n\\u00a0 \\nint\\nMGTransferGlobalCoarseningToolsDefinition mg_transfer_global_coarsening.h:110\\nMGTransferGlobalCoarseningTools::create_next_polynomial_coarsening_degreeunsigned int create_next_polynomial_coarsening_degree(const unsigned int degree, const PolynomialCoarseningSequenceType &p_sequence)Definition mg_transfer_global_coarsening.cc:22\\nPhysics::Elasticity::Kinematics::lTensor< 2, dim, Number > l(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\nNext, we will create all data structures additionally needed on each multigrid level. This involves determining constraints with homogeneous Dirichlet boundary conditions, and building the operator just like on the active level.\\n\\u00a0     MGLevelObject<AffineConstraints<typename VectorType::value_type>>\\n\\u00a0       constraints(minlevel, maxlevel);\\n\\u00a0 \\n\\u00a0     for (unsigned int level = minlevel; level <= maxlevel; ++level)\\n\\u00a0       {\\n\\u00a0         const auto &dof_handler = dof_handlers[level];\\n\\u00a0         auto       &constraint  = constraints[level];\\n\\u00a0 \\n\\u00a0         constraint.reinit(dof_handler.locally_owned_dofs(),\\n\\u00a0                           DoFTools::extract_locally_relevant_dofs(dof_handler));\\n\\u00a0 \\n\\u00a0         DoFTools::make_hanging_node_constraints(dof_handler, constraint);\\n\\u00a0         VectorTools::interpolate_boundary_values(mapping_collection,\\n\\u00a0                                                  dof_handler,\\n\\u00a0                                                  0,\\n\\u00a0                                                  Functions::ZeroFunction<dim>(),\\n\\u00a0                                                  constraint);\\n\\u00a0         constraint.close();\\n\\u00a0 \\n\\u00a0         VectorType dummy;\\n\\u00a0 \\n\\u00a0         operators[level] = std::make_unique<OperatorType>(mapping_collection,\\n\\u00a0                                                           dof_handler,\\n\\u00a0                                                           quadrature_collection,\\n\\u00a0                                                           constraint,\\n\\u00a0                                                           dummy);\\n\\u00a0       }\\n\\u00a0 \\nFunctions::ZeroFunctionDefinition function.h:510\\nVectorTools::interpolate_boundary_valuesvoid interpolate_boundary_values(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const std::map< types::boundary_id, const Function< spacedim, number > * > &function_map, std::map< types::global_dof_index, number > &boundary_values, const ComponentMask &component_mask={})\\nSet up intergrid operators and collect transfer operators within a single operator as needed by the Multigrid solver class.\\n\\u00a0     for (unsigned int level = minlevel; level < maxlevel; ++level)\\n\\u00a0       transfers[level + 1].reinit(dof_handlers[level + 1],\\n\\u00a0                                   dof_handlers[level],\\n\\u00a0                                   constraints[level + 1],\\n\\u00a0                                   constraints[level]);\\n\\u00a0 \\n\\u00a0     MGTransferGlobalCoarsening<dim, VectorType> transfer(\\n\\u00a0       transfers, [&](const auto l, auto &vec) {\\n\\u00a0         operators[l]->initialize_dof_vector(vec);\\n\\u00a0       });\\n\\u00a0 \\nMGTransferMFDefinition mg_transfer_global_coarsening.h:990\\nFinally, proceed to solve the problem with multigrid.\\n\\u00a0     mg_solve(solver_control,\\n\\u00a0              dst,\\n\\u00a0              src,\\n\\u00a0              mg_data,\\n\\u00a0              dof_handler,\\n\\u00a0              system_matrix,\\n\\u00a0              operators,\\n\\u00a0              transfer);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n The LaplaceProblem class template\\nNow we will finally declare the main class of this program, which solves the Laplace equation on subsequently refined function spaces. Its structure will look familiar as it is similar to the main classes of step-27 and step-40. There are basically just two additions:\\nThe SparseMatrix object that would hold the system matrix has been replaced by an object of the LaplaceOperator class for the MatrixFree formulation.\\nAn object of parallel::CellWeights, which will help us with load balancing, has been added.\\n\\n\\u00a0   template <int dim>\\n\\u00a0   class LaplaceProblem\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     LaplaceProblem(const Parameters &parameters);\\n\\u00a0 \\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void initialize_grid();\\n\\u00a0     void setup_system();\\n\\u00a0     void print_diagnostics();\\n\\u00a0     void solve_system();\\n\\u00a0     void compute_indicators();\\n\\u00a0     void adapt_resolution();\\n\\u00a0     void output_results(const unsigned int cycle);\\n\\u00a0 \\n\\u00a0     MPI_Comm mpi_communicator;\\n\\u00a0 \\n\\u00a0     const Parameters prm;\\n\\u00a0 \\n\\u00a0     parallel::distributed::Triangulation<dim> triangulation;\\n\\u00a0     DoFHandler<dim>                           dof_handler;\\n\\u00a0 \\n\\u00a0     hp::MappingCollection<dim> mapping_collection;\\n\\u00a0     hp::FECollection<dim>      fe_collection;\\n\\u00a0     hp::QCollection<dim>       quadrature_collection;\\n\\u00a0     hp::QCollection<dim - 1>   face_quadrature_collection;\\n\\u00a0 \\n\\u00a0     IndexSet locally_owned_dofs;\\n\\u00a0     IndexSet locally_relevant_dofs;\\n\\u00a0 \\n\\u00a0     AffineConstraints<double> constraints;\\n\\u00a0 \\n\\u00a0     LaplaceOperator<dim, double>               laplace_operator;\\n\\u00a0     LinearAlgebra::distributed::Vector<double> locally_relevant_solution;\\n\\u00a0     LinearAlgebra::distributed::Vector<double> system_rhs;\\n\\u00a0 \\n\\u00a0     std::unique_ptr<FESeries::Legendre<dim>>    legendre;\\n\\u00a0     std::unique_ptr<parallel::CellWeights<dim>> cell_weights;\\n\\u00a0 \\n\\u00a0     Vector<float> estimated_error_per_cell;\\n\\u00a0     Vector<float> hp_decision_indicators;\\n\\u00a0 \\n\\u00a0     ConditionalOStream pcout;\\n\\u00a0     TimerOutput        computing_timer;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nConditionalOStreamDefinition conditional_ostream.h:80\\nIndexSetDefinition index_set.h:70\\nMPI_Comm\\nTimerOutputDefinition timer.h:549\\nVectorDefinition vector.h:120\\nhp::FECollectionDefinition fe_collection.h:61\\nparallel::distributed::TriangulationDefinition tria.h:268\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\n The LaplaceProblem class implementation\\n Constructor\\nThe constructor starts with an initializer list that looks similar to the one of step-40. We again prepare the ConditionalOStream object to allow only the first process to output anything over the console, and initialize the computing timer properly.\\n\\u00a0   template <int dim>\\n\\u00a0   LaplaceProblem<dim>::LaplaceProblem(const Parameters &parameters)\\n\\u00a0     : mpi_communicator(MPI_COMM_WORLD)\\n\\u00a0     , prm(parameters)\\n\\u00a0     , triangulation(mpi_communicator)\\n\\u00a0     , dof_handler(triangulation)\\n\\u00a0     , pcout(std::cout,\\n\\u00a0             (Utilities::MPI::this_mpi_process(mpi_communicator) == 0))\\n\\u00a0     , computing_timer(mpi_communicator,\\n\\u00a0                       pcout,\\n\\u00a0                       TimerOutput::never,\\n\\u00a0                       TimerOutput::wall_times)\\n\\u00a0   {\\n\\u00a0     Assert(prm.min_h_level <= prm.max_h_level,\\n\\u00a0            ExcMessage(\\n\\u00a0              \\\"Triangulation level limits have been incorrectly set up.\\\"));\\n\\u00a0     Assert(prm.min_p_degree <= prm.max_p_degree,\\n\\u00a0            ExcMessage(\\\"FECollection degrees have been incorrectly set up.\\\"));\\n\\u00a0 \\nInitializeLibrary::MPI@ MPI\\nUtilitiesDefinition communication_pattern_base.h:30\\nWe need to prepare the data structures for the hp-functionality in the actual body of the constructor, and create corresponding objects for every degree in the specified range from the parameter struct. As we are only dealing with non-distorted rectangular cells, a linear mapping object is sufficient in this context.\\nIn the Parameters struct, we provide ranges for levels on which the function space is operating with a reasonable resolution. The multigrid algorithm requires linear elements on the coarsest possible level. So we start with the lowest polynomial degree and fill the collection with consecutively higher degrees until the user-specified maximum is reached.\\n\\u00a0     mapping_collection.push_back(MappingQ1<dim>());\\n\\u00a0 \\n\\u00a0     for (unsigned int degree = 1; degree <= prm.max_p_degree; ++degree)\\n\\u00a0       {\\n\\u00a0         fe_collection.push_back(FE_Q<dim>(degree));\\n\\u00a0         quadrature_collection.push_back(QGauss<dim>(degree + 1));\\n\\u00a0         face_quadrature_collection.push_back(QGauss<dim - 1>(degree + 1));\\n\\u00a0       }\\n\\u00a0 \\nFE_QDefinition fe_q.h:554\\nMappingQ1Definition mapping_q1.h:55\\nQGaussDefinition quadrature_lib.h:40\\nAs our FECollection contains more finite elements than we want to use for the finite element approximation of our solution, we would like to limit the range on which active FE indices can operate on. For this, the FECollection class allows to register a hierarchy that determines the succeeding and preceding finite element in case of of p-refinement and p-coarsening, respectively. All functions in the hp::Refinement namespace consult this hierarchy to determine future FE indices. We will register such a hierarchy that only works on finite elements with polynomial degrees in the proposed range [min_p_degree, max_p_degree].\\n\\u00a0     const unsigned int min_fe_index = prm.min_p_degree - 1;\\n\\u00a0     fe_collection.set_hierarchy(\\n\\u00a0       /*next_index=*/\\n\\u00a0       [](const typename hp::FECollection<dim> &fe_collection,\\n\\u00a0          const unsigned int                    fe_index) -> unsigned int {\\n\\u00a0         return ((fe_index + 1) < fe_collection.size()) ? fe_index + 1 :\\n\\u00a0                                                          fe_index;\\n\\u00a0       },\\n\\u00a0       /*previous_index=*/\\n\\u00a0       [min_fe_index](const typename hp::FECollection<dim> &,\\n\\u00a0                      const unsigned int fe_index) -> unsigned int {\\n\\u00a0         Assert(fe_index >= min_fe_index,\\n\\u00a0                ExcMessage(\\\"Finite element is not part of hierarchy!\\\"));\\n\\u00a0         return (fe_index > min_fe_index) ? fe_index - 1 : fe_index;\\n\\u00a0       });\\n\\u00a0 \\nhp::Collection::sizeunsigned int size() constDefinition collection.h:308\\nWe initialize the FESeries::Legendre object in the default configuration for smoothness estimation.\\n\\u00a0     legendre = std::make_unique<FESeries::Legendre<dim>>(\\n\\u00a0       SmoothnessEstimator::Legendre::default_fe_series(fe_collection));\\n\\u00a0 \\nSmoothnessEstimator::Legendre::default_fe_seriesFESeries::Legendre< dim, spacedim > default_fe_series(const hp::FECollection< dim, spacedim > &fe_collection, const unsigned int component=numbers::invalid_unsigned_int)Definition smoothness_estimator.cc:288\\nThe next part is going to be tricky. During execution of refinement, a few hp-algorithms need to interfere with the actual refinement process on the Triangulation object. We do this by connecting several functions to Triangulation::Signals: signals will be called at different stages during the actual refinement process and trigger all connected functions. We require this functionality for load balancing and to limit the polynomial degrees of neighboring cells.\\nFor the former, we would like to assign a weight to every cell that is proportional to the number of degrees of freedom of its future finite element. The library offers a class parallel::CellWeights that allows to easily attach individual weights at the right place during the refinement process, i.e., after all refine and coarsen flags have been set correctly for hp-adaptation and right before repartitioning for load balancing is about to happen. Functions can be registered that will attach weights in the form that \\\\(a (n_\\\\text{dofs})^b\\\\) with a provided pair of parameters \\\\((a,b)\\\\). We register such a function in the following.\\nFor load balancing, efficient solvers like the one we use should scale linearly with the number of degrees of freedom owned. We set the parameters for cell weighting correspondingly: A weighting factor of \\\\(1\\\\) and an exponent of \\\\(1\\\\) (see the definitions of the weighting_factor and weighting_exponent above).\\n\\u00a0     cell_weights = std::make_unique<parallel::CellWeights<dim>>(\\n\\u00a0       dof_handler,\\n\\u00a0       parallel::CellWeights<dim>::ndofs_weighting(\\n\\u00a0         {prm.weighting_factor, prm.weighting_exponent}));\\n\\u00a0 \\nparallel::CellWeights::ndofs_weightingstatic WeightingFunction ndofs_weighting(const std::pair< float, float > &coefficients)Definition cell_weights.cc:77\\nIn h-adaptive applications, we ensure a 2:1 mesh balance by limiting the difference of refinement levels of neighboring cells to one. With the second call in the following code snippet, we will ensure the same for p-levels on neighboring cells: levels of future finite elements are not allowed to differ by more than a specified difference. The function hp::Refinement::limit_p_level_difference takes care of this, but needs to be connected to a very specific signal in the parallel context. The issue is that we need to know how the mesh will be actually refined to set future FE indices accordingly. As we ask the p4est oracle to perform refinement, we need to ensure that the Triangulation has been updated with the adaptation flags of the oracle first. An instantiation of parallel::distributed::TemporarilyMatchRefineFlags does exactly that for the duration of its life. Thus, we will create an object of this class right before limiting the p-level difference, and connect the corresponding lambda function to the signal Triangulation::Signals::post_p4est_refinement, which will be triggered after the oracle got refined, but before the Triangulation is refined. Furthermore, we specify that this function will be connected to the front of the signal, to ensure that the modification is performed before any other function connected to the same signal.\\n\\u00a0     triangulation.signals.post_p4est_refinement.connect(\\n\\u00a0       [&, min_fe_index]() {\\n\\u00a0         const parallel::distributed::TemporarilyMatchRefineFlags<dim>\\n\\u00a0           refine_modifier(triangulation);\\n\\u00a0         hp::Refinement::limit_p_level_difference(dof_handler,\\n\\u00a0                                                  prm.max_p_level_difference,\\n\\u00a0                                                  /*contains=*/min_fe_index);\\n\\u00a0       },\\n\\u00a0       boost::signals2::at_front);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTriangulation::signalsSignals signalsDefinition tria.h:2513\\nparallel::distributed::TemporarilyMatchRefineFlagsDefinition tria.h:1144\\nhp::Refinement::limit_p_level_differencebool limit_p_level_difference(const DoFHandler< dim, spacedim > &dof_handler, const unsigned int max_difference=1, const unsigned int contains_fe_index=0)Definition refinement.cc:810\\n LaplaceProblem::initialize_grid\\nFor a L-shaped domain, we could use the function GridGenerator::hyper_L() as demonstrated in step-50. However in the 2d case, that particular function removes the first quadrant, while we need the fourth quadrant removed in our scenario. Thus, we will use a different function GridGenerator::subdivided_hyper_L() which gives us more options to create the mesh. Furthermore, we formulate that function in a way that it also generates a 3d mesh: the 2d L-shaped domain will basically elongated by 1 in the positive z-direction.\\nWe first pretend to build a GridGenerator::subdivided_hyper_rectangle(). The parameters that we need to provide are Point objects for the lower left and top right corners, as well as the number of repetitions that the base mesh will have in each direction. We provide them for the first two dimensions and treat the higher third dimension separately.\\nTo create a L-shaped domain, we need to remove the excess cells. For this, we specify the cells_to_remove accordingly. We would like to remove one cell in every cell from the negative direction, but remove one from the positive x-direction.\\nOn the coarse grid, we set the initial active FE indices and distribute the degrees of freedom once. We do that in order to assign the hp::FECollection to the DoFHandler, so that all cells know how many DoFs they are going to have. This step is mandatory for the weighted load balancing algorithm, which will be called implicitly in parallel::distributed::Triangulation::refine_global().\\n\\u00a0   template <int dim>\\n\\u00a0   void LaplaceProblem<dim>::initialize_grid()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"initialize grid\\\");\\n\\u00a0 \\n\\u00a0     std::vector<unsigned int> repetitions(dim);\\n\\u00a0     Point<dim>                bottom_left, top_right;\\n\\u00a0     for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0       if (d < 2)\\n\\u00a0         {\\n\\u00a0           repetitions[d] = 2;\\n\\u00a0           bottom_left[d] = -1.;\\n\\u00a0           top_right[d]   = 1.;\\n\\u00a0         }\\n\\u00a0       else\\n\\u00a0         {\\n\\u00a0           repetitions[d] = 1;\\n\\u00a0           bottom_left[d] = 0.;\\n\\u00a0           top_right[d]   = 1.;\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0     std::vector<int> cells_to_remove(dim, 1);\\n\\u00a0     cells_to_remove[0] = -1;\\n\\u00a0 \\n\\u00a0     GridGenerator::subdivided_hyper_L(\\n\\u00a0       triangulation, repetitions, bottom_left, top_right, cells_to_remove);\\n\\u00a0 \\n\\u00a0     const unsigned int min_fe_index = prm.min_p_degree - 1;\\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       if (cell->is_locally_owned())\\n\\u00a0         cell->set_active_fe_index(min_fe_index);\\n\\u00a0 \\n\\u00a0     dof_handler.distribute_dofs(fe_collection);\\n\\u00a0 \\n\\u00a0     triangulation.refine_global(prm.min_h_level);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTimerOutput::ScopeDefinition timer.h:557\\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nGridGenerator::subdivided_hyper_Lvoid subdivided_hyper_L(Triangulation< dim, spacedim > &tria, const std::vector< unsigned int > &repetitions, const Point< dim > &bottom_left, const Point< dim > &top_right, const std::vector< int > &n_cells_to_remove)\\n LaplaceProblem::setup_system\\nThis function looks exactly the same to the one of step-40, but you will notice the absence of the system matrix as well as the scaffold that surrounds it. Instead, we will initialize the MatrixFree formulation of the laplace_operator here. For boundary conditions, we will use the Solution class introduced earlier in this tutorial.\\n\\u00a0   template <int dim>\\n\\u00a0   void LaplaceProblem<dim>::setup_system()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"setup system\\\");\\n\\u00a0 \\n\\u00a0     dof_handler.distribute_dofs(fe_collection);\\n\\u00a0 \\n\\u00a0     locally_owned_dofs = dof_handler.locally_owned_dofs();\\n\\u00a0     locally_relevant_dofs =\\n\\u00a0       DoFTools::extract_locally_relevant_dofs(dof_handler);\\n\\u00a0 \\n\\u00a0     locally_relevant_solution.reinit(locally_owned_dofs,\\n\\u00a0                                      locally_relevant_dofs,\\n\\u00a0                                      mpi_communicator);\\n\\u00a0     system_rhs.reinit(locally_owned_dofs, mpi_communicator);\\n\\u00a0 \\n\\u00a0     constraints.clear();\\n\\u00a0     constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n\\u00a0     DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n\\u00a0     VectorTools::interpolate_boundary_values(\\n\\u00a0       mapping_collection, dof_handler, 0, Solution<dim>(), constraints);\\n\\u00a0     constraints.close();\\n\\u00a0 \\n\\u00a0     laplace_operator.reinit(mapping_collection,\\n\\u00a0                             dof_handler,\\n\\u00a0                             quadrature_collection,\\n\\u00a0                             constraints,\\n\\u00a0                             system_rhs);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n LaplaceProblem::print_diagnostics\\nThis is a function that prints additional diagnostics about the equation system and its partitioning. In addition to the usual global number of active cells and degrees of freedom, we also output their local equivalents. For a regulated output, we will communicate the local quantities with a Utilities::MPI::gather operation to the first process which will then output all information. Output of local quantities is limited to the first 8 processes to avoid cluttering the terminal.\\nFurthermore, we would like to print the frequencies of the polynomial degrees in the numerical discretization. Since this information is only stored locally, we will count the finite elements on locally owned cells and later communicate them via Utilities::MPI::sum.\\n\\u00a0   template <int dim>\\n\\u00a0   void LaplaceProblem<dim>::print_diagnostics()\\n\\u00a0   {\\n\\u00a0     const unsigned int first_n_processes =\\n\\u00a0       std::min<unsigned int>(8,\\n\\u00a0                              Utilities::MPI::n_mpi_processes(mpi_communicator));\\n\\u00a0     const bool output_cropped =\\n\\u00a0       first_n_processes < Utilities::MPI::n_mpi_processes(mpi_communicator);\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       pcout << \\\"   Number of active cells:       \\\"\\n\\u00a0             << triangulation.n_global_active_cells() << std::endl\\n\\u00a0             << \\\"     by partition:              \\\";\\n\\u00a0 \\n\\u00a0       std::vector<unsigned int> n_active_cells_per_subdomain =\\n\\u00a0         Utilities::MPI::gather(mpi_communicator,\\n\\u00a0                                triangulation.n_locally_owned_active_cells());\\n\\u00a0       for (unsigned int i = 0; i < first_n_processes; ++i)\\n\\u00a0         pcout << ' ' << n_active_cells_per_subdomain[i];\\n\\u00a0       if (output_cropped)\\n\\u00a0         pcout << \\\" ...\\\";\\n\\u00a0       pcout << std::endl;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       pcout << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n\\u00a0             << std::endl\\n\\u00a0             << \\\"     by partition:              \\\";\\n\\u00a0 \\n\\u00a0       std::vector<types::global_dof_index> n_dofs_per_subdomain =\\n\\u00a0         Utilities::MPI::gather(mpi_communicator,\\n\\u00a0                                dof_handler.n_locally_owned_dofs());\\n\\u00a0       for (unsigned int i = 0; i < first_n_processes; ++i)\\n\\u00a0         pcout << ' ' << n_dofs_per_subdomain[i];\\n\\u00a0       if (output_cropped)\\n\\u00a0         pcout << \\\" ...\\\";\\n\\u00a0       pcout << std::endl;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       std::vector<types::global_dof_index> n_constraints_per_subdomain =\\n\\u00a0         Utilities::MPI::gather(mpi_communicator, constraints.n_constraints());\\n\\u00a0 \\n\\u00a0       pcout << \\\"   Number of constraints:        \\\"\\n\\u00a0             << std::accumulate(n_constraints_per_subdomain.begin(),\\n\\u00a0                                n_constraints_per_subdomain.end(),\\n\\u00a0                                0)\\n\\u00a0             << std::endl\\n\\u00a0             << \\\"     by partition:              \\\";\\n\\u00a0       for (unsigned int i = 0; i < first_n_processes; ++i)\\n\\u00a0         pcout << ' ' << n_constraints_per_subdomain[i];\\n\\u00a0       if (output_cropped)\\n\\u00a0         pcout << \\\" ...\\\";\\n\\u00a0       pcout << std::endl;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       std::vector<unsigned int> n_fe_indices(fe_collection.size(), 0);\\n\\u00a0       for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0         if (cell->is_locally_owned())\\n\\u00a0           n_fe_indices[cell->active_fe_index()]++;\\n\\u00a0 \\n\\u00a0       Utilities::MPI::sum(n_fe_indices, mpi_communicator, n_fe_indices);\\n\\u00a0 \\n\\u00a0       pcout << \\\"   Frequencies of poly. degrees:\\\";\\n\\u00a0       for (unsigned int i = 0; i < fe_collection.size(); ++i)\\n\\u00a0         if (n_fe_indices[i] > 0)\\n\\u00a0           pcout << ' ' << fe_collection[i].degree << ':' << n_fe_indices[i];\\n\\u00a0       pcout << std::endl;\\n\\u00a0     }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nparallel::TriangulationBase::n_global_active_cellsvirtual types::global_cell_index n_global_active_cells() const overrideDefinition tria_base.cc:151\\nparallel::TriangulationBase::n_locally_owned_active_cellsunsigned int n_locally_owned_active_cells() constDefinition tria_base.cc:131\\nUtilities::MPI::sumT sum(const T &t, const MPI_Comm mpi_communicator)\\nUtilities::MPI::n_mpi_processesunsigned int n_mpi_processes(const MPI_Comm mpi_communicator)Definition mpi.cc:92\\nUtilities::MPI::gatherstd::vector< T > gather(const MPI_Comm comm, const T &object_to_send, const unsigned int root_process=0)\\nstd::min::VectorizedArray< Number, width > min(const ::VectorizedArray< Number, width > &, const ::VectorizedArray< Number, width > &)Definition vectorization.h:6960\\n LaplaceProblem::solve_system\\nThe scaffold around the solution is similar to the one of step-40. We prepare a vector that matches the requirements of MatrixFree and collect the locally-relevant degrees of freedoms we solved the equation system. The solution happens with the function introduced earlier.\\n\\u00a0   template <int dim>\\n\\u00a0   void LaplaceProblem<dim>::solve_system()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"solve system\\\");\\n\\u00a0 \\n\\u00a0     LinearAlgebra::distributed::Vector<double> completely_distributed_solution;\\n\\u00a0     laplace_operator.initialize_dof_vector(completely_distributed_solution);\\n\\u00a0 \\n\\u00a0     SolverControl solver_control(system_rhs.size(),\\n\\u00a0                                  prm.tolerance_factor * system_rhs.l2_norm());\\n\\u00a0 \\n\\u00a0     solve_with_gmg(solver_control,\\n\\u00a0                    laplace_operator,\\n\\u00a0                    completely_distributed_solution,\\n\\u00a0                    system_rhs,\\n\\u00a0                    prm.mg_data,\\n\\u00a0                    mapping_collection,\\n\\u00a0                    dof_handler,\\n\\u00a0                    quadrature_collection);\\n\\u00a0 \\n\\u00a0     pcout << \\\"   Solved in \\\" << solver_control.last_step() << \\\" iterations.\\\"\\n\\u00a0           << std::endl;\\n\\u00a0 \\n\\u00a0     constraints.distribute(completely_distributed_solution);\\n\\u00a0 \\n\\u00a0     locally_relevant_solution.copy_locally_owned_data_from(\\n\\u00a0       completely_distributed_solution);\\n\\u00a0     locally_relevant_solution.update_ghost_values();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nSolverControl::last_stepunsigned int last_step() constDefinition solver_control.cc:129\\n LaplaceProblem::compute_indicators\\nThis function contains only a part of the typical refine_grid function from other tutorials and is new in that sense. Here, we will only calculate all indicators for adaptation with actually refining the grid. We do this for the purpose of writing all indicators to the file system, so we store them for later.\\nSince we are dealing the an elliptic problem, we will make use of the KellyErrorEstimator again, but with a slight difference. Modifying the scaling factor of the underlying face integrals to be dependent on the actual polynomial degree of the neighboring elements is favorable in hp-adaptive applications [70]. We can do this by specifying the very last parameter from the additional ones you notices. The others are actually just the defaults.\\nFor the purpose of hp-adaptation, we will calculate smoothness estimates with the strategy presented in the tutorial introduction and use the implementation in SmoothnessEstimator::Legendre. In the Parameters struct, we set the minimal polynomial degree to 2 as it seems that the smoothness estimation algorithms have trouble with linear elements.\\n\\u00a0   template <int dim>\\n\\u00a0   void LaplaceProblem<dim>::compute_indicators()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"compute indicators\\\");\\n\\u00a0 \\n\\u00a0     estimated_error_per_cell.grow_or_shrink(triangulation.n_active_cells());\\n\\u00a0     KellyErrorEstimator<dim>::estimate(\\n\\u00a0       dof_handler,\\n\\u00a0       face_quadrature_collection,\\n\\u00a0       std::map<types::boundary_id, const Function<dim> *>(),\\n\\u00a0       locally_relevant_solution,\\n\\u00a0       estimated_error_per_cell,\\n\\u00a0       /*component_mask=*/ComponentMask(),\\n\\u00a0       /*coefficients=*/nullptr,\\n\\u00a0       /*n_threads=*/numbers::invalid_unsigned_int,\\n\\u00a0       /*subdomain_id=*/numbers::invalid_subdomain_id,\\n\\u00a0       /*material_id=*/numbers::invalid_material_id,\\n\\u00a0       /*strategy=*/\\n\\u00a0       KellyErrorEstimator<dim>::Strategy::face_diameter_over_twice_max_degree);\\n\\u00a0 \\n\\u00a0     hp_decision_indicators.grow_or_shrink(triangulation.n_active_cells());\\n\\u00a0     SmoothnessEstimator::Legendre::coefficient_decay(*legendre,\\n\\u00a0                                                      dof_handler,\\n\\u00a0                                                      locally_relevant_solution,\\n\\u00a0                                                      hp_decision_indicators);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nComponentMaskDefinition component_mask.h:81\\nKellyErrorEstimatorDefinition error_estimator.h:263\\nKellyErrorEstimator::estimatestatic void estimate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Quadrature< dim - 1 > &quadrature, const std::map< types::boundary_id, const Function< spacedim, Number > * > &neumann_bc, const ReadVector< Number > &solution, Vector< float > &error, const ComponentMask &component_mask={}, const Function< spacedim > *coefficients=nullptr, const unsigned int n_threads=numbers::invalid_unsigned_int, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id, const types::material_id material_id=numbers::invalid_material_id, const Strategy strategy=cell_diameter_over_24)\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nSmoothnessEstimator::Legendre::coefficient_decayvoid coefficient_decay(FESeries::Legendre< dim, spacedim > &fe_legendre, const DoFHandler< dim, spacedim > &dof_handler, const VectorType &solution, Vector< float > &smoothness_indicators, const VectorTools::NormType regression_strategy=VectorTools::Linfty_norm, const double smallest_abs_coefficient=1e-10, const bool only_flagged_cells=false)Definition smoothness_estimator.cc:101\\nnumbers::invalid_material_idconst types::material_id invalid_material_idDefinition types.h:277\\nnumbers::invalid_subdomain_idconst types::subdomain_id invalid_subdomain_idDefinition types.h:341\\nnumbers::invalid_unsigned_intstatic const unsigned int invalid_unsigned_intDefinition types.h:220\\n LaplaceProblem::adapt_resolution\\nWith the previously calculated indicators, we will finally flag all cells for adaptation and also execute refinement in this function. As in previous tutorials, we will use the \\\"fixed number\\\" strategy, but now for hp-adaptation.\\n\\u00a0   template <int dim>\\n\\u00a0   void LaplaceProblem<dim>::adapt_resolution()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"adapt resolution\\\");\\n\\u00a0 \\nFirst, we will set refine and coarsen flags based on the error estimates on each cell. There is nothing new here.\\nWe will use general refine and coarsen fractions that have been elaborated in the other deal.II tutorials: using the fixed number strategy, we will flag 30% of all cells for refinement and 3% for coarsening, as provided in the Parameters struct.\\n\\u00a0     parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number(\\n\\u00a0       triangulation,\\n\\u00a0       estimated_error_per_cell,\\n\\u00a0       prm.refine_fraction,\\n\\u00a0       prm.coarsen_fraction);\\n\\u00a0 \\nparallel::distributed::GridRefinement::refine_and_coarsen_fixed_numbervoid refine_and_coarsen_fixed_number(::Triangulation< dim, spacedim > &tria, const ::Vector< Number > &criteria, const double top_fraction_of_cells, const double bottom_fraction_of_cells, const types::global_cell_index max_n_cells=std::numeric_limits< types::global_cell_index >::max())Definition grid_refinement.cc:503\\nNext, we will make all adjustments for hp-adaptation. We want to refine and coarsen those cells flagged in the previous step, but need to decide if we would like to do it by adjusting the grid resolution or the polynomial degree.\\nThe next function call sets future FE indices according to the previously calculated smoothness indicators as p-adaptation indicators. These indices will only be set on those cells that have refine or coarsen flags assigned.\\nFor the p-adaptation fractions, we will take an educated guess. Since we only expect a single singularity in our scenario, i.e., in the origin of the domain, and a smooth solution anywhere else, we would like to strongly prefer to use p-adaptation over h-adaptation. This reflects in our choice of a fraction of 90% for both p-refinement and p-coarsening.\\n\\u00a0     hp::Refinement::p_adaptivity_fixed_number(dof_handler,\\n\\u00a0                                               hp_decision_indicators,\\n\\u00a0                                               prm.p_refine_fraction,\\n\\u00a0                                               prm.p_coarsen_fraction);\\n\\u00a0 \\nhp::Refinement::p_adaptivity_fixed_numbervoid p_adaptivity_fixed_number(const DoFHandler< dim, spacedim > &dof_handler, const Vector< Number > &criteria, const double p_refine_fraction=0.5, const double p_coarsen_fraction=0.5, const ComparisonFunction< std_cxx20::type_identity_t< Number > > &compare_refine=std::greater_equal< Number >(), const ComparisonFunction< std_cxx20::type_identity_t< Number > > &compare_coarsen=std::less_equal< Number >())Definition refinement.cc:246\\nAfter setting all indicators, we will remove those that exceed the specified limits of the provided level ranges in the Parameters struct. This limitation naturally arises for p-adaptation as the number of supplied finite elements is limited. In addition, we registered a custom hierarchy for p-adaptation in the constructor. Now, we need to do this manually in the h-adaptive context like in step-31.\\nWe will iterate over all cells on the designated min and max levels and remove the corresponding flags. As an alternative, we could also flag these cells for p-adaptation by setting future FE indices accordingly instead of simply clearing the refine and coarsen flags.\\n\\u00a0     Assert(triangulation.n_levels() >= prm.min_h_level + 1 &&\\n\\u00a0              triangulation.n_levels() <= prm.max_h_level + 1,\\n\\u00a0            ExcInternalError());\\n\\u00a0 \\n\\u00a0     if (triangulation.n_levels() > prm.max_h_level)\\n\\u00a0       for (const auto &cell :\\n\\u00a0            triangulation.active_cell_iterators_on_level(prm.max_h_level))\\n\\u00a0         cell->clear_refine_flag();\\n\\u00a0 \\n\\u00a0     for (const auto &cell :\\n\\u00a0          triangulation.active_cell_iterators_on_level(prm.min_h_level))\\n\\u00a0       cell->clear_coarsen_flag();\\n\\u00a0 \\nTriangulation::n_levelsunsigned int n_levels() const\\nAt this stage, we have both the future FE indices and the classic refine and coarsen flags set. The latter will be interpreted by Triangulation::execute_coarsening_and_refinement() for h-adaptation, and our previous modification ensures that the resulting Triangulation stays within the specified level range.\\nNow, we would like to only impose one type of adaptation on cells, which is what the next function will sort out for us. In short, on cells which have both types of indicators assigned, we will favor the p-adaptation one and remove the h-adaptation one.\\n\\u00a0     hp::Refinement::choose_p_over_h(dof_handler);\\n\\u00a0 \\nhp::Refinement::choose_p_over_hvoid choose_p_over_h(const DoFHandler< dim, spacedim > &dof_handler)Definition refinement.cc:699\\nIn the end, we are left to execute coarsening and refinement. Here, not only the grid will be updated, but also all previous future FE indices will become active.\\nRemember that we have attached functions to triangulation signals in the constructor, will be triggered in this function call. So there is even more happening: weighted repartitioning will be performed to ensure load balancing, as well as we will limit the difference of p-levels between neighboring cells.\\n\\u00a0     triangulation.execute_coarsening_and_refinement();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nparallel::distributed::Triangulation::execute_coarsening_and_refinementvirtual void execute_coarsening_and_refinement() overrideDefinition tria.cc:3320\\n LaplaceProblem::output_results\\nWriting results to the file system in parallel applications works exactly like in step-40. In addition to the data containers that we prepared throughout the tutorial, we would also like to write out the polynomial degree of each finite element on the grid as well as the subdomain each cell belongs to. We prepare necessary containers for this in the scope of this function.\\n\\u00a0   template <int dim>\\n\\u00a0   void LaplaceProblem<dim>::output_results(const unsigned int cycle)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"output results\\\");\\n\\u00a0 \\n\\u00a0     Vector<float> fe_degrees(triangulation.n_active_cells());\\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       if (cell->is_locally_owned())\\n\\u00a0         fe_degrees(cell->active_cell_index()) = cell->get_fe().degree;\\n\\u00a0 \\n\\u00a0     Vector<float> subdomain(triangulation.n_active_cells());\\n\\u00a0     for (auto &subd : subdomain)\\n\\u00a0       subd = triangulation.locally_owned_subdomain();\\n\\u00a0 \\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0     data_out.attach_dof_handler(dof_handler);\\n\\u00a0     data_out.add_data_vector(locally_relevant_solution, \\\"solution\\\");\\n\\u00a0     data_out.add_data_vector(fe_degrees, \\\"fe_degree\\\");\\n\\u00a0     data_out.add_data_vector(subdomain, \\\"subdomain\\\");\\n\\u00a0     data_out.add_data_vector(estimated_error_per_cell, \\\"error\\\");\\n\\u00a0     data_out.add_data_vector(hp_decision_indicators, \\\"hp_indicator\\\");\\n\\u00a0     data_out.build_patches(mapping_collection);\\n\\u00a0 \\n\\u00a0     data_out.write_vtu_with_pvtu_record(\\n\\u00a0       \\\"./\\\", \\\"solution\\\", cycle, mpi_communicator, 2, 1);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOutDefinition data_out.h:147\\n LaplaceProblem::run\\nThe actual run function again looks very familiar to step-40. The only addition is the bracketed section that precedes the actual cycle loop. Here, we will pre-calculate the Legendre transformation matrices. In general, these will be calculated on the fly via lazy allocation whenever a certain matrix is needed. For timing purposes however, we would like to calculate them all at once before the actual time measurement begins. We will thus designate their calculation to their own scope.\\n\\u00a0   template <int dim>\\n\\u00a0   void LaplaceProblem<dim>::run()\\n\\u00a0   {\\n\\u00a0     pcout << \\\"Running with Trilinos on \\\"\\n\\u00a0           << Utilities::MPI::n_mpi_processes(mpi_communicator)\\n\\u00a0           << \\\" MPI rank(s)...\\\" << std::endl;\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       pcout << \\\"Calculating transformation matrices...\\\" << std::endl;\\n\\u00a0       TimerOutput::Scope t(computing_timer, \\\"calculate transformation\\\");\\n\\u00a0       legendre->precalculate_all_transformation_matrices();\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     for (unsigned int cycle = 0; cycle < prm.n_cycles; ++cycle)\\n\\u00a0       {\\n\\u00a0         pcout << \\\"Cycle \\\" << cycle << ':' << std::endl;\\n\\u00a0 \\n\\u00a0         if (cycle == 0)\\n\\u00a0           initialize_grid();\\n\\u00a0         else\\n\\u00a0           adapt_resolution();\\n\\u00a0 \\n\\u00a0         setup_system();\\n\\u00a0 \\n\\u00a0         print_diagnostics();\\n\\u00a0 \\n\\u00a0         solve_system();\\n\\u00a0 \\n\\u00a0         compute_indicators();\\n\\u00a0 \\n\\u00a0         if (Utilities::MPI::n_mpi_processes(mpi_communicator) <= 32)\\n\\u00a0           output_results(cycle);\\n\\u00a0 \\n\\u00a0         computing_timer.print_summary();\\n\\u00a0         computing_timer.reset();\\n\\u00a0 \\n\\u00a0         pcout << std::endl;\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 } // namespace Step75\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n main()\\nThe final function is the main function that will ultimately create and run a LaplaceOperator instantiation. Its structure is similar to most other tutorial programs.\\n\\u00a0 int main(int argc, char *argv[])\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       using namespace dealii;\\n\\u00a0       using namespace Step75;\\n\\u00a0 \\n\\u00a0       Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n\\u00a0 \\n\\u00a0       Parameters        prm;\\n\\u00a0       LaplaceProblem<2> laplace_problem(prm);\\n\\u00a0       laplace_problem.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0 \\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return 0;\\n\\u00a0 }\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\n Results\\nWhen you run the program with the given parameters on four processes in release mode, your terminal output should look like this: Running with Trilinos on 4 MPI rank(s)...\\nCalculating transformation matrices...\\nCycle 0:\\n   Number of active cells:       3072\\n     by partition:               768 768 768 768\\n   Number of degrees of freedom: 12545\\n     by partition:               3201 3104 3136 3104\\n   Number of constraints:        542\\n     by partition:               165 74 138 165\\n   Frequencies of poly. degrees: 2:3072\\n   Solved in 7 iterations.\\n \\n \\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |     0.172s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| calculate transformation        |         1 |    0.0194s |        11% |\\n| compute indicators              |         1 |   0.00676s |       3.9% |\\n| initialize grid                 |         1 |     0.011s |       6.4% |\\n| output results                  |         1 |    0.0343s |        20% |\\n| setup system                    |         1 |   0.00839s |       4.9% |\\n| solve system                    |         1 |    0.0896s |        52% |\\n+---------------------------------+-----------+------------+------------+\\n \\n \\nCycle 1:\\n   Number of active cells:       3351\\n     by partition:               875 761 843 872\\n   Number of degrees of freedom: 18228\\n     by partition:               4535 4735 4543 4415\\n   Number of constraints:        1202\\n     by partition:               303 290 326 283\\n   Frequencies of poly. degrees: 2:2522 3:829\\n   Solved in 7 iterations.\\n \\n \\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |     0.165s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| adapt resolution                |         1 |   0.00473s |       2.9% |\\n| compute indicators              |         1 |   0.00764s |       4.6% |\\n| output results                  |         1 |    0.0243s |        15% |\\n| setup system                    |         1 |   0.00662s |         4% |\\n| solve system                    |         1 |     0.121s |        74% |\\n+---------------------------------+-----------+------------+------------+\\n \\n \\n...\\n \\n \\nCycle 7:\\n   Number of active cells:       5610\\n     by partition:               1324 1483 1482 1321\\n   Number of degrees of freedom: 82047\\n     by partition:               21098 19960 20111 20878\\n   Number of constraints:        14357\\n     by partition:               3807 3229 3554 3767\\n   Frequencies of poly. degrees: 2:1126 3:1289 4:2725 5:465 6:5\\n   Solved in 7 iterations.\\n \\n \\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |      1.83s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| adapt resolution                |         1 |   0.00834s |      0.46% |\\n| compute indicators              |         1 |    0.0178s |      0.97% |\\n| output results                  |         1 |    0.0434s |       2.4% |\\n| setup system                    |         1 |    0.0165s |       0.9% |\\n| solve system                    |         1 |      1.74s |        95% |\\n+---------------------------------+-----------+------------+------------+\\nWhen running the code with more processes, you will notice slight differences in the number of active cells and degrees of freedom. This is due to the fact that solver and preconditioner depend on the partitioning of the problem, which might yield to slight differences of the solution in the last digits and ultimately yields to different adaptation behavior.\\nFurthermore, the number of iterations for the solver stays about the same in all cycles despite hp-adaptation, indicating the robustness of the proposed algorithms and promising good scalability for even larger problem sizes and on more processes.\\nLet us have a look at the graphical output of the program. After all refinement cycles in the given parameter configuration, the actual discretized function space looks like the following with its partitioning on twelve processes on the left and the polynomial degrees of finite elements on the right. In the left picture, each color represents a unique subdomain. In the right picture, the lightest color corresponds to the polynomial degree two and the darkest one corresponds to degree six:\\n        Possibilities for extensions\\nThis tutorial shows only one particular way how to use parallel hp-adaptive finite element methods. In the following paragraphs, you will get to know which alternatives are possible. Most of these extensions are already part of https://github.com/marcfehling/hpbox/, which provides you with implementation examples that you can play around with.\\nDifferent hp-decision strategies\\nThe deal.II library offers multiple strategies to decide which type of adaptation to impose on cells: either adjust the grid resolution or change the polynomial degree. We only presented the Legendre coefficient decay strategy in this tutorial, while step-27 demonstrated the Fourier equivalent of the same idea.\\nSee the \\\"possibilities for extensions\\\" section of step-27 for an overview over these strategies, or the corresponding documentation for a detailed description.\\nThere, another strategy is mentioned that has not been shown in any tutorial so far: the strategy based on refinement history. The usage of this method for parallel distributed applications is more tricky than the others, so we will highlight the challenges that come along with it. We need information about the final state of refinement flags, and we need to transfer the solution across refined meshes. For the former, we need to attach the hp::Refinement::predict_error() function to the Triangulation::Signals::post_p4est_refinement signal in a way that it will be called after the hp::Refinement::limit_p_level_difference() function. At this stage, all refinement flags and future FE indices are terminally set and a reliable prediction of the error is possible. The predicted error then needs to be transferred across refined meshes with the aid of parallel::distributed::CellDataTransfer.\\nTry implementing one of these strategies into this tutorial and observe the subtle changes to the results. You will notice that all strategies are capable of identifying the singularities near the reentrant corners and will perform \\\\(h\\\\)-refinement in these regions, while preferring \\\\(p\\\\)-refinement in the bulk domain. A detailed comparison of these strategies is presented in [84] .\\nSolve with matrix-based methods\\nThis tutorial focuses solely on matrix-free strategies. All hp-adaptive algorithms however also work with matrix-based approaches in the parallel distributed context.\\nTo create a system matrix, you can either use the LaplaceOperator::get_system_matrix() function, or use an assemble_system() function similar to the one of step-27. You can then pass the system matrix to the solver as usual.\\nYou can time the results of both matrix-based and matrix-free implementations, quantify the speed-up, and convince yourself which variant is faster.\\nMultigrid variants\\nFor sake of simplicity, we have restricted ourselves to a single type of coarse-grid solver (CG with AMG), smoother (Chebyshev smoother with point Jacobi preconditioner), and geometric-coarsening scheme (global coarsening) within the multigrid algorithm. Feel free to try out alternatives and investigate their performance and robustness.\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2021 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Authors: Marc Fehling, Colorado State University, 2021\\n *          Peter Munch, Technical University of Munich and Helmholtz-Zentrum\\n *                       hereon, 2021\\n *          Wolfgang Bangerth, Colorado State University, 2021\\n */\\n \\n \\n#include <deal.II/base/conditional_ostream.h>\\n#include <deal.II/base/index_set.h>\\n#include <deal.II/base/mpi.h>\\n#include <deal.II/base/quadrature_lib.h>\\n#include <deal.II/base/timer.h>\\n \\n#include <deal.II/distributed/grid_refinement.h>\\n#include <deal.II/distributed/tria.h>\\n \\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/dofs/dof_tools.h>\\n \\n#include <deal.II/grid/grid_generator.h>\\n \\n#include <deal.II/fe/fe_q.h>\\n#include <deal.II/fe/fe_series.h>\\n#include <deal.II/fe/mapping_q1.h>\\n \\n#include <deal.II/hp/fe_collection.h>\\n#include <deal.II/hp/refinement.h>\\n \\n#include <deal.II/lac/affine_constraints.h>\\n#include <deal.II/lac/dynamic_sparsity_pattern.h>\\n#include <deal.II/lac/precondition.h>\\n#include <deal.II/lac/solver_cg.h>\\n#include <deal.II/lac/trilinos_precondition.h>\\n#include <deal.II/lac/trilinos_sparse_matrix.h>\\n#include <deal.II/lac/trilinos_sparsity_pattern.h>\\n#include <deal.II/lac/vector.h>\\n \\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/error_estimator.h>\\n#include <deal.II/numerics/smoothness_estimator.h>\\n#include <deal.II/numerics/vector_tools.h>\\n \\n#include <algorithm>\\n#include <fstream>\\n#include <iostream>\\n \\n#include <deal.II/distributed/cell_weights.h>\\n \\n#include <deal.II/base/function.h>\\n#include <deal.II/base/geometric_utilities.h>\\n \\n#include <deal.II/matrix_free/matrix_free.h>\\n#include <deal.II/matrix_free/fe_evaluation.h>\\n#include <deal.II/matrix_free/tools.h>\\n \\n#include <deal.II/lac/la_parallel_vector.h>\\n \\n#include <deal.II/multigrid/mg_coarse.h>\\n#include <deal.II/multigrid/mg_constrained_dofs.h>\\n#include <deal.II/multigrid/mg_matrix.h>\\n#include <deal.II/multigrid/mg_smoother.h>\\n#include <deal.II/multigrid/mg_tools.h>\\n#include <deal.II/multigrid/mg_transfer_global_coarsening.h>\\n#include <deal.II/multigrid/multigrid.h>\\n \\nnamespace Step75\\n{\\n using namespace dealii;\\n \\n \\n template <int dim>\\n class Solution : public Function<dim>\\n  {\\n public:\\n    Solution()\\n      : Function<dim>()\\n    {\\n Assert(dim > 1, ExcNotImplemented());\\n    }\\n \\n virtual double value(const Point<dim> &p,\\n const unsigned int /*component*/) const override\\n {\\n const std::array<double, 2> polar =\\n GeometricUtilities::Coordinates::to_spherical(Point<2>(p[0], p[1]));\\n \\n constexpr const double alpha = 2. / 3.;\\n return std::pow(polar[0], alpha) * std::sin(alpha * polar[1]);\\n    }\\n  };\\n \\n \\n \\n \\n struct MultigridParameters\\n  {\\n struct\\n    {\\n      std::string  type            = \\\"cg_with_amg\\\";\\n unsigned int maxiter         = 10000;\\n double       abstol          = 1e-20;\\n double       reltol          = 1e-4;\\n unsigned int smoother_sweeps = 1;\\n unsigned int n_cycles        = 1;\\n      std::string  smoother_type   = \\\"ILU\\\";\\n    } coarse_solver;\\n \\n struct\\n    {\\n      std::string  type                = \\\"chebyshev\\\";\\n double       smoothing_range     = 20;\\n unsigned int degree              = 5;\\n unsigned int eig_cg_n_iterations = 20;\\n    } smoother;\\n \\n struct\\n    {\\n MGTransferGlobalCoarseningTools::PolynomialCoarseningSequenceType\\n        p_sequence = MGTransferGlobalCoarseningTools::\\n          PolynomialCoarseningSequenceType::decrease_by_one;\\n bool perform_h_transfer = true;\\n    } transfer;\\n  };\\n \\n \\n \\n struct Parameters\\n  {\\n unsigned int n_cycles         = 8;\\n double       tolerance_factor = 1e-12;\\n \\n    MultigridParameters mg_data;\\n \\n unsigned int min_h_level            = 5;\\n unsigned int max_h_level            = 12;\\n unsigned int min_p_degree           = 2;\\n unsigned int max_p_degree           = 6;\\n unsigned int max_p_level_difference = 1;\\n \\n double refine_fraction    = 0.3;\\n double coarsen_fraction   = 0.03;\\n double p_refine_fraction  = 0.9;\\n double p_coarsen_fraction = 0.9;\\n \\n double weighting_factor   = 1.;\\n double weighting_exponent = 1.;\\n  };\\n \\n \\n \\n \\n template <int dim, typename number>\\n class LaplaceOperator : public Subscriptor\\n  {\\n public:\\n using VectorType = LinearAlgebra::distributed::Vector<number>;\\n \\n using FECellIntegrator = FEEvaluation<dim, -1, 0, 1, number>;\\n \\n    LaplaceOperator() = default;\\n \\n    LaplaceOperator(const hp::MappingCollection<dim> &mapping,\\n const DoFHandler<dim>            &dof_handler,\\n const hp::QCollection<dim>       &quad,\\n const AffineConstraints<number>  &constraints,\\n                    VectorType                       &system_rhs);\\n \\n void reinit(const hp::MappingCollection<dim> &mapping,\\n const DoFHandler<dim>            &dof_handler,\\n const hp::QCollection<dim>       &quad,\\n const AffineConstraints<number>  &constraints,\\n                VectorType                       &system_rhs);\\n \\n types::global_dof_index m() const;\\n \\n    number el(unsigned int, unsigned int) const;\\n \\n void initialize_dof_vector(VectorType &vec) const;\\n \\n void vmult(VectorType &dst, const VectorType &src) const;\\n \\n void Tvmult(VectorType &dst, const VectorType &src) const;\\n \\n const TrilinosWrappers::SparseMatrix &get_system_matrix() const;\\n \\n void compute_inverse_diagonal(VectorType &diagonal) const;\\n \\n private:\\n void do_cell_integral_local(FECellIntegrator &integrator) const;\\n \\n void do_cell_integral_global(FECellIntegrator &integrator,\\n                                 VectorType       &dst,\\n const VectorType &src) const;\\n \\n \\n void do_cell_integral_range(\\n const MatrixFree<dim, number>               &matrix_free,\\n      VectorType                                  &dst,\\n const VectorType                            &src,\\n const std::pair<unsigned int, unsigned int> &range) const;\\n \\n MatrixFree<dim, number> matrix_free;\\n \\n AffineConstraints<number>              constraints;\\n mutable TrilinosWrappers::SparseMatrix system_matrix;\\n  };\\n \\n \\n \\n template <int dim, typename number>\\n  LaplaceOperator<dim, number>::LaplaceOperator(\\n const hp::MappingCollection<dim> &mapping,\\n const DoFHandler<dim>            &dof_handler,\\n const hp::QCollection<dim>       &quad,\\n const AffineConstraints<number>  &constraints,\\n    VectorType                       &system_rhs)\\n  {\\n    this->reinit(mapping, dof_handler, quad, constraints, system_rhs);\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n void LaplaceOperator<dim, number>::reinit(\\n const hp::MappingCollection<dim> &mapping,\\n const DoFHandler<dim>            &dof_handler,\\n const hp::QCollection<dim>       &quad,\\n const AffineConstraints<number>  &constraints,\\n    VectorType                       &system_rhs)\\n  {\\n    this->system_matrix.clear();\\n \\n    this->constraints.copy_from(constraints);\\n \\n typename MatrixFree<dim, number>::AdditionalData data;\\n    data.mapping_update_flags = update_gradients;\\n \\n    matrix_free.reinit(mapping, dof_handler, constraints, quad, data);\\n \\n    {\\n AffineConstraints<number> constraints_without_dbc(\\n        dof_handler.locally_owned_dofs(),\\n DoFTools::extract_locally_relevant_dofs(dof_handler));\\n \\n DoFTools::make_hanging_node_constraints(dof_handler,\\n                                              constraints_without_dbc);\\n      constraints_without_dbc.close();\\n \\n      VectorType b, x;\\n \\n      this->initialize_dof_vector(system_rhs);\\n \\n MatrixFree<dim, number> matrix_free;\\n      matrix_free.reinit(\\n        mapping, dof_handler, constraints_without_dbc, quad, data);\\n \\n      matrix_free.initialize_dof_vector(b);\\n      matrix_free.initialize_dof_vector(x);\\n \\n      constraints.distribute(x);\\n \\n      matrix_free.cell_loop(&LaplaceOperator::do_cell_integral_range,\\n this,\\n                            b,\\n                            x);\\n \\n      constraints.set_zero(b);\\n \\n      system_rhs -= b;\\n    }\\n  }\\n \\n \\n \\n \\n template <int dim, typename number>\\n types::global_dof_index LaplaceOperator<dim, number>::m() const\\n {\\n return matrix_free.get_dof_handler().n_dofs();\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n  number LaplaceOperator<dim, number>::el(unsigned int, unsigned int) const\\n {\\n DEAL_II_NOT_IMPLEMENTED();\\n return 0;\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n void\\n  LaplaceOperator<dim, number>::initialize_dof_vector(VectorType &vec) const\\n {\\n    matrix_free.initialize_dof_vector(vec);\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n void LaplaceOperator<dim, number>::vmult(VectorType       &dst,\\n const VectorType &src) const\\n {\\n    this->matrix_free.cell_loop(\\n      &LaplaceOperator::do_cell_integral_range, this, dst, src, true);\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n void LaplaceOperator<dim, number>::Tvmult(VectorType       &dst,\\n const VectorType &src) const\\n {\\n    this->vmult(dst, src);\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n void LaplaceOperator<dim, number>::compute_inverse_diagonal(\\n    VectorType &diagonal) const\\n {\\n    this->matrix_free.initialize_dof_vector(diagonal);\\n MatrixFreeTools::compute_diagonal(matrix_free,\\n                                      diagonal,\\n                                      &LaplaceOperator::do_cell_integral_local,\\n this);\\n \\n for (auto &i : diagonal)\\n      i = (std::abs(i) > 1.0e-10) ? (1.0 / i) : 1.0;\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n const TrilinosWrappers::SparseMatrix &\\n  LaplaceOperator<dim, number>::get_system_matrix() const\\n {\\n if (system_matrix.m() == 0 && system_matrix.n() == 0)\\n      {\\n const auto &dof_handler = this->matrix_free.get_dof_handler();\\n \\n TrilinosWrappers::SparsityPattern dsp(\\n          dof_handler.locally_owned_dofs(),\\n          dof_handler.get_triangulation().get_communicator());\\n \\n DoFTools::make_sparsity_pattern(dof_handler, dsp, this->constraints);\\n \\n        dsp.compress();\\n        system_matrix.reinit(dsp);\\n \\n MatrixFreeTools::compute_matrix(\\n          matrix_free,\\n          constraints,\\n          system_matrix,\\n          &LaplaceOperator::do_cell_integral_local,\\n this);\\n      }\\n \\n return this->system_matrix;\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n void LaplaceOperator<dim, number>::do_cell_integral_local(\\n    FECellIntegrator &integrator) const\\n {\\n    integrator.evaluate(EvaluationFlags::gradients);\\n \\n for (const unsigned int q : integrator.quadrature_point_indices())\\n      integrator.submit_gradient(integrator.get_gradient(q), q);\\n \\n    integrator.integrate(EvaluationFlags::gradients);\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n void LaplaceOperator<dim, number>::do_cell_integral_global(\\n    FECellIntegrator &integrator,\\n    VectorType       &dst,\\n const VectorType &src) const\\n {\\n    integrator.gather_evaluate(src, EvaluationFlags::gradients);\\n \\n for (const unsigned int q : integrator.quadrature_point_indices())\\n      integrator.submit_gradient(integrator.get_gradient(q), q);\\n \\n    integrator.integrate_scatter(EvaluationFlags::gradients, dst);\\n  }\\n \\n \\n \\n template <int dim, typename number>\\n void LaplaceOperator<dim, number>::do_cell_integral_range(\\n const MatrixFree<dim, number>               &matrix_free,\\n    VectorType                                  &dst,\\n const VectorType                            &src,\\n const std::pair<unsigned int, unsigned int> &range) const\\n {\\n    FECellIntegrator integrator(matrix_free, range);\\n \\n for (unsigned cell = range.first; cell < range.second; ++cell)\\n      {\\n        integrator.reinit(cell);\\n \\n        do_cell_integral_global(integrator, dst, src);\\n      }\\n  }\\n \\n \\n \\n \\n \\n template <typename VectorType,\\n int dim,\\n typename SystemMatrixType,\\n typename LevelMatrixType,\\n typename MGTransferType>\\n static void\\n  mg_solve(SolverControl             &solver_control,\\n           VectorType                &dst,\\n const VectorType          &src,\\n const MultigridParameters &mg_data,\\n const DoFHandler<dim>     &dof,\\n const SystemMatrixType    &fine_matrix,\\n const MGLevelObject<std::unique_ptr<LevelMatrixType>> &mg_matrices,\\n const MGTransferType                                  &mg_transfer)\\n  {\\n AssertThrow(mg_data.coarse_solver.type == \\\"cg_with_amg\\\",\\n                ExcNotImplemented());\\n AssertThrow(mg_data.smoother.type == \\\"chebyshev\\\", ExcNotImplemented());\\n \\n const unsigned int min_level = mg_matrices.min_level();\\n const unsigned int max_level = mg_matrices.max_level();\\n \\n using SmootherPreconditionerType = DiagonalMatrix<VectorType>;\\n using SmootherType               = PreconditionChebyshev<LevelMatrixType,\\n                                               VectorType,\\n                                               SmootherPreconditionerType>;\\n using PreconditionerType = PreconditionMG<dim, VectorType, MGTransferType>;\\n \\n mg::Matrix<VectorType> mg_matrix(mg_matrices);\\n \\n MGLevelObject<typename SmootherType::AdditionalData> smoother_data(\\n      min_level, max_level);\\n \\n for (unsigned int level = min_level; level <= max_level; ++level)\\n      {\\n        smoother_data[level].preconditioner =\\n          std::make_shared<SmootherPreconditionerType>();\\n        mg_matrices[level]->compute_inverse_diagonal(\\n          smoother_data[level].preconditioner->get_vector());\\n        smoother_data[level].smoothing_range = mg_data.smoother.smoothing_range;\\n        smoother_data[level].degree          = mg_data.smoother.degree;\\n        smoother_data[level].eig_cg_n_iterations =\\n          mg_data.smoother.eig_cg_n_iterations;\\n      }\\n \\n MGSmootherPrecondition<LevelMatrixType, SmootherType, VectorType>\\n      mg_smoother;\\n    mg_smoother.initialize(mg_matrices, smoother_data);\\n \\n ReductionControl coarse_grid_solver_control(mg_data.coarse_solver.maxiter,\\n                                                mg_data.coarse_solver.abstol,\\n                                                mg_data.coarse_solver.reltol,\\n false,\\n false);\\n SolverCG<VectorType> coarse_grid_solver(coarse_grid_solver_control);\\n \\n    std::unique_ptr<MGCoarseGridBase<VectorType>> mg_coarse;\\n \\n TrilinosWrappers::PreconditionAMG                 precondition_amg;\\n TrilinosWrappers::PreconditionAMG::AdditionalData amg_data;\\n    amg_data.smoother_sweeps = mg_data.coarse_solver.smoother_sweeps;\\n    amg_data.n_cycles        = mg_data.coarse_solver.n_cycles;\\n    amg_data.smoother_type   = mg_data.coarse_solver.smoother_type.c_str();\\n \\n    precondition_amg.initialize(mg_matrices[min_level]->get_system_matrix(),\\n                                amg_data);\\n \\n    mg_coarse =\\n      std::make_unique<MGCoarseGridIterativeSolver<VectorType,\\n SolverCG<VectorType>,\\n                                                   LevelMatrixType,\\n decltype(precondition_amg)>>(\\n        coarse_grid_solver, *mg_matrices[min_level], precondition_amg);\\n \\n Multigrid<VectorType> mg(\\n      mg_matrix, *mg_coarse, mg_transfer, mg_smoother, mg_smoother);\\n \\n    PreconditionerType preconditioner(dof, mg, mg_transfer);\\n \\n SolverCG<VectorType>(solver_control)\\n      .solve(fine_matrix, dst, src, preconditioner);\\n  }\\n \\n \\n \\n \\n template <typename VectorType, typename OperatorType, int dim>\\n void solve_with_gmg(SolverControl                   &solver_control,\\n const OperatorType              &system_matrix,\\n                      VectorType                      &dst,\\n const VectorType                &src,\\n const MultigridParameters       &mg_data,\\n const hp::MappingCollection<dim> mapping_collection,\\n const DoFHandler<dim>           &dof_handler,\\n const hp::QCollection<dim>      &quadrature_collection)\\n  {\\n MGLevelObject<DoFHandler<dim>>                     dof_handlers;\\n MGLevelObject<std::unique_ptr<OperatorType>>       operators;\\n MGLevelObject<MGTwoLevelTransfer<dim, VectorType>> transfers;\\n \\n    std::vector<std::shared_ptr<const Triangulation<dim>>>\\n      coarse_grid_triangulations;\\n if (mg_data.transfer.perform_h_transfer)\\n      coarse_grid_triangulations =\\n MGTransferGlobalCoarseningTools::create_geometric_coarsening_sequence(\\n          dof_handler.get_triangulation());\\n else\\n      coarse_grid_triangulations.emplace_back(\\n        &(dof_handler.get_triangulation()), [](auto *) {});\\n \\n const unsigned int n_h_levels = coarse_grid_triangulations.size() - 1;\\n \\n const auto get_max_active_fe_degree = [&](const auto &dof_handler) {\\n unsigned int max = 0;\\n \\n for (auto &cell : dof_handler.active_cell_iterators())\\n        if (cell->is_locally_owned())\\n max =\\n std::max(max, dof_handler.get_fe(cell->active_fe_index()).degree);\\n \\n return Utilities::MPI::max(max, MPI_COMM_WORLD);\\n    };\\n \\n const unsigned int n_p_levels =\\n MGTransferGlobalCoarseningTools::create_polynomial_coarsening_sequence(\\n        get_max_active_fe_degree(dof_handler), mg_data.transfer.p_sequence)\\n        .size();\\n \\n    std::map<unsigned int, unsigned int> fe_index_for_degree;\\n for (unsigned int i = 0; i < dof_handler.get_fe_collection().size(); ++i)\\n      {\\n const unsigned int degree = dof_handler.get_fe(i).degree;\\n Assert(fe_index_for_degree.find(degree) == fe_index_for_degree.end(),\\n               ExcMessage(\\\"FECollection does not contain unique degrees.\\\"));\\n        fe_index_for_degree[degree] = i;\\n      }\\n \\n unsigned int minlevel = 0;\\n unsigned int maxlevel = n_h_levels + n_p_levels - 1;\\n \\n    dof_handlers.resize(minlevel, maxlevel);\\n    operators.resize(minlevel, maxlevel);\\n    transfers.resize(minlevel, maxlevel);\\n \\n for (unsigned int l = 0; l < n_h_levels; ++l)\\n      {\\n        dof_handlers[l].reinit(*coarse_grid_triangulations[l]);\\n        dof_handlers[l].distribute_dofs(dof_handler.get_fe_collection());\\n      }\\n \\n for (unsigned int i = 0, l = maxlevel; i < n_p_levels; ++i, --l)\\n      {\\n        dof_handlers[l].reinit(dof_handler.get_triangulation());\\n \\n if (l == maxlevel) // finest level\\n          {\\n auto &dof_handler_mg = dof_handlers[l];\\n \\n auto cell_other = dof_handler.begin_active();\\n for (auto &cell : dof_handler_mg.active_cell_iterators())\\n              {\\n if (cell->is_locally_owned())\\n                  cell->set_active_fe_index(cell_other->active_fe_index());\\n                ++cell_other;\\n              }\\n          }\\n else // coarse level\\n          {\\n auto &dof_handler_fine   = dof_handlers[l + 1];\\n auto &dof_handler_coarse = dof_handlers[l + 0];\\n \\n auto cell_other = dof_handler_fine.begin_active();\\n for (auto &cell : dof_handler_coarse.active_cell_iterators())\\n              {\\n if (cell->is_locally_owned())\\n                  {\\n const unsigned int next_degree =\\n MGTransferGlobalCoarseningTools::\\n                        create_next_polynomial_coarsening_degree(\\n                          cell_other->get_fe().degree,\\n                          mg_data.transfer.p_sequence);\\n Assert(fe_index_for_degree.find(next_degree) !=\\n                             fe_index_for_degree.end(),\\n                           ExcMessage(\\\"Next polynomial degree in sequence \\\"\\n \\\"does not exist in FECollection.\\\"));\\n \\n                    cell->set_active_fe_index(fe_index_for_degree[next_degree]);\\n                  }\\n                ++cell_other;\\n              }\\n          }\\n \\n        dof_handlers[l].distribute_dofs(dof_handler.get_fe_collection());\\n      }\\n \\n MGLevelObject<AffineConstraints<typename VectorType::value_type>>\\n      constraints(minlevel, maxlevel);\\n \\n for (unsigned int level = minlevel; level <= maxlevel; ++level)\\n      {\\n const auto &dof_handler = dof_handlers[level];\\n auto       &constraint  = constraints[level];\\n \\n        constraint.reinit(dof_handler.locally_owned_dofs(),\\n DoFTools::extract_locally_relevant_dofs(dof_handler));\\n \\n DoFTools::make_hanging_node_constraints(dof_handler, constraint);\\n VectorTools::interpolate_boundary_values(mapping_collection,\\n                                                 dof_handler,\\n                                                 0,\\n Functions::ZeroFunction<dim>(),\\n                                                 constraint);\\n        constraint.close();\\n \\n        VectorType dummy;\\n \\n        operators[level] = std::make_unique<OperatorType>(mapping_collection,\\n                                                          dof_handler,\\n                                                          quadrature_collection,\\n                                                          constraint,\\n                                                          dummy);\\n      }\\n \\n for (unsigned int level = minlevel; level < maxlevel; ++level)\\n      transfers[level + 1].reinit(dof_handlers[level + 1],\\n                                  dof_handlers[level],\\n                                  constraints[level + 1],\\n                                  constraints[level]);\\n \\n MGTransferGlobalCoarsening<dim, VectorType> transfer(\\n      transfers, [&](const auto l, auto &vec) {\\n        operators[l]->initialize_dof_vector(vec);\\n      });\\n \\n    mg_solve(solver_control,\\n             dst,\\n             src,\\n             mg_data,\\n             dof_handler,\\n             system_matrix,\\n             operators,\\n             transfer);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n class LaplaceProblem\\n  {\\n public:\\n    LaplaceProblem(const Parameters &parameters);\\n \\n void run();\\n \\n private:\\n void initialize_grid();\\n void setup_system();\\n void print_diagnostics();\\n void solve_system();\\n void compute_indicators();\\n void adapt_resolution();\\n void output_results(const unsigned int cycle);\\n \\n MPI_Comm mpi_communicator;\\n \\n const Parameters prm;\\n \\n parallel::distributed::Triangulation<dim> triangulation;\\n DoFHandler<dim>                           dof_handler;\\n \\n hp::MappingCollection<dim> mapping_collection;\\n hp::FECollection<dim>      fe_collection;\\n hp::QCollection<dim>       quadrature_collection;\\n hp::QCollection<dim - 1>   face_quadrature_collection;\\n \\n IndexSet locally_owned_dofs;\\n IndexSet locally_relevant_dofs;\\n \\n AffineConstraints<double> constraints;\\n \\n    LaplaceOperator<dim, double>               laplace_operator;\\n LinearAlgebra::distributed::Vector<double> locally_relevant_solution;\\n LinearAlgebra::distributed::Vector<double> system_rhs;\\n \\n    std::unique_ptr<FESeries::Legendre<dim>>    legendre;\\n    std::unique_ptr<parallel::CellWeights<dim>> cell_weights;\\n \\n Vector<float> estimated_error_per_cell;\\n Vector<float> hp_decision_indicators;\\n \\n ConditionalOStream pcout;\\n TimerOutput        computing_timer;\\n  };\\n \\n \\n \\n \\n \\n template <int dim>\\n  LaplaceProblem<dim>::LaplaceProblem(const Parameters &parameters)\\n    : mpi_communicator(MPI_COMM_WORLD)\\n    , prm(parameters)\\n    , triangulation(mpi_communicator)\\n    , dof_handler(triangulation)\\n    , pcout(std::cout,\\n            (Utilities::MPI::this_mpi_process(mpi_communicator) == 0))\\n    , computing_timer(mpi_communicator,\\n                      pcout,\\n TimerOutput::never,\\n TimerOutput::wall_times)\\n  {\\n Assert(prm.min_h_level <= prm.max_h_level,\\n           ExcMessage(\\n \\\"Triangulation level limits have been incorrectly set up.\\\"));\\n Assert(prm.min_p_degree <= prm.max_p_degree,\\n           ExcMessage(\\\"FECollection degrees have been incorrectly set up.\\\"));\\n \\n    mapping_collection.push_back(MappingQ1<dim>());\\n \\n for (unsigned int degree = 1; degree <= prm.max_p_degree; ++degree)\\n      {\\n        fe_collection.push_back(FE_Q<dim>(degree));\\n        quadrature_collection.push_back(QGauss<dim>(degree + 1));\\n        face_quadrature_collection.push_back(QGauss<dim - 1>(degree + 1));\\n      }\\n \\n const unsigned int min_fe_index = prm.min_p_degree - 1;\\n    fe_collection.set_hierarchy(\\n /*next_index=*/\\n      [](const typename hp::FECollection<dim> &fe_collection,\\n const unsigned int                    fe_index) -> unsigned int {\\n return ((fe_index + 1) < fe_collection.size()) ? fe_index + 1 :\\n                                                         fe_index;\\n      },\\n /*previous_index=*/\\n      [min_fe_index](const typename hp::FECollection<dim> &,\\n const unsigned int fe_index) -> unsigned int {\\n Assert(fe_index >= min_fe_index,\\n               ExcMessage(\\\"Finite element is not part of hierarchy!\\\"));\\n return (fe_index > min_fe_index) ? fe_index - 1 : fe_index;\\n      });\\n \\n legendre = std::make_unique<FESeries::Legendre<dim>>(\\n SmoothnessEstimator::Legendre::default_fe_series(fe_collection));\\n \\n    cell_weights = std::make_unique<parallel::CellWeights<dim>>(\\n      dof_handler,\\n parallel::CellWeights<dim>::ndofs_weighting(\\n        {prm.weighting_factor, prm.weighting_exponent}));\\n \\n triangulation.signals.post_p4est_refinement.connect(\\n      [&, min_fe_index]() {\\n const parallel::distributed::TemporarilyMatchRefineFlags<dim>\\n          refine_modifier(triangulation);\\n hp::Refinement::limit_p_level_difference(dof_handler,\\n                                                 prm.max_p_level_difference,\\n /*contains=*/min_fe_index);\\n      },\\n      boost::signals2::at_front);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void LaplaceProblem<dim>::initialize_grid()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"initialize grid\\\");\\n \\n    std::vector<unsigned int> repetitions(dim);\\n Point<dim>                bottom_left, top_right;\\n for (unsigned int d = 0; d < dim; ++d)\\n if (d < 2)\\n        {\\n          repetitions[d] = 2;\\n          bottom_left[d] = -1.;\\n          top_right[d]   = 1.;\\n        }\\n else\\n        {\\n          repetitions[d] = 1;\\n          bottom_left[d] = 0.;\\n          top_right[d]   = 1.;\\n        }\\n \\n    std::vector<int> cells_to_remove(dim, 1);\\n    cells_to_remove[0] = -1;\\n \\n GridGenerator::subdivided_hyper_L(\\n triangulation, repetitions, bottom_left, top_right, cells_to_remove);\\n \\n const unsigned int min_fe_index = prm.min_p_degree - 1;\\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      if (cell->is_locally_owned())\\n        cell->set_active_fe_index(min_fe_index);\\n \\n    dof_handler.distribute_dofs(fe_collection);\\n \\n triangulation.refine_global(prm.min_h_level);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void LaplaceProblem<dim>::setup_system()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"setup system\\\");\\n \\n    dof_handler.distribute_dofs(fe_collection);\\n \\n    locally_owned_dofs = dof_handler.locally_owned_dofs();\\n    locally_relevant_dofs =\\n DoFTools::extract_locally_relevant_dofs(dof_handler);\\n \\n    locally_relevant_solution.reinit(locally_owned_dofs,\\n                                     locally_relevant_dofs,\\n                                     mpi_communicator);\\n    system_rhs.reinit(locally_owned_dofs, mpi_communicator);\\n \\n    constraints.clear();\\n    constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n VectorTools::interpolate_boundary_values(\\n      mapping_collection, dof_handler, 0, Solution<dim>(), constraints);\\n    constraints.close();\\n \\n    laplace_operator.reinit(mapping_collection,\\n                            dof_handler,\\n                            quadrature_collection,\\n                            constraints,\\n                            system_rhs);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void LaplaceProblem<dim>::print_diagnostics()\\n  {\\n const unsigned int first_n_processes =\\n std::min<unsigned int>(8,\\n Utilities::MPI::n_mpi_processes(mpi_communicator));\\n const bool output_cropped =\\n      first_n_processes < Utilities::MPI::n_mpi_processes(mpi_communicator);\\n \\n    {\\n      pcout << \\\"   Number of active cells:       \\\"\\n            << triangulation.n_global_active_cells() << std::endl\\n            << \\\"     by partition:              \\\";\\n \\n      std::vector<unsigned int> n_active_cells_per_subdomain =\\n Utilities::MPI::gather(mpi_communicator,\\n triangulation.n_locally_owned_active_cells());\\n for (unsigned int i = 0; i < first_n_processes; ++i)\\n        pcout << ' ' << n_active_cells_per_subdomain[i];\\n if (output_cropped)\\n        pcout << \\\" ...\\\";\\n      pcout << std::endl;\\n    }\\n \\n    {\\n      pcout << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n            << std::endl\\n            << \\\"     by partition:              \\\";\\n \\n      std::vector<types::global_dof_index> n_dofs_per_subdomain =\\n Utilities::MPI::gather(mpi_communicator,\\n                               dof_handler.n_locally_owned_dofs());\\n for (unsigned int i = 0; i < first_n_processes; ++i)\\n        pcout << ' ' << n_dofs_per_subdomain[i];\\n if (output_cropped)\\n        pcout << \\\" ...\\\";\\n      pcout << std::endl;\\n    }\\n \\n    {\\n      std::vector<types::global_dof_index> n_constraints_per_subdomain =\\n Utilities::MPI::gather(mpi_communicator, constraints.n_constraints());\\n \\n      pcout << \\\"   Number of constraints:        \\\"\\n            << std::accumulate(n_constraints_per_subdomain.begin(),\\n                               n_constraints_per_subdomain.end(),\\n                               0)\\n            << std::endl\\n            << \\\"     by partition:              \\\";\\n for (unsigned int i = 0; i < first_n_processes; ++i)\\n        pcout << ' ' << n_constraints_per_subdomain[i];\\n if (output_cropped)\\n        pcout << \\\" ...\\\";\\n      pcout << std::endl;\\n    }\\n \\n    {\\n      std::vector<unsigned int> n_fe_indices(fe_collection.size(), 0);\\n for (const auto &cell : dof_handler.active_cell_iterators())\\n        if (cell->is_locally_owned())\\n          n_fe_indices[cell->active_fe_index()]++;\\n \\n Utilities::MPI::sum(n_fe_indices, mpi_communicator, n_fe_indices);\\n \\n      pcout << \\\"   Frequencies of poly. degrees:\\\";\\n for (unsigned int i = 0; i < fe_collection.size(); ++i)\\n if (n_fe_indices[i] > 0)\\n          pcout << ' ' << fe_collection[i].degree << ':' << n_fe_indices[i];\\n      pcout << std::endl;\\n    }\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void LaplaceProblem<dim>::solve_system()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"solve system\\\");\\n \\n LinearAlgebra::distributed::Vector<double> completely_distributed_solution;\\n    laplace_operator.initialize_dof_vector(completely_distributed_solution);\\n \\n SolverControl solver_control(system_rhs.size(),\\n                                 prm.tolerance_factor * system_rhs.l2_norm());\\n \\n    solve_with_gmg(solver_control,\\n                   laplace_operator,\\n                   completely_distributed_solution,\\n                   system_rhs,\\n                   prm.mg_data,\\n                   mapping_collection,\\n                   dof_handler,\\n                   quadrature_collection);\\n \\n    pcout << \\\"   Solved in \\\" << solver_control.last_step() << \\\" iterations.\\\"\\n          << std::endl;\\n \\n    constraints.distribute(completely_distributed_solution);\\n \\n    locally_relevant_solution.copy_locally_owned_data_from(\\n      completely_distributed_solution);\\n    locally_relevant_solution.update_ghost_values();\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void LaplaceProblem<dim>::compute_indicators()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"compute indicators\\\");\\n \\n    estimated_error_per_cell.grow_or_shrink(triangulation.n_active_cells());\\n KellyErrorEstimator<dim>::estimate(\\n      dof_handler,\\n      face_quadrature_collection,\\n      std::map<types::boundary_id, const Function<dim> *>(),\\n      locally_relevant_solution,\\n      estimated_error_per_cell,\\n /*component_mask=*/ComponentMask(),\\n /*coefficients=*/nullptr,\\n /*n_threads=*/numbers::invalid_unsigned_int,\\n /*subdomain_id=*/numbers::invalid_subdomain_id,\\n /*material_id=*/numbers::invalid_material_id,\\n /*strategy=*/\\n KellyErrorEstimator<dim>::Strategy::face_diameter_over_twice_max_degree);\\n \\n    hp_decision_indicators.grow_or_shrink(triangulation.n_active_cells());\\n SmoothnessEstimator::Legendre::coefficient_decay(*legendre,\\n                                                     dof_handler,\\n                                                     locally_relevant_solution,\\n                                                     hp_decision_indicators);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void LaplaceProblem<dim>::adapt_resolution()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"adapt resolution\\\");\\n \\n parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number(\\n triangulation,\\n      estimated_error_per_cell,\\n      prm.refine_fraction,\\n      prm.coarsen_fraction);\\n \\n hp::Refinement::p_adaptivity_fixed_number(dof_handler,\\n                                              hp_decision_indicators,\\n                                              prm.p_refine_fraction,\\n                                              prm.p_coarsen_fraction);\\n \\n Assert(triangulation.n_levels() >= prm.min_h_level + 1 &&\\n triangulation.n_levels() <= prm.max_h_level + 1,\\n           ExcInternalError());\\n \\n if (triangulation.n_levels() > prm.max_h_level)\\n for (const auto &cell :\\n triangulation.active_cell_iterators_on_level(prm.max_h_level))\\n        cell->clear_refine_flag();\\n \\n for (const auto &cell :\\n triangulation.active_cell_iterators_on_level(prm.min_h_level))\\n      cell->clear_coarsen_flag();\\n \\n hp::Refinement::choose_p_over_h(dof_handler);\\n \\n triangulation.execute_coarsening_and_refinement();\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void LaplaceProblem<dim>::output_results(const unsigned int cycle)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"output results\\\");\\n \\n Vector<float> fe_degrees(triangulation.n_active_cells());\\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      if (cell->is_locally_owned())\\n        fe_degrees(cell->active_cell_index()) = cell->get_fe().degree;\\n \\n Vector<float> subdomain(triangulation.n_active_cells());\\n for (auto &subd : subdomain)\\n      subd = triangulation.locally_owned_subdomain();\\n \\n DataOut<dim> data_out;\\n    data_out.attach_dof_handler(dof_handler);\\n    data_out.add_data_vector(locally_relevant_solution, \\\"solution\\\");\\n    data_out.add_data_vector(fe_degrees, \\\"fe_degree\\\");\\n    data_out.add_data_vector(subdomain, \\\"subdomain\\\");\\n    data_out.add_data_vector(estimated_error_per_cell, \\\"error\\\");\\n    data_out.add_data_vector(hp_decision_indicators, \\\"hp_indicator\\\");\\n    data_out.build_patches(mapping_collection);\\n \\n    data_out.write_vtu_with_pvtu_record(\\n \\\"./\\\", \\\"solution\\\", cycle, mpi_communicator, 2, 1);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void LaplaceProblem<dim>::run()\\n  {\\n    pcout << \\\"Running with Trilinos on \\\"\\n          << Utilities::MPI::n_mpi_processes(mpi_communicator)\\n          << \\\" MPI rank(s)...\\\" << std::endl;\\n \\n    {\\n      pcout << \\\"Calculating transformation matrices...\\\" << std::endl;\\n TimerOutput::Scope t(computing_timer, \\\"calculate transformation\\\");\\n legendre->precalculate_all_transformation_matrices();\\n    }\\n \\n for (unsigned int cycle = 0; cycle < prm.n_cycles; ++cycle)\\n      {\\n        pcout << \\\"Cycle \\\" << cycle << ':' << std::endl;\\n \\n if (cycle == 0)\\n          initialize_grid();\\n else\\n          adapt_resolution();\\n \\n        setup_system();\\n \\n        print_diagnostics();\\n \\n        solve_system();\\n \\n        compute_indicators();\\n \\n if (Utilities::MPI::n_mpi_processes(mpi_communicator) <= 32)\\n          output_results(cycle);\\n \\n        computing_timer.print_summary();\\n        computing_timer.reset();\\n \\n        pcout << std::endl;\\n      }\\n  }\\n} // namespace Step75\\n \\n \\n \\n \\nint main(int argc, char *argv[])\\n{\\n try\\n    {\\n using namespace dealii;\\n using namespace Step75;\\n \\n Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n \\n      Parameters        prm;\\n      LaplaceProblem<2> laplace_problem(prm);\\n      laplace_problem.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n \\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n \\n return 0;\\n}\\naffine_constraints.h\\ncell_weights.h\\nAffineConstraints::closevoid close()\\nAffineConstraints::distributevoid distribute(VectorType &vec) const\\nAffineConstraints::reinitvoid reinit()\\nAffineConstraints::copy_fromvoid copy_from(const AffineConstraints< other_number > &other)Definition affine_constraints.h:2778\\nAffineConstraints::clearvoid clear()\\nAffineConstraints::n_constraintssize_type n_constraints() constDefinition affine_constraints.h:2463\\nAffineConstraints::set_zerovoid set_zero(VectorType &vec) constDefinition affine_constraints.h:2450\\nDataOutInterface::write_vtu_with_pvtu_recordstd::string write_vtu_with_pvtu_record(const std::string &directory, const std::string &filename_without_extension, const unsigned int counter, const MPI_Comm mpi_communicator, const unsigned int n_digits_for_counter=numbers::invalid_unsigned_int, const unsigned int n_groups=0) constDefinition data_out_base.cc:7854\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\nDoFHandler::get_fe_collectionconst hp::FECollection< dim, spacedim > & get_fe_collection() const\\nDoFHandler::get_feconst FiniteElement< dim, spacedim > & get_fe(const types::fe_index index=0) const\\nDoFHandler::get_triangulationconst Triangulation< dim, spacedim > & get_triangulation() const\\nDoFHandler::locally_owned_dofsconst IndexSet & locally_owned_dofs() const\\nDoFHandler::begin_activeactive_cell_iterator begin_active(const unsigned int level=0) const\\nFiniteElementData::degreeconst unsigned int degreeDefinition fe_data.h:452\\nMGLevelObject::resizevoid resize(const unsigned int new_minlevel, const unsigned int new_maxlevel, Args &&...args)Definition mg_level_object.h:256\\nMatrixFree::get_dof_handlerconst DoFHandler< dim > & get_dof_handler(const unsigned int dof_handler_index=0) const\\nMatrixFree::initialize_dof_vectorvoid initialize_dof_vector(VectorType &vec, const unsigned int dof_handler_index=0) const\\nMatrixFree::cell_loopvoid cell_loop(const std::function< void(const MatrixFree< dim, Number, VectorizedArrayType > &, OutVector &, const InVector &, const std::pair< unsigned int, unsigned int > &)> &cell_operation, OutVector &dst, const InVector &src, const bool zero_dst_vector=false) const\\nTriangulation::get_communicatorvirtual MPI_Comm get_communicator() const\\nTrilinosWrappers::PreconditionAMG::initializevoid initialize(const SparseMatrix &matrix, const AdditionalData &additional_data=AdditionalData())Definition trilinos_precondition_ml.cc:221\\nhp::MappingCollection::push_backvoid push_back(const Mapping< dim, spacedim > &new_mapping)Definition mapping_collection.cc:48\\nhp::QCollection::push_backvoid push_back(const Quadrature< dim_in > &new_quadrature)Definition q_collection.h:215\\nconditional_ostream.h\\ngrid_refinement.h\\ntria.h\\ndof_handler.h\\ndof_tools.h\\ndynamic_sparsity_pattern.h\\nerror_estimator.h\\nfe_collection.h\\nfe_evaluation.h\\nfe_q.h\\nfe_series.h\\nfunction.h\\ngeometric_utilities.h\\ngrid_generator.h\\nindex_set.h\\nla_parallel_vector.h\\nmapping_q1.h\\nmatrix_free.h\\nmg_coarse.h\\nmg_constrained_dofs.h\\nmg_matrix.h\\nmg_smoother.h\\nmg_tools.h\\nmg_transfer_global_coarsening.h\\nmpi.h\\nmultigrid.h\\nLAPACKSupport::diagonal@ diagonalMatrix is diagonal.Definition lapack_support.h:118\\nMGTransferGlobalCoarseningTools::PolynomialCoarseningSequenceType::decrease_by_one@ decrease_by_one\\nPhysics::Elasticity::Kinematics::eSymmetricTensor< 2, dim, Number > e(const Tensor< 2, dim, Number > &F)\\nPhysics::Elasticity::Kinematics::bSymmetricTensor< 2, dim, Number > b(const Tensor< 2, dim, Number > &F)\\nPhysics::Elasticity::Kinematics::dSymmetricTensor< 2, dim, Number > d(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\nUtilities::MPI::this_mpi_processunsigned int this_mpi_process(const MPI_Comm mpi_communicator)Definition mpi.cc:107\\nVectorTools::EvaluationFlags::max@ maxDefinition vector_tools_evaluate.h:55\\nWorkStream::internal::tbb_no_coloring::runvoid run(const Iterator &begin, const std_cxx20::type_identity_t< Iterator > &end, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length, const unsigned int chunk_size)Definition work_stream.h:471\\ninternal::reinitvoid reinit(MatrixBlock< MatrixType > &v, const BlockSparsityPattern &p)Definition matrix_block.h:617\\nstd_cxx17::legendredouble legendre(unsigned int l, double x)Definition cmath.h:65\\nstd::abs::VectorizedArray< Number, width > abs(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6927\\ndata_out.h\\nprecondition.h\\nquadrature_lib.h\\nrefinement.h\\nsmoothness_estimator.h\\nsolver_cg.h\\nTrilinosWrappers::PreconditionAMG::AdditionalData::n_cyclesunsigned int n_cyclesDefinition trilinos_precondition.h:1457\\nTrilinosWrappers::PreconditionAMG::AdditionalData::smoother_typeconst char * smoother_typeDefinition trilinos_precondition.h:1561\\ntimer.h\\ntools.h\\ntrilinos_precondition.h\\ntrilinos_sparse_matrix.h\\ntrilinos_sparsity_pattern.h\\nvector.h\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"