"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_32.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-32 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-32 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-32 tutorial program\\n\\n\\nThis tutorial depends on step-31, step-55.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\n Using the \\\"right\\\" pressure \\n The scaling of discretized equations \\n Changes to the Stokes preconditioner and solver \\n Changes to the artificial viscosity stabilization \\n Locally conservative Stokes discretization \\n Higher order mappings for curved boundaries \\n Parallelization on clusters \\n Parallelization within individual nodes of a cluster \\n The testcase \\n Implementation details \\n Outlook \\n\\n The commented program\\n\\nInclude files\\nEquation data\\nPreconditioning the Stokes system\\nDefinition of assembly data structures\\nThe BoussinesqFlowProblem class template\\nBoussinesqFlowProblem class implementation\\n\\nBoussinesqFlowProblem::Parameters\\nBoussinesqFlowProblem::BoussinesqFlowProblem\\nThe BoussinesqFlowProblem helper functions\\n\\nBoussinesqFlowProblem::get_maximal_velocity\\nBoussinesqFlowProblem::get_cfl_number\\nBoussinesqFlowProblem::get_entropy_variation\\nBoussinesqFlowProblem::get_extrapolated_temperature_range\\nBoussinesqFlowProblem::compute_viscosity\\n\\nThe BoussinesqFlowProblem setup functions\\nThe BoussinesqFlowProblem assembly functions\\n\\nStokes preconditioner assembly\\nStokes system assembly\\nTemperature matrix assembly\\nTemperature right hand side assembly\\n\\nBoussinesqFlowProblem::solve\\nBoussinesqFlowProblem::output_results\\nBoussinesqFlowProblem::refine_mesh\\nBoussinesqFlowProblem::run\\n\\nThe main function\\n\\n\\n Results\\n\\nComparison of results with step-31\\nResults for a 2d circular shell testcase\\nResults for a 3d spherical shell testcase\\nPossibilities for extensions\\n\\n The plain program\\n   \\n\\n\\nThis program was contributed by Martin Kronbichler, Wolfgang Bangerth, and Timo Heister.\\nThis material is based upon work partly supported by the National Science Foundation under Award No. EAR-0426271 and The California Institute of Technology; and in a continuation by the National Science Foundation under Award No. EAR-0949446 and The University of California \\u2013 Davis. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author and do not necessarily reflect the views of the National Science Foundation, The California Institute of Technology, or of The University of California \\u2013 Davis.\\nThe work discussed here is also presented in the following publication:  M. Kronbichler, T. Heister, W. Bangerth: High Accuracy Mantle Convection Simulation through Modern Numerical Methods, Geophysical Journal International, 2012, 191, 12-29. [DOI] \\nThe continuation of development of this program has led to the much larger open source code ASPECT (see http://aspect.geodynamics.org/) which is much more flexible in solving many kinds of related problems. \\n Introduction\\nThis program does pretty much exactly what step-31 already does: it solves the Boussinesq equations that describe the motion of a fluid whose temperature is not in equilibrium. As such, all the equations we have described in step-31 still hold: we solve the same general partial differential equation (with only minor modifications to adjust for more realism in the problem setting), using the same finite element scheme, the same time stepping algorithm, and more or less the same stabilization method for the temperature advection-diffusion equation. As a consequence, you may first want to understand that program \\u2014 and its implementation \\u2014 before you work on the current one.\\nThe difference between step-31 and the current program is that here we want to do things in parallel, using both the availability of many machines in a cluster (with parallelization based on MPI) as well as many processor cores within a single machine (with parallelization based on threads). This program's main job is therefore to introduce the changes that are necessary to utilize the availability of these parallel compute resources. In this regard, it builds on the step-40 program that first introduces the necessary classes for much of the parallel functionality, and on step-55 that shows how this is done for a vector-valued problem.\\nIn addition to these changes, we also use a slightly different preconditioner, and we will have to make a number of changes that have to do with the fact that we want to solve a realistic problem here, not a model problem. The latter, in particular, will require that we think about scaling issues as well as what all those parameters and coefficients in the equations under consideration actually mean. We will discuss first the issues that affect changes in the mathematical formulation and solver structure, then how to parallelize things, and finally the actual testcase we will consider.\\nUsing the \\\"right\\\" pressure \\nIn step-31, we used the following Stokes model for the velocity and pressure field:      \\n\\\\begin{eqnarray*}\\n  -\\\\nabla \\\\cdot (2 \\\\eta \\\\varepsilon ({\\\\mathbf u})) + \\\\nabla p &=&\\n  -\\\\rho \\\\; \\\\beta \\\\; T \\\\mathbf{g},\\n  \\\\\\\\\\n  \\\\nabla \\\\cdot {\\\\mathbf u} &=& 0.\\n\\\\end{eqnarray*}\\n\\n The right hand side of the first equation appears a wee bit unmotivated. Here's how things should really be. We need the external forces that act on the fluid, which we assume are given by gravity only. In the current case, we assume that the fluid does expand slightly for the purposes of this gravity force, but not enough that we need to modify the incompressibility condition (the second equation). What this means is that for the purpose of the right hand side, we can assume that \\\\(\\\\rho=\\\\rho(T)\\\\). An assumption that may not be entirely justified is that we can assume that the changes of density as a function of temperature are small, leading to an expression of the form  \\\\(\\\\rho(T) = \\\\rho_{\\\\text{ref}}\\n[1-\\\\beta(T-T_{\\\\text{ref}})]\\\\), i.e., the density equals \\\\(\\\\rho_{\\\\text{ref}}\\\\) at reference temperature and decreases linearly as the temperature increases (as the material expands). The force balance equation then looks properly written like this:    \\n\\\\begin{eqnarray*}\\n  -\\\\nabla \\\\cdot (2 \\\\eta \\\\varepsilon ({\\\\mathbf u})) + \\\\nabla p &=&\\n  \\\\rho_{\\\\text{ref}} [1-\\\\beta(T-T_{\\\\text{ref}})] \\\\mathbf{g}.\\n\\\\end{eqnarray*}\\n\\n Now note that the gravity force results from a gravity potential as \\\\(\\\\mathbf g=-\\\\nabla \\\\varphi\\\\), so that we can re-write this as follows:     \\n\\\\begin{eqnarray*}\\n  -\\\\nabla \\\\cdot (2 \\\\eta \\\\varepsilon ({\\\\mathbf u})) + \\\\nabla p &=&\\n  -\\\\rho_{\\\\text{ref}} \\\\; \\\\beta\\\\; T\\\\; \\\\mathbf{g}\\n  -\\\\rho_{\\\\text{ref}} [1+\\\\beta T_{\\\\text{ref}}] \\\\nabla\\\\varphi.\\n\\\\end{eqnarray*}\\n\\n The second term on the right is time independent, and so we could introduce a new \\\"dynamic\\\" pressure  \\\\(p_{\\\\text{dyn}}=p+\\\\rho_{\\\\text{ref}}\\n[1+\\\\beta T_{\\\\text{ref}}] \\\\varphi=p_{\\\\text{total}}-p_{\\\\text{static}}\\\\) with which the Stokes equations would read:      \\n\\\\begin{eqnarray*}\\n  -\\\\nabla \\\\cdot (2 \\\\eta \\\\varepsilon ({\\\\mathbf u})) + \\\\nabla p_{\\\\text{dyn}} &=&\\n  -\\\\rho_{\\\\text{ref}} \\\\; \\\\beta \\\\; T \\\\; \\\\mathbf{g},\\n  \\\\\\\\\\n  \\\\nabla \\\\cdot {\\\\mathbf u} &=& 0.\\n\\\\end{eqnarray*}\\n\\n This is exactly the form we used in step-31, and it was appropriate to do so because all changes in the fluid flow are only driven by the dynamic pressure that results from temperature differences. (In other words: Any contribution to the right hand side that results from taking the gradient of a scalar field have no effect on the velocity field.)\\nOn the other hand, we will here use the form of the Stokes equations that considers the total pressure instead:      \\n\\\\begin{eqnarray*}\\n  -\\\\nabla \\\\cdot (2 \\\\eta \\\\varepsilon ({\\\\mathbf u})) + \\\\nabla p &=&\\n  \\\\rho(T)\\\\; \\\\mathbf{g},\\n  \\\\\\\\\\n  \\\\nabla \\\\cdot {\\\\mathbf u} &=& 0.\\n\\\\end{eqnarray*}\\n\\n There are several advantages to this:\\n\\nThis way we can plot the pressure in our program in such a way that it actually shows the total pressure that includes the effects of temperature differences as well as the static pressure of the overlying rocks. Since the pressure does not appear any further in any of the other equations, whether to use one or the other is more a matter of taste than of correctness. The flow field is exactly the same, but we get a pressure that we can now compare with values that are given in geophysical books as those that hold at the bottom of the earth mantle, for example.\\nIf we wanted to make the model even more realistic, we would have to take into account that many of the material parameters (e.g. the viscosity, the density, etc) not only depend on the temperature but also the total pressure.\\nThe model above assumed a linear dependence  \\\\(\\\\rho(T) = \\\\rho_{\\\\text{ref}}\\n  [1-\\\\beta(T-T_{\\\\text{ref}})]\\\\) and assumed that \\\\(\\\\beta\\\\) is small. In practice, this may not be so. In fact, realistic models are certainly not linear, and \\\\(\\\\beta\\\\) may also not be small for at least part of the temperature range because the density's behavior is substantially dependent not only on thermal expansion but by phase changes.\\nA final reason to do this is discussed in the results section and concerns possible extensions to the model we use here. It has to do with the fact that the temperature equation (see below) we use here does not include a term that contains the pressure. It should, however: rock, like gas, heats up as you compress it. Consequently, material that rises up cools adiabatically, and cold material that sinks down heats adiabatically. We discuss this further below.\\n\\nNoteThere is, however, a downside to this procedure. In the earth, the dynamic pressure is several orders of magnitude smaller than the total pressure. If we use the equations above and solve all variables to, say, 4 digits of accuracy, then we may be able to get the velocity and the total pressure right, but we will have no accuracy at all if we compute the dynamic pressure by subtracting from the total pressure the static part  \\\\(p_\\\\text{static}=\\\\rho_{\\\\text{ref}}\\n[1+\\\\beta T_{\\\\text{ref}}] \\\\varphi\\\\). If, for example, the dynamic pressure is six orders of magnitude smaller than the static pressure, then we need to solve the overall pressure to at least seven digits of accuracy to get anything remotely accurate. That said, in practice this turns out not to be a limiting factor.\\nThe scaling of discretized equations \\nRemember that we want to solve the following set of equations:            \\n\\\\begin{eqnarray*}\\n  -\\\\nabla \\\\cdot (2 \\\\eta \\\\varepsilon ({\\\\mathbf u})) + \\\\nabla p &=&\\n  \\\\rho(T) \\\\mathbf{g},\\n  \\\\\\\\\\n  \\\\nabla \\\\cdot {\\\\mathbf u} &=& 0,\\n  \\\\\\\\\\n  \\\\frac{\\\\partial T}{\\\\partial t}\\n  +\\n  {\\\\mathbf u} \\\\cdot \\\\nabla T\\n  -\\n  \\\\nabla \\\\cdot \\\\kappa \\\\nabla T &=& \\\\gamma,\\n\\\\end{eqnarray*}\\n\\n augmented by appropriate boundary and initial conditions. As discussed in step-31, we will solve this set of equations by solving for a Stokes problem first in each time step, and then moving the temperature equation forward by one time interval.\\nThe problem under consideration in this current section is with the Stokes problem: if we discretize it as usual, we get a linear system                \\n\\\\begin{eqnarray*}\\n  M \\\\; X\\n  =\\n  \\\\left(\\\\begin{array}{cc}\\n    A & B^T \\\\\\\\ B & 0\\n  \\\\end{array}\\\\right)\\n  \\\\left(\\\\begin{array}{c}\\n    U \\\\\\\\ P\\n  \\\\end{array}\\\\right)\\n  =\\n  \\\\left(\\\\begin{array}{c}\\n    F_U \\\\\\\\ 0\\n  \\\\end{array}\\\\right)\\n  =\\n  F\\n\\\\end{eqnarray*}\\n\\n which in this program we will solve with a FGMRES solver. This solver iterates until the residual of these linear equations is below a certain tolerance, i.e., until          \\n\\\\[\\n  \\\\left\\\\|\\n  \\\\left(\\\\begin{array}{c}\\n    F_U - A U^{(k)} - B P^{(k)}\\n    \\\\\\\\\\n    B^T U^{(k)}\\n  \\\\end{array}\\\\right)\\n  \\\\right\\\\|\\n  < \\\\text{Tol}.\\n\\\\]\\n\\n This does not make any sense from the viewpoint of physical units: the quantities involved here have physical units so that the first part of the residual has units  \\\\(\\\\frac{\\\\text{Pa}}{\\\\text{m}}\\n\\\\text{m}^{\\\\text{dim}}\\\\) (most easily established by considering the term \\\\((\\\\nabla \\\\cdot \\\\mathbf v, p)_{\\\\Omega}\\\\) and considering that the pressure has units \\\\(\\\\text{Pa}=\\\\frac{\\\\text{kg}}{\\\\text{m}\\\\;\\\\text{s}^2}\\\\) and the integration yields a factor of \\\\(\\\\text{m}^{\\\\text{dim}}\\\\)), whereas the second part of the residual has units \\\\(\\\\frac{\\\\text{m}^{\\\\text{dim}}}{\\\\text{s}}\\\\). Taking the norm of this residual vector would yield a quantity with units  \\\\(\\\\text{m}^{\\\\text{dim}-1} \\\\sqrt{\\\\left(\\\\text{Pa}\\\\right)^2 +\\n       \\\\left(\\\\frac{\\\\text{m}}{\\\\text{s}}\\\\right)^2}\\\\). This, quite obviously, does not make sense, and we should not be surprised that doing so is eventually going to come back hurting us.\\nSo why is this an issue here, but not in step-31? The reason back there is that everything was nicely balanced: velocities were on the order of one, the pressure likewise, the viscosity was one, and the domain had a diameter of \\\\(\\\\sqrt{2}\\\\). As a result, while nonsensical, nothing bad happened. On the other hand, as we will explain below, things here will not be that simply scaled: \\\\(\\\\eta\\\\) will be around \\\\(10^{21}\\\\), velocities on the order of \\\\(10^{-8}\\\\), pressure around \\\\(10^8\\\\), and the diameter of the domain is \\\\(10^7\\\\). In other words, the order of magnitude for the first equation is going to be  \\\\(\\\\eta\\\\text{div}\\\\varepsilon(\\\\mathbf u) \\\\approx 10^{21} \\\\frac{10^{-8}}{(10^7)^2}\\n\\\\approx 10^{-1}\\\\), whereas the second equation will be around \\\\(\\\\text{div}{\\\\mathbf u}\\\\approx \\\\frac{10^{-8}}{10^7} \\\\approx 10^{-15}\\\\). Well, so what this will lead to is this: if the solver wants to make the residual small, it will almost entirely focus on the first set of equations because they are so much bigger, and ignore the divergence equation that describes mass conservation. That's exactly what happens: unless we set the tolerance to extremely small values, the resulting flow field is definitely not divergence free. As an auxiliary problem, it turns out that it is difficult to find a tolerance that always works; in practice, one often ends up with a tolerance that requires 30 or 40 iterations for most time steps, and 10,000 for some others.\\nSo what's a numerical analyst to do in a case like this? The answer is to start at the root and first make sure that everything is mathematically consistent first. In our case, this means that if we want to solve the system of Stokes equations jointly, we have to scale them so that they all have the same physical dimensions. In our case, this means multiplying the second equation by something that has units \\\\(\\\\frac{\\\\text{Pa}\\\\;\\\\text{s}}{\\\\text{m}}\\\\); one choice is to multiply with \\\\(\\\\frac{\\\\eta}{L}\\\\) where \\\\(L\\\\) is a typical lengthscale in our domain (which experiments show is best chosen to be the diameter of plumes \\u2014 around 10 km \\u2014 rather than the diameter of the domain). Using these numbers for \\\\(\\\\eta\\\\) and \\\\(L\\\\), this factor is around \\\\(10^{17}\\\\). So, we now get this for the Stokes system:      \\n\\\\begin{eqnarray*}\\n  -\\\\nabla \\\\cdot (2 \\\\eta \\\\varepsilon ({\\\\mathbf u})) + \\\\nabla p &=&\\n  \\\\rho(T) \\\\; \\\\mathbf{g},\\n  \\\\\\\\\\n  \\\\frac{\\\\eta}{L} \\\\nabla \\\\cdot {\\\\mathbf u} &=& 0.\\n\\\\end{eqnarray*}\\n\\n The trouble with this is that the result is not symmetric any more (we have \\\\(\\\\frac{\\\\eta}{L} \\\\nabla \\\\cdot\\\\) at the bottom left, but not its transpose operator at the top right). This, however, can be cured by introducing a scaled pressure \\\\(\\\\hat p = \\\\frac{L}{\\\\eta}p\\\\), and we get the scaled equations       \\n\\\\begin{eqnarray*}\\n  -\\\\nabla \\\\cdot (2 \\\\eta \\\\varepsilon ({\\\\mathbf u})) +\\n  \\\\nabla \\\\left(\\\\frac{\\\\eta}{L} \\\\hat p\\\\right) &=&\\n  \\\\rho(T) \\\\; \\\\mathbf{g},\\n  \\\\\\\\\\n  \\\\frac{\\\\eta}{L} \\\\nabla \\\\cdot {\\\\mathbf u} &=& 0.\\n\\\\end{eqnarray*}\\n\\n This is now symmetric. Obviously, we can easily recover the original pressure \\\\(p\\\\) from the scaled pressure \\\\(\\\\hat p\\\\) that we compute as a result of this procedure.\\nIn the program below, we will introduce a factor EquationData::pressure_scaling that corresponds to \\\\(\\\\frac{\\\\eta}{L}\\\\), and we will use this factor in the assembly of the system matrix and preconditioner. Because it is annoying and error prone, we will recover the unscaled pressure immediately following the solution of the linear system, i.e., the solution vector's pressure component will immediately be unscaled to retrieve the physical pressure. Since the solver uses the fact that we can use a good initial guess by extrapolating the previous solutions, we also have to scale the pressure immediately before solving.\\nChanges to the Stokes preconditioner and solver \\nIn this tutorial program, we apply a variant of the preconditioner used in step-31. That preconditioner was built to operate on the system matrix \\\\(M\\\\) in block form such that the product matrix          \\n\\\\begin{eqnarray*}\\n  P^{-1} M\\n  =\\n  \\\\left(\\\\begin{array}{cc}\\n    A^{-1} & 0 \\\\\\\\ S^{-1} B A^{-1} & -S^{-1}\\n  \\\\end{array}\\\\right)\\n  \\\\left(\\\\begin{array}{cc}\\n    A & B^T \\\\\\\\ B & 0\\n  \\\\end{array}\\\\right)\\n\\\\end{eqnarray*}\\n\\n is of a form that Krylov-based iterative solvers like GMRES can solve in a few iterations. We then replaced the exact inverse of \\\\(A\\\\) by the action of an AMG preconditioner \\\\(\\\\tilde{A}\\\\) based on a vector Laplace matrix, approximated the Schur complement \\\\(S = B A^{-1} B^T\\\\) by a mass matrix \\\\(M_p\\\\) on the pressure space and wrote an InverseMatrix class for implementing the action of \\\\(M_p^{-1}\\\\approx S^{-1}\\\\) on vectors. In the InverseMatrix class, we used a CG solve with an incomplete Cholesky (IC) preconditioner for performing the inner solves.\\nAn observation one can make is that we use just the action of a preconditioner for approximating the velocity inverse \\\\(A^{-1}\\\\) (and the outer GMRES iteration takes care of the approximate character of the inverse), whereas we use a more or less exact inverse for \\\\(M_p^{-1}\\\\), realized by a fully converged CG solve. This appears unbalanced, but there's system to this madness: almost all the effort goes into the upper left block to which we apply the AMG preconditioner, whereas even an exact inversion of the pressure mass matrix costs basically nothing. Consequently, if it helps us reduce the overall number of iterations somewhat, then this effort is well spent.\\nThat said, even though the solver worked well for step-31, we have a problem here that is a bit more complicated (cells are deformed, the pressure varies by orders of magnitude, and we want to plan ahead for more complicated physics), and so we'll change a few things slightly:\\n\\nFor more complex problems, it turns out that using just a single AMG V-cycle as preconditioner is not always sufficient. The outer solver converges just fine most of the time in a reasonable number of iterations (say, less than 50) but there are the occasional time step where it suddenly takes 700 or so. What exactly is going on there is hard to determine, but the problem can be avoided by using a more accurate solver for the top left block. Consequently, we'll want to use a CG iteration to invert the top left block of the preconditioner matrix, and use the AMG as a preconditioner for the CG solver.\\nThe downside of this is that, of course, the Stokes preconditioner becomes much more expensive (approximately 10 times more expensive than when we just use a single V-cycle). Our strategy then is this: let's do up to 30 GMRES iterations with just the V-cycle as a preconditioner and if that doesn't yield convergence, then take the best approximation of the Stokes solution obtained after this first round of iterations and use that as the starting guess for iterations where we use the full inner solver with a rather lenient tolerance as preconditioner. In all our experiments this leads to convergence in only a few additional iterations.\\nOne thing we need to pay attention to is that when using a CG with a lenient tolerance in the preconditioner, then \\\\(y = \\\\tilde A^{-1} r\\\\) is no longer a linear function of \\\\(r\\\\) (it is, of course, if we have a very stringent tolerance in our solver, or if we only apply a single V-cycle). This is a problem since now our preconditioner is no longer a linear operator; in other words, every time GMRES uses it the preconditioner looks different. The standard GMRES solver can't deal with this, leading to slow convergence or even breakdown, but the F-GMRES variant is designed to deal with exactly this kind of situation and we consequently use it.\\nOn the other hand, once we have settled on using F-GMRES we can relax the tolerance used in inverting the preconditioner for \\\\(S\\\\). In step-31, we ran a preconditioned CG method on \\\\(\\\\tilde S\\\\) until the residual had been reduced by 7 orders of magnitude. Here, we can again be more lenient because we know that the outer preconditioner doesn't suffer.\\nIn step-31, we used a left preconditioner in which we first invert the top left block of the preconditioner matrix, then apply the bottom left (divergence) one, and then invert the bottom right. In other words, the application of the preconditioner acts as a lower left block triangular matrix. Another option is to use a right preconditioner that here would be upper right block triangulation, i.e., we first invert the bottom right Schur complement, apply the top right (gradient) operator and then invert the elliptic top left block. To a degree, which one to choose is a matter of taste. That said, there is one significant advantage to a right preconditioner in GMRES-type solvers: the residual with which we determine whether we should stop the iteration is the true residual, not the norm of the preconditioned equations. Consequently, it is much simpler to compare it to the stopping criterion we typically use, namely the norm of the right hand side vector. In writing this code we found that the scaling issues we discussed above also made it difficult to determine suitable stopping criteria for left-preconditioned linear systems, and consequently this program uses a right preconditioner.\\nIn step-31, we used an IC (incomplete Cholesky) preconditioner for the pressure mass matrix in the Schur complement preconditioner and for the solution of the temperature system. Here, we could in principle do the same, but we do choose an even simpler preconditioner, namely a Jacobi preconditioner for both systems. This is because here we target at massively parallel computations, where the decompositions for IC/ILU would have to be performed block-wise for the locally owned degrees of freedom on each processor. This means, that the preconditioner gets more like a Jacobi preconditioner anyway, so we rather start from that variant straight away. Note that we only use the Jacobi preconditioners for CG solvers with mass matrices, where they give optimal (h-independent) convergence anyway, even though they usually require about twice as many iterations as an IC preconditioner.\\n\\nAs a final note, let us remark that in step-31 we computed the Schur complement \\\\(S=B A^{-1} B^T\\\\) by approximating \\\\(-\\\\text{div}(-\\\\eta\\\\Delta)^{-1}\\\\nabla \\\\approx \\\\frac 1{\\\\eta} \\\\mathbf{1}\\\\). Now, however, we have re-scaled the \\\\(B\\\\) and \\\\(B^T\\\\) operators. So \\\\(S\\\\) should now approximate  \\\\(-\\\\frac{\\\\eta}{L}\\\\text{div}(-\\\\eta\\\\Delta)^{-1}\\\\nabla \\\\frac{\\\\eta}{L} \\\\approx\\n\\\\left(\\\\frac{\\\\eta}{L}\\\\right)^2 \\\\frac 1{\\\\eta} \\\\mathbf{1}\\\\). We use the discrete form of the right hand side of this as our approximation \\\\(\\\\tilde S\\\\) to \\\\(S\\\\).\\nChanges to the artificial viscosity stabilization \\nSimilarly to step-31, we will use an artificial viscosity for stabilization based on a residual of the equation. As a difference to step-31, we will provide two slightly different definitions of the stabilization parameter. For \\\\(\\\\alpha=1\\\\), we use the same definition as in step-31:             \\n\\\\begin{eqnarray*}\\n  \\\\nu_\\\\alpha(T)|_K\\n  =\\n  \\\\nu_1(T)|_K\\n  =\\n  \\\\beta\\n  \\\\|\\\\mathbf{u}\\\\|_{L^\\\\infty(K)}\\n  h_K\\n  \\\\min\\\\left\\\\{\\n    1,\\n    \\\\frac{\\\\|R_1(T)\\\\|_{L^\\\\infty(K)}}{c(\\\\mathbf{u},T)}\\n  \\\\right\\\\}\\n\\\\end{eqnarray*}\\n\\n where we compute the viscosity from a residual \\\\(\\\\|R_1(T)\\\\|_{L^\\\\infty(K)}\\\\) of the equation, limited by a diffusion proportional to the mesh size \\\\(h_K\\\\) in regions where the residual is large (around steep gradients). This definition has been shown to work well for the given case, \\\\(\\\\alpha = 1\\\\) in step-31, but it is usually less effective as the diffusion for \\\\(\\\\alpha=2\\\\). For that case, we choose a slightly more readable definition of the viscosity,   \\n\\\\begin{eqnarray*}\\n  \\\\nu_2(T)|_K = \\\\min (\\\\nu_h^\\\\mathrm{max}|_K,\\\\nu_h^\\\\mathrm{E}|_K)\\n\\\\end{eqnarray*}\\n\\n where the first term gives again the maximum dissipation (similarly to a first order upwind scheme),   \\n\\\\begin{eqnarray*}\\n  \\\\nu^\\\\mathrm{max}_h|_K = \\\\beta h_K \\\\|\\\\mathbf {u}\\\\|_{L^\\\\infty(K)}\\n\\\\end{eqnarray*}\\n\\n and the entropy viscosity is defined as    \\n\\\\begin{eqnarray*}\\n  \\\\nu^\\\\mathrm{E}_h|_K = c_R \\\\frac{h_K^2 \\\\|R_\\\\mathrm{2,E}(T)\\\\|_{L^\\\\infty(K)}}\\n  {\\\\|E(T) - \\\\bar{E}(T)\\\\|_{L^\\\\infty(\\\\Omega)} }.\\n\\\\end{eqnarray*}\\n\\nThis formula is described in the article J.-L. Guermond, R. Pasquetti, & B. Popov, 2011. Entropy viscosity method for nonlinear conservation laws, J. Comput. Phys., 230, 4248\\u20134267. Compared to the case \\\\(\\\\alpha = 1\\\\), the residual is computed from the temperature entropy, \\\\(E(T) = \\\\frac12 (T-T_m)^2\\\\) with \\\\(T_m\\\\) an average temperature (we choose the mean between the maximum and minimum temperature in the computation), which gives the following formula    \\n\\\\begin{eqnarray*}\\n R_\\\\mathrm{E}(T) = \\\\frac{\\\\partial E(T)}{\\\\partial t} +\\n    (T-T_\\\\mathrm{m}) \\\\left(\\\\mathbf{u} \\\\cdot \\\\nabla T -  \\\\kappa \\\\nabla^2 T - \\\\gamma\\\\right).\\n\\\\end{eqnarray*}\\n\\n The denominator in the formula for \\\\(\\\\nu^\\\\mathrm{E}_h|_K\\\\) is computed as the global deviation of the entropy from the space-averaged entropy  \\\\(\\\\bar{E}(T) =\\n\\\\int_\\\\Omega E(T) d\\\\mathbf{x}/\\\\int_\\\\Omega d\\\\mathbf{x}\\\\). As in step-31, we evaluate the artificial viscosity from the temperature and velocity at two previous time levels, in order to avoid a nonlinearity in its definition.\\nThe above definitions of the viscosity are simple, but depend on two parameters, namely \\\\(\\\\beta\\\\) and \\\\(c_R\\\\). For the current program, we want to go about this issue a bit more systematically for both parameters in the case \\\\(\\\\alpha =1\\\\), using the same line of reasoning with which we chose two other parameters in our discretization, \\\\(c_k\\\\) and \\\\(\\\\beta\\\\), in the results section of step-31. In particular, remember that we would like to make the artificial viscosity as small as possible while keeping it as large as necessary. In the following, let us describe the general strategy one may follow. The computations shown here were done with an earlier version of the program and so the actual numerical values you get when running the program may no longer match those shown here; that said, the general approach remains valid and has been used to find the values of the parameters actually used in the program.\\nTo see what is happening, note that below we will impose boundary conditions for the temperature between 973 and 4273 Kelvin, and initial conditions are also chosen in this range; for these considerations, we run the program without internal heat sources or sinks, and consequently the temperature should always be in this range, barring any internal oscillations. If the minimal temperature drops below 973 Kelvin, then we need to add stabilization by either increasing \\\\(\\\\beta\\\\) or decreasing \\\\(c_R\\\\).\\nAs we did in step-31, we first determine an optimal value of \\\\(\\\\beta\\\\) by using the \\\"traditional\\\" formula       \\n\\\\begin{eqnarray*}\\n  \\\\nu_\\\\alpha(T)|_K\\n  =\\n  \\\\beta\\n  \\\\|\\\\mathbf{u}\\\\|_{L^\\\\infty(K)}\\n    h_K,\\n\\\\end{eqnarray*}\\n\\n which we know to be stable if only \\\\(\\\\beta\\\\) is large enough. Doing a couple hundred time steps (on a coarser mesh than the one shown in the program, and with a different viscosity that affects transport velocities and therefore time step sizes) in 2d will produce the following graph:\\n\\nAs can be seen, values \\\\(\\\\beta \\\\le 0.05\\\\) are too small whereas \\\\(\\\\beta=0.052\\\\) appears to work, at least to the time horizon shown here. As a remark on the side, there are at least two questions one may wonder here: First, what happens at the time when the solution becomes unstable? Looking at the graphical output, we can see that with the unreasonably coarse mesh chosen for these experiments, around time \\\\(t=10^{15}\\\\) seconds the plumes of hot material that have been rising towards the cold outer boundary and have then spread sideways are starting to get close to each other, squeezing out the cold material in-between. This creates a layer of cells into which fluids flows from two opposite sides and flows out toward a third, apparently a scenario that then produce these instabilities without sufficient stabilization. Second: In step-31, we used \\\\(\\\\beta=0.015\\\\cdot\\\\text{dim}\\\\); why does this not work here? The answer to this is not entirely clear \\u2013 stabilization parameters are certainly known to depend on things like the shape of cells, for which we had squares in step-31 but have trapezoids in the current program. Whatever the exact cause, we at least have a value of \\\\(\\\\beta\\\\), namely 0.052 for 2d, that works for the current program. A similar set of experiments can be made in 3d where we find that \\\\(\\\\beta=0.078\\\\) is a good choice \\u2014 neatly leading to the formula \\\\(\\\\beta=0.026 \\\\cdot \\\\textrm{dim}\\\\).\\nWith this value fixed, we can go back to the original formula for the viscosity \\\\(\\\\nu\\\\) and play with the constant \\\\(c_R\\\\), making it as large as possible in order to make \\\\(\\\\nu\\\\) as small as possible. This gives us a picture like this:\\n\\nConsequently, \\\\(c_R=0.1\\\\) would appear to be the right value here. While this graph has been obtained for an exponent \\\\(\\\\alpha=1\\\\), in the program we use \\\\(\\\\alpha=2\\\\) instead, and in that case one has to re-tune the parameter (and observe that \\\\(c_R\\\\) appears in the numerator and not in the denominator). It turns out that \\\\(c_R=1\\\\) works with \\\\(\\\\alpha=2\\\\).\\nLocally conservative Stokes discretization \\nThe standard Taylor-Hood discretization for Stokes, using the  \\\\(Q_{k+1}^d\\n\\\\times Q_k\\\\) element, is globally conservative, i.e.  \\\\(\\\\int_{\\\\partial\\\\Omega}\\n\\\\mathbf n \\\\cdot \\\\mathbf u_h = 0\\\\). This can easily be seen: the weak form of the divergence equation reads  \\\\((q_h, \\\\textrm{div}\\\\; \\\\mathbf u_h)=0, \\\\forall\\nq_h\\\\in Q_h\\\\). Because the pressure space does contain the function \\\\(q_h=1\\\\), we get     \\n\\\\begin{align*}\\n  0 = (1, \\\\textrm{div}\\\\; \\\\mathbf u_h)_\\\\Omega\\n  = \\\\int_\\\\Omega \\\\textrm{div}\\\\; \\\\mathbf u_h\\n  = \\\\int_{\\\\partial\\\\Omega} \\\\mathbf n \\\\cdot \\\\mathbf u_h\\n\\\\end{align*}\\n\\n by the divergence theorem. This property is important: if we want to use the velocity field \\\\(u_h\\\\) to transport along other quantities (such as the temperature in the current equations, but it could also be concentrations of chemical substances or entirely artificial tracer quantities) then the conservation property guarantees that the amount of the quantity advected remains constant.\\nThat said, there are applications where this global property is not enough. Rather, we would like that it holds locally, on every cell. This can be achieved by using the space \\\\(Q_{k+1}^d \\\\times DGP_k\\\\) for discretization, where we have replaced the continuous space of tensor product polynomials of degree \\\\(k\\\\) for the pressure by the discontinuous space of the complete polynomials of the same degree. (Note that tensor product polynomials in 2d contain the functions \\\\(1, x, y, xy\\\\), whereas the complete polynomials only have the functions \\\\(1,x,y\\\\).) This space turns out to be stable for the Stokes equation.\\nBecause the space is discontinuous, we can now in particular choose the test function \\\\(q_h(\\\\mathbf x)=\\\\chi_K(\\\\mathbf x)\\\\), i.e. the characteristic function of cell \\\\(K\\\\). We then get in a similar fashion as above       \\n\\\\begin{align*}\\n  0\\n  = (q_h, \\\\textrm{div}\\\\; \\\\mathbf u_h)_\\\\Omega\\n  = (1, \\\\textrm{div}\\\\; \\\\mathbf u_h)_K\\n  = \\\\int_K \\\\textrm{div}\\\\; \\\\mathbf u_h\\n  = \\\\int_{\\\\partial K} \\\\mathbf n \\\\cdot \\\\mathbf u_h,\\n\\\\end{align*}\\n\\n showing the conservation property for cell \\\\(K\\\\). This clearly holds for each cell individually.\\nThere are good reasons to use this discretization. As mentioned above, this element guarantees conservation of advected quantities on each cell individually. A second advantage is that the pressure mass matrix we use as a preconditioner in place of the Schur complement becomes block diagonal and consequently very easy to invert. However, there are also downsides. For one, there are now more pressure variables, increasing the overall size of the problem, although this doesn't seem to cause much harm in practice. More importantly, though, the fact that now the divergence integrated over each cell is zero when it wasn't before does not guarantee that the divergence is pointwise smaller. In fact, as one can easily verify, the \\\\(L_2\\\\) norm of the divergence is larger for this than for the standard Taylor-Hood discretization. (However, both converge at the same rate to zero, since it is easy to see that    \\\\(\\\\|\\\\textrm{div}\\\\; u_h\\\\|=\\n\\\\|\\\\textrm{div}\\\\; (u-u_h)\\\\|=\\n\\\\|\\\\textrm{trace}\\\\; \\\\nabla (u-u_h)\\\\|\\\\le\\n\\\\|\\\\nabla (u-u_h)\\\\|={\\\\cal O}(h^{k+2})\\\\).) It is therefore not a priori clear that the error is indeed smaller just because we now have more degrees of freedom.\\nGiven these considerations, it remains unclear which discretization one should prefer. Consequently, we leave that up to the user and make it a parameter in the input file which one to use.\\nHigher order mappings for curved boundaries \\nIn the program, we will use a spherical shell as domain. This means that the inner and outer boundary of the domain are no longer \\\"straight\\\" (by which we usually mean that they are bilinear surfaces that can be represented by the FlatManifold class). Rather, they are curved and it seems prudent to use a curved approximation in the program if we are already using higher order finite elements for the velocity. Consequently, we will introduce a member variable of type MappingQ that denotes such a mapping (step-10 and step-11 introduce such mappings for the first time) and that we will use in all computations.\\nParallelization on clusters \\nRunning convection codes in 3d with significant Rayleigh numbers requires a lot of computations \\u2014 in the case of whole earth simulations on the order of one or several hundred million unknowns. This can obviously not be done with a single machine any more (at least not in 2010 when we started writing this code). Consequently, we need to parallelize it. Parallelization of scientific codes across multiple machines in a cluster of computers is almost always done using the Message Passing Interface (MPI). This program is no exception to that, and it follows the general spirit of the step-17 and step-18 programs in this though in practice it borrows more from step-40 in which we first introduced the classes and strategies we use when we want to completely distribute all computations, and step-55 that shows how to do that for vector-valued problems: including, for example, splitting the mesh up into a number of parts so that each processor only stores its own share plus some ghost cells, and using strategies where no processor potentially has enough memory to hold the entries of the combined solution vector locally. The goal is to run this code on hundreds or maybe even thousands of processors, at reasonable scalability.\\nNoteEven though it has a larger number, step-40 comes logically before the current program. The same is true for step-55. You will probably want to look at these programs before you try to understand what we do here.\\nMPI is a rather awkward interface to program with. It is a semi-object oriented set of functions, and while one uses it to send data around a network, one needs to explicitly describe the data types because the MPI functions insist on getting the address of the data as void* objects rather than deducing the data type automatically through overloading or templates. We've already seen in step-17 and step-18 how to avoid almost all of MPI by putting all the communication necessary into either the deal.II library or, in those programs, into PETSc. We'll do something similar here: like in step-40 and step-55, deal.II and the underlying p4est library are responsible for all the communication necessary for distributing the mesh, and we will let the Trilinos library (along with the wrappers in namespace TrilinosWrappers) deal with parallelizing the linear algebra components. We have already used Trilinos in step-31, and will do so again here, with the difference that we will use its parallel capabilities.\\nTrilinos consists of a significant number of packages, implementing basic parallel linear algebra operations (the Epetra package), different solver and preconditioner packages, and on to things that are of less importance to deal.II (e.g., optimization, uncertainty quantification, etc). deal.II's Trilinos interfaces encapsulate many of the things Trilinos offers that are of relevance to PDE solvers, and provides wrapper classes (in namespace TrilinosWrappers) that make the Trilinos matrix, vector, solver and preconditioner classes look very much the same as deal.II's own implementations of this functionality. However, as opposed to deal.II's classes, they can be used in parallel if we give them the necessary information. As a consequence, there are two Trilinos classes that we have to deal with directly (rather than through wrappers), both of which are part of Trilinos' Epetra library of basic linear algebra and tool classes: \\n\\nThe Epetra_Comm class is an abstraction of an MPI \\\"communicator\\\", i.e., it describes how many and which machines can communicate with each other. Each distributed object, such as a sparse matrix or a vector for which we may want to store parts on different machines, needs to have a communicator object to know how many parts there are, where they can be found, and how they can be accessed.\\nIn this program, we only really use one communicator object \\u2013 based on the MPI variable MPI_COMM_WORLD \\u2013 that encompasses all processes that work together. It would be perfectly legitimate to start a process on \\\\(N\\\\) machines but only store vectors on a subset of these by producing a communicator object that only encompasses this subset of machines; there is really no compelling reason to do so here, however.\\n\\n\\n\\nThe IndexSet class is used to describe which elements of a vector or which rows of a matrix should reside on the current machine that is part of a communicator. To create such an object, you need to know (i) the total number of elements or rows, (ii) the indices of the elements you want to store locally. We will set up these partitioners in the BoussinesqFlowProblem::setup_dofs function below and then hand it to every parallel object we create.\\nUnlike PETSc, Trilinos makes no assumption that the elements of a vector need to be partitioned into contiguous chunks. At least in principle, we could store all elements with even indices on one processor and all odd ones on another. That's not very efficient, of course, but it's possible. Furthermore, the elements of these partitionings do not necessarily be mutually exclusive. This is important because when postprocessing solutions, we need access to all locally relevant or at least the locally active degrees of freedom (see the topic on Parallel computing with multiple processors using distributed memory for a definition, as well as the discussion in step-40). Which elements the Trilinos vector considers as locally owned is not important to us then. All we care about is that it stores those elements locally that we need. \\n\\n\\nThere are a number of other concepts relevant to distributing the mesh to a number of processors; you may want to take a look at the Parallel computing with multiple processors using distributed memory topic and step-40 or step-55 before trying to understand this program. The rest of the program is almost completely agnostic about the fact that we don't store all objects completely locally. There will be a few points where we have to limit loops over all cells to those that are locally owned, or where we need to distinguish between vectors that store only locally owned elements and those that store everything that is locally relevant (see this glossary entry), but by and large the amount of heavy lifting necessary to make the program run in parallel is well hidden in the libraries upon which this program builds. In any case, we will comment on these locations as we get to them in the program code.\\nParallelization within individual nodes of a cluster \\nThe second strategy to parallelize a program is to make use of the fact that most computers today have more than one processor that all have access to the same memory. In other words, in this model, we don't explicitly have to say which pieces of data reside where \\u2013 all of the data we need is directly accessible and all we have to do is split processing this data between the available processors. We will then couple this with the MPI parallelization outlined above, i.e., we will have all the processors on a machine work together to, for example, assemble the local contributions to the global matrix for the cells that this machine actually \\\"owns\\\" but not for those cells that are owned by other machines. We will use this strategy for four kinds of operations we frequently do in this program: assembly of the Stokes and temperature matrices, assembly of the matrix that forms the Stokes preconditioner, and assembly of the right hand side of the temperature system.\\nAll of these operations essentially look as follows: we need to loop over all cells for which cell->subdomain_id() equals the index our machine has within the communicator object used for all communication (i.e., MPI_COMM_WORLD, as explained above). The test we are actually going to use for this, and which describes in a concise way why we test this condition, is cell->is_locally_owned(). On each such cell we need to assemble the local contributions to the global matrix or vector, and then we have to copy each cell's contribution into the global matrix or vector. Note that the first part of this (the loop) defines a range of iterators on which something has to happen. The second part, assembly of local contributions is something that takes the majority of CPU time in this sequence of steps, and is a typical example of things that can be done in parallel: each cell's contribution is entirely independent of all other cells' contributions. The third part, copying into the global matrix, must not happen in parallel since we are modifying one object and so several threads can not at the same time read an existing matrix element, add their contribution, and write the sum back into memory without danger of producing a race condition.\\ndeal.II has a class that is made for exactly this workflow: WorkStream, first discussed in step-9 and step-13. Its use is also extensively documented in the topic on Parallel computing with multiple processors accessing shared memory (in the section on the WorkStream class) and we won't repeat here the rationale and detailed instructions laid out there, though you will want to read through this topic to understand the distinction between scratch space and per-cell data. Suffice it to mention that we need the following:\\n\\nAn iterator range for those cells on which we are supposed to work. This is provided by the FilteredIterator class which acts just like every other cell iterator in deal.II with the exception that it skips all cells that do not satisfy a particular predicate (i.e., a criterion that evaluates to true or false). In our case, the predicate is whether a cell is locally owned.\\nA function that does the work on each cell for each of the tasks identified above, i.e., functions that assemble the local contributions to Stokes matrix and preconditioner, temperature matrix, and temperature right hand side. These are the BoussinesqFlowProblem::local_assemble_stokes_system, BoussinesqFlowProblem::local_assemble_stokes_preconditioner, BoussinesqFlowProblem::local_assemble_temperature_matrix, and BoussinesqFlowProblem::local_assemble_temperature_rhs functions in the code below. These four functions can all have several instances running in parallel at the same time.\\nFunctions that copy the result of the previous ones into the global object and that run sequentially to avoid race conditions. These are the BoussinesqFlowProblem::copy_local_to_global_stokes_system, BoussinesqFlowProblem::copy_local_to_global_stokes_preconditioner, BoussinesqFlowProblem::copy_local_to_global_temperature_matrix, and BoussinesqFlowProblem::copy_local_to_global_temperature_rhs functions.\\n\\nWe will comment on a few more points in the actual code, but in general their structure should be clear from the discussion in Parallel computing with multiple processors accessing shared memory.\\nThe underlying technology for WorkStream identifies \\\"tasks\\\" that need to be worked on (e.g. assembling local contributions on a cell) and schedules these tasks automatically to available processors. WorkStream creates these tasks automatically, by splitting the iterator range into suitable chunks.\\nNoteUsing multiple threads within each MPI process only makes sense if you have fewer MPI processes running on each node of your cluster than there are processor cores on this machine. Otherwise, MPI will already keep your processors busy and you won't get any additional speedup from using threads. For example, if your cluster nodes have 8 cores as they often have at the time of writing this, and if your batch scheduler puts 8 MPI processes on each node, then using threads doesn't make the program any faster. Consequently, you probably want to either configure your deal.II without threads, or set the number of threads in Utilities::MPI::MPI_InitFinalize to 1 (third argument), or \\\"export DEAL_II_NUM_THREADS=1\\\" before running. That said, at the time of writing this, we only use the WorkStream class for assembling (parts of) linear systems, while 75% or more of the run time of the program is spent in the linear solvers that are not parallelized \\u2014 in other words, the best we could hope is to parallelize the remaining 25%.\\nThe testcase \\nThe setup for this program is mildly reminiscent of the problem we wanted to solve in the first place (see the introduction of step-31): convection in the earth mantle. As a consequence, we choose the following data, all of which appears in the program in units of meters and seconds (the SI system) even if we list them here in other units. We do note, however, that these choices are essentially still only exemplary, and not meant to result in a completely realistic description of convection in the earth mantle: for that, more and more difficult physics would have to be implemented, and several other aspects are currently missing from this program as well. We will come back to this issue in the results section again, but state for now that providing a realistic description is a goal of the ASPECT code in development at the time of writing this.\\nAs a reminder, let us again state the equations we want to solve are these:             \\n\\\\begin{eqnarray*}\\n  -\\\\nabla \\\\cdot (2 \\\\eta \\\\varepsilon ({\\\\mathbf u})) +\\n  \\\\nabla \\\\left( \\\\frac{\\\\eta}{L} \\\\hat p\\\\right) &=&\\n  \\\\rho(T) \\\\mathbf{g},\\n  \\\\\\\\\\n  \\\\frac{\\\\eta}{L} \\\\nabla \\\\cdot {\\\\mathbf u} &=& 0,\\n  \\\\\\\\\\n  \\\\frac{\\\\partial T}{\\\\partial t}\\n  +\\n  {\\\\mathbf u} \\\\cdot \\\\nabla T\\n  -\\n  \\\\nabla \\\\cdot \\\\kappa \\\\nabla T &=& \\\\gamma,\\n\\\\end{eqnarray*}\\n\\n augmented by boundary and initial conditions. We then have to choose data for the following quantities: \\n\\nThe domain is an annulus (in 2d) or a spherical shell (in 3d) with inner and outer radii that match that of the earth: the total radius of the earth is 6371km, with the mantle starting at a depth of around 35km (just under the solid earth crust composed of continental and oceanic plates) to a depth of 2890km (where the outer earth core starts). The radii are therefore  \\\\(R_0=(6371-2890)\\\\text{km},\\n  R_1=(6371-35)\\\\text{km}\\\\). This domain is conveniently generated using the GridGenerator::hyper_shell() function.\\n\\n\\n\\nAt the interface between crust and mantle, the temperature is between 500 and 900 degrees Celsius, whereas at its bottom it is around 4000 degrees Celsius (see, for example, this Wikipedia entry). In Kelvin, we therefore choose \\\\(T_0=(4000+273)\\\\text{K}\\\\), \\\\(T_1=(500+273)\\\\text{K}\\\\) as boundary conditions at the inner and outer edge.\\nIn addition to this, we also have to specify some initial conditions for the temperature field. The real temperature field of the earth is quite complicated as a consequence of the convection that has been going on for more than four billion years \\u2013 in fact, it is the properties of this temperature distribution that we want to explore with programs like this. As a consequence, we don't really have anything useful to offer here, but we can hope that if we start with something and let things run for a while that the exact initial conditions don't matter that much any more \\u2014 as is in fact suggested by looking at the pictures shown in the results section below. The initial temperature field we use here is given in terms of the radius by      \\n\\\\begin{align*}\\n    s &= \\\\frac{\\\\|\\\\mathbf x\\\\|-R_0}{R_1-R_0}, \\\\\\\\\\n    \\\\varphi &= \\\\arctan \\\\frac{y}{x}, \\\\\\\\\\n    \\\\tau &= s + \\\\frac 15 s(1-s) \\\\sin(6\\\\varphi) q(z), \\\\\\\\\\n    T(\\\\mathbf x) &= T_0(1-\\\\tau) + T_1\\\\tau,\\n  \\\\end{align*}\\n\\n where        \\n\\\\begin{align*}\\n    q(z) = \\\\left\\\\{\\n    \\\\begin{array}{ll}\\n      1 & \\\\text{in 2d} \\\\\\\\\\n      \\\\max\\\\{0, \\\\cos(\\\\pi |z/R_1|)\\\\} & \\\\text{in 3d}\\n    \\\\end{array}\\n    \\\\right. .\\n  \\\\end{align*}\\n\\n This complicated function is essentially a perturbation of a linear profile between the inner and outer temperatures. In 2d, the function \\\\(\\\\tau=\\\\tau(\\\\mathbf x)\\\\) looks like this (I got the picture from this page):\\n\\nThe point of this profile is that if we had used \\\\(s\\\\) instead of \\\\(\\\\tau\\\\) in the definition of \\\\(T(\\\\mathbf x)\\\\) then it would simply be a linear interpolation. \\\\(\\\\tau\\\\) has the same function values as \\\\(s\\\\) on the inner and outer boundaries (zero and one, respectively), but it stretches the temperature profile a bit depending on the angle and the \\\\(z\\\\) value in 3d, producing an angle-dependent perturbation of the linearly interpolating field. We will see in the results section that this is an entirely unphysical temperature field (though it will make for interesting images) as the equilibrium state for the temperature will be an almost constant temperature with boundary layers at the inner and outer boundary.\\n\\n\\n\\nThe right hand side of the temperature equation contains the rate of internal heating \\\\(\\\\gamma\\\\). The earth does heat naturally through several mechanisms: radioactive decay, chemical separation (heavier elements sink to the bottom, lighter ones rise to the top; the countercurrents dissipate energy equal to the loss of potential energy by this separation process); heat release by crystallization of liquid metal as the solid inner core of the earth grows; and heat dissipation from viscous friction as the fluid moves.\\nChemical separation is difficult to model since it requires modeling mantle material as multiple phases; it is also a relatively small effect. Crystallization heat is even more difficult since it is confined to areas where temperature and pressure allow for phase changes, i.e., a discontinuous process. Given the difficulties in modeling these two phenomena, we will neglect them.\\nThe other two are readily handled and, given the way we scaled the temperature equation, lead to the equation      \\n\\\\[\\n    \\\\gamma(\\\\mathbf x)\\n     =\\n     \\\\frac{\\\\rho q+2\\\\eta \\\\varepsilon(\\\\mathbf u):\\\\varepsilon(\\\\mathbf u)}\\n     {\\\\rho c_p},\\n  \\\\]\\n\\n where \\\\(q\\\\) is the radiogenic heating in \\\\(\\\\frac{W}{kg}\\\\), and the second term in the enumerator is viscous friction heating. \\\\(\\\\rho\\\\) is the density and \\\\(c_p\\\\) is the specific heat. The literature provides the following approximate values: \\\\(c_p=1250 \\\\frac{J}{kg\\\\; K}, q=7.4\\\\cdot 10^{-12}\\\\frac{W}{kg}\\\\). The other parameters are discussed elsewhere in this section.\\nWe neglect one internal heat source, namely adiabatic heating here, which will lead to a surprising temperature field. This point is commented on in detail in the results section below.\\n\\n\\n\\nFor the velocity we choose as boundary conditions \\\\(\\\\mathbf{v}=0\\\\) at the inner radius (i.e., the fluid sticks to the earth core) and \\\\(\\\\mathbf{n}\\\\cdot\\\\mathbf{v}=0\\\\) at the outer radius (i.e., the fluid flows tangentially along the bottom of the earth crust). Neither of these is physically overly correct: certainly, on both boundaries, fluids can flow tangentially, but they will incur a shear stress through friction against the medium at the other side of the interface (the metallic core and the crust, respectively). Such a situation could be modeled by a Robin-type boundary condition for the tangential velocity; in either case, the normal (vertical) velocity would be zero, although even that is not entirely correct since continental plates also have vertical motion (see, for example, the phenomenon of post-glacial rebound). But to already make things worse for the tangential velocity, the medium on the other side is in motion as well, so the shear stress would, in the simplest case, be proportional to the velocity difference, leading to a boundary condition of the form       \\n\\\\begin{align*}\\n    \\\\mathbf{n}\\\\cdot [2\\\\eta \\\\varepsilon(\\\\mathbf v)]\\n    &=\\n    s \\\\mathbf{n} \\\\times [\\\\mathbf v - \\\\mathbf v_0],\\n    \\\\\\\\\\n    \\\\mathbf{n} \\\\cdot \\\\mathbf v &= 0,\\n  \\\\end{align*}\\n\\n with a proportionality constant \\\\(s\\\\). Rather than going down this route, however, we go with the choice of zero (stick) and tangential flow boundary conditions.\\nAs a side note of interest, we may also have chosen tangential flow conditions on both inner and outer boundary. That has a significant drawback, however: it leaves the velocity not uniquely defined. The reason is that all velocity fields \\\\(\\\\hat{\\\\mathbf v}\\\\) that correspond to a solid body rotation around the center of the domain satisfy  \\\\(\\\\mathrm{div}\\\\;\\n  \\\\varepsilon(\\\\hat{\\\\mathbf v})=0, \\\\mathrm{div} \\\\;\\\\hat{\\\\mathbf v} = 0\\\\), and \\\\(\\\\mathbf{n} \\\\cdot \\\\hat{\\\\mathbf v} = 0\\\\). As a consequence, if \\\\(\\\\mathbf v\\\\) satisfies equations and boundary conditions, then so does  \\\\(\\\\mathbf v +\\n  \\\\hat{\\\\mathbf v}\\\\). That's certainly not a good situation that we would like to avoid. The traditional way to work around this is to pick an arbitrary point on the boundary and call this your fixed point by choosing the velocity to be zero in all components there. (In 3d one has to choose two points.) Since this program isn't meant to be too realistic to begin with, we avoid this complication by simply fixing the velocity along the entire interior boundary.\\n\\n\\n\\nTo first order, the gravity vector always points downward. The question for a body as big as the earth is just: where is \\\"up\\\". The naive answer of course is \\\"radially inward, towards the center of the earth\\\". So at the surface of the earth, we have     \\n\\\\[\\n    \\\\mathbf g\\n    =\\n    -9.81 \\\\frac{\\\\text{m}}{\\\\text{s}^2} \\\\frac{\\\\mathbf x}{\\\\|\\\\mathbf x\\\\|},\\n  \\\\]\\n\\n where \\\\(9.81 \\\\frac{\\\\text{m}}{\\\\text{s}^2}\\\\) happens to be the average gravity acceleration at the earth surface. But in the earth interior, the question becomes a bit more complicated: at the (bary-)center of the earth, for example, you have matter pulling equally hard in all directions, and so \\\\(\\\\mathbf g=0\\\\). In between, the net force is described as follows: let us define the gravity potential by       \\n\\\\[\\n    \\\\varphi(\\\\mathbf x)\\n    =\\n    \\\\int_{\\\\text{earth}}\\n    -G \\\\frac{\\\\rho(\\\\mathbf y)}{\\\\|\\\\mathbf x-\\\\mathbf y\\\\|}\\n    \\\\ \\\\text{d}y,\\n  \\\\]\\n\\n then \\\\(\\\\mathbf g(\\\\mathbf x) = -\\\\nabla \\\\varphi(\\\\mathbf x)\\\\). If we assume that the density \\\\(\\\\rho\\\\) is constant throughout the earth, we can produce an analytical expression for the gravity vector (don't try to integrate above equation somehow \\u2013 it leads to elliptic integrals; a simpler way is to notice that  \\\\(-\\\\Delta\\\\varphi(\\\\mathbf x) = -4\\\\pi G \\\\rho\\n  \\\\chi_{\\\\text{earth}}(\\\\mathbf x)\\\\) and solving this partial differential equation in all of \\\\({\\\\mathbb R}^3\\\\) exploiting the radial symmetry):            \\n\\\\[\\n    \\\\mathbf g(\\\\mathbf x) =\\n    \\\\left\\\\{\\n      \\\\begin{array}{ll}\\n        -\\\\frac{4}{3}\\\\pi G \\\\rho \\\\|\\\\mathbf x\\\\| \\\\frac{\\\\mathbf x}{\\\\|\\\\mathbf x\\\\|}\\n        & \\\\text{for} \\\\ \\\\|\\\\mathbf x\\\\|<R_1, \\\\\\\\\\n        -\\\\frac{4}{3}\\\\pi G \\\\rho R^3 \\\\frac{1}{\\\\|\\\\mathbf x\\\\|^2}\\n        \\\\frac{\\\\mathbf x}{\\\\|\\\\mathbf x\\\\|}\\n        & \\\\text{for} \\\\ \\\\|\\\\mathbf x\\\\|\\\\ge R_1.\\n      \\\\end{array}\\n    \\\\right.\\n  \\\\]\\n\\n The factor \\\\(-\\\\frac{\\\\mathbf x}{\\\\|\\\\mathbf x\\\\|}\\\\) is the unit vector pointing radially inward. Of course, within this problem, we are only interested in the branch that pertains to within the earth, i.e.,  \\\\(\\\\|\\\\mathbf\\n  x\\\\|<R_1\\\\). We would therefore only consider the expression        \\n\\\\[\\n    \\\\mathbf g(\\\\mathbf x) =\\n        -\\\\frac{4}{3}\\\\pi G \\\\rho \\\\|\\\\mathbf x\\\\| \\\\frac{\\\\mathbf x}{\\\\|\\\\mathbf x\\\\|}\\n        =\\n        -\\\\frac{4}{3}\\\\pi G \\\\rho \\\\mathbf x\\n        =\\n        - 9.81 \\\\frac{\\\\mathbf x}{R_1} \\\\frac{\\\\text{m}}{\\\\text{s}^2},\\n  \\\\]\\n\\n where we can infer the last expression because we know Earth's gravity at the surface (where \\\\(\\\\|x\\\\|=R_1\\\\)).\\nOne can derive a more general expression by integrating the differential equation for \\\\(\\\\varphi(r)\\\\) in the case that the density distribution is radially symmetric, i.e.,  \\\\(\\\\rho(\\\\mathbf\\n  x)=\\\\rho(\\\\|\\\\mathbf x\\\\|)=\\\\rho(r)\\\\). In that case, one would get    \\n\\\\[\\n    \\\\varphi(r)\\n    = 4\\\\pi G \\\\int_0^r \\\\frac 1{s^2} \\\\int_0^s t^2 \\\\rho(t) \\\\; dt \\\\; ds.\\n  \\\\]\\n\\nThere are two problems with this, however: (i) The Earth is not homogeneous, i.e., the density \\\\(\\\\rho\\\\) depends on \\\\(\\\\mathbf x\\\\); in fact it is not even a function that only depends on the radius \\\\(r=\\\\|\\\\mathbf x\\\\|\\\\). In reality, gravity therefore does not always decrease as we get deeper: because the earth core is so much denser than the mantle, gravity actually peaks at around  \\\\(10.7\\n  \\\\frac{\\\\text{m}}{\\\\text{s}^2}\\\\) at the core mantle boundary (see this article). (ii) The density, and by consequence the gravity vector, is not even constant in time: after all, the problem we want to solve is the time dependent upwelling of hot, less dense material and the downwelling of cold dense material. This leads to a gravity vector that varies with space and time, and does not always point straight down.\\nIn order to not make the situation more complicated than necessary, we could use the approximation that at the inner boundary of the mantle, gravity is \\\\(10.7 \\\\frac{\\\\text{m}}{\\\\text{s}^2}\\\\) and at the outer boundary it is \\\\(9.81 \\\\frac{\\\\text{m}}{\\\\text{s}^2}\\\\), in each case pointing radially inward, and that in between gravity varies linearly with the radial distance from the earth center. That said, it isn't that hard to actually be slightly more realistic and assume (as we do below) that the earth mantle has constant density. In that case, the equation above can be integrated and we get an expression for \\\\(\\\\|\\\\mathbf{g}\\\\|\\\\) where we can fit constants to match the gravity at the top and bottom of the earth mantle to obtain    \\n\\\\[\\n    \\\\|\\\\mathbf{g}\\\\|\\n    = 1.245\\\\cdot 10^{-6} \\\\frac{1}{\\\\textrm{s}^2} r + 7.714\\\\cdot 10^{13} \\\\frac{\\\\textrm{m}^3}{\\\\textrm{s}^2}\\\\frac{1}{r^2}.\\n  \\\\]\\n\\n\\n\\n\\nThe density of the earth mantle varies spatially, but not by very much. \\\\(\\\\rho_{\\\\text{ref}}=3300 \\\\frac{\\\\text{kg}}{\\\\text{m}^3}\\\\) is a relatively good average value for the density at reference temperature \\\\(T_{\\\\text{ref}}=293\\\\) Kelvin.\\n\\n\\n\\nThe thermal expansion coefficient \\\\(\\\\beta\\\\) also varies with depth (through its dependence on temperature and pressure). Close to the surface, it appears to be on the order of \\\\(\\\\beta=45\\\\cdot 10^{-6} \\\\frac 1{\\\\text{K}}\\\\), whereas at the core mantle boundary, it may be closer to  \\\\(\\\\beta=10\\\\cdot\\n  10^{-6} \\\\frac 1{\\\\text{K}}\\\\). As a reasonable value, let us choose \\\\(\\\\beta=2\\\\cdot 10^{-5} \\\\frac 1{\\\\text{K}}\\\\). The density as a function of temperature is then \\\\(\\\\rho(T)=[1-\\\\beta(T-T_{\\\\text{ref}})]\\\\rho_{\\\\text{ref}}\\\\).\\n\\n\\n\\nThe second to last parameter we need to specify is the viscosity \\\\(\\\\eta\\\\). This is a tough one, because rocks at the temperatures and pressure typical for the earth mantle flow so slowly that the viscosity can not be determined accurately in the laboratory. So how do we know about the viscosity of the mantle? The most commonly used route is to consider that during and after ice ages, ice shields form and disappear on time scales that are shorter than the time scale of flow in the mantle. As a consequence, continents slowly sink into the earth mantle under the added weight of an ice shield, and they rise up again slowly after the ice shield has disappeared again (this is called postglacial rebound). By measuring the speed of this rebound, we can infer the viscosity of the material that flows into the area vacated under the rebounding continental plates.\\nUsing this technique, values around   \\\\(\\\\eta=10^{21} \\\\text{Pa}\\\\;\\\\text{s}\\n  = 10^{21} \\\\frac{\\\\text{N}\\\\;\\\\text{s}}{\\\\text{m}^2}\\n  = 10^{21} \\\\frac{\\\\text{kg}}{\\\\text{m}\\\\;\\\\text{s}}\\\\) have been found as the most likely, though the error bar on this is at least one order of magnitude.\\nWhile we will use this value, we again have to caution that there are many physical reasons to assume that this is not the correct value. First, it should really be made dependent on temperature: hotter material is most likely to be less viscous than colder material. In reality, however, the situation is even more complex. Most rocks in the mantle undergo phase changes as temperature and pressure change: depending on temperature and pressure, different crystal configurations are thermodynamically favored over others, even if the chemical composition of the mantle were homogeneous. For example, the common mantle material MgSiO3 exists in its perovskite structure throughout most of the mantle, but in the lower mantle the same substance is stable only as post-perovskite. Clearly, to compute realistic viscosities, we would not only need to know the exact chemical composition of the mantle and the viscosities of all materials, but we would also have to compute the thermodynamically most stable configurations for all materials at each quadrature point. This is at the time of writing this program not a feasible suggestion.\\n\\n\\n\\nOur last material parameter is the thermal diffusivity \\\\(\\\\kappa\\\\), which is defined as \\\\(\\\\kappa=\\\\frac{k}{\\\\rho c_p}\\\\) where \\\\(k\\\\) is the thermal conductivity, \\\\(\\\\rho\\\\) the density, and \\\\(c_p\\\\) the specific heat. For this, the literature indicates that it increases from around \\\\(0.7\\\\) in the upper mantle to around \\\\(1.7 \\\\frac{\\\\text{mm}^2}{\\\\text{s}}\\\\) in the lower mantle, though the exact value is not really all that important: heat transport through convection is several orders of magnitude more important than through thermal conduction. It may be of interest to know that perovskite, the most abundant material in the earth mantle, appears to become transparent at pressures above around 120 GPa (see, for example, J. Badro et al., Science 305, 383-386 (2004)); in the lower mantle, it may therefore be that heat transport through radiative transfer is more efficient than through thermal conduction.\\nIn view of these considerations, let us choose \\\\(\\\\kappa=1 \\\\frac{\\\\text{mm}^2}{\\\\text{s}} =10^{-6} \\\\frac{\\\\text{m}^2}{\\\\text{s}}\\\\) for the purpose of this program. \\n\\n\\nAll of these pieces of equation data are defined in the program in the EquationData namespace. When run, the program produces long-term maximal velocities around 10-40 centimeters per year (see the results section below), approximately the physically correct order of magnitude. We will set the end time to 1 billion years.\\nNoteThe choice of the constants and material parameters above follows in large part the comprehensive book \\\"Mantle Convection in the Earth and Planets,\\nPart 1\\\" by G. Schubert and D. L. Turcotte and P. Olson (Cambridge, 2001). It contains extensive discussion of ways to make the program more realistic.\\nImplementation details \\nCompared to step-31, this program has a number of noteworthy differences:\\n\\nThe EquationData namespace is significantly larger, reflecting the fact that we now have much more physics to deal with. That said, most of this additional physical detail is rather self-contained in functions in this one namespace, and does not proliferate throughout the rest of the program.\\nOf more obvious visibility is the fact that we have put a good number of parameters into an input file handled by the ParameterHandler class (see, for example, step-29, for ways to set up run-time parameter files with this class). This often makes sense when one wants to avoid re-compiling the program just because one wants to play with a single parameter (think, for example, of parameter studies determining the best values of the stabilization constants discussed above), in particular given that it takes a nontrivial amount of time to re-compile programs of the current size. To just give an overview of the kinds of parameters we have moved from fixed values into the input file, here is a listing of a typical step-32.prm file : # Listing of Parameters\\n# ---------------------\\n# The end time of the simulation in years.\\nset End time                            = 1e8\\n \\n# Whether graphical output is to be generated or not. You may not want to get\\n# graphical output if the number of processors is large.\\nset Generate graphical output           = false\\n \\n# The number of adaptive refinement steps performed after initial global\\n# refinement.\\nset Initial adaptive refinement         = 1\\n \\n# The number of global refinement steps performed on the initial coarse mesh,\\n# before the problem is first solved there.\\nset Initial global refinement           = 1\\n \\n# The number of time steps between each generation of graphical output files.\\nset Time steps between graphical output = 50\\n \\n# The number of time steps after which the mesh is to be adapted based on\\n# computed error indicators.\\nset Time steps between mesh refinement  = 10\\n \\n \\nsubsection Discretization\\n  # The polynomial degree to use for the velocity variables in the Stokes\\n  # system.\\n  set Stokes velocity polynomial degree       = 2\\n \\n  # The polynomial degree to use for the temperature variable.\\n  set Temperature polynomial degree           = 2\\n \\n  # Whether to use a Stokes discretization that is locally conservative at the\\n  # expense of a larger number of degrees of freedom, or to go with a cheaper\\n  # discretization that does not locally conserve mass (although it is\\n  # globally conservative.\\n  set Use locally conservative discretization = true\\nend\\n \\n \\nsubsection Stabilization parameters\\n  # The exponent in the entropy viscosity stabilization.\\n  set alpha = 2\\n \\n  # The beta factor in the artificial viscosity stabilization. An appropriate\\n  # value for 2d is 0.052 and 0.078 for 3d.\\n  set beta  = 0.078\\n \\n  # The c_R factor in the entropy viscosity stabilization.\\n  set c_R   = 0.5\\nend\\n\\nThere are, obviously, a good number of changes that have to do with the fact that we want to run our program on a possibly very large number of machines. Although one may suspect that this requires us to completely re-structure our code, that isn't in fact the case (although the classes that implement much of this functionality in deal.II certainly look very different from an implementation viewpoint, but this doesn't reflect in their public interface). Rather, the changes are mostly subtle, and the overall structure of the main class is pretty much unchanged. That said, the devil is in the detail: getting parallel computing right, without deadlocks, ensuring that the right data is available at the right place (see, for example, the discussion on fully distributed vectors vs. vectors with ghost elements), and avoiding bottlenecks is difficult and discussions on this topic will appear in a good number of places in this program.\\n\\nOutlook \\nThis is a tutorial program. That means that at least most of its focus needs to lie on demonstrating ways of using deal.II and associated libraries, and not diluting this teaching lesson by focusing overly much on physical details. Despite the lengthy section above on the choice of physical parameters, the part of the program devoted to this is actually quite short and self contained.\\nThat said, both step-31 and the current step-32 have not come about by chance but are certainly meant as wayposts along the path to a more comprehensive program that will simulate convection in the earth mantle. We call this code ASPECT (short for Advanced Solver for Problems in Earth's ConvecTion); its development is funded by the Computational Infrastructure in Geodynamics initiative with support from the National Science Foundation. More information on ASPECT is available at its homepage.\\n The commented program\\n Include files\\nThe first task as usual is to include the functionality of these well-known deal.II library files and some C++ header files.\\n\\u00a0 #include <deal.II/base/quadrature_lib.h>\\n\\u00a0 #include <deal.II/base/function.h>\\n\\u00a0 #include <deal.II/base/utilities.h>\\n\\u00a0 #include <deal.II/base/conditional_ostream.h>\\n\\u00a0 #include <deal.II/base/work_stream.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 #include <deal.II/base/parameter_handler.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/full_matrix.h>\\n\\u00a0 #include <deal.II/lac/solver_bicgstab.h>\\n\\u00a0 #include <deal.II/lac/solver_cg.h>\\n\\u00a0 #include <deal.II/lac/solver_gmres.h>\\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/lac/block_sparsity_pattern.h>\\n\\u00a0 #include <deal.II/lac/trilinos_parallel_block_vector.h>\\n\\u00a0 #include <deal.II/lac/trilinos_sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/trilinos_block_sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/trilinos_precondition.h>\\n\\u00a0 #include <deal.II/lac/trilinos_solver.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/grid/tria.h>\\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/filtered_iterator.h>\\n\\u00a0 #include <deal.II/grid/manifold_lib.h>\\n\\u00a0 #include <deal.II/grid/grid_tools.h>\\n\\u00a0 #include <deal.II/grid/grid_refinement.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/dofs/dof_renumbering.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 #include <deal.II/fe/fe_dgq.h>\\n\\u00a0 #include <deal.II/fe/fe_dgp.h>\\n\\u00a0 #include <deal.II/fe/fe_system.h>\\n\\u00a0 #include <deal.II/fe/fe_values.h>\\n\\u00a0 #include <deal.II/fe/mapping_q.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/error_estimator.h>\\n\\u00a0 #include <deal.II/numerics/solution_transfer.h>\\n\\u00a0 \\n\\u00a0 #include <fstream>\\n\\u00a0 #include <iostream>\\n\\u00a0 #include <limits>\\n\\u00a0 #include <locale>\\n\\u00a0 #include <string>\\n\\u00a0 \\nThis is the only include file that is new: It introduces the parallel::distributed::SolutionTransfer equivalent of the SolutionTransfer class to take a solution from on mesh to the next one upon mesh refinement, but in the case of parallel distributed triangulations:\\n\\u00a0 #include <deal.II/distributed/solution_transfer.h>\\n\\u00a0 \\nThe following classes are used in parallel distributed computations and have all already been introduced in step-40:\\n\\u00a0 #include <deal.II/base/index_set.h>\\n\\u00a0 #include <deal.II/distributed/tria.h>\\n\\u00a0 #include <deal.II/distributed/grid_refinement.h>\\n\\u00a0 \\n\\u00a0 \\nThe next step is like in all previous tutorial programs: We put everything into a namespace of its own and then import the deal.II classes and functions into it:\\n\\u00a0 namespace Step32\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n Equation data\\nIn the following namespace, we define the various pieces of equation data that describe the problem. This corresponds to the various aspects of making the problem at least slightly realistic and that were exhaustively discussed in the description of the testcase in the introduction.\\nWe start with a few coefficients that have constant values (the comment after the value indicates its physical units):\\n\\u00a0   namespace EquationData\\n\\u00a0   {\\n\\u00a0     constexpr double eta                   = 1e21;    /* Pa s       */\\n\\u00a0     constexpr double kappa                 = 1e-6;    /* m^2 / s    */\\n\\u00a0     constexpr double reference_density     = 3300;    /* kg / m^3   */\\n\\u00a0     constexpr double reference_temperature = 293;     /* K          */\\n\\u00a0     constexpr double expansion_coefficient = 2e-5;    /* 1/K        */\\n\\u00a0     constexpr double specific_heat         = 1250;    /* J / K / kg */\\n\\u00a0     constexpr double radiogenic_heating    = 7.4e-12; /* W / kg     */\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     constexpr double R0 = 6371000. - 2890000.; /* m          */\\n\\u00a0     constexpr double R1 = 6371000. - 35000.;   /* m          */\\n\\u00a0 \\n\\u00a0     constexpr double T0 = 4000 + 273; /* K          */\\n\\u00a0     constexpr double T1 = 700 + 273;  /* K          */\\n\\u00a0 \\n\\u00a0 \\nThe next set of definitions are for functions that encode the density as a function of temperature, the gravity vector, and the initial values for the temperature. Again, all of these (along with the values they compute) are discussed in the introduction:\\n\\u00a0     double density(const double temperature)\\n\\u00a0     {\\n\\u00a0       return (\\n\\u00a0         reference_density *\\n\\u00a0         (1 - expansion_coefficient * (temperature - reference_temperature)));\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <int dim>\\n\\u00a0     Tensor<1, dim> gravity_vector(const Point<dim> &p)\\n\\u00a0     {\\n\\u00a0       const double r = p.norm();\\n\\u00a0       return -(1.245e-6 * r + 7.714e13 / r / r) * p / r;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <int dim>\\n\\u00a0     class TemperatureInitialValues : public Function<dim>\\n\\u00a0     {\\n\\u00a0     public:\\n\\u00a0       TemperatureInitialValues()\\n\\u00a0         : Function<dim>(1)\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0       virtual double value(const Point<dim>  &p,\\n\\u00a0                            const unsigned int component = 0) const override;\\n\\u00a0 \\n\\u00a0       virtual void vector_value(const Point<dim> &p,\\n\\u00a0                                 Vector<double>   &value) const override;\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <int dim>\\n\\u00a0     double TemperatureInitialValues<dim>::value(const Point<dim> &p,\\n\\u00a0                                                 const unsigned int) const\\n\\u00a0     {\\n\\u00a0       const double r = p.norm();\\n\\u00a0       const double h = R1 - R0;\\n\\u00a0 \\n\\u00a0       const double s = (r - R0) / h;\\n\\u00a0       const double q =\\n\\u00a0         (dim == 3) ? std::max(0.0, cos(numbers::PI * abs(p[2] / R1))) : 1.0;\\n\\u00a0       const double phi = std::atan2(p[0], p[1]);\\n\\u00a0       const double tau = s + 0.2 * s * (1 - s) * std::sin(6 * phi) * q;\\n\\u00a0 \\n\\u00a0       return T0 * (1.0 - tau) + T1 * tau;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <int dim>\\n\\u00a0     void\\n\\u00a0     TemperatureInitialValues<dim>::vector_value(const Point<dim> &p,\\n\\u00a0                                                 Vector<double>   &values) const\\n\\u00a0     {\\n\\u00a0       for (unsigned int c = 0; c < this->n_components; ++c)\\n\\u00a0         values(c) = TemperatureInitialValues<dim>::value(p, c);\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\nFunctionDefinition function.h:152\\nFunction::valuevirtual RangeNumberType value(const Point< dim > &p, const unsigned int component=0) const\\nFunction::vector_valuevirtual void vector_value(const Point< dim > &p, Vector< RangeNumberType > &values) const\\nPointDefinition point.h:111\\nTensorDefinition tensor.h:471\\nTensor::normnumbers::NumberTraits< Number >::real_type norm() const\\nVectorDefinition vector.h:120\\nnumbers::PIstatic constexpr double PIDefinition numbers.h:259\\nstd::max::VectorizedArray< Number, width > max(const ::VectorizedArray< Number, width > &, const ::VectorizedArray< Number, width > &)Definition vectorization.h:6943\\nstd::cos::VectorizedArray< Number, width > cos(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6609\\nstd::sin::VectorizedArray< Number, width > sin(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6589\\nstd::abs::VectorizedArray< Number, width > abs(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6927\\nAs mentioned in the introduction we need to rescale the pressure to avoid the relative ill-conditioning of the momentum and mass conservation equations. The scaling factor is \\\\(\\\\frac{\\\\eta}{L}\\\\) where \\\\(L\\\\) was a typical length scale. By experimenting it turns out that a good length scale is the diameter of plumes, which is around 10 km:\\n\\u00a0     constexpr double pressure_scaling = eta / 10000;\\n\\u00a0 \\nThe final number in this namespace is a constant that denotes the number of seconds per (average, tropical) year. We use this only when generating screen output: internally, all computations of this program happen in SI units (kilogram, meter, seconds) but writing geological times in seconds yields numbers that one can't relate to reality, and so we convert to years using the factor defined here:\\n\\u00a0     const double year_in_seconds = 60 * 60 * 24 * 365.2425;\\n\\u00a0 \\n\\u00a0   } // namespace EquationData\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Preconditioning the Stokes system\\nThis namespace implements the preconditioner. As discussed in the introduction, this preconditioner differs in a number of key portions from the one used in step-31. Specifically, it is a right preconditioner, implementing the matrix     \\n\\\\begin{align*}\\n   \\\\left(\\\\begin{array}{cc}A^{-1} & -A^{-1}B^TS^{-1}\\n   \\\\\\\\0 & S^{-1}\\n   \\\\end{array}\\\\right)\\n   \\\\end{align*}\\n\\n where the two inverse matrix operations are approximated by linear solvers or, if the right flag is given to the constructor of this class, by a single AMG V-cycle for the velocity block. The three code blocks of the vmult function implement the multiplications with the three blocks of this preconditioner matrix and should be self explanatory if you have read through step-31 or the discussion of composing solvers in step-20.\\n\\u00a0   namespace LinearSolvers\\n\\u00a0   {\\n\\u00a0     template <class PreconditionerTypeA, class PreconditionerTypeMp>\\n\\u00a0     class BlockSchurPreconditioner : public Subscriptor\\n\\u00a0     {\\n\\u00a0     public:\\n\\u00a0       BlockSchurPreconditioner(const TrilinosWrappers::BlockSparseMatrix &S,\\n\\u00a0                                const TrilinosWrappers::BlockSparseMatrix &Spre,\\n\\u00a0                                const PreconditionerTypeMp &Mppreconditioner,\\n\\u00a0                                const PreconditionerTypeA  &Apreconditioner,\\n\\u00a0                                const bool                  do_solve_A)\\n\\u00a0         : stokes_matrix(&S)\\n\\u00a0         , stokes_preconditioner_matrix(&Spre)\\n\\u00a0         , mp_preconditioner(Mppreconditioner)\\n\\u00a0         , a_preconditioner(Apreconditioner)\\n\\u00a0         , do_solve_A(do_solve_A)\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0       void vmult(TrilinosWrappers::MPI::BlockVector       &dst,\\n\\u00a0                  const TrilinosWrappers::MPI::BlockVector &src) const\\n\\u00a0       {\\n\\u00a0         TrilinosWrappers::MPI::Vector utmp(src.block(0));\\n\\u00a0 \\n\\u00a0         {\\n\\u00a0           SolverControl solver_control(5000, 1e-6 * src.block(1).l2_norm());\\n\\u00a0 \\n\\u00a0           SolverCG<TrilinosWrappers::MPI::Vector> solver(solver_control);\\n\\u00a0 \\n\\u00a0           solver.solve(stokes_preconditioner_matrix->block(1, 1),\\n\\u00a0                        dst.block(1),\\n\\u00a0                        src.block(1),\\n\\u00a0                        mp_preconditioner);\\n\\u00a0 \\n\\u00a0           dst.block(1) *= -1.0;\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0         {\\n\\u00a0           stokes_matrix->block(0, 1).vmult(utmp, dst.block(1));\\n\\u00a0           utmp *= -1.0;\\n\\u00a0           utmp.add(src.block(0));\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0         if (do_solve_A == true)\\n\\u00a0           {\\n\\u00a0             SolverControl solver_control(5000, utmp.l2_norm() * 1e-2);\\n\\u00a0             TrilinosWrappers::SolverCG solver(solver_control);\\n\\u00a0             solver.solve(stokes_matrix->block(0, 0),\\n\\u00a0                          dst.block(0),\\n\\u00a0                          utmp,\\n\\u00a0                          a_preconditioner);\\n\\u00a0           }\\n\\u00a0         else\\n\\u00a0           a_preconditioner.vmult(dst.block(0), utmp);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     private:\\n\\u00a0       const SmartPointer<const TrilinosWrappers::BlockSparseMatrix>\\n\\u00a0         stokes_matrix;\\n\\u00a0       const SmartPointer<const TrilinosWrappers::BlockSparseMatrix>\\n\\u00a0                                   stokes_preconditioner_matrix;\\n\\u00a0       const PreconditionerTypeMp &mp_preconditioner;\\n\\u00a0       const PreconditionerTypeA  &a_preconditioner;\\n\\u00a0       const bool                  do_solve_A;\\n\\u00a0     };\\n\\u00a0   } // namespace LinearSolvers\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nBlockVectorDefinition block_vector.h:71\\nSmartPointerDefinition smartpointer.h:93\\nSolverCGDefinition solver_cg.h:179\\nSolverControlDefinition solver_control.h:67\\nSubscriptorDefinition subscriptor.h:60\\nTrilinosWrappers::BlockSparseMatrixDefinition trilinos_block_sparse_matrix.h:72\\nTrilinosWrappers::SolverCGDefinition trilinos_solver.h:375\\nInitializeLibrary::MPI@ MPI\\nTrilinosWrappersDefinition template_constraints.h:652\\n Definition of assembly data structures\\nAs described in the introduction, we will use the WorkStream mechanism discussed in the Parallel computing with multiple processors accessing shared memory topic to parallelize operations among the processors of a single machine. The WorkStream class requires that data is passed around in two kinds of data structures, one for scratch data and one to pass data from the assembly function to the function that copies local contributions into global objects.\\nThe following namespace (and the two sub-namespaces) contains a collection of data structures that serve this purpose, one pair for each of the four operations discussed in the introduction that we will want to parallelize. Each assembly routine gets two sets of data: a Scratch array that collects all the classes and arrays that are used for the calculation of the cell contribution, and a CopyData array that keeps local matrices and vectors which will be written into the global matrix. Whereas CopyData is a container for the final data that is written into the global matrices and vector (and, thus, absolutely necessary), the Scratch arrays are merely there for performance reasons \\u2014 it would be much more expensive to set up a FEValues object on each cell, than creating it only once and updating some derivative data.\\nstep-31 had four assembly routines: One for the preconditioner matrix of the Stokes system, one for the Stokes matrix and right hand side, one for the temperature matrices and one for the right hand side of the temperature equation. We here organize the scratch arrays and CopyData objects for each of those four assembly components using a struct environment (since we consider these as temporary objects we pass around, rather than classes that implement functionality of their own, though this is a more subjective point of view to distinguish between structs and classes).\\nRegarding the Scratch objects, each struct is equipped with a constructor that creates an FEValues object using the FiniteElement, Quadrature, Mapping (which describes the interpolation of curved boundaries), and The interplay of UpdateFlags, Mapping, and FiniteElement in FEValues instances. Moreover, we manually implement a copy constructor (since the FEValues class is not copyable by itself), and provide some additional vector fields that are used to hold intermediate data during the computation of local contributions.\\nLet us start with the scratch arrays and, specifically, the one used for assembly of the Stokes preconditioner:\\n\\u00a0   namespace Assembly\\n\\u00a0   {\\n\\u00a0     namespace Scratch\\n\\u00a0     {\\n\\u00a0       template <int dim>\\n\\u00a0       struct StokesPreconditioner\\n\\u00a0       {\\n\\u00a0         StokesPreconditioner(const FiniteElement<dim> &stokes_fe,\\n\\u00a0                              const Quadrature<dim>    &stokes_quadrature,\\n\\u00a0                              const Mapping<dim>       &mapping,\\n\\u00a0                              const UpdateFlags         update_flags);\\n\\u00a0 \\n\\u00a0         StokesPreconditioner(const StokesPreconditioner &data);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0         FEValues<dim> stokes_fe_values;\\n\\u00a0 \\n\\u00a0         std::vector<Tensor<2, dim>> grad_phi_u;\\n\\u00a0         std::vector<double>         phi_p;\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       StokesPreconditioner<dim>::StokesPreconditioner(\\n\\u00a0         const FiniteElement<dim> &stokes_fe,\\n\\u00a0         const Quadrature<dim>    &stokes_quadrature,\\n\\u00a0         const Mapping<dim>       &mapping,\\n\\u00a0         const UpdateFlags         update_flags)\\n\\u00a0         : stokes_fe_values(mapping, stokes_fe, stokes_quadrature, update_flags)\\n\\u00a0         , grad_phi_u(stokes_fe.n_dofs_per_cell())\\n\\u00a0         , phi_p(stokes_fe.n_dofs_per_cell())\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       StokesPreconditioner<dim>::StokesPreconditioner(\\n\\u00a0         const StokesPreconditioner &scratch)\\n\\u00a0         : stokes_fe_values(scratch.stokes_fe_values.get_mapping(),\\n\\u00a0                            scratch.stokes_fe_values.get_fe(),\\n\\u00a0                            scratch.stokes_fe_values.get_quadrature(),\\n\\u00a0                            scratch.stokes_fe_values.get_update_flags())\\n\\u00a0         , grad_phi_u(scratch.grad_phi_u)\\n\\u00a0         , phi_p(scratch.phi_p)\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFEValuesDefinition fe_values.h:63\\nFiniteElementDefinition fe.h:655\\nMappingAbstract base class for mapping classes.Definition mapping.h:318\\nQuadratureDefinition quadrature.h:123\\nint\\nUpdateFlagsUpdateFlagsDefinition fe_update_flags.h:64\\nThe next one is the scratch object used for the assembly of the full Stokes system. Observe that we derive the StokesSystem scratch class from the StokesPreconditioner class above. We do this because all the objects that are necessary for the assembly of the preconditioner are also needed for the actual matrix system and right hand side, plus some extra data. This makes the program more compact. Note also that the assembly of the Stokes system and the temperature right hand side further down requires data from temperature and velocity, respectively, so we actually need two FEValues objects for those two cases.\\n\\u00a0       template <int dim>\\n\\u00a0       struct StokesSystem : public StokesPreconditioner<dim>\\n\\u00a0       {\\n\\u00a0         StokesSystem(const FiniteElement<dim> &stokes_fe,\\n\\u00a0                      const Mapping<dim>       &mapping,\\n\\u00a0                      const Quadrature<dim>    &stokes_quadrature,\\n\\u00a0                      const UpdateFlags         stokes_update_flags,\\n\\u00a0                      const FiniteElement<dim> &temperature_fe,\\n\\u00a0                      const UpdateFlags         temperature_update_flags);\\n\\u00a0 \\n\\u00a0         StokesSystem(const StokesSystem<dim> &data);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0         FEValues<dim> temperature_fe_values;\\n\\u00a0 \\n\\u00a0         std::vector<Tensor<1, dim>>          phi_u;\\n\\u00a0         std::vector<SymmetricTensor<2, dim>> grads_phi_u;\\n\\u00a0         std::vector<double>                  div_phi_u;\\n\\u00a0 \\n\\u00a0         std::vector<double> old_temperature_values;\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       StokesSystem<dim>::StokesSystem(\\n\\u00a0         const FiniteElement<dim> &stokes_fe,\\n\\u00a0         const Mapping<dim>       &mapping,\\n\\u00a0         const Quadrature<dim>    &stokes_quadrature,\\n\\u00a0         const UpdateFlags         stokes_update_flags,\\n\\u00a0         const FiniteElement<dim> &temperature_fe,\\n\\u00a0         const UpdateFlags         temperature_update_flags)\\n\\u00a0         : StokesPreconditioner<dim>(stokes_fe,\\n\\u00a0                                     stokes_quadrature,\\n\\u00a0                                     mapping,\\n\\u00a0                                     stokes_update_flags)\\n\\u00a0         , temperature_fe_values(mapping,\\n\\u00a0                                 temperature_fe,\\n\\u00a0                                 stokes_quadrature,\\n\\u00a0                                 temperature_update_flags)\\n\\u00a0         , phi_u(stokes_fe.n_dofs_per_cell())\\n\\u00a0         , grads_phi_u(stokes_fe.n_dofs_per_cell())\\n\\u00a0         , div_phi_u(stokes_fe.n_dofs_per_cell())\\n\\u00a0         , old_temperature_values(stokes_quadrature.size())\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       StokesSystem<dim>::StokesSystem(const StokesSystem<dim> &scratch)\\n\\u00a0         : StokesPreconditioner<dim>(scratch)\\n\\u00a0         , temperature_fe_values(\\n\\u00a0             scratch.temperature_fe_values.get_mapping(),\\n\\u00a0             scratch.temperature_fe_values.get_fe(),\\n\\u00a0             scratch.temperature_fe_values.get_quadrature(),\\n\\u00a0             scratch.temperature_fe_values.get_update_flags())\\n\\u00a0         , phi_u(scratch.phi_u)\\n\\u00a0         , grads_phi_u(scratch.grads_phi_u)\\n\\u00a0         , div_phi_u(scratch.div_phi_u)\\n\\u00a0         , old_temperature_values(scratch.old_temperature_values)\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\nAfter defining the objects used in the assembly of the Stokes system, we do the same for the assembly of the matrices necessary for the temperature system. The general structure is very similar:\\n\\u00a0       template <int dim>\\n\\u00a0       struct TemperatureMatrix\\n\\u00a0       {\\n\\u00a0         TemperatureMatrix(const FiniteElement<dim> &temperature_fe,\\n\\u00a0                           const Mapping<dim>       &mapping,\\n\\u00a0                           const Quadrature<dim>    &temperature_quadrature);\\n\\u00a0 \\n\\u00a0         TemperatureMatrix(const TemperatureMatrix &data);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0         FEValues<dim> temperature_fe_values;\\n\\u00a0 \\n\\u00a0         std::vector<double>         phi_T;\\n\\u00a0         std::vector<Tensor<1, dim>> grad_phi_T;\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       TemperatureMatrix<dim>::TemperatureMatrix(\\n\\u00a0         const FiniteElement<dim> &temperature_fe,\\n\\u00a0         const Mapping<dim>       &mapping,\\n\\u00a0         const Quadrature<dim>    &temperature_quadrature)\\n\\u00a0         : temperature_fe_values(mapping,\\n\\u00a0                                 temperature_fe,\\n\\u00a0                                 temperature_quadrature,\\n\\u00a0                                 update_values | update_gradients |\\n\\u00a0                                   update_JxW_values)\\n\\u00a0         , phi_T(temperature_fe.n_dofs_per_cell())\\n\\u00a0         , grad_phi_T(temperature_fe.n_dofs_per_cell())\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       TemperatureMatrix<dim>::TemperatureMatrix(\\n\\u00a0         const TemperatureMatrix &scratch)\\n\\u00a0         : temperature_fe_values(\\n\\u00a0             scratch.temperature_fe_values.get_mapping(),\\n\\u00a0             scratch.temperature_fe_values.get_fe(),\\n\\u00a0             scratch.temperature_fe_values.get_quadrature(),\\n\\u00a0             scratch.temperature_fe_values.get_update_flags())\\n\\u00a0         , phi_T(scratch.phi_T)\\n\\u00a0         , grad_phi_T(scratch.grad_phi_T)\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\nupdate_values@ update_valuesShape function values.Definition fe_update_flags.h:75\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nThe final scratch object is used in the assembly of the right hand side of the temperature system. This object is significantly larger than the ones above because a lot more quantities enter the computation of the right hand side of the temperature equation. In particular, the temperature values and gradients of the previous two time steps need to be evaluated at the quadrature points, as well as the velocities and the strain rates (i.e. the symmetric gradients of the velocity) that enter the right hand side as friction heating terms. Despite the number of terms, the following should be rather self explanatory:\\n\\u00a0       template <int dim>\\n\\u00a0       struct TemperatureRHS\\n\\u00a0       {\\n\\u00a0         TemperatureRHS(const FiniteElement<dim> &temperature_fe,\\n\\u00a0                        const FiniteElement<dim> &stokes_fe,\\n\\u00a0                        const Mapping<dim>       &mapping,\\n\\u00a0                        const Quadrature<dim>    &quadrature);\\n\\u00a0 \\n\\u00a0         TemperatureRHS(const TemperatureRHS &data);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0         FEValues<dim> temperature_fe_values;\\n\\u00a0         FEValues<dim> stokes_fe_values;\\n\\u00a0 \\n\\u00a0         std::vector<double>         phi_T;\\n\\u00a0         std::vector<Tensor<1, dim>> grad_phi_T;\\n\\u00a0 \\n\\u00a0         std::vector<Tensor<1, dim>> old_velocity_values;\\n\\u00a0         std::vector<Tensor<1, dim>> old_old_velocity_values;\\n\\u00a0 \\n\\u00a0         std::vector<SymmetricTensor<2, dim>> old_strain_rates;\\n\\u00a0         std::vector<SymmetricTensor<2, dim>> old_old_strain_rates;\\n\\u00a0 \\n\\u00a0         std::vector<double>         old_temperature_values;\\n\\u00a0         std::vector<double>         old_old_temperature_values;\\n\\u00a0         std::vector<Tensor<1, dim>> old_temperature_grads;\\n\\u00a0         std::vector<Tensor<1, dim>> old_old_temperature_grads;\\n\\u00a0         std::vector<double>         old_temperature_laplacians;\\n\\u00a0         std::vector<double>         old_old_temperature_laplacians;\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       TemperatureRHS<dim>::TemperatureRHS(\\n\\u00a0         const FiniteElement<dim> &temperature_fe,\\n\\u00a0         const FiniteElement<dim> &stokes_fe,\\n\\u00a0         const Mapping<dim>       &mapping,\\n\\u00a0         const Quadrature<dim>    &quadrature)\\n\\u00a0         : temperature_fe_values(mapping,\\n\\u00a0                                 temperature_fe,\\n\\u00a0                                 quadrature,\\n\\u00a0                                 update_values | update_gradients |\\n\\u00a0                                   update_hessians | update_quadrature_points |\\n\\u00a0                                   update_JxW_values)\\n\\u00a0         , stokes_fe_values(mapping,\\n\\u00a0                            stokes_fe,\\n\\u00a0                            quadrature,\\n\\u00a0                            update_values | update_gradients)\\n\\u00a0         , phi_T(temperature_fe.n_dofs_per_cell())\\n\\u00a0         , grad_phi_T(temperature_fe.n_dofs_per_cell())\\n\\u00a0         ,\\n\\u00a0 \\n\\u00a0         old_velocity_values(quadrature.size())\\n\\u00a0         , old_old_velocity_values(quadrature.size())\\n\\u00a0         , old_strain_rates(quadrature.size())\\n\\u00a0         , old_old_strain_rates(quadrature.size())\\n\\u00a0         ,\\n\\u00a0 \\n\\u00a0         old_temperature_values(quadrature.size())\\n\\u00a0         , old_old_temperature_values(quadrature.size())\\n\\u00a0         , old_temperature_grads(quadrature.size())\\n\\u00a0         , old_old_temperature_grads(quadrature.size())\\n\\u00a0         , old_temperature_laplacians(quadrature.size())\\n\\u00a0         , old_old_temperature_laplacians(quadrature.size())\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       TemperatureRHS<dim>::TemperatureRHS(const TemperatureRHS &scratch)\\n\\u00a0         : temperature_fe_values(\\n\\u00a0             scratch.temperature_fe_values.get_mapping(),\\n\\u00a0             scratch.temperature_fe_values.get_fe(),\\n\\u00a0             scratch.temperature_fe_values.get_quadrature(),\\n\\u00a0             scratch.temperature_fe_values.get_update_flags())\\n\\u00a0         , stokes_fe_values(scratch.stokes_fe_values.get_mapping(),\\n\\u00a0                            scratch.stokes_fe_values.get_fe(),\\n\\u00a0                            scratch.stokes_fe_values.get_quadrature(),\\n\\u00a0                            scratch.stokes_fe_values.get_update_flags())\\n\\u00a0         , phi_T(scratch.phi_T)\\n\\u00a0         , grad_phi_T(scratch.grad_phi_T)\\n\\u00a0         ,\\n\\u00a0 \\n\\u00a0         old_velocity_values(scratch.old_velocity_values)\\n\\u00a0         , old_old_velocity_values(scratch.old_old_velocity_values)\\n\\u00a0         , old_strain_rates(scratch.old_strain_rates)\\n\\u00a0         , old_old_strain_rates(scratch.old_old_strain_rates)\\n\\u00a0         ,\\n\\u00a0 \\n\\u00a0         old_temperature_values(scratch.old_temperature_values)\\n\\u00a0         , old_old_temperature_values(scratch.old_old_temperature_values)\\n\\u00a0         , old_temperature_grads(scratch.old_temperature_grads)\\n\\u00a0         , old_old_temperature_grads(scratch.old_old_temperature_grads)\\n\\u00a0         , old_temperature_laplacians(scratch.old_temperature_laplacians)\\n\\u00a0         , old_old_temperature_laplacians(scratch.old_old_temperature_laplacians)\\n\\u00a0       {}\\n\\u00a0     } // namespace Scratch\\n\\u00a0 \\n\\u00a0 \\nupdate_hessians@ update_hessiansSecond derivatives of shape functions.Definition fe_update_flags.h:87\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\nThe CopyData objects are even simpler than the Scratch objects as all they have to do is to store the results of local computations until they can be copied into the global matrix or vector objects. These structures therefore only need to provide a constructor, a copy operation, and some arrays for local matrix, local vectors and the relation between local and global degrees of freedom (a.k.a. local_dof_indices). Again, we have one such structure for each of the four operations we will parallelize using the WorkStream class:\\n\\u00a0     namespace CopyData\\n\\u00a0     {\\n\\u00a0       template <int dim>\\n\\u00a0       struct StokesPreconditioner\\n\\u00a0       {\\n\\u00a0         StokesPreconditioner(const FiniteElement<dim> &stokes_fe);\\n\\u00a0         StokesPreconditioner(const StokesPreconditioner &data);\\n\\u00a0         StokesPreconditioner &operator=(const StokesPreconditioner &) = default;\\n\\u00a0 \\n\\u00a0         FullMatrix<double>                   local_matrix;\\n\\u00a0         std::vector<types::global_dof_index> local_dof_indices;\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       StokesPreconditioner<dim>::StokesPreconditioner(\\n\\u00a0         const FiniteElement<dim> &stokes_fe)\\n\\u00a0         : local_matrix(stokes_fe.n_dofs_per_cell(), stokes_fe.n_dofs_per_cell())\\n\\u00a0         , local_dof_indices(stokes_fe.n_dofs_per_cell())\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       StokesPreconditioner<dim>::StokesPreconditioner(\\n\\u00a0         const StokesPreconditioner &data)\\n\\u00a0         : local_matrix(data.local_matrix)\\n\\u00a0         , local_dof_indices(data.local_dof_indices)\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       struct StokesSystem : public StokesPreconditioner<dim>\\n\\u00a0       {\\n\\u00a0         StokesSystem(const FiniteElement<dim> &stokes_fe);\\n\\u00a0 \\n\\u00a0         Vector<double> local_rhs;\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       StokesSystem<dim>::StokesSystem(const FiniteElement<dim> &stokes_fe)\\n\\u00a0         : StokesPreconditioner<dim>(stokes_fe)\\n\\u00a0         , local_rhs(stokes_fe.n_dofs_per_cell())\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       struct TemperatureMatrix\\n\\u00a0       {\\n\\u00a0         TemperatureMatrix(const FiniteElement<dim> &temperature_fe);\\n\\u00a0 \\n\\u00a0         FullMatrix<double>                   local_mass_matrix;\\n\\u00a0         FullMatrix<double>                   local_stiffness_matrix;\\n\\u00a0         std::vector<types::global_dof_index> local_dof_indices;\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       TemperatureMatrix<dim>::TemperatureMatrix(\\n\\u00a0         const FiniteElement<dim> &temperature_fe)\\n\\u00a0         : local_mass_matrix(temperature_fe.n_dofs_per_cell(),\\n\\u00a0                             temperature_fe.n_dofs_per_cell())\\n\\u00a0         , local_stiffness_matrix(temperature_fe.n_dofs_per_cell(),\\n\\u00a0                                  temperature_fe.n_dofs_per_cell())\\n\\u00a0         , local_dof_indices(temperature_fe.n_dofs_per_cell())\\n\\u00a0       {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       struct TemperatureRHS\\n\\u00a0       {\\n\\u00a0         TemperatureRHS(const FiniteElement<dim> &temperature_fe);\\n\\u00a0 \\n\\u00a0         Vector<double>                       local_rhs;\\n\\u00a0         std::vector<types::global_dof_index> local_dof_indices;\\n\\u00a0         FullMatrix<double>                   matrix_for_bc;\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0       template <int dim>\\n\\u00a0       TemperatureRHS<dim>::TemperatureRHS(\\n\\u00a0         const FiniteElement<dim> &temperature_fe)\\n\\u00a0         : local_rhs(temperature_fe.n_dofs_per_cell())\\n\\u00a0         , local_dof_indices(temperature_fe.n_dofs_per_cell())\\n\\u00a0         , matrix_for_bc(temperature_fe.n_dofs_per_cell(),\\n\\u00a0                         temperature_fe.n_dofs_per_cell())\\n\\u00a0       {}\\n\\u00a0     } // namespace CopyData\\n\\u00a0   }   // namespace Assembly\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFullMatrixDefinition full_matrix.h:79\\n The BoussinesqFlowProblem class template\\nThis is the declaration of the main class. It is very similar to step-31 but there are a number differences we will comment on below.\\nThe top of the class is essentially the same as in step-31, listing the public methods and a set of private functions that do the heavy lifting. Compared to step-31 there are only two additions to this section: the function get_cfl_number() that computes the maximum CFL number over all cells which we then compute the global time step from, and the function get_entropy_variation() that is used in the computation of the entropy stabilization. It is akin to the get_extrapolated_temperature_range() we have used in step-31 for this purpose, but works on the entropy instead of the temperature instead.\\n\\u00a0   template <int dim>\\n\\u00a0   class BoussinesqFlowProblem\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     struct Parameters;\\n\\u00a0     BoussinesqFlowProblem(Parameters &parameters);\\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void   setup_dofs();\\n\\u00a0     void   assemble_stokes_preconditioner();\\n\\u00a0     void   build_stokes_preconditioner();\\n\\u00a0     void   assemble_stokes_system();\\n\\u00a0     void   assemble_temperature_matrix();\\n\\u00a0     void   assemble_temperature_system(const double maximal_velocity);\\n\\u00a0     double get_maximal_velocity() const;\\n\\u00a0     double get_cfl_number() const;\\n\\u00a0     double get_entropy_variation(const double average_temperature) const;\\n\\u00a0     std::pair<double, double> get_extrapolated_temperature_range() const;\\n\\u00a0     void                      solve();\\n\\u00a0     void                      output_results();\\n\\u00a0     void                      refine_mesh(const unsigned int max_grid_level);\\n\\u00a0 \\n\\u00a0     double compute_viscosity(\\n\\u00a0       const std::vector<double>         &old_temperature,\\n\\u00a0       const std::vector<double>         &old_old_temperature,\\n\\u00a0       const std::vector<Tensor<1, dim>> &old_temperature_grads,\\n\\u00a0       const std::vector<Tensor<1, dim>> &old_old_temperature_grads,\\n\\u00a0       const std::vector<double>         &old_temperature_laplacians,\\n\\u00a0       const std::vector<double>         &old_old_temperature_laplacians,\\n\\u00a0       const std::vector<Tensor<1, dim>> &old_velocity_values,\\n\\u00a0       const std::vector<Tensor<1, dim>> &old_old_velocity_values,\\n\\u00a0       const std::vector<SymmetricTensor<2, dim>> &old_strain_rates,\\n\\u00a0       const std::vector<SymmetricTensor<2, dim>> &old_old_strain_rates,\\n\\u00a0       const double                                global_u_infty,\\n\\u00a0       const double                                global_T_variation,\\n\\u00a0       const double                                average_temperature,\\n\\u00a0       const double                                global_entropy_variation,\\n\\u00a0       const double                                cell_diameter) const;\\n\\u00a0 \\n\\u00a0   public:\\nSymmetricTensorDefinition symmetric_tensor.h:719\\nThe first significant new component is the definition of a struct for the parameters according to the discussion in the introduction. This structure is initialized by reading from a parameter file during construction of this object.\\n\\u00a0     struct Parameters\\n\\u00a0     {\\n\\u00a0       Parameters(const std::string &parameter_filename);\\n\\u00a0 \\n\\u00a0       static void declare_parameters(ParameterHandler &prm);\\n\\u00a0       void        parse_parameters(ParameterHandler &prm);\\n\\u00a0 \\n\\u00a0       double end_time;\\n\\u00a0 \\n\\u00a0       unsigned int initial_global_refinement;\\n\\u00a0       unsigned int initial_adaptive_refinement;\\n\\u00a0 \\n\\u00a0       bool         generate_graphical_output;\\n\\u00a0       unsigned int graphical_output_interval;\\n\\u00a0 \\n\\u00a0       unsigned int adaptive_refinement_interval;\\n\\u00a0 \\n\\u00a0       double stabilization_alpha;\\n\\u00a0       double stabilization_c_R;\\n\\u00a0       double stabilization_beta;\\n\\u00a0 \\n\\u00a0       unsigned int stokes_velocity_degree;\\n\\u00a0       bool         use_locally_conservative_discretization;\\n\\u00a0 \\n\\u00a0       unsigned int temperature_degree;\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     Parameters &parameters;\\n\\u00a0 \\nParameterHandlerDefinition parameter_handler.h:855\\nThe pcout (for parallel std::cout) object is used to simplify writing output: each MPI process can use this to generate output as usual, but since each of these processes will (hopefully) produce the same output it will just be replicated many times over; with the ConditionalOStream class, only the output generated by one MPI process will actually be printed to screen, whereas the output by all the other threads will simply be forgotten.\\n\\u00a0     ConditionalOStream pcout;\\n\\u00a0 \\nConditionalOStreamDefinition conditional_ostream.h:80\\nThe following member variables will then again be similar to those in step-31 (and to other tutorial programs). As mentioned in the introduction, we fully distribute computations, so we will have to use the parallel::distributed::Triangulation class (see step-40) but the remainder of these variables is rather standard with two exceptions:\\n\\nThe mapping variable is used to denote a higher-order polynomial mapping. As mentioned in the introduction, we use this mapping when forming integrals through quadrature for all cells.\\nIn a bit of naming confusion, you will notice below that some of the variables from namespace TrilinosWrappers are taken from namespace TrilinosWrappers::MPI (such as the right hand side vectors) whereas others are not (such as the various matrices). This is due to legacy reasons. We will frequently have to query velocities and temperatures at arbitrary quadrature points; consequently, rather than importing ghost information of a vector whenever we need access to degrees of freedom that are relevant locally but owned by another processor, we solve linear systems in parallel but then immediately initialize a vector including ghost entries of the solution for further processing. The various *_solution vectors are therefore filled immediately after solving their respective linear system in parallel and will always contain values for all locally relevant degrees of freedom; the fully distributed vectors that we obtain from the solution process and that only ever contain the locally owned degrees of freedom are destroyed immediately after the solution process and after we have copied the relevant values into the member variable vectors.\\n\\n\\u00a0     parallel::distributed::Triangulation<dim> triangulation;\\n\\u00a0     double                                    global_Omega_diameter;\\n\\u00a0 \\n\\u00a0     const MappingQ<dim> mapping;\\n\\u00a0 \\n\\u00a0     const FESystem<dim>       stokes_fe;\\n\\u00a0     DoFHandler<dim>           stokes_dof_handler;\\n\\u00a0     AffineConstraints<double> stokes_constraints;\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::BlockSparseMatrix stokes_matrix;\\n\\u00a0     TrilinosWrappers::BlockSparseMatrix stokes_preconditioner_matrix;\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::MPI::BlockVector stokes_solution;\\n\\u00a0     TrilinosWrappers::MPI::BlockVector old_stokes_solution;\\n\\u00a0     TrilinosWrappers::MPI::BlockVector stokes_rhs;\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     const FE_Q<dim>           temperature_fe;\\n\\u00a0     DoFHandler<dim>           temperature_dof_handler;\\n\\u00a0     AffineConstraints<double> temperature_constraints;\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::SparseMatrix temperature_mass_matrix;\\n\\u00a0     TrilinosWrappers::SparseMatrix temperature_stiffness_matrix;\\n\\u00a0     TrilinosWrappers::SparseMatrix temperature_matrix;\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::MPI::Vector temperature_solution;\\n\\u00a0     TrilinosWrappers::MPI::Vector old_temperature_solution;\\n\\u00a0     TrilinosWrappers::MPI::Vector old_old_temperature_solution;\\n\\u00a0     TrilinosWrappers::MPI::Vector temperature_rhs;\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     double       time_step;\\n\\u00a0     double       old_time_step;\\n\\u00a0     unsigned int timestep_number;\\n\\u00a0 \\n\\u00a0     std::shared_ptr<TrilinosWrappers::PreconditionAMG>    Amg_preconditioner;\\n\\u00a0     std::shared_ptr<TrilinosWrappers::PreconditionJacobi> Mp_preconditioner;\\n\\u00a0     std::shared_ptr<TrilinosWrappers::PreconditionJacobi> T_preconditioner;\\n\\u00a0 \\n\\u00a0     bool rebuild_stokes_matrix;\\n\\u00a0     bool rebuild_stokes_preconditioner;\\n\\u00a0     bool rebuild_temperature_matrices;\\n\\u00a0     bool rebuild_temperature_preconditioner;\\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nDoFHandlerDefinition dof_handler.h:317\\nFESystemDefinition fe_system.h:208\\nFE_QDefinition fe_q.h:554\\nMappingQDefinition mapping_q.h:110\\nTrilinosWrappers::MPI::BlockVectorDefinition trilinos_parallel_block_vector.h:75\\nTrilinosWrappers::MPI::VectorDefinition trilinos_vector.h:405\\nTrilinosWrappers::SparseMatrixDefinition trilinos_sparse_matrix.h:550\\nparallel::distributed::TriangulationDefinition tria.h:268\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\nThe next member variable, computing_timer is used to conveniently account for compute time spent in certain \\\"sections\\\" of the code that are repeatedly entered. For example, we will enter (and leave) sections for Stokes matrix assembly and would like to accumulate the run time spent in this section over all time steps. Every so many time steps as well as at the end of the program (through the destructor of the TimerOutput class) we will then produce a nice summary of the times spent in the different sections into which we categorize the run-time of this program.\\n\\u00a0     TimerOutput computing_timer;\\n\\u00a0 \\nTimerOutputDefinition timer.h:549\\nAfter these member variables we have a number of auxiliary functions that have been broken out of the ones listed above. Specifically, there are first three functions that we call from setup_dofs and then the ones that do the assembling of linear systems:\\n\\u00a0     void setup_stokes_matrix(\\n\\u00a0       const std::vector<IndexSet> &stokes_partitioning,\\n\\u00a0       const std::vector<IndexSet> &stokes_relevant_partitioning);\\n\\u00a0     void setup_stokes_preconditioner(\\n\\u00a0       const std::vector<IndexSet> &stokes_partitioning,\\n\\u00a0       const std::vector<IndexSet> &stokes_relevant_partitioning);\\n\\u00a0     void setup_temperature_matrices(\\n\\u00a0       const IndexSet &temperature_partitioning,\\n\\u00a0       const IndexSet &temperature_relevant_partitioning);\\n\\u00a0 \\n\\u00a0 \\nIndexSetDefinition index_set.h:70\\nFollowing the task-based parallelization paradigm, we split all the assembly routines into two parts: a first part that can do all the calculations on a certain cell without taking care of other threads, and a second part (which is writing the local data into the global matrices and vectors) which can be entered by only one thread at a time. In order to implement that, we provide functions for each of those two steps for all the four assembly routines that we use in this program. The following eight functions do exactly this:\\n\\u00a0     void local_assemble_stokes_preconditioner(\\n\\u00a0       const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0       Assembly::Scratch::StokesPreconditioner<dim>         &scratch,\\n\\u00a0       Assembly::CopyData::StokesPreconditioner<dim>        &data);\\n\\u00a0 \\n\\u00a0     void copy_local_to_global_stokes_preconditioner(\\n\\u00a0       const Assembly::CopyData::StokesPreconditioner<dim> &data);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     void local_assemble_stokes_system(\\n\\u00a0       const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0       Assembly::Scratch::StokesSystem<dim>                 &scratch,\\n\\u00a0       Assembly::CopyData::StokesSystem<dim>                &data);\\n\\u00a0 \\n\\u00a0     void copy_local_to_global_stokes_system(\\n\\u00a0       const Assembly::CopyData::StokesSystem<dim> &data);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     void local_assemble_temperature_matrix(\\n\\u00a0       const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0       Assembly::Scratch::TemperatureMatrix<dim>            &scratch,\\n\\u00a0       Assembly::CopyData::TemperatureMatrix<dim>           &data);\\n\\u00a0 \\n\\u00a0     void copy_local_to_global_temperature_matrix(\\n\\u00a0       const Assembly::CopyData::TemperatureMatrix<dim> &data);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0     void local_assemble_temperature_rhs(\\n\\u00a0       const std::pair<double, double> global_T_range,\\n\\u00a0       const double                    global_max_velocity,\\n\\u00a0       const double                    global_entropy_variation,\\n\\u00a0       const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0       Assembly::Scratch::TemperatureRHS<dim>               &scratch,\\n\\u00a0       Assembly::CopyData::TemperatureRHS<dim>              &data);\\n\\u00a0 \\n\\u00a0     void copy_local_to_global_temperature_rhs(\\n\\u00a0       const Assembly::CopyData::TemperatureRHS<dim> &data);\\n\\u00a0 \\nDoFHandler::active_cell_iteratortypename ActiveSelector::active_cell_iterator active_cell_iteratorDefinition dof_handler.h:440\\nFinally, we forward declare a member class that we will define later on and that will be used to compute a number of quantities from our solution vectors that we'd like to put into the output files for visualization.\\n\\u00a0     class Postprocessor;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n BoussinesqFlowProblem class implementation\\n BoussinesqFlowProblem::Parameters\\nHere comes the definition of the parameters for the Stokes problem. We allow to set the end time for the simulation, the level of refinements (both global and adaptive, which in the sum specify what maximum level the cells are allowed to have), and the interval between refinements in the time stepping.\\nThen, we let the user specify constants for the stabilization parameters (as discussed in the introduction), the polynomial degree for the Stokes velocity space, whether to use the locally conservative discretization based on FE_DGP elements for the pressure or not (FE_Q elements for pressure), and the polynomial degree for the temperature interpolation.\\nThe constructor checks for a valid input file (if not, a file with default parameters for the quantities is written), and eventually parses the parameters.\\n\\u00a0   template <int dim>\\n\\u00a0   BoussinesqFlowProblem<dim>::Parameters::Parameters(\\n\\u00a0     const std::string &parameter_filename)\\n\\u00a0     : end_time(1e8)\\n\\u00a0     , initial_global_refinement(2)\\n\\u00a0     , initial_adaptive_refinement(2)\\n\\u00a0     , adaptive_refinement_interval(10)\\n\\u00a0     , stabilization_alpha(2)\\n\\u00a0     , stabilization_c_R(0.11)\\n\\u00a0     , stabilization_beta(0.078)\\n\\u00a0     , stokes_velocity_degree(2)\\n\\u00a0     , use_locally_conservative_discretization(true)\\n\\u00a0     , temperature_degree(2)\\n\\u00a0   {\\n\\u00a0     ParameterHandler prm;\\n\\u00a0     BoussinesqFlowProblem<dim>::Parameters::declare_parameters(prm);\\n\\u00a0 \\n\\u00a0     std::ifstream parameter_file(parameter_filename);\\n\\u00a0 \\n\\u00a0     if (!parameter_file)\\n\\u00a0       {\\n\\u00a0         parameter_file.close();\\n\\u00a0 \\n\\u00a0         std::ofstream parameter_out(parameter_filename);\\n\\u00a0         prm.print_parameters(parameter_out, ParameterHandler::Text);\\n\\u00a0 \\n\\u00a0         AssertThrow(\\n\\u00a0           false,\\n\\u00a0           ExcMessage(\\n\\u00a0             \\\"Input parameter file <\\\" + parameter_filename +\\n\\u00a0             \\\"> not found. Creating a template file of the same name.\\\"));\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     prm.parse_input(parameter_file);\\n\\u00a0     parse_parameters(prm);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nParameterHandler::Text@ TextDefinition parameter_handler.h:903\\nAssertThrow#define AssertThrow(cond, exc)Definition exceptions.h:1739\\nNext we have a function that declares the parameters that we expect in the input file, together with their data types, default values and a description:\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::Parameters::declare_parameters(\\n\\u00a0     ParameterHandler &prm)\\n\\u00a0   {\\n\\u00a0     prm.declare_entry(\\\"End time\\\",\\n\\u00a0                       \\\"1e8\\\",\\n\\u00a0                       Patterns::Double(0),\\n\\u00a0                       \\\"The end time of the simulation in years.\\\");\\n\\u00a0     prm.declare_entry(\\\"Initial global refinement\\\",\\n\\u00a0                       \\\"2\\\",\\n\\u00a0                       Patterns::Integer(0),\\n\\u00a0                       \\\"The number of global refinement steps performed on \\\"\\n\\u00a0                       \\\"the initial coarse mesh, before the problem is first \\\"\\n\\u00a0                       \\\"solved there.\\\");\\n\\u00a0     prm.declare_entry(\\\"Initial adaptive refinement\\\",\\n\\u00a0                       \\\"2\\\",\\n\\u00a0                       Patterns::Integer(0),\\n\\u00a0                       \\\"The number of adaptive refinement steps performed after \\\"\\n\\u00a0                       \\\"initial global refinement.\\\");\\n\\u00a0     prm.declare_entry(\\\"Time steps between mesh refinement\\\",\\n\\u00a0                       \\\"10\\\",\\n\\u00a0                       Patterns::Integer(1),\\n\\u00a0                       \\\"The number of time steps after which the mesh is to be \\\"\\n\\u00a0                       \\\"adapted based on computed error indicators.\\\");\\n\\u00a0     prm.declare_entry(\\\"Generate graphical output\\\",\\n\\u00a0                       \\\"false\\\",\\n\\u00a0                       Patterns::Bool(),\\n\\u00a0                       \\\"Whether graphical output is to be generated or not. \\\"\\n\\u00a0                       \\\"You may not want to get graphical output if the number \\\"\\n\\u00a0                       \\\"of processors is large.\\\");\\n\\u00a0     prm.declare_entry(\\\"Time steps between graphical output\\\",\\n\\u00a0                       \\\"50\\\",\\n\\u00a0                       Patterns::Integer(1),\\n\\u00a0                       \\\"The number of time steps between each generation of \\\"\\n\\u00a0                       \\\"graphical output files.\\\");\\n\\u00a0 \\n\\u00a0     prm.enter_subsection(\\\"Stabilization parameters\\\");\\n\\u00a0     {\\n\\u00a0       prm.declare_entry(\\\"alpha\\\",\\n\\u00a0                         \\\"2\\\",\\n\\u00a0                         Patterns::Double(1, 2),\\n\\u00a0                         \\\"The exponent in the entropy viscosity stabilization.\\\");\\n\\u00a0       prm.declare_entry(\\\"c_R\\\",\\n\\u00a0                         \\\"0.11\\\",\\n\\u00a0                         Patterns::Double(0),\\n\\u00a0                         \\\"The c_R factor in the entropy viscosity \\\"\\n\\u00a0                         \\\"stabilization.\\\");\\n\\u00a0       prm.declare_entry(\\\"beta\\\",\\n\\u00a0                         \\\"0.078\\\",\\n\\u00a0                         Patterns::Double(0),\\n\\u00a0                         \\\"The beta factor in the artificial viscosity \\\"\\n\\u00a0                         \\\"stabilization. An appropriate value for 2d is 0.052 \\\"\\n\\u00a0                         \\\"and 0.078 for 3d.\\\");\\n\\u00a0     }\\n\\u00a0     prm.leave_subsection();\\n\\u00a0 \\n\\u00a0     prm.enter_subsection(\\\"Discretization\\\");\\n\\u00a0     {\\n\\u00a0       prm.declare_entry(\\n\\u00a0         \\\"Stokes velocity polynomial degree\\\",\\n\\u00a0         \\\"2\\\",\\n\\u00a0         Patterns::Integer(1),\\n\\u00a0         \\\"The polynomial degree to use for the velocity variables \\\"\\n\\u00a0         \\\"in the Stokes system.\\\");\\n\\u00a0       prm.declare_entry(\\n\\u00a0         \\\"Temperature polynomial degree\\\",\\n\\u00a0         \\\"2\\\",\\n\\u00a0         Patterns::Integer(1),\\n\\u00a0         \\\"The polynomial degree to use for the temperature variable.\\\");\\n\\u00a0       prm.declare_entry(\\n\\u00a0         \\\"Use locally conservative discretization\\\",\\n\\u00a0         \\\"true\\\",\\n\\u00a0         Patterns::Bool(),\\n\\u00a0         \\\"Whether to use a Stokes discretization that is locally \\\"\\n\\u00a0         \\\"conservative at the expense of a larger number of degrees \\\"\\n\\u00a0         \\\"of freedom, or to go with a cheaper discretization \\\"\\n\\u00a0         \\\"that does not locally conserve mass (although it is \\\"\\n\\u00a0         \\\"globally conservative.\\\");\\n\\u00a0     }\\n\\u00a0     prm.leave_subsection();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nPatterns::BoolDefinition patterns.h:980\\nPatterns::DoubleDefinition patterns.h:291\\nPatterns::IntegerDefinition patterns.h:188\\nAnd then we need a function that reads the contents of the ParameterHandler object we get by reading the input file and puts the results into variables that store the values of the parameters we have previously declared:\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::Parameters::parse_parameters(\\n\\u00a0     ParameterHandler &prm)\\n\\u00a0   {\\n\\u00a0     end_time                  = prm.get_double(\\\"End time\\\");\\n\\u00a0     initial_global_refinement = prm.get_integer(\\\"Initial global refinement\\\");\\n\\u00a0     initial_adaptive_refinement =\\n\\u00a0       prm.get_integer(\\\"Initial adaptive refinement\\\");\\n\\u00a0 \\n\\u00a0     adaptive_refinement_interval =\\n\\u00a0       prm.get_integer(\\\"Time steps between mesh refinement\\\");\\n\\u00a0 \\n\\u00a0     generate_graphical_output = prm.get_bool(\\\"Generate graphical output\\\");\\n\\u00a0     graphical_output_interval =\\n\\u00a0       prm.get_integer(\\\"Time steps between graphical output\\\");\\n\\u00a0 \\n\\u00a0     prm.enter_subsection(\\\"Stabilization parameters\\\");\\n\\u00a0     {\\n\\u00a0       stabilization_alpha = prm.get_double(\\\"alpha\\\");\\n\\u00a0       stabilization_c_R   = prm.get_double(\\\"c_R\\\");\\n\\u00a0       stabilization_beta  = prm.get_double(\\\"beta\\\");\\n\\u00a0     }\\n\\u00a0     prm.leave_subsection();\\n\\u00a0 \\n\\u00a0     prm.enter_subsection(\\\"Discretization\\\");\\n\\u00a0     {\\n\\u00a0       stokes_velocity_degree =\\n\\u00a0         prm.get_integer(\\\"Stokes velocity polynomial degree\\\");\\n\\u00a0       temperature_degree = prm.get_integer(\\\"Temperature polynomial degree\\\");\\n\\u00a0       use_locally_conservative_discretization =\\n\\u00a0         prm.get_bool(\\\"Use locally conservative discretization\\\");\\n\\u00a0     }\\n\\u00a0     prm.leave_subsection();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n BoussinesqFlowProblem::BoussinesqFlowProblem\\nThe constructor of the problem is very similar to the constructor in step-31. What is different is the parallel communication: Trilinos uses a message passing interface (MPI) for data distribution. When entering the BoussinesqFlowProblem class, we have to decide how the parallelization is to be done. We choose a rather simple strategy and let all processors that are running the program work together, specified by the communicator MPI_COMM_WORLD. Next, we create the output stream (as we already did in step-18) that only generates output on the first MPI process and is completely forgetful on all others. The implementation of this idea is to check the process number when pcout gets a true argument, and it uses the std::cout stream for output. If we are one processor five, for instance, then we will give a false argument to pcout, which means that the output of that processor will not be printed. With the exception of the mapping object (for which we use polynomials of degree 4) all but the final member variable are exactly the same as in step-31.\\nThis final object, the TimerOutput object, is then told to restrict output to the pcout stream (processor 0), and then we specify that we want to get a summary table at the end of the program which shows us wallclock times (as opposed to CPU times). We will manually also request intermediate summaries every so many time steps in the run() function below.\\n\\u00a0   template <int dim>\\n\\u00a0   BoussinesqFlowProblem<dim>::BoussinesqFlowProblem(Parameters &parameters_)\\n\\u00a0     : parameters(parameters_)\\n\\u00a0     , pcout(std::cout, (Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0))\\n\\u00a0     ,\\n\\u00a0 \\n\\u00a0     triangulation(MPI_COMM_WORLD,\\n\\u00a0                   typename Triangulation<dim>::MeshSmoothing(\\n\\u00a0                     Triangulation<dim>::smoothing_on_refinement |\\n\\u00a0                     Triangulation<dim>::smoothing_on_coarsening))\\n\\u00a0     ,\\n\\u00a0 \\n\\u00a0     global_Omega_diameter(0.)\\n\\u00a0     ,\\n\\u00a0 \\n\\u00a0     mapping(4)\\n\\u00a0     ,\\n\\u00a0 \\n\\u00a0     stokes_fe(FE_Q<dim>(parameters.stokes_velocity_degree) ^ dim,\\n\\u00a0               (parameters.use_locally_conservative_discretization ?\\n\\u00a0                  static_cast<const FiniteElement<dim> &>(\\n\\u00a0                    FE_DGP<dim>(parameters.stokes_velocity_degree - 1)) :\\n\\u00a0                  static_cast<const FiniteElement<dim> &>(\\n\\u00a0                    FE_Q<dim>(parameters.stokes_velocity_degree - 1))))\\n\\u00a0     ,\\n\\u00a0 \\n\\u00a0     stokes_dof_handler(triangulation)\\n\\u00a0     ,\\n\\u00a0 \\n\\u00a0     temperature_fe(parameters.temperature_degree)\\n\\u00a0     , temperature_dof_handler(triangulation)\\n\\u00a0     ,\\n\\u00a0 \\n\\u00a0     time_step(0)\\n\\u00a0     , old_time_step(0)\\n\\u00a0     , timestep_number(0)\\n\\u00a0     , rebuild_stokes_matrix(true)\\n\\u00a0     , rebuild_stokes_preconditioner(true)\\n\\u00a0     , rebuild_temperature_matrices(true)\\n\\u00a0     , rebuild_temperature_preconditioner(true)\\n\\u00a0     ,\\n\\u00a0 \\n\\u00a0     computing_timer(MPI_COMM_WORLD,\\n\\u00a0                     pcout,\\n\\u00a0                     TimerOutput::summary,\\n\\u00a0                     TimerOutput::wall_times)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFE_DGPDefinition fe_dgp.h:311\\nTriangulationDefinition tria.h:1323\\nUtilitiesDefinition communication_pattern_base.h:30\\nstdSTL namespace.\\n The BoussinesqFlowProblem helper functions\\n BoussinesqFlowProblem::get_maximal_velocity\\nExcept for two small details, the function to compute the global maximum of the velocity is the same as in step-31. The first detail is actually common to all functions that implement loops over all cells in the triangulation: When operating in parallel, each processor can only work on a chunk of cells since each processor only has a certain part of the entire triangulation. This chunk of cells that we want to work on is identified via a so-called subdomain_id, as we also did in step-18. All we need to change is hence to perform the cell-related operations only on cells that are owned by the current process (as opposed to ghost or artificial cells), i.e. for which the subdomain id equals the number of the process ID. Since this is a commonly used operation, there is a shortcut for this operation: we can ask whether the cell is owned by the current processor using cell->is_locally_owned().\\nThe second difference is the way we calculate the maximum value. Before, we could simply have a double variable that we checked against on each quadrature point for each cell. Now, we have to be a bit more careful since each processor only operates on a subset of cells. What we do is to first let each processor calculate the maximum among its cells, and then do a global communication operation Utilities::MPI::max that computes the maximum value among all the maximum values of the individual processors. MPI provides such a call, but it's even simpler to use the respective function in namespace Utilities::MPI using the MPI communicator object since that will do the right thing even if we work without MPI and on a single machine only. The call to Utilities::MPI::max needs two arguments, namely the local maximum (input) and the MPI communicator, which is MPI_COMM_WORLD in this example.\\n\\u00a0   template <int dim>\\n\\u00a0   double BoussinesqFlowProblem<dim>::get_maximal_velocity() const\\n\\u00a0   {\\n\\u00a0     const QIterated<dim> quadrature_formula(QTrapezoid<1>(),\\n\\u00a0                                             parameters.stokes_velocity_degree);\\n\\u00a0     const unsigned int   n_q_points = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     FEValues<dim>               fe_values(mapping,\\n\\u00a0                             stokes_fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_values);\\n\\u00a0     std::vector<Tensor<1, dim>> velocity_values(n_q_points);\\n\\u00a0 \\n\\u00a0     const FEValuesExtractors::Vector velocities(0);\\n\\u00a0 \\n\\u00a0     double max_local_velocity = 0;\\n\\u00a0 \\n\\u00a0     for (const auto &cell : stokes_dof_handler.active_cell_iterators())\\n\\u00a0       if (cell->is_locally_owned())\\n\\u00a0         {\\n\\u00a0           fe_values.reinit(cell);\\n\\u00a0           fe_values[velocities].get_function_values(stokes_solution,\\n\\u00a0                                                     velocity_values);\\n\\u00a0 \\n\\u00a0           for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0             max_local_velocity =\\n\\u00a0               std::max(max_local_velocity, velocity_values[q].norm());\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0     return Utilities::MPI::max(max_local_velocity, MPI_COMM_WORLD);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nQIteratedDefinition quadrature.h:435\\nQTrapezoidDefinition quadrature_lib.h:191\\nUtilities::MPI::maxT max(const T &t, const MPI_Comm mpi_communicator)\\nFEValuesExtractors::VectorDefinition fe_values_extractors.h:150\\n BoussinesqFlowProblem::get_cfl_number\\nThe next function does something similar, but we now compute the CFL number, i.e., maximal velocity on a cell divided by the cell diameter. This number is necessary to determine the time step size, as we use a semi-explicit time stepping scheme for the temperature equation (see step-31 for a discussion). We compute it in the same way as above: Compute the local maximum over all locally owned cells, then exchange it via MPI to find the global maximum.\\n\\u00a0   template <int dim>\\n\\u00a0   double BoussinesqFlowProblem<dim>::get_cfl_number() const\\n\\u00a0   {\\n\\u00a0     const QIterated<dim> quadrature_formula(QTrapezoid<1>(),\\n\\u00a0                                             parameters.stokes_velocity_degree);\\n\\u00a0     const unsigned int   n_q_points = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     FEValues<dim>               fe_values(mapping,\\n\\u00a0                             stokes_fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_values);\\n\\u00a0     std::vector<Tensor<1, dim>> velocity_values(n_q_points);\\n\\u00a0 \\n\\u00a0     const FEValuesExtractors::Vector velocities(0);\\n\\u00a0 \\n\\u00a0     double max_local_cfl = 0;\\n\\u00a0 \\n\\u00a0     for (const auto &cell : stokes_dof_handler.active_cell_iterators())\\n\\u00a0       if (cell->is_locally_owned())\\n\\u00a0         {\\n\\u00a0           fe_values.reinit(cell);\\n\\u00a0           fe_values[velocities].get_function_values(stokes_solution,\\n\\u00a0                                                     velocity_values);\\n\\u00a0 \\n\\u00a0           double max_local_velocity = 1e-10;\\n\\u00a0           for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0             max_local_velocity =\\n\\u00a0               std::max(max_local_velocity, velocity_values[q].norm());\\n\\u00a0           max_local_cfl =\\n\\u00a0             std::max(max_local_cfl, max_local_velocity / cell->diameter());\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0     return Utilities::MPI::max(max_local_cfl, MPI_COMM_WORLD);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n BoussinesqFlowProblem::get_entropy_variation\\nNext comes the computation of the global entropy variation \\\\(\\\\|E(T)-\\\\bar{E}(T)\\\\|_\\\\infty\\\\) where the entropy \\\\(E\\\\) is defined as discussed in the introduction. This is needed for the evaluation of the stabilization in the temperature equation as explained in the introduction. The entropy variation is actually only needed if we use \\\\(\\\\alpha=2\\\\) as a power in the residual computation. The infinity norm is computed by the maxima over quadrature points, as usual in discrete computations.\\nIn order to compute this quantity, we first have to find the space-average \\\\(\\\\bar{E}(T)\\\\) and then evaluate the maximum. However, that means that we would need to perform two loops. We can avoid the overhead by noting that   \\\\(\\\\|E(T)-\\\\bar{E}(T)\\\\|_\\\\infty =\\n   \\\\max\\\\big(E_{\\\\textrm{max}}(T)-\\\\bar{E}(T),\\n   \\\\bar{E}(T)-E_{\\\\textrm{min}}(T)\\\\big)\\\\), i.e., the maximum out of the deviation from the average entropy in positive and negative directions. The four quantities we need for the latter formula (maximum entropy, minimum entropy, average entropy, area) can all be evaluated in the same loop over all cells, so we choose this simpler variant.\\n\\u00a0   template <int dim>\\n\\u00a0   double BoussinesqFlowProblem<dim>::get_entropy_variation(\\n\\u00a0     const double average_temperature) const\\n\\u00a0   {\\n\\u00a0     if (parameters.stabilization_alpha != 2)\\n\\u00a0       return 1.;\\n\\u00a0 \\n\\u00a0     const QGauss<dim>  quadrature_formula(parameters.temperature_degree + 1);\\n\\u00a0     const unsigned int n_q_points = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     FEValues<dim>       fe_values(temperature_fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_values | update_JxW_values);\\n\\u00a0     std::vector<double> old_temperature_values(n_q_points);\\n\\u00a0     std::vector<double> old_old_temperature_values(n_q_points);\\n\\u00a0 \\nQGaussDefinition quadrature_lib.h:40\\nIn the two functions above we computed the maximum of numbers that were all non-negative, so we knew that zero was certainly a lower bound. On the other hand, here we need to find the maximum deviation from the average value, i.e., we will need to know the maximal and minimal values of the entropy for which we don't a priori know the sign.\\nTo compute it, we can therefore start with the largest and smallest possible values we can store in a double precision number: The minimum is initialized with a bigger and the maximum with a smaller number than any one that is going to appear. We are then guaranteed that these numbers will be overwritten in the loop on the first cell or, if this processor does not own any cells, in the communication step at the latest. The following loop then computes the minimum and maximum local entropy as well as keeps track of the area/volume of the part of the domain we locally own and the integral over the entropy on it:\\n\\u00a0     double min_entropy = std::numeric_limits<double>::max(),\\n\\u00a0            max_entropy = -std::numeric_limits<double>::max(), area = 0,\\n\\u00a0            entropy_integrated = 0;\\n\\u00a0 \\n\\u00a0     for (const auto &cell : temperature_dof_handler.active_cell_iterators())\\n\\u00a0       if (cell->is_locally_owned())\\n\\u00a0         {\\n\\u00a0           fe_values.reinit(cell);\\n\\u00a0           fe_values.get_function_values(old_temperature_solution,\\n\\u00a0                                         old_temperature_values);\\n\\u00a0           fe_values.get_function_values(old_old_temperature_solution,\\n\\u00a0                                         old_old_temperature_values);\\n\\u00a0           for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0             {\\n\\u00a0               const double T =\\n\\u00a0                 (old_temperature_values[q] + old_old_temperature_values[q]) / 2;\\n\\u00a0               const double entropy =\\n\\u00a0                 ((T - average_temperature) * (T - average_temperature));\\n\\u00a0 \\n\\u00a0               min_entropy = std::min(min_entropy, entropy);\\n\\u00a0               max_entropy = std::max(max_entropy, entropy);\\n\\u00a0               area += fe_values.JxW(q);\\n\\u00a0               entropy_integrated += fe_values.JxW(q) * entropy;\\n\\u00a0             }\\n\\u00a0         }\\n\\u00a0 \\nstd::min::VectorizedArray< Number, width > min(const ::VectorizedArray< Number, width > &, const ::VectorizedArray< Number, width > &)Definition vectorization.h:6960\\nNow we only need to exchange data between processors: we need to sum the two integrals (area, entropy_integrated), and get the extrema for maximum and minimum. We could do this through four different data exchanges, but we can it with two: Utilities::MPI::sum also exists in a variant that takes an array of values that are all to be summed up. And we can also utilize the Utilities::MPI::max function by realizing that forming the minimum over the minimal entropies equals forming the negative of the maximum over the negative of the minimal entropies; this maximum can then be combined with forming the maximum over the maximal entropies.\\n\\u00a0     const double local_sums[2]   = {entropy_integrated, area},\\n\\u00a0                  local_maxima[2] = {-min_entropy, max_entropy};\\n\\u00a0     double global_sums[2], global_maxima[2];\\n\\u00a0 \\n\\u00a0     Utilities::MPI::sum(local_sums, MPI_COMM_WORLD, global_sums);\\n\\u00a0     Utilities::MPI::max(local_maxima, MPI_COMM_WORLD, global_maxima);\\n\\u00a0 \\nUtilities::MPI::sumT sum(const T &t, const MPI_Comm mpi_communicator)\\nHaving computed everything this way, we can then compute the average entropy and find the \\\\(L^\\\\infty\\\\) norm by taking the larger of the deviation of the maximum or minimum from the average:\\n\\u00a0     const double average_entropy = global_sums[0] / global_sums[1];\\n\\u00a0     const double entropy_diff    = std::max(global_maxima[1] - average_entropy,\\n\\u00a0                                          average_entropy - (-global_maxima[0]));\\n\\u00a0     return entropy_diff;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n BoussinesqFlowProblem::get_extrapolated_temperature_range\\nThe next function computes the minimal and maximal value of the extrapolated temperature over the entire domain. Again, this is only a slightly modified version of the respective function in step-31. As in the function above, we collect local minima and maxima and then compute the global extrema using the same trick as above.\\nAs already discussed in step-31, the function needs to distinguish between the first and all following time steps because it uses a higher order temperature extrapolation scheme when at least two previous time steps are available.\\n\\u00a0   template <int dim>\\n\\u00a0   std::pair<double, double>\\n\\u00a0   BoussinesqFlowProblem<dim>::get_extrapolated_temperature_range() const\\n\\u00a0   {\\n\\u00a0     const QIterated<dim> quadrature_formula(QTrapezoid<1>(),\\n\\u00a0                                             parameters.temperature_degree);\\n\\u00a0     const unsigned int   n_q_points = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     FEValues<dim>       fe_values(mapping,\\n\\u00a0                             temperature_fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_values);\\n\\u00a0     std::vector<double> old_temperature_values(n_q_points);\\n\\u00a0     std::vector<double> old_old_temperature_values(n_q_points);\\n\\u00a0 \\n\\u00a0     double min_local_temperature = std::numeric_limits<double>::max(),\\n\\u00a0            max_local_temperature = -std::numeric_limits<double>::max();\\n\\u00a0 \\n\\u00a0     if (timestep_number != 0)\\n\\u00a0       {\\n\\u00a0         for (const auto &cell : temperature_dof_handler.active_cell_iterators())\\n\\u00a0           if (cell->is_locally_owned())\\n\\u00a0             {\\n\\u00a0               fe_values.reinit(cell);\\n\\u00a0               fe_values.get_function_values(old_temperature_solution,\\n\\u00a0                                             old_temperature_values);\\n\\u00a0               fe_values.get_function_values(old_old_temperature_solution,\\n\\u00a0                                             old_old_temperature_values);\\n\\u00a0 \\n\\u00a0               for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0                 {\\n\\u00a0                   const double temperature =\\n\\u00a0                     (1. + time_step / old_time_step) *\\n\\u00a0                       old_temperature_values[q] -\\n\\u00a0                     time_step / old_time_step * old_old_temperature_values[q];\\n\\u00a0 \\n\\u00a0                   min_local_temperature =\\n\\u00a0                     std::min(min_local_temperature, temperature);\\n\\u00a0                   max_local_temperature =\\n\\u00a0                     std::max(max_local_temperature, temperature);\\n\\u00a0                 }\\n\\u00a0             }\\n\\u00a0       }\\n\\u00a0     else\\n\\u00a0       {\\n\\u00a0         for (const auto &cell : temperature_dof_handler.active_cell_iterators())\\n\\u00a0           if (cell->is_locally_owned())\\n\\u00a0             {\\n\\u00a0               fe_values.reinit(cell);\\n\\u00a0               fe_values.get_function_values(old_temperature_solution,\\n\\u00a0                                             old_temperature_values);\\n\\u00a0 \\n\\u00a0               for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0                 {\\n\\u00a0                   const double temperature = old_temperature_values[q];\\n\\u00a0 \\n\\u00a0                   min_local_temperature =\\n\\u00a0                     std::min(min_local_temperature, temperature);\\n\\u00a0                   max_local_temperature =\\n\\u00a0                     std::max(max_local_temperature, temperature);\\n\\u00a0                 }\\n\\u00a0             }\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     double local_extrema[2] = {-min_local_temperature, max_local_temperature};\\n\\u00a0     double global_extrema[2];\\n\\u00a0     Utilities::MPI::max(local_extrema, MPI_COMM_WORLD, global_extrema);\\n\\u00a0 \\n\\u00a0     return std::make_pair(-global_extrema[0], global_extrema[1]);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\ninternal::reinitvoid reinit(MatrixBlock< MatrixType > &v, const BlockSparsityPattern &p)Definition matrix_block.h:617\\n BoussinesqFlowProblem::compute_viscosity\\nThe function that calculates the viscosity is purely local and so needs no communication at all. It is mostly the same as in step-31 but with an updated formulation of the viscosity if \\\\(\\\\alpha=2\\\\) is chosen:\\n\\u00a0   template <int dim>\\n\\u00a0   double BoussinesqFlowProblem<dim>::compute_viscosity(\\n\\u00a0     const std::vector<double>                  &old_temperature,\\n\\u00a0     const std::vector<double>                  &old_old_temperature,\\n\\u00a0     const std::vector<Tensor<1, dim>>          &old_temperature_grads,\\n\\u00a0     const std::vector<Tensor<1, dim>>          &old_old_temperature_grads,\\n\\u00a0     const std::vector<double>                  &old_temperature_laplacians,\\n\\u00a0     const std::vector<double>                  &old_old_temperature_laplacians,\\n\\u00a0     const std::vector<Tensor<1, dim>>          &old_velocity_values,\\n\\u00a0     const std::vector<Tensor<1, dim>>          &old_old_velocity_values,\\n\\u00a0     const std::vector<SymmetricTensor<2, dim>> &old_strain_rates,\\n\\u00a0     const std::vector<SymmetricTensor<2, dim>> &old_old_strain_rates,\\n\\u00a0     const double                                global_u_infty,\\n\\u00a0     const double                                global_T_variation,\\n\\u00a0     const double                                average_temperature,\\n\\u00a0     const double                                global_entropy_variation,\\n\\u00a0     const double                                cell_diameter) const\\n\\u00a0   {\\n\\u00a0     if (global_u_infty == 0)\\n\\u00a0       return 5e-3 * cell_diameter;\\n\\u00a0 \\n\\u00a0     const unsigned int n_q_points = old_temperature.size();\\n\\u00a0 \\n\\u00a0     double max_residual = 0;\\n\\u00a0     double max_velocity = 0;\\n\\u00a0 \\n\\u00a0     for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0       {\\n\\u00a0         const Tensor<1, dim> u =\\n\\u00a0           (old_velocity_values[q] + old_old_velocity_values[q]) / 2;\\n\\u00a0 \\n\\u00a0         const SymmetricTensor<2, dim> strain_rate =\\n\\u00a0           (old_strain_rates[q] + old_old_strain_rates[q]) / 2;\\n\\u00a0 \\n\\u00a0         const double T = (old_temperature[q] + old_old_temperature[q]) / 2;\\n\\u00a0         const double dT_dt =\\n\\u00a0           (old_temperature[q] - old_old_temperature[q]) / old_time_step;\\n\\u00a0         const double u_grad_T =\\n\\u00a0           u * (old_temperature_grads[q] + old_old_temperature_grads[q]) / 2;\\n\\u00a0 \\n\\u00a0         const double kappa_Delta_T =\\n\\u00a0           EquationData::kappa *\\n\\u00a0           (old_temperature_laplacians[q] + old_old_temperature_laplacians[q]) /\\n\\u00a0           2;\\n\\u00a0         const double gamma =\\n\\u00a0           ((EquationData::radiogenic_heating * EquationData::density(T) +\\n\\u00a0             2 * EquationData::eta * strain_rate * strain_rate) /\\n\\u00a0            (EquationData::density(T) * EquationData::specific_heat));\\n\\u00a0 \\n\\u00a0         double residual = std::abs(dT_dt + u_grad_T - kappa_Delta_T - gamma);\\n\\u00a0         if (parameters.stabilization_alpha == 2)\\n\\u00a0           residual *= std::abs(T - average_temperature);\\n\\u00a0 \\n\\u00a0         max_residual = std::max(residual, max_residual);\\n\\u00a0         max_velocity = std::max(std::sqrt(u * u), max_velocity);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     const double max_viscosity =\\n\\u00a0       (parameters.stabilization_beta * max_velocity * cell_diameter);\\n\\u00a0     if (timestep_number == 0)\\n\\u00a0       return max_viscosity;\\n\\u00a0     else\\n\\u00a0       {\\n\\u00a0         Assert(old_time_step > 0, ExcInternalError());\\n\\u00a0 \\n\\u00a0         double entropy_viscosity;\\n\\u00a0         if (parameters.stabilization_alpha == 2)\\n\\u00a0           entropy_viscosity =\\n\\u00a0             (parameters.stabilization_c_R * cell_diameter * cell_diameter *\\n\\u00a0              max_residual / global_entropy_variation);\\n\\u00a0         else\\n\\u00a0           entropy_viscosity =\\n\\u00a0             (parameters.stabilization_c_R * cell_diameter *\\n\\u00a0              global_Omega_diameter * max_velocity * max_residual /\\n\\u00a0              (global_u_infty * global_T_variation));\\n\\u00a0 \\n\\u00a0         return std::min(max_viscosity, entropy_viscosity);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nAssert#define Assert(cond, exc)Definition exceptions.h:1638\\nstd::sqrt::VectorizedArray< Number, width > sqrt(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6869\\n The BoussinesqFlowProblem setup functions\\nThe following three functions set up the Stokes matrix, the matrix used for the Stokes preconditioner, and the temperature matrix. The code is mostly the same as in step-31, but it has been broken out into three functions of their own for simplicity.\\nThe main functional difference between the code here and that in step-31 is that the matrices we want to set up are distributed across multiple processors. Since we still want to build up the sparsity pattern first for efficiency reasons, we could continue to build the entire sparsity pattern as a BlockDynamicSparsityPattern, as we did in step-31. However, that would be inefficient: every processor would build the same sparsity pattern, but only initialize a small part of the matrix using it. It also violates the principle that every processor should only work on those cells it owns (and, if necessary the layer of ghost cells around it).\\nRather, we use an object of type TrilinosWrappers::BlockSparsityPattern, which is (obviously) a wrapper around a sparsity pattern object provided by Trilinos. The advantage is that the Trilinos sparsity pattern class can communicate across multiple processors: if this processor fills in all the nonzero entries that result from the cells it owns, and every other processor does so as well, then at the end after some MPI communication initiated by the compress() call, we will have the globally assembled sparsity pattern available with which the global matrix can be initialized.\\nThere is one important aspect when initializing Trilinos sparsity patterns in parallel: In addition to specifying the locally owned rows and columns of the matrices via the stokes_partitioning index set, we also supply information about all the rows we are possibly going to write into when assembling on a certain processor. The set of locally relevant rows contains all such rows (possibly also a few unnecessary ones, but it is difficult to find the exact row indices before actually getting indices on all cells and resolving constraints). This additional information allows to exactly determine the structure for the off-processor data found during assembly. While Trilinos matrices are able to collect this information on the fly as well (when initializing them from some other reinit method), it is less efficient and leads to problems when assembling matrices with multiple threads. In this program, we pessimistically assume that only one processor at a time can write into the matrix while assembly (whereas the computation is parallel), which is fine for Trilinos matrices. In practice, one can do better by hinting WorkStream at cells that do not share vertices, allowing for parallelism among those cells (see the graph coloring algorithms and WorkStream with colored iterators argument). However, that only works when only one MPI processor is present because Trilinos' internal data structures for accumulating off-processor data on the fly are not thread safe. With the initialization presented here, there is no such problem and one could safely introduce graph coloring for this algorithm.\\nThe only other change we need to make is to tell the DoFTools::make_sparsity_pattern() function that it is only supposed to work on a subset of cells, namely the ones whose subdomain_id equals the number of the current processor, and to ignore all other cells.\\nThis strategy is replicated across all three of the following functions.\\nNote that Trilinos matrices store the information contained in the sparsity patterns, so we can safely release the sp variable once the matrix has been given the sparsity structure.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::setup_stokes_matrix(\\n\\u00a0     const std::vector<IndexSet> &stokes_partitioning,\\n\\u00a0     const std::vector<IndexSet> &stokes_relevant_partitioning)\\n\\u00a0   {\\n\\u00a0     stokes_matrix.clear();\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::BlockSparsityPattern sp(stokes_partitioning,\\n\\u00a0                                               stokes_partitioning,\\n\\u00a0                                               stokes_relevant_partitioning,\\n\\u00a0                                               MPI_COMM_WORLD);\\n\\u00a0 \\n\\u00a0     Table<2, DoFTools::Coupling> coupling(dim + 1, dim + 1);\\n\\u00a0     for (unsigned int c = 0; c < dim + 1; ++c)\\n\\u00a0       for (unsigned int d = 0; d < dim + 1; ++d)\\n\\u00a0         if (!((c == dim) && (d == dim)))\\n\\u00a0           coupling[c][d] = DoFTools::always;\\n\\u00a0         else\\n\\u00a0           coupling[c][d] = DoFTools::none;\\n\\u00a0 \\n\\u00a0     DoFTools::make_sparsity_pattern(stokes_dof_handler,\\n\\u00a0                                     coupling,\\n\\u00a0                                     sp,\\n\\u00a0                                     stokes_constraints,\\n\\u00a0                                     false,\\n\\u00a0                                     Utilities::MPI::this_mpi_process(\\n\\u00a0                                       MPI_COMM_WORLD));\\n\\u00a0     sp.compress();\\n\\u00a0 \\n\\u00a0     stokes_matrix.reinit(sp);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::setup_stokes_preconditioner(\\n\\u00a0     const std::vector<IndexSet> &stokes_partitioning,\\n\\u00a0     const std::vector<IndexSet> &stokes_relevant_partitioning)\\n\\u00a0   {\\n\\u00a0     Amg_preconditioner.reset();\\n\\u00a0     Mp_preconditioner.reset();\\n\\u00a0 \\n\\u00a0     stokes_preconditioner_matrix.clear();\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::BlockSparsityPattern sp(stokes_partitioning,\\n\\u00a0                                               stokes_partitioning,\\n\\u00a0                                               stokes_relevant_partitioning,\\n\\u00a0                                               MPI_COMM_WORLD);\\n\\u00a0 \\n\\u00a0     Table<2, DoFTools::Coupling> coupling(dim + 1, dim + 1);\\n\\u00a0     for (unsigned int c = 0; c < dim + 1; ++c)\\n\\u00a0       for (unsigned int d = 0; d < dim + 1; ++d)\\n\\u00a0         if (c == d)\\n\\u00a0           coupling[c][d] = DoFTools::always;\\n\\u00a0         else\\n\\u00a0           coupling[c][d] = DoFTools::none;\\n\\u00a0 \\n\\u00a0     DoFTools::make_sparsity_pattern(stokes_dof_handler,\\n\\u00a0                                     coupling,\\n\\u00a0                                     sp,\\n\\u00a0                                     stokes_constraints,\\n\\u00a0                                     false,\\n\\u00a0                                     Utilities::MPI::this_mpi_process(\\n\\u00a0                                       MPI_COMM_WORLD));\\n\\u00a0     sp.compress();\\n\\u00a0 \\n\\u00a0     stokes_preconditioner_matrix.reinit(sp);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::setup_temperature_matrices(\\n\\u00a0     const IndexSet &temperature_partitioner,\\n\\u00a0     const IndexSet &temperature_relevant_partitioner)\\n\\u00a0   {\\n\\u00a0     T_preconditioner.reset();\\n\\u00a0     temperature_mass_matrix.clear();\\n\\u00a0     temperature_stiffness_matrix.clear();\\n\\u00a0     temperature_matrix.clear();\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::SparsityPattern sp(temperature_partitioner,\\n\\u00a0                                          temperature_partitioner,\\n\\u00a0                                          temperature_relevant_partitioner,\\n\\u00a0                                          MPI_COMM_WORLD);\\n\\u00a0     DoFTools::make_sparsity_pattern(temperature_dof_handler,\\n\\u00a0                                     sp,\\n\\u00a0                                     temperature_constraints,\\n\\u00a0                                     false,\\n\\u00a0                                     Utilities::MPI::this_mpi_process(\\n\\u00a0                                       MPI_COMM_WORLD));\\n\\u00a0     sp.compress();\\n\\u00a0 \\n\\u00a0     temperature_matrix.reinit(sp);\\n\\u00a0     temperature_mass_matrix.reinit(sp);\\n\\u00a0     temperature_stiffness_matrix.reinit(sp);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTableDefinition array_view.h:39\\nTrilinosWrappers::BlockSparsityPatternDefinition block_sparsity_pattern.h:665\\nTrilinosWrappers::SparsityPatternDefinition trilinos_sparsity_pattern.h:275\\nDoFTools::make_sparsity_patternvoid make_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, SparsityPatternBase &sparsity_pattern, const AffineConstraints< number > &constraints={}, const bool keep_constrained_dofs=true, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id)Definition dof_tools_sparsity.cc:56\\nDoFTools::none@ noneDefinition dof_tools.h:243\\nDoFTools::always@ alwaysDefinition dof_tools.h:247\\nUtilities::MPI::this_mpi_processunsigned int this_mpi_process(const MPI_Comm mpi_communicator)Definition mpi.cc:107\\nThe remainder of the setup function (after splitting out the three functions above) mostly has to deal with the things we need to do for parallelization across processors. Because setting all of this up is a significant compute time expense of the program, we put everything we do here into a timer group so that we can get summary information about the fraction of time spent in this part of the program at its end.\\nAt the top as usual we enumerate degrees of freedom and sort them by component/block, followed by writing their numbers to the screen from processor zero. The DoFHandler::distributed_dofs() function, when applied to a parallel::distributed::Triangulation object, sorts degrees of freedom in such a way that all degrees of freedom associated with subdomain zero come before all those associated with subdomain one, etc. For the Stokes part, this entails, however, that velocities and pressures become intermixed, but this is trivially solved by sorting again by blocks; it is worth noting that this latter operation leaves the relative ordering of all velocities and pressures alone, i.e. within the velocity block we will still have all those associated with subdomain zero before all velocities associated with subdomain one, etc. This is important since we store each of the blocks of this matrix distributed across all processors and want this to be done in such a way that each processor stores that part of the matrix that is roughly equal to the degrees of freedom located on those cells that it will actually work on.\\nWhen printing the numbers of degrees of freedom, note that these numbers are going to be large if we use many processors. Consequently, we let the stream put a comma separator in between every three digits. The state of the stream, using the locale, is saved from before to after this operation. While slightly opaque, the code works because the default locale (which we get using the constructor call std::locale(\\\"\\\")) implies printing numbers with a comma separator for every third digit (i.e., thousands, millions, billions).\\nIn this function as well as many below, we measure how much time we spend here and collect that in a section called \\\"Setup dof\\n   systems\\\" across function invocations. This is done using an TimerOutput::Scope object that gets a timer going in the section with above name of the computing_timer object upon construction of the local variable; the timer is stopped again when the destructor of the timing_section variable is called. This, of course, happens either at the end of the function, or if we leave the function through a return statement or when an exception is thrown somewhere \\u2013 in other words, whenever we leave this function in any way. The use of such \\\"scope\\\" objects therefore makes sure that we do not have to manually add code that tells the timer to stop at every location where this function may be left.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::setup_dofs()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope timing_section(computing_timer, \\\"Setup dof systems\\\");\\n\\u00a0 \\n\\u00a0     stokes_dof_handler.distribute_dofs(stokes_fe);\\n\\u00a0 \\n\\u00a0     std::vector<unsigned int> stokes_sub_blocks(dim + 1, 0);\\n\\u00a0     stokes_sub_blocks[dim] = 1;\\n\\u00a0     DoFRenumbering::component_wise(stokes_dof_handler, stokes_sub_blocks);\\n\\u00a0 \\n\\u00a0     temperature_dof_handler.distribute_dofs(temperature_fe);\\n\\u00a0 \\n\\u00a0     const std::vector<types::global_dof_index> stokes_dofs_per_block =\\n\\u00a0       DoFTools::count_dofs_per_fe_block(stokes_dof_handler, stokes_sub_blocks);\\n\\u00a0 \\n\\u00a0     const types::global_dof_index n_u = stokes_dofs_per_block[0],\\n\\u00a0                                   n_p = stokes_dofs_per_block[1],\\n\\u00a0                                   n_T = temperature_dof_handler.n_dofs();\\n\\u00a0 \\n\\u00a0     std::locale s = pcout.get_stream().getloc();\\n\\u00a0     pcout.get_stream().imbue(std::locale(\\\"\\\"));\\n\\u00a0     pcout << \\\"Number of active cells: \\\" << triangulation.n_global_active_cells()\\n\\u00a0           << \\\" (on \\\" << triangulation.n_levels() << \\\" levels)\\\" << std::endl\\n\\u00a0           << \\\"Number of degrees of freedom: \\\" << n_u + n_p + n_T << \\\" (\\\" << n_u\\n\\u00a0           << '+' << n_p << '+' << n_T << ')' << std::endl\\n\\u00a0           << std::endl;\\n\\u00a0     pcout.get_stream().imbue(s);\\n\\u00a0 \\n\\u00a0 \\nTimerOutput::ScopeDefinition timer.h:557\\nTriangulation::n_levelsunsigned int n_levels() const\\nparallel::TriangulationBase::n_global_active_cellsvirtual types::global_cell_index n_global_active_cells() const overrideDefinition tria_base.cc:151\\nunsigned int\\nDoFRenumbering::component_wisevoid component_wise(DoFHandler< dim, spacedim > &dof_handler, const std::vector< unsigned int > &target_component=std::vector< unsigned int >())Definition dof_renumbering.cc:666\\nDoFTools::count_dofs_per_fe_blockstd::vector< types::global_dof_index > count_dofs_per_fe_block(const DoFHandler< dim, spacedim > &dof, const std::vector< unsigned int > &target_block=std::vector< unsigned int >())Definition dof_tools.cc:2104\\nAfter this, we have to set up the various partitioners (of type IndexSet, see the introduction) that describe which parts of each matrix or vector will be stored where, then call the functions that actually set up the matrices, and at the end also resize the various vectors we keep around in this program.\\n\\u00a0     const IndexSet &stokes_locally_owned_index_set =\\n\\u00a0       stokes_dof_handler.locally_owned_dofs();\\n\\u00a0     const IndexSet stokes_locally_relevant_set =\\n\\u00a0       DoFTools::extract_locally_relevant_dofs(stokes_dof_handler);\\n\\u00a0 \\n\\u00a0     std::vector<IndexSet> stokes_partitioning;\\n\\u00a0     stokes_partitioning.push_back(\\n\\u00a0       stokes_locally_owned_index_set.get_view(0, n_u));\\n\\u00a0     stokes_partitioning.push_back(\\n\\u00a0       stokes_locally_owned_index_set.get_view(n_u, n_u + n_p));\\n\\u00a0 \\n\\u00a0     std::vector<IndexSet> stokes_relevant_partitioning;\\n\\u00a0     stokes_relevant_partitioning.push_back(\\n\\u00a0       stokes_locally_relevant_set.get_view(0, n_u));\\n\\u00a0     stokes_relevant_partitioning.push_back(\\n\\u00a0       stokes_locally_relevant_set.get_view(n_u, n_u + n_p));\\n\\u00a0 \\n\\u00a0     const IndexSet temperature_partitioning =\\n\\u00a0       temperature_dof_handler.locally_owned_dofs();\\n\\u00a0     const IndexSet temperature_relevant_partitioning =\\n\\u00a0       DoFTools::extract_locally_relevant_dofs(temperature_dof_handler);\\n\\u00a0 \\nDoFTools::extract_locally_relevant_dofsIndexSet extract_locally_relevant_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1164\\nFollowing this, we can compute constraints for the solution vectors, including hanging node constraints and homogeneous and inhomogeneous boundary values for the Stokes and temperature fields. Note that as for everything else, the constraint objects can not hold all constraints on every processor. Rather, each processor needs to store only those that are actually necessary for correctness given that it only assembles linear systems on cells it owns. As discussed in the this paper, the set of constraints we need to know about is exactly the set of constraints on all locally relevant degrees of freedom, so this is what we use to initialize the constraint objects.\\n\\u00a0     {\\n\\u00a0       stokes_constraints.clear();\\n\\u00a0       stokes_constraints.reinit(stokes_locally_owned_index_set,\\n\\u00a0                                 stokes_locally_relevant_set);\\n\\u00a0 \\n\\u00a0       DoFTools::make_hanging_node_constraints(stokes_dof_handler,\\n\\u00a0                                               stokes_constraints);\\n\\u00a0 \\n\\u00a0       const FEValuesExtractors::Vector velocity_components(0);\\n\\u00a0       VectorTools::interpolate_boundary_values(\\n\\u00a0         stokes_dof_handler,\\n\\u00a0         0,\\n\\u00a0         Functions::ZeroFunction<dim>(dim + 1),\\n\\u00a0         stokes_constraints,\\n\\u00a0         stokes_fe.component_mask(velocity_components));\\n\\u00a0 \\n\\u00a0       std::set<types::boundary_id> no_normal_flux_boundaries;\\n\\u00a0       no_normal_flux_boundaries.insert(1);\\n\\u00a0       VectorTools::compute_no_normal_flux_constraints(stokes_dof_handler,\\n\\u00a0                                                       0,\\n\\u00a0                                                       no_normal_flux_boundaries,\\n\\u00a0                                                       stokes_constraints,\\n\\u00a0                                                       mapping);\\n\\u00a0       stokes_constraints.close();\\n\\u00a0     }\\n\\u00a0     {\\n\\u00a0       temperature_constraints.clear();\\n\\u00a0       temperature_constraints.reinit(temperature_partitioning,\\n\\u00a0                                      temperature_relevant_partitioning);\\n\\u00a0 \\n\\u00a0       DoFTools::make_hanging_node_constraints(temperature_dof_handler,\\n\\u00a0                                               temperature_constraints);\\n\\u00a0       VectorTools::interpolate_boundary_values(\\n\\u00a0         temperature_dof_handler,\\n\\u00a0         0,\\n\\u00a0         EquationData::TemperatureInitialValues<dim>(),\\n\\u00a0         temperature_constraints);\\n\\u00a0       VectorTools::interpolate_boundary_values(\\n\\u00a0         temperature_dof_handler,\\n\\u00a0         1,\\n\\u00a0         EquationData::TemperatureInitialValues<dim>(),\\n\\u00a0         temperature_constraints);\\n\\u00a0       temperature_constraints.close();\\n\\u00a0     }\\n\\u00a0 \\nFunctions::ZeroFunctionDefinition function.h:510\\nDoFTools::make_hanging_node_constraintsvoid make_hanging_node_constraints(const DoFHandler< dim, spacedim > &dof_handler, AffineConstraints< number > &constraints)Definition dof_tools_constraints.cc:3073\\nVectorTools::compute_no_normal_flux_constraintsvoid compute_no_normal_flux_constraints(const DoFHandler< dim, spacedim > &dof_handler, const unsigned int first_vector_component, const std::set< types::boundary_id > &boundary_ids, AffineConstraints< number > &constraints, const Mapping< dim, spacedim > &mapping=(ReferenceCells::get_hypercube< dim >() .template get_default_linear_mapping< dim, spacedim >()), const bool use_manifold_for_normal=true)\\nVectorTools::interpolate_boundary_valuesvoid interpolate_boundary_values(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const std::map< types::boundary_id, const Function< spacedim, number > * > &function_map, std::map< types::global_dof_index, number > &boundary_values, const ComponentMask &component_mask={})\\nAll this done, we can then initialize the various matrix and vector objects to their proper sizes. At the end, we also record that all matrices and preconditioners have to be re-computed at the beginning of the next time step. Note how we initialize the vectors for the Stokes and temperature right hand sides: These are writable vectors (last boolean argument set to true) that have the correct one-to-one partitioning of locally owned elements but are still given the relevant partitioning for means of figuring out the vector entries that are going to be set right away. As for matrices, this allows for writing local contributions into the vector with multiple threads (always assuming that the same vector entry is not accessed by multiple threads at the same time). The other vectors only allow for read access of individual elements, including ghosts, but are not suitable for solvers.\\n\\u00a0     setup_stokes_matrix(stokes_partitioning, stokes_relevant_partitioning);\\n\\u00a0     setup_stokes_preconditioner(stokes_partitioning,\\n\\u00a0                                 stokes_relevant_partitioning);\\n\\u00a0     setup_temperature_matrices(temperature_partitioning,\\n\\u00a0                                temperature_relevant_partitioning);\\n\\u00a0 \\n\\u00a0     stokes_rhs.reinit(stokes_partitioning,\\n\\u00a0                       stokes_relevant_partitioning,\\n\\u00a0                       MPI_COMM_WORLD,\\n\\u00a0                       true);\\n\\u00a0     stokes_solution.reinit(stokes_relevant_partitioning, MPI_COMM_WORLD);\\n\\u00a0     old_stokes_solution.reinit(stokes_solution);\\n\\u00a0 \\n\\u00a0     temperature_rhs.reinit(temperature_partitioning,\\n\\u00a0                            temperature_relevant_partitioning,\\n\\u00a0                            MPI_COMM_WORLD,\\n\\u00a0                            true);\\n\\u00a0     temperature_solution.reinit(temperature_relevant_partitioning,\\n\\u00a0                                 MPI_COMM_WORLD);\\n\\u00a0     old_temperature_solution.reinit(temperature_solution);\\n\\u00a0     old_old_temperature_solution.reinit(temperature_solution);\\n\\u00a0 \\n\\u00a0     rebuild_stokes_matrix              = true;\\n\\u00a0     rebuild_stokes_preconditioner      = true;\\n\\u00a0     rebuild_temperature_matrices       = true;\\n\\u00a0     rebuild_temperature_preconditioner = true;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n The BoussinesqFlowProblem assembly functions\\nFollowing the discussion in the introduction and in the Parallel computing with multiple processors accessing shared memory topic, we split the assembly functions into different parts:\\n\\n\\nThe local calculations of matrices and right hand sides, given a certain cell as input (these functions are named local_assemble_* below). The resulting function is, in other words, essentially the body of the loop over all cells in step-31. Note, however, that these functions store the result from the local calculations in variables of classes from the CopyData namespace.\\n\\n\\n\\nThese objects are then given to the second step which writes the local data into the global data structures (these functions are named copy_local_to_global_* below). These functions are pretty trivial.\\n\\n\\n\\nThese two subfunctions are then used in the respective assembly routine (called assemble_* below), where a WorkStream object is set up and runs over all the cells that belong to the processor's subdomain. \\n\\n Stokes preconditioner assembly\\nLet us start with the functions that builds the Stokes preconditioner. The first two of these are pretty trivial, given the discussion above. Note in particular that the main point in using the scratch data object is that we want to avoid allocating any objects on the free space each time we visit a new cell. As a consequence, the assembly function below only has automatic local variables, and everything else is accessed through the scratch data object, which is allocated only once before we start the loop over all cells:\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::local_assemble_stokes_preconditioner(\\n\\u00a0     const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0     Assembly::Scratch::StokesPreconditioner<dim>         &scratch,\\n\\u00a0     Assembly::CopyData::StokesPreconditioner<dim>        &data)\\n\\u00a0   {\\n\\u00a0     const unsigned int dofs_per_cell = stokes_fe.n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points =\\n\\u00a0       scratch.stokes_fe_values.n_quadrature_points;\\n\\u00a0 \\n\\u00a0     const FEValuesExtractors::Vector velocities(0);\\n\\u00a0     const FEValuesExtractors::Scalar pressure(dim);\\n\\u00a0 \\n\\u00a0     scratch.stokes_fe_values.reinit(cell);\\n\\u00a0     cell->get_dof_indices(data.local_dof_indices);\\n\\u00a0 \\n\\u00a0     data.local_matrix = 0;\\n\\u00a0 \\n\\u00a0     for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0       {\\n\\u00a0         for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n\\u00a0           {\\n\\u00a0             scratch.grad_phi_u[k] =\\n\\u00a0               scratch.stokes_fe_values[velocities].gradient(k, q);\\n\\u00a0             scratch.phi_p[k] = scratch.stokes_fe_values[pressure].value(k, q);\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0           for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0             data.local_matrix(i, j) +=\\n\\u00a0               (EquationData::eta *\\n\\u00a0                  scalar_product(scratch.grad_phi_u[i], scratch.grad_phi_u[j]) +\\n\\u00a0                (1. / EquationData::eta) * EquationData::pressure_scaling *\\n\\u00a0                  EquationData::pressure_scaling *\\n\\u00a0                  (scratch.phi_p[i] * scratch.phi_p[j])) *\\n\\u00a0               scratch.stokes_fe_values.JxW(q);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::copy_local_to_global_stokes_preconditioner(\\n\\u00a0     const Assembly::CopyData::StokesPreconditioner<dim> &data)\\n\\u00a0   {\\n\\u00a0     stokes_constraints.distribute_local_to_global(data.local_matrix,\\n\\u00a0                                                   data.local_dof_indices,\\n\\u00a0                                                   stokes_preconditioner_matrix);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nFEValuesExtractors::ScalarDefinition fe_values_extractors.h:95\\nNow for the function that actually puts things together, using the WorkStream functions. WorkStream::run needs a start and end iterator to enumerate the cells it is supposed to work on. Typically, one would use DoFHandler::begin_active() and DoFHandler::end() for that but here we actually only want the subset of cells that in fact are owned by the current processor. This is where the FilteredIterator class comes into play: you give it a range of cells and it provides an iterator that only iterates over that subset of cells that satisfy a certain predicate (a predicate is a function of one argument that either returns true or false). The predicate we use here is IteratorFilters::LocallyOwnedCell, i.e., it returns true exactly if the cell is owned by the current processor. The resulting iterator range is then exactly what we need.\\nWith this obstacle out of the way, we call the WorkStream::run function with this set of cells, scratch and copy objects, and with pointers to two functions: the local assembly and copy-local-to-global function. These functions need to have very specific signatures: three arguments in the first and one argument in the latter case (see the documentation of the WorkStream::run function for the meaning of these arguments). Note how we use a lambda functions to create a function object that satisfies this requirement. It uses function arguments for the local assembly function that specify cell, scratch data, and copy data, as well as function argument for the copy function that expects the data to be written into the global matrix (also see the discussion in step-13's assemble_linear_system() function). On the other hand, the implicit zeroth argument of member functions (namely the this pointer of the object on which that member function is to operate on) is bound to the this pointer of the current function and is captured. The WorkStream::run function, as a consequence, does not need to know anything about the object these functions work on.\\nWhen the WorkStream is executed, it will create several local assembly routines of the first kind for several cells and let some available processors work on them. The function that needs to be synchronized, i.e., the write operation into the global matrix, however, is executed by only one thread at a time in the prescribed order. Of course, this only holds for the parallelization on a single MPI process. Different MPI processes will have their own WorkStream objects and do that work completely independently (and in different memory spaces). In a distributed calculation, some data will accumulate at degrees of freedom that are not owned by the respective processor. It would be inefficient to send data around every time we encounter such a dof. What happens instead is that the Trilinos sparse matrix will keep that data and send it to the owner at the end of assembly, by calling the compress() command.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::assemble_stokes_preconditioner()\\n\\u00a0   {\\n\\u00a0     stokes_preconditioner_matrix = 0;\\n\\u00a0 \\n\\u00a0     const QGauss<dim> quadrature_formula(parameters.stokes_velocity_degree + 1);\\n\\u00a0 \\n\\u00a0     using CellFilter =\\n\\u00a0       FilteredIterator<typename DoFHandler<dim>::active_cell_iterator>;\\n\\u00a0 \\n\\u00a0     auto worker =\\n\\u00a0       [this](const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0              Assembly::Scratch::StokesPreconditioner<dim>         &scratch,\\n\\u00a0              Assembly::CopyData::StokesPreconditioner<dim>        &data) {\\n\\u00a0         this->local_assemble_stokes_preconditioner(cell, scratch, data);\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0     auto copier =\\n\\u00a0       [this](const Assembly::CopyData::StokesPreconditioner<dim> &data) {\\n\\u00a0         this->copy_local_to_global_stokes_preconditioner(data);\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0     WorkStream::run(CellFilter(IteratorFilters::LocallyOwnedCell(),\\n\\u00a0                                stokes_dof_handler.begin_active()),\\n\\u00a0                     CellFilter(IteratorFilters::LocallyOwnedCell(),\\n\\u00a0                                stokes_dof_handler.end()),\\n\\u00a0                     worker,\\n\\u00a0                     copier,\\n\\u00a0                     Assembly::Scratch::StokesPreconditioner<dim>(\\n\\u00a0                       stokes_fe,\\n\\u00a0                       quadrature_formula,\\n\\u00a0                       mapping,\\n\\u00a0                       update_JxW_values | update_values | update_gradients),\\n\\u00a0                     Assembly::CopyData::StokesPreconditioner<dim>(stokes_fe));\\n\\u00a0 \\n\\u00a0     stokes_preconditioner_matrix.compress(VectorOperation::add);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFilteredIteratorDefinition filtered_iterator.h:633\\nIteratorFilters::LocallyOwnedCellDefinition filtered_iterator.h:195\\nWorkStream::runvoid run(const std::vector< std::vector< Iterator > > &colored_iterators, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length=2 *MultithreadInfo::n_threads(), const unsigned int chunk_size=8)Definition work_stream.h:1272\\nVectorOperation::add@ addDefinition vector_operation.h:53\\nThe final function in this block initiates assembly of the Stokes preconditioner matrix and then in fact builds the Stokes preconditioner. It is mostly the same as in the serial case. The only difference to step-31 is that we use a Jacobi preconditioner for the pressure mass matrix instead of IC, as discussed in the introduction.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::build_stokes_preconditioner()\\n\\u00a0   {\\n\\u00a0     if (rebuild_stokes_preconditioner == false)\\n\\u00a0       return;\\n\\u00a0 \\n\\u00a0     TimerOutput::Scope timer_section(computing_timer,\\n\\u00a0                                      \\\"   Build Stokes preconditioner\\\");\\n\\u00a0     pcout << \\\"   Rebuilding Stokes preconditioner...\\\" << std::flush;\\n\\u00a0 \\n\\u00a0     assemble_stokes_preconditioner();\\n\\u00a0 \\n\\u00a0     std::vector<std::vector<bool>>   constant_modes;\\n\\u00a0     const FEValuesExtractors::Vector velocity_components(0);\\n\\u00a0     DoFTools::extract_constant_modes(stokes_dof_handler,\\n\\u00a0                                      stokes_fe.component_mask(\\n\\u00a0                                        velocity_components),\\n\\u00a0                                      constant_modes);\\n\\u00a0 \\n\\u00a0     Mp_preconditioner =\\n\\u00a0       std::make_shared<TrilinosWrappers::PreconditionJacobi>();\\n\\u00a0     Amg_preconditioner = std::make_shared<TrilinosWrappers::PreconditionAMG>();\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::PreconditionAMG::AdditionalData Amg_data;\\n\\u00a0     Amg_data.constant_modes        = constant_modes;\\n\\u00a0     Amg_data.elliptic              = true;\\n\\u00a0     Amg_data.higher_order_elements = true;\\n\\u00a0     Amg_data.smoother_sweeps       = 2;\\n\\u00a0     Amg_data.aggregation_threshold = 0.02;\\n\\u00a0 \\n\\u00a0     Mp_preconditioner->initialize(stokes_preconditioner_matrix.block(1, 1));\\n\\u00a0     Amg_preconditioner->initialize(stokes_preconditioner_matrix.block(0, 0),\\n\\u00a0                                    Amg_data);\\n\\u00a0 \\n\\u00a0     rebuild_stokes_preconditioner = false;\\n\\u00a0 \\n\\u00a0     pcout << std::endl;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nDoFTools::extract_constant_modesvoid extract_constant_modes(const DoFHandler< dim, spacedim > &dof_handler, const ComponentMask &component_mask, std::vector< std::vector< bool > > &constant_modes)Definition dof_tools.cc:1416\\nTrilinosWrappers::PreconditionAMG::AdditionalDataDefinition trilinos_precondition.h:1330\\nTrilinosWrappers::PreconditionAMG::AdditionalData::constant_modesstd::vector< std::vector< bool > > constant_modesDefinition trilinos_precondition.h:1500\\n Stokes system assembly\\nThe next three functions implement the assembly of the Stokes system, again split up into a part performing local calculations, one for writing the local data into the global matrix and vector, and one for actually running the loop over all cells with the help of the WorkStream class. Note that the assembly of the Stokes matrix needs only to be done in case we have changed the mesh. Otherwise, just the (temperature-dependent) right hand side needs to be calculated here. Since we are working with distributed matrices and vectors, we have to call the respective compress() functions in the end of the assembly in order to send non-local data to the owner process.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::local_assemble_stokes_system(\\n\\u00a0     const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0     Assembly::Scratch::StokesSystem<dim>                 &scratch,\\n\\u00a0     Assembly::CopyData::StokesSystem<dim>                &data)\\n\\u00a0   {\\n\\u00a0     const unsigned int dofs_per_cell =\\n\\u00a0       scratch.stokes_fe_values.get_fe().n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points =\\n\\u00a0       scratch.stokes_fe_values.n_quadrature_points;\\n\\u00a0 \\n\\u00a0     const FEValuesExtractors::Vector velocities(0);\\n\\u00a0     const FEValuesExtractors::Scalar pressure(dim);\\n\\u00a0 \\n\\u00a0     scratch.stokes_fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0     const typename DoFHandler<dim>::active_cell_iterator temperature_cell =\\n\\u00a0       cell->as_dof_handler_iterator(temperature_dof_handler);\\n\\u00a0     scratch.temperature_fe_values.reinit(temperature_cell);\\n\\u00a0 \\n\\u00a0     if (rebuild_stokes_matrix)\\n\\u00a0       data.local_matrix = 0;\\n\\u00a0     data.local_rhs = 0;\\n\\u00a0 \\n\\u00a0     scratch.temperature_fe_values.get_function_values(\\n\\u00a0       old_temperature_solution, scratch.old_temperature_values);\\n\\u00a0 \\n\\u00a0     for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0       {\\n\\u00a0         const double old_temperature = scratch.old_temperature_values[q];\\n\\u00a0 \\n\\u00a0         for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n\\u00a0           {\\n\\u00a0             scratch.phi_u[k] = scratch.stokes_fe_values[velocities].value(k, q);\\n\\u00a0             if (rebuild_stokes_matrix)\\n\\u00a0               {\\n\\u00a0                 scratch.grads_phi_u[k] =\\n\\u00a0                   scratch.stokes_fe_values[velocities].symmetric_gradient(k, q);\\n\\u00a0                 scratch.div_phi_u[k] =\\n\\u00a0                   scratch.stokes_fe_values[velocities].divergence(k, q);\\n\\u00a0                 scratch.phi_p[k] =\\n\\u00a0                   scratch.stokes_fe_values[pressure].value(k, q);\\n\\u00a0               }\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         if (rebuild_stokes_matrix == true)\\n\\u00a0           for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0             for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0               data.local_matrix(i, j) +=\\n\\u00a0                 (EquationData::eta * 2 *\\n\\u00a0                    (scratch.grads_phi_u[i] * scratch.grads_phi_u[j]) -\\n\\u00a0                  (EquationData::pressure_scaling * scratch.div_phi_u[i] *\\n\\u00a0                   scratch.phi_p[j]) -\\n\\u00a0                  (EquationData::pressure_scaling * scratch.phi_p[i] *\\n\\u00a0                   scratch.div_phi_u[j])) *\\n\\u00a0                 scratch.stokes_fe_values.JxW(q);\\n\\u00a0 \\n\\u00a0         const Tensor<1, dim> gravity = EquationData::gravity_vector(\\n\\u00a0           scratch.stokes_fe_values.quadrature_point(q));\\n\\u00a0 \\n\\u00a0         for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0           data.local_rhs(i) += (EquationData::density(old_temperature) *\\n\\u00a0                                 gravity * scratch.phi_u[i]) *\\n\\u00a0                                scratch.stokes_fe_values.JxW(q);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     cell->get_dof_indices(data.local_dof_indices);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::copy_local_to_global_stokes_system(\\n\\u00a0     const Assembly::CopyData::StokesSystem<dim> &data)\\n\\u00a0   {\\n\\u00a0     if (rebuild_stokes_matrix == true)\\n\\u00a0       stokes_constraints.distribute_local_to_global(data.local_matrix,\\n\\u00a0                                                     data.local_rhs,\\n\\u00a0                                                     data.local_dof_indices,\\n\\u00a0                                                     stokes_matrix,\\n\\u00a0                                                     stokes_rhs);\\n\\u00a0     else\\n\\u00a0       stokes_constraints.distribute_local_to_global(data.local_rhs,\\n\\u00a0                                                     data.local_dof_indices,\\n\\u00a0                                                     stokes_rhs);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::assemble_stokes_system()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope timer_section(computing_timer,\\n\\u00a0                                      \\\"   Assemble Stokes system\\\");\\n\\u00a0 \\n\\u00a0     if (rebuild_stokes_matrix == true)\\n\\u00a0       stokes_matrix = 0;\\n\\u00a0 \\n\\u00a0     stokes_rhs = 0;\\n\\u00a0 \\n\\u00a0     const QGauss<dim> quadrature_formula(parameters.stokes_velocity_degree + 1);\\n\\u00a0 \\n\\u00a0     using CellFilter =\\n\\u00a0       FilteredIterator<typename DoFHandler<2>::active_cell_iterator>;\\n\\u00a0 \\n\\u00a0     WorkStream::run(\\n\\u00a0       CellFilter(IteratorFilters::LocallyOwnedCell(),\\n\\u00a0                  stokes_dof_handler.begin_active()),\\n\\u00a0       CellFilter(IteratorFilters::LocallyOwnedCell(), stokes_dof_handler.end()),\\n\\u00a0       [this](const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0              Assembly::Scratch::StokesSystem<dim>                 &scratch,\\n\\u00a0              Assembly::CopyData::StokesSystem<dim>                &data) {\\n\\u00a0         this->local_assemble_stokes_system(cell, scratch, data);\\n\\u00a0       },\\n\\u00a0       [this](const Assembly::CopyData::StokesSystem<dim> &data) {\\n\\u00a0         this->copy_local_to_global_stokes_system(data);\\n\\u00a0       },\\n\\u00a0       Assembly::Scratch::StokesSystem<dim>(\\n\\u00a0         stokes_fe,\\n\\u00a0         mapping,\\n\\u00a0         quadrature_formula,\\n\\u00a0         (update_values | update_quadrature_points | update_JxW_values |\\n\\u00a0          (rebuild_stokes_matrix == true ? update_gradients : UpdateFlags(0))),\\n\\u00a0         temperature_fe,\\n\\u00a0         update_values),\\n\\u00a0       Assembly::CopyData::StokesSystem<dim>(stokes_fe));\\n\\u00a0 \\n\\u00a0     if (rebuild_stokes_matrix == true)\\n\\u00a0       stokes_matrix.compress(VectorOperation::add);\\n\\u00a0     stokes_rhs.compress(VectorOperation::add);\\n\\u00a0 \\n\\u00a0     rebuild_stokes_matrix = false;\\n\\u00a0 \\n\\u00a0     pcout << std::endl;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n Temperature matrix assembly\\nThe task to be performed by the next three functions is to calculate a mass matrix and a Laplace matrix on the temperature system. These will be combined in order to yield the semi-implicit time stepping matrix that consists of the mass matrix plus a time step-dependent weight factor times the Laplace matrix. This function is again essentially the body of the loop over all cells from step-31.\\nThe two following functions perform similar services as the ones above.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::local_assemble_temperature_matrix(\\n\\u00a0     const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0     Assembly::Scratch::TemperatureMatrix<dim>            &scratch,\\n\\u00a0     Assembly::CopyData::TemperatureMatrix<dim>           &data)\\n\\u00a0   {\\n\\u00a0     const unsigned int dofs_per_cell =\\n\\u00a0       scratch.temperature_fe_values.get_fe().n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points =\\n\\u00a0       scratch.temperature_fe_values.n_quadrature_points;\\n\\u00a0 \\n\\u00a0     scratch.temperature_fe_values.reinit(cell);\\n\\u00a0     cell->get_dof_indices(data.local_dof_indices);\\n\\u00a0 \\n\\u00a0     data.local_mass_matrix      = 0;\\n\\u00a0     data.local_stiffness_matrix = 0;\\n\\u00a0 \\n\\u00a0     for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0       {\\n\\u00a0         for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n\\u00a0           {\\n\\u00a0             scratch.grad_phi_T[k] =\\n\\u00a0               scratch.temperature_fe_values.shape_grad(k, q);\\n\\u00a0             scratch.phi_T[k] = scratch.temperature_fe_values.shape_value(k, q);\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0           for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0             {\\n\\u00a0               data.local_mass_matrix(i, j) +=\\n\\u00a0                 (scratch.phi_T[i] * scratch.phi_T[j] *\\n\\u00a0                  scratch.temperature_fe_values.JxW(q));\\n\\u00a0               data.local_stiffness_matrix(i, j) +=\\n\\u00a0                 (EquationData::kappa * scratch.grad_phi_T[i] *\\n\\u00a0                  scratch.grad_phi_T[j] * scratch.temperature_fe_values.JxW(q));\\n\\u00a0             }\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::copy_local_to_global_temperature_matrix(\\n\\u00a0     const Assembly::CopyData::TemperatureMatrix<dim> &data)\\n\\u00a0   {\\n\\u00a0     temperature_constraints.distribute_local_to_global(data.local_mass_matrix,\\n\\u00a0                                                        data.local_dof_indices,\\n\\u00a0                                                        temperature_mass_matrix);\\n\\u00a0     temperature_constraints.distribute_local_to_global(\\n\\u00a0       data.local_stiffness_matrix,\\n\\u00a0       data.local_dof_indices,\\n\\u00a0       temperature_stiffness_matrix);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::assemble_temperature_matrix()\\n\\u00a0   {\\n\\u00a0     if (rebuild_temperature_matrices == false)\\n\\u00a0       return;\\n\\u00a0 \\n\\u00a0     TimerOutput::Scope timer_section(computing_timer,\\n\\u00a0                                      \\\"   Assemble temperature matrices\\\");\\n\\u00a0     temperature_mass_matrix      = 0;\\n\\u00a0     temperature_stiffness_matrix = 0;\\n\\u00a0 \\n\\u00a0     const QGauss<dim> quadrature_formula(parameters.temperature_degree + 2);\\n\\u00a0 \\n\\u00a0     using CellFilter =\\n\\u00a0       FilteredIterator<typename DoFHandler<2>::active_cell_iterator>;\\n\\u00a0 \\n\\u00a0     WorkStream::run(\\n\\u00a0       CellFilter(IteratorFilters::LocallyOwnedCell(),\\n\\u00a0                  temperature_dof_handler.begin_active()),\\n\\u00a0       CellFilter(IteratorFilters::LocallyOwnedCell(),\\n\\u00a0                  temperature_dof_handler.end()),\\n\\u00a0       [this](const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0              Assembly::Scratch::TemperatureMatrix<dim>            &scratch,\\n\\u00a0              Assembly::CopyData::TemperatureMatrix<dim>           &data) {\\n\\u00a0         this->local_assemble_temperature_matrix(cell, scratch, data);\\n\\u00a0       },\\n\\u00a0       [this](const Assembly::CopyData::TemperatureMatrix<dim> &data) {\\n\\u00a0         this->copy_local_to_global_temperature_matrix(data);\\n\\u00a0       },\\n\\u00a0       Assembly::Scratch::TemperatureMatrix<dim>(temperature_fe,\\n\\u00a0                                                 mapping,\\n\\u00a0                                                 quadrature_formula),\\n\\u00a0       Assembly::CopyData::TemperatureMatrix<dim>(temperature_fe));\\n\\u00a0 \\n\\u00a0     temperature_mass_matrix.compress(VectorOperation::add);\\n\\u00a0     temperature_stiffness_matrix.compress(VectorOperation::add);\\n\\u00a0 \\n\\u00a0     rebuild_temperature_matrices       = false;\\n\\u00a0     rebuild_temperature_preconditioner = true;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n Temperature right hand side assembly\\nThis is the last assembly function. It calculates the right hand side of the temperature system, which includes the convection and the stabilization terms. It includes a lot of evaluations of old solutions at the quadrature points (which are necessary for calculating the artificial viscosity of stabilization), but is otherwise similar to the other assembly functions. Notice, once again, how we resolve the dilemma of having inhomogeneous boundary conditions, by just making a right hand side at this point (compare the comments for the project() function above): We create some matrix columns with exactly the values that would be entered for the temperature stiffness matrix, in case we have inhomogeneously constrained dofs. That will account for the correct balance of the right hand side vector with the matrix system of temperature.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::local_assemble_temperature_rhs(\\n\\u00a0     const std::pair<double, double> global_T_range,\\n\\u00a0     const double                    global_max_velocity,\\n\\u00a0     const double                    global_entropy_variation,\\n\\u00a0     const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0     Assembly::Scratch::TemperatureRHS<dim>               &scratch,\\n\\u00a0     Assembly::CopyData::TemperatureRHS<dim>              &data)\\n\\u00a0   {\\n\\u00a0     const bool use_bdf2_scheme = (timestep_number != 0);\\n\\u00a0 \\n\\u00a0     const unsigned int dofs_per_cell =\\n\\u00a0       scratch.temperature_fe_values.get_fe().n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points =\\n\\u00a0       scratch.temperature_fe_values.n_quadrature_points;\\n\\u00a0 \\n\\u00a0     const FEValuesExtractors::Vector velocities(0);\\n\\u00a0 \\n\\u00a0     data.local_rhs     = 0;\\n\\u00a0     data.matrix_for_bc = 0;\\n\\u00a0     cell->get_dof_indices(data.local_dof_indices);\\n\\u00a0 \\n\\u00a0     scratch.temperature_fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0     typename DoFHandler<dim>::active_cell_iterator stokes_cell =\\n\\u00a0       cell->as_dof_handler_iterator(stokes_dof_handler);\\n\\u00a0     scratch.stokes_fe_values.reinit(stokes_cell);\\n\\u00a0 \\n\\u00a0     scratch.temperature_fe_values.get_function_values(\\n\\u00a0       old_temperature_solution, scratch.old_temperature_values);\\n\\u00a0     scratch.temperature_fe_values.get_function_values(\\n\\u00a0       old_old_temperature_solution, scratch.old_old_temperature_values);\\n\\u00a0 \\n\\u00a0     scratch.temperature_fe_values.get_function_gradients(\\n\\u00a0       old_temperature_solution, scratch.old_temperature_grads);\\n\\u00a0     scratch.temperature_fe_values.get_function_gradients(\\n\\u00a0       old_old_temperature_solution, scratch.old_old_temperature_grads);\\n\\u00a0 \\n\\u00a0     scratch.temperature_fe_values.get_function_laplacians(\\n\\u00a0       old_temperature_solution, scratch.old_temperature_laplacians);\\n\\u00a0     scratch.temperature_fe_values.get_function_laplacians(\\n\\u00a0       old_old_temperature_solution, scratch.old_old_temperature_laplacians);\\n\\u00a0 \\n\\u00a0     scratch.stokes_fe_values[velocities].get_function_values(\\n\\u00a0       stokes_solution, scratch.old_velocity_values);\\n\\u00a0     scratch.stokes_fe_values[velocities].get_function_values(\\n\\u00a0       old_stokes_solution, scratch.old_old_velocity_values);\\n\\u00a0     scratch.stokes_fe_values[velocities].get_function_symmetric_gradients(\\n\\u00a0       stokes_solution, scratch.old_strain_rates);\\n\\u00a0     scratch.stokes_fe_values[velocities].get_function_symmetric_gradients(\\n\\u00a0       old_stokes_solution, scratch.old_old_strain_rates);\\n\\u00a0 \\n\\u00a0     const double nu =\\n\\u00a0       compute_viscosity(scratch.old_temperature_values,\\n\\u00a0                         scratch.old_old_temperature_values,\\n\\u00a0                         scratch.old_temperature_grads,\\n\\u00a0                         scratch.old_old_temperature_grads,\\n\\u00a0                         scratch.old_temperature_laplacians,\\n\\u00a0                         scratch.old_old_temperature_laplacians,\\n\\u00a0                         scratch.old_velocity_values,\\n\\u00a0                         scratch.old_old_velocity_values,\\n\\u00a0                         scratch.old_strain_rates,\\n\\u00a0                         scratch.old_old_strain_rates,\\n\\u00a0                         global_max_velocity,\\n\\u00a0                         global_T_range.second - global_T_range.first,\\n\\u00a0                         0.5 * (global_T_range.second + global_T_range.first),\\n\\u00a0                         global_entropy_variation,\\n\\u00a0                         cell->diameter());\\n\\u00a0 \\n\\u00a0     for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0       {\\n\\u00a0         for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n\\u00a0           {\\n\\u00a0             scratch.phi_T[k] = scratch.temperature_fe_values.shape_value(k, q);\\n\\u00a0             scratch.grad_phi_T[k] =\\n\\u00a0               scratch.temperature_fe_values.shape_grad(k, q);\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0         const double T_term_for_rhs =\\n\\u00a0           (use_bdf2_scheme ?\\n\\u00a0              (scratch.old_temperature_values[q] *\\n\\u00a0                 (1 + time_step / old_time_step) -\\n\\u00a0               scratch.old_old_temperature_values[q] * (time_step * time_step) /\\n\\u00a0                 (old_time_step * (time_step + old_time_step))) :\\n\\u00a0              scratch.old_temperature_values[q]);\\n\\u00a0 \\n\\u00a0         const double ext_T =\\n\\u00a0           (use_bdf2_scheme ? (scratch.old_temperature_values[q] *\\n\\u00a0                                 (1 + time_step / old_time_step) -\\n\\u00a0                               scratch.old_old_temperature_values[q] *\\n\\u00a0                                 time_step / old_time_step) :\\n\\u00a0                              scratch.old_temperature_values[q]);\\n\\u00a0 \\n\\u00a0         const Tensor<1, dim> ext_grad_T =\\n\\u00a0           (use_bdf2_scheme ? (scratch.old_temperature_grads[q] *\\n\\u00a0                                 (1 + time_step / old_time_step) -\\n\\u00a0                               scratch.old_old_temperature_grads[q] * time_step /\\n\\u00a0                                 old_time_step) :\\n\\u00a0                              scratch.old_temperature_grads[q]);\\n\\u00a0 \\n\\u00a0         const Tensor<1, dim> extrapolated_u =\\n\\u00a0           (use_bdf2_scheme ?\\n\\u00a0              (scratch.old_velocity_values[q] * (1 + time_step / old_time_step) -\\n\\u00a0               scratch.old_old_velocity_values[q] * time_step / old_time_step) :\\n\\u00a0              scratch.old_velocity_values[q]);\\n\\u00a0 \\n\\u00a0         const SymmetricTensor<2, dim> extrapolated_strain_rate =\\n\\u00a0           (use_bdf2_scheme ?\\n\\u00a0              (scratch.old_strain_rates[q] * (1 + time_step / old_time_step) -\\n\\u00a0               scratch.old_old_strain_rates[q] * time_step / old_time_step) :\\n\\u00a0              scratch.old_strain_rates[q]);\\n\\u00a0 \\n\\u00a0         const double gamma =\\n\\u00a0           ((EquationData::radiogenic_heating * EquationData::density(ext_T) +\\n\\u00a0             2 * EquationData::eta * extrapolated_strain_rate *\\n\\u00a0               extrapolated_strain_rate) /\\n\\u00a0            (EquationData::density(ext_T) * EquationData::specific_heat));\\n\\u00a0 \\n\\u00a0         for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0           {\\n\\u00a0             data.local_rhs(i) +=\\n\\u00a0               (T_term_for_rhs * scratch.phi_T[i] -\\n\\u00a0                time_step * extrapolated_u * ext_grad_T * scratch.phi_T[i] -\\n\\u00a0                time_step * nu * ext_grad_T * scratch.grad_phi_T[i] +\\n\\u00a0                time_step * gamma * scratch.phi_T[i]) *\\n\\u00a0               scratch.temperature_fe_values.JxW(q);\\n\\u00a0 \\n\\u00a0             if (temperature_constraints.is_inhomogeneously_constrained(\\n\\u00a0                   data.local_dof_indices[i]))\\n\\u00a0               {\\n\\u00a0                 for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0                   data.matrix_for_bc(j, i) +=\\n\\u00a0                     (scratch.phi_T[i] * scratch.phi_T[j] *\\n\\u00a0                        (use_bdf2_scheme ? ((2 * time_step + old_time_step) /\\n\\u00a0                                            (time_step + old_time_step)) :\\n\\u00a0                                           1.) +\\n\\u00a0                      scratch.grad_phi_T[i] * scratch.grad_phi_T[j] *\\n\\u00a0                        EquationData::kappa * time_step) *\\n\\u00a0                     scratch.temperature_fe_values.JxW(q);\\n\\u00a0               }\\n\\u00a0           }\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::copy_local_to_global_temperature_rhs(\\n\\u00a0     const Assembly::CopyData::TemperatureRHS<dim> &data)\\n\\u00a0   {\\n\\u00a0     temperature_constraints.distribute_local_to_global(data.local_rhs,\\n\\u00a0                                                        data.local_dof_indices,\\n\\u00a0                                                        temperature_rhs,\\n\\u00a0                                                        data.matrix_for_bc);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nIn the function that runs the WorkStream for actually calculating the right hand side, we also generate the final matrix. As mentioned above, it is a sum of the mass matrix and the Laplace matrix, times some time step-dependent weight. This weight is specified by the BDF-2 time integration scheme, see the introduction in step-31. What is new in this tutorial program (in addition to the use of MPI parallelization and the WorkStream class), is that we now precompute the temperature preconditioner as well. The reason is that the setup of the Jacobi preconditioner takes a noticeable time compared to the solver because we usually only need between 10 and 20 iterations for solving the temperature system (this might sound strange, as Jacobi really only consists of a diagonal, but in Trilinos it is derived from more general framework for point relaxation preconditioners which is a bit inefficient). Hence, it is more efficient to precompute the preconditioner, even though the matrix entries may slightly change because the time step might change. This is not too big a problem because we remesh every few time steps (and regenerate the preconditioner then).\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::assemble_temperature_system(\\n\\u00a0     const double maximal_velocity)\\n\\u00a0   {\\n\\u00a0     const bool use_bdf2_scheme = (timestep_number != 0);\\n\\u00a0 \\n\\u00a0     if (use_bdf2_scheme == true)\\n\\u00a0       {\\n\\u00a0         temperature_matrix.copy_from(temperature_mass_matrix);\\n\\u00a0         temperature_matrix *=\\n\\u00a0           (2 * time_step + old_time_step) / (time_step + old_time_step);\\n\\u00a0         temperature_matrix.add(time_step, temperature_stiffness_matrix);\\n\\u00a0       }\\n\\u00a0     else\\n\\u00a0       {\\n\\u00a0         temperature_matrix.copy_from(temperature_mass_matrix);\\n\\u00a0         temperature_matrix.add(time_step, temperature_stiffness_matrix);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     if (rebuild_temperature_preconditioner == true)\\n\\u00a0       {\\n\\u00a0         T_preconditioner =\\n\\u00a0           std::make_shared<TrilinosWrappers::PreconditionJacobi>();\\n\\u00a0         T_preconditioner->initialize(temperature_matrix);\\n\\u00a0         rebuild_temperature_preconditioner = false;\\n\\u00a0       }\\n\\u00a0 \\nThe next part is computing the right hand side vectors. To do so, we first compute the average temperature \\\\(T_m\\\\) that we use for evaluating the artificial viscosity stabilization through the residual  \\\\(E(T) =\\n   (T-T_m)^2\\\\). We do this by defining the midpoint between maximum and minimum temperature as average temperature in the definition of the entropy viscosity. An alternative would be to use the integral average, but the results are not very sensitive to this choice. The rest then only requires calling WorkStream::run again, binding the arguments to the local_assemble_temperature_rhs function that are the same in every call to the correct values:\\n\\u00a0     temperature_rhs = 0;\\n\\u00a0 \\n\\u00a0     const QGauss<dim> quadrature_formula(parameters.temperature_degree + 2);\\n\\u00a0     const std::pair<double, double> global_T_range =\\n\\u00a0       get_extrapolated_temperature_range();\\n\\u00a0 \\n\\u00a0     const double average_temperature =\\n\\u00a0       0.5 * (global_T_range.first + global_T_range.second);\\n\\u00a0     const double global_entropy_variation =\\n\\u00a0       get_entropy_variation(average_temperature);\\n\\u00a0 \\n\\u00a0     using CellFilter =\\n\\u00a0       FilteredIterator<typename DoFHandler<2>::active_cell_iterator>;\\n\\u00a0 \\n\\u00a0     auto worker =\\n\\u00a0       [this, global_T_range, maximal_velocity, global_entropy_variation](\\n\\u00a0         const typename DoFHandler<dim>::active_cell_iterator &cell,\\n\\u00a0         Assembly::Scratch::TemperatureRHS<dim>               &scratch,\\n\\u00a0         Assembly::CopyData::TemperatureRHS<dim>              &data) {\\n\\u00a0         this->local_assemble_temperature_rhs(global_T_range,\\n\\u00a0                                              maximal_velocity,\\n\\u00a0                                              global_entropy_variation,\\n\\u00a0                                              cell,\\n\\u00a0                                              scratch,\\n\\u00a0                                              data);\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0     auto copier = [this](const Assembly::CopyData::TemperatureRHS<dim> &data) {\\n\\u00a0       this->copy_local_to_global_temperature_rhs(data);\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0     WorkStream::run(CellFilter(IteratorFilters::LocallyOwnedCell(),\\n\\u00a0                                temperature_dof_handler.begin_active()),\\n\\u00a0                     CellFilter(IteratorFilters::LocallyOwnedCell(),\\n\\u00a0                                temperature_dof_handler.end()),\\n\\u00a0                     worker,\\n\\u00a0                     copier,\\n\\u00a0                     Assembly::Scratch::TemperatureRHS<dim>(\\n\\u00a0                       temperature_fe, stokes_fe, mapping, quadrature_formula),\\n\\u00a0                     Assembly::CopyData::TemperatureRHS<dim>(temperature_fe));\\n\\u00a0 \\n\\u00a0     temperature_rhs.compress(VectorOperation::add);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n BoussinesqFlowProblem::solve\\nThis function solves the linear systems in each time step of the Boussinesq problem. First, we work on the Stokes system and then on the temperature system. In essence, it does the same things as the respective function in step-31. However, there are a few changes here.\\nThe first change is related to the way we store our solution: we keep the vectors with locally owned degrees of freedom plus ghost nodes on each MPI node. When we enter a solver which is supposed to perform matrix-vector products with a distributed matrix, this is not the appropriate form, though. There, we will want to have the solution vector to be distributed in the same way as the matrix, i.e. without any ghosts. So what we do first is to generate a distributed vector called distributed_stokes_solution and put only the locally owned dofs into that, which is neatly done by the operator= of the Trilinos vector.\\nNext, we scale the pressure solution (or rather, the initial guess) for the solver so that it matches with the length scales in the matrices, as discussed in the introduction. We also immediately scale the pressure solution back to the correct units after the solution is completed. We also need to set the pressure values at hanging nodes to zero. This we also did in step-31 in order not to disturb the Schur complement by some vector entries that actually are irrelevant during the solve stage. As a difference to step-31, here we do it only for the locally owned pressure dofs. After solving for the Stokes solution, each processor copies the distributed solution back into the solution vector that also includes ghost elements.\\nThe third and most obvious change is that we have two variants for the Stokes solver: A fast solver that sometimes breaks down, and a robust solver that is slower. This is what we already discussed in the introduction. Here is how we realize it: First, we perform 30 iterations with the fast solver based on the simple preconditioner based on the AMG V-cycle instead of an approximate solve (this is indicated by the false argument to the LinearSolvers::BlockSchurPreconditioner object). If we converge, everything is fine. If we do not converge, the solver control object will throw an exception SolverControl::NoConvergence. Usually, this would abort the program because we don't catch them in our usual solve() functions. This is certainly not what we want to happen here. Rather, we want to switch to the strong solver and continue the solution process with whatever vector we got so far. Hence, we catch the exception with the C++ try/catch mechanism. We then simply go through the same solver sequence again in the catch clause, this time passing the true flag to the preconditioner for the strong solver, signaling an approximate CG solve.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::solve()\\n\\u00a0   {\\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope timer_section(computing_timer,\\n\\u00a0                                        \\\"   Solve Stokes system\\\");\\n\\u00a0 \\n\\u00a0       pcout << \\\"   Solving Stokes system... \\\" << std::flush;\\n\\u00a0 \\n\\u00a0       TrilinosWrappers::MPI::BlockVector distributed_stokes_solution(\\n\\u00a0         stokes_rhs);\\n\\u00a0       distributed_stokes_solution = stokes_solution;\\n\\u00a0 \\n\\u00a0       distributed_stokes_solution.block(1) /= EquationData::pressure_scaling;\\n\\u00a0 \\n\\u00a0       const unsigned int\\n\\u00a0         start = (distributed_stokes_solution.block(0).size() +\\n\\u00a0                  distributed_stokes_solution.block(1).local_range().first),\\n\\u00a0         end   = (distributed_stokes_solution.block(0).size() +\\n\\u00a0                distributed_stokes_solution.block(1).local_range().second);\\n\\u00a0       for (unsigned int i = start; i < end; ++i)\\n\\u00a0         if (stokes_constraints.is_constrained(i))\\n\\u00a0           distributed_stokes_solution(i) = 0;\\n\\u00a0 \\n\\u00a0 \\n\\u00a0       PrimitiveVectorMemory<TrilinosWrappers::MPI::BlockVector> mem;\\n\\u00a0 \\n\\u00a0       unsigned int  n_iterations     = 0;\\n\\u00a0       const double  solver_tolerance = 1e-8 * stokes_rhs.l2_norm();\\n\\u00a0       SolverControl solver_control(30, solver_tolerance);\\n\\u00a0 \\n\\u00a0       try\\n\\u00a0         {\\n\\u00a0           const LinearSolvers::BlockSchurPreconditioner<\\n\\u00a0             TrilinosWrappers::PreconditionAMG,\\n\\u00a0             TrilinosWrappers::PreconditionJacobi>\\n\\u00a0             preconditioner(stokes_matrix,\\n\\u00a0                            stokes_preconditioner_matrix,\\n\\u00a0                            *Mp_preconditioner,\\n\\u00a0                            *Amg_preconditioner,\\n\\u00a0                            false);\\n\\u00a0 \\n\\u00a0           SolverFGMRES<TrilinosWrappers::MPI::BlockVector> solver(\\n\\u00a0             solver_control,\\n\\u00a0             mem,\\n\\u00a0             SolverFGMRES<TrilinosWrappers::MPI::BlockVector>::AdditionalData(\\n\\u00a0               30));\\n\\u00a0           solver.solve(stokes_matrix,\\n\\u00a0                        distributed_stokes_solution,\\n\\u00a0                        stokes_rhs,\\n\\u00a0                        preconditioner);\\n\\u00a0 \\n\\u00a0           n_iterations = solver_control.last_step();\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       catch (SolverControl::NoConvergence &)\\n\\u00a0         {\\n\\u00a0           const LinearSolvers::BlockSchurPreconditioner<\\n\\u00a0             TrilinosWrappers::PreconditionAMG,\\n\\u00a0             TrilinosWrappers::PreconditionJacobi>\\n\\u00a0             preconditioner(stokes_matrix,\\n\\u00a0                            stokes_preconditioner_matrix,\\n\\u00a0                            *Mp_preconditioner,\\n\\u00a0                            *Amg_preconditioner,\\n\\u00a0                            true);\\n\\u00a0 \\n\\u00a0           SolverControl solver_control_refined(stokes_matrix.m(),\\n\\u00a0                                                solver_tolerance);\\n\\u00a0           SolverFGMRES<TrilinosWrappers::MPI::BlockVector> solver(\\n\\u00a0             solver_control_refined,\\n\\u00a0             mem,\\n\\u00a0             SolverFGMRES<TrilinosWrappers::MPI::BlockVector>::AdditionalData(\\n\\u00a0               50));\\n\\u00a0           solver.solve(stokes_matrix,\\n\\u00a0                        distributed_stokes_solution,\\n\\u00a0                        stokes_rhs,\\n\\u00a0                        preconditioner);\\n\\u00a0 \\n\\u00a0           n_iterations =\\n\\u00a0             (solver_control.last_step() + solver_control_refined.last_step());\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0       stokes_constraints.distribute(distributed_stokes_solution);\\n\\u00a0 \\n\\u00a0       distributed_stokes_solution.block(1) *= EquationData::pressure_scaling;\\n\\u00a0 \\n\\u00a0       stokes_solution = distributed_stokes_solution;\\n\\u00a0       pcout << n_iterations << \\\" iterations.\\\" << std::endl;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\nPrimitiveVectorMemoryDefinition vector_memory.h:237\\nSolverControl::NoConvergenceDefinition solver_control.h:95\\nSolverFGMRESDefinition solver_gmres.h:648\\nTrilinosWrappers::PreconditionAMGDefinition trilinos_precondition.h:1321\\nTrilinosWrappers::PreconditionJacobiDefinition trilinos_precondition.h:248\\nSolverFGMRES::AdditionalDataDefinition solver_gmres.h:654\\nNow let's turn to the temperature part: First, we compute the time step size. We found that we need smaller time steps for 3d than for 2d for the shell geometry. This is because the cells are more distorted in that case (it is the smallest edge length that determines the CFL number). Instead of computing the time step from maximum velocity and minimal mesh size as in step-31, we compute local CFL numbers, i.e., on each cell we compute the maximum velocity times the mesh size, and compute the maximum of them. Hence, we need to choose the factor in front of the time step slightly smaller. (We later re-considered this approach towards time stepping. If you're curious about this, you may want to read the time stepping section in [114] .)\\nAfter temperature right hand side assembly, we solve the linear system for temperature (with fully distributed vectors without ghost elements and using the solution from the last timestep as our initial guess for the iterative solver), apply constraints, and copy the vector back to one with ghosts.\\nIn the end, we extract the temperature range similarly to step-31 to produce some output (for example in order to help us choose the stabilization constants, as discussed in the introduction). The only difference is that we need to exchange maxima over all processors.\\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope timer_section(computing_timer,\\n\\u00a0                                        \\\"   Assemble temperature rhs\\\");\\n\\u00a0 \\n\\u00a0       old_time_step = time_step;\\n\\u00a0 \\n\\u00a0       const double scaling = (dim == 3 ? 0.25 : 1.0);\\n\\u00a0       time_step            = (scaling / (2.1 * dim * std::sqrt(1. * dim)) /\\n\\u00a0                    (parameters.temperature_degree * get_cfl_number()));\\n\\u00a0 \\n\\u00a0       const double maximal_velocity = get_maximal_velocity();\\n\\u00a0       pcout << \\\"   Maximal velocity: \\\"\\n\\u00a0             << maximal_velocity * EquationData::year_in_seconds * 100\\n\\u00a0             << \\\" cm/year\\\" << std::endl;\\n\\u00a0       pcout << \\\"   \\\"\\n\\u00a0             << \\\"Time step: \\\" << time_step / EquationData::year_in_seconds\\n\\u00a0             << \\\" years\\\" << std::endl;\\n\\u00a0 \\n\\u00a0       assemble_temperature_system(maximal_velocity);\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope timer_section(computing_timer,\\n\\u00a0                                        \\\"   Solve temperature system\\\");\\n\\u00a0 \\n\\u00a0       SolverControl solver_control(temperature_matrix.m(),\\n\\u00a0                                    1e-12 * temperature_rhs.l2_norm());\\n\\u00a0       SolverCG<TrilinosWrappers::MPI::Vector> cg(solver_control);\\n\\u00a0 \\n\\u00a0       TrilinosWrappers::MPI::Vector distributed_temperature_solution(\\n\\u00a0         temperature_rhs);\\n\\u00a0       distributed_temperature_solution = old_temperature_solution;\\n\\u00a0 \\n\\u00a0       cg.solve(temperature_matrix,\\n\\u00a0                distributed_temperature_solution,\\n\\u00a0                temperature_rhs,\\n\\u00a0                *T_preconditioner);\\n\\u00a0 \\n\\u00a0       temperature_constraints.distribute(distributed_temperature_solution);\\n\\u00a0       temperature_solution = distributed_temperature_solution;\\n\\u00a0 \\n\\u00a0       pcout << \\\"   \\\" << solver_control.last_step()\\n\\u00a0             << \\\" CG iterations for temperature\\\" << std::endl;\\n\\u00a0 \\n\\u00a0       double temperature[2] = {std::numeric_limits<double>::max(),\\n\\u00a0                                -std::numeric_limits<double>::max()};\\n\\u00a0       double global_temperature[2];\\n\\u00a0 \\n\\u00a0       for (unsigned int i =\\n\\u00a0              distributed_temperature_solution.local_range().first;\\n\\u00a0            i < distributed_temperature_solution.local_range().second;\\n\\u00a0            ++i)\\n\\u00a0         {\\n\\u00a0           temperature[0] =\\n\\u00a0             std::min<double>(temperature[0],\\n\\u00a0                              distributed_temperature_solution(i));\\n\\u00a0           temperature[1] =\\n\\u00a0             std::max<double>(temperature[1],\\n\\u00a0                              distributed_temperature_solution(i));\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       temperature[0] *= -1.0;\\n\\u00a0       Utilities::MPI::max(temperature, MPI_COMM_WORLD, global_temperature);\\n\\u00a0       global_temperature[0] *= -1.0;\\n\\u00a0 \\n\\u00a0       pcout << \\\"   Temperature range: \\\" << global_temperature[0] << ' '\\n\\u00a0             << global_temperature[1] << std::endl;\\n\\u00a0     }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n BoussinesqFlowProblem::output_results\\nNext comes the function that generates the output. The quantities to output could be introduced manually like we did in step-31. An alternative is to hand this task over to a class PostProcessor that inherits from the class DataPostprocessor, which can be attached to DataOut. This allows us to output derived quantities from the solution, like the friction heating included in this example. It overloads the virtual function DataPostprocessor::evaluate_vector_field(), which is then internally called from DataOut::build_patches(). We have to give it values of the numerical solution, its derivatives, normals to the cell, the actual evaluation points and any additional quantities. This follows the same procedure as discussed in step-29 and other programs.\\n\\u00a0   template <int dim>\\n\\u00a0   class BoussinesqFlowProblem<dim>::Postprocessor\\n\\u00a0     : public DataPostprocessor<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     Postprocessor(const unsigned int partition, const double minimal_pressure);\\n\\u00a0 \\n\\u00a0     virtual void evaluate_vector_field(\\n\\u00a0       const DataPostprocessorInputs::Vector<dim> &inputs,\\n\\u00a0       std::vector<Vector<double>> &computed_quantities) const override;\\n\\u00a0 \\n\\u00a0     virtual std::vector<std::string> get_names() const override;\\n\\u00a0 \\n\\u00a0     virtual std::vector<\\n\\u00a0       DataComponentInterpretation::DataComponentInterpretation>\\n\\u00a0     get_data_component_interpretation() const override;\\n\\u00a0 \\n\\u00a0     virtual UpdateFlags get_needed_update_flags() const override;\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     const unsigned int partition;\\n\\u00a0     const double       minimal_pressure;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   BoussinesqFlowProblem<dim>::Postprocessor::Postprocessor(\\n\\u00a0     const unsigned int partition,\\n\\u00a0     const double       minimal_pressure)\\n\\u00a0     : partition(partition)\\n\\u00a0     , minimal_pressure(minimal_pressure)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\nDataPostprocessorDefinition data_postprocessor.h:583\\nDataComponentInterpretation::DataComponentInterpretationDataComponentInterpretationDefinition data_component_interpretation.h:47\\nDataPostprocessorInputs::VectorDefinition data_postprocessor.h:400\\nHere we define the names for the variables we want to output. These are the actual solution values for velocity, pressure, and temperature, as well as the friction heating and to each cell the number of the processor that owns it. This allows us to visualize the partitioning of the domain among the processors. Except for the velocity, which is vector-valued, all other quantities are scalar.\\n\\u00a0   template <int dim>\\n\\u00a0   std::vector<std::string>\\n\\u00a0   BoussinesqFlowProblem<dim>::Postprocessor::get_names() const\\n\\u00a0   {\\n\\u00a0     std::vector<std::string> solution_names(dim, \\\"velocity\\\");\\n\\u00a0     solution_names.emplace_back(\\\"p\\\");\\n\\u00a0     solution_names.emplace_back(\\\"T\\\");\\n\\u00a0     solution_names.emplace_back(\\\"friction_heating\\\");\\n\\u00a0     solution_names.emplace_back(\\\"partition\\\");\\n\\u00a0 \\n\\u00a0     return solution_names;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n\\u00a0   BoussinesqFlowProblem<dim>::Postprocessor::get_data_component_interpretation()\\n\\u00a0     const\\n\\u00a0   {\\n\\u00a0     std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n\\u00a0       interpretation(dim,\\n\\u00a0                      DataComponentInterpretation::component_is_part_of_vector);\\n\\u00a0 \\n\\u00a0     interpretation.push_back(DataComponentInterpretation::component_is_scalar);\\n\\u00a0     interpretation.push_back(DataComponentInterpretation::component_is_scalar);\\n\\u00a0     interpretation.push_back(DataComponentInterpretation::component_is_scalar);\\n\\u00a0     interpretation.push_back(DataComponentInterpretation::component_is_scalar);\\n\\u00a0 \\n\\u00a0     return interpretation;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   UpdateFlags\\n\\u00a0   BoussinesqFlowProblem<dim>::Postprocessor::get_needed_update_flags() const\\n\\u00a0   {\\n\\u00a0     return update_values | update_gradients | update_quadrature_points;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nDataComponentInterpretation::component_is_scalar@ component_is_scalarDefinition data_component_interpretation.h:52\\nDataComponentInterpretation::component_is_part_of_vector@ component_is_part_of_vectorDefinition data_component_interpretation.h:58\\nNow we implement the function that computes the derived quantities. As we also did for the output, we rescale the velocity from its SI units to something more readable, namely cm/year. Next, the pressure is scaled to be between 0 and the maximum pressure. This makes it more easily comparable \\u2013 in essence making all pressure variables positive or zero. Temperature is taken as is, and the friction heating is computed as \\\\(2 \\\\eta \\\\varepsilon(\\\\mathbf{u}) \\\\cdot \\\\varepsilon(\\\\mathbf{u})\\\\).\\nThe quantities we output here are more for illustration, rather than for actual scientific value. We come back to this briefly in the results section of this program and explain what one may in fact be interested in.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::Postprocessor::evaluate_vector_field(\\n\\u00a0     const DataPostprocessorInputs::Vector<dim> &inputs,\\n\\u00a0     std::vector<Vector<double>>                &computed_quantities) const\\n\\u00a0   {\\n\\u00a0     const unsigned int n_evaluation_points = inputs.solution_values.size();\\n\\u00a0     Assert(inputs.solution_gradients.size() == n_evaluation_points,\\n\\u00a0            ExcInternalError());\\n\\u00a0     Assert(computed_quantities.size() == n_evaluation_points,\\n\\u00a0            ExcInternalError());\\n\\u00a0     Assert(inputs.solution_values[0].size() == dim + 2, ExcInternalError());\\n\\u00a0 \\n\\u00a0     for (unsigned int p = 0; p < n_evaluation_points; ++p)\\n\\u00a0       {\\n\\u00a0         for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0           computed_quantities[p](d) = (inputs.solution_values[p](d) *\\n\\u00a0                                        EquationData::year_in_seconds * 100);\\n\\u00a0 \\n\\u00a0         const double pressure =\\n\\u00a0           (inputs.solution_values[p](dim) - minimal_pressure);\\n\\u00a0         computed_quantities[p](dim) = pressure;\\n\\u00a0 \\n\\u00a0         const double temperature        = inputs.solution_values[p](dim + 1);\\n\\u00a0         computed_quantities[p](dim + 1) = temperature;\\n\\u00a0 \\n\\u00a0         Tensor<2, dim> grad_u;\\n\\u00a0         for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0           grad_u[d] = inputs.solution_gradients[p][d];\\n\\u00a0         const SymmetricTensor<2, dim> strain_rate = symmetrize(grad_u);\\n\\u00a0         computed_quantities[p](dim + 2) =\\n\\u00a0           2 * EquationData::eta * strain_rate * strain_rate;\\n\\u00a0 \\n\\u00a0         computed_quantities[p](dim + 3) = partition;\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nsymmetrizeDEAL_II_HOST constexpr SymmetricTensor< 2, dim, Number > symmetrize(const Tensor< 2, dim, Number > &t)Definition symmetric_tensor.h:3534\\nThe output_results() function has a similar task to the one in step-31. However, here we are going to demonstrate a different technique on how to merge output from different DoFHandler objects. The way we're going to achieve this recombination is to create a joint DoFHandler that collects both components, the Stokes solution and the temperature solution. This can be nicely done by combining the finite elements from the two systems to form one FESystem, and let this collective system define a new DoFHandler object. To be sure that everything was done correctly, we perform a sanity check that ensures that we got all the dofs from both Stokes and temperature even in the combined system. We then combine the data vectors. Unfortunately, there is no straight-forward relation that tells us how to sort Stokes and temperature vector into the joint vector. The way we can get around this trouble is to rely on the information collected in the FESystem. For each dof on a cell, the joint finite element knows to which equation component (velocity component, pressure, or temperature) it belongs \\u2013 that's the information we need! So we step through all cells (with iterators into all three DoFHandlers moving in sync), and for each joint cell dof, we read out that component using the FiniteElement::system_to_base_index function (see there for a description of what the various parts of its return value contain). We also need to keep track whether we're on a Stokes dof or a temperature dof, which is contained in joint_fe.system_to_base_index(i).first.first. Eventually, the dof_indices data structures on either of the three systems tell us how the relation between global vector and local dofs looks like on the present cell, which concludes this tedious work. We make sure that each processor only works on the subdomain it owns locally (and not on ghost or artificial cells) when building the joint solution vector. The same will then have to be done in DataOut::build_patches(), but that function does so automatically.\\nWhat we end up with is a set of patches that we can write using the functions in DataOutBase in a variety of output formats. Here, we then have to pay attention that what each processor writes is really only its own part of the domain, i.e. we will want to write each processor's contribution into a separate file. This we do by adding an additional number to the filename when we write the solution. This is not really new, we did it similarly in step-40. Note that we write in the compressed format .vtu instead of plain vtk files, which saves quite some storage.\\nAll the rest of the work is done in the PostProcessor class.\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::output_results()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope timer_section(computing_timer, \\\"Postprocessing\\\");\\n\\u00a0 \\n\\u00a0     const FESystem<dim> joint_fe(stokes_fe, 1, temperature_fe, 1);\\n\\u00a0 \\n\\u00a0     DoFHandler<dim> joint_dof_handler(triangulation);\\n\\u00a0     joint_dof_handler.distribute_dofs(joint_fe);\\n\\u00a0     Assert(joint_dof_handler.n_dofs() ==\\n\\u00a0              stokes_dof_handler.n_dofs() + temperature_dof_handler.n_dofs(),\\n\\u00a0            ExcInternalError());\\n\\u00a0 \\n\\u00a0     TrilinosWrappers::MPI::Vector joint_solution;\\n\\u00a0     joint_solution.reinit(joint_dof_handler.locally_owned_dofs(),\\n\\u00a0                           MPI_COMM_WORLD);\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       std::vector<types::global_dof_index> local_joint_dof_indices(\\n\\u00a0         joint_fe.n_dofs_per_cell());\\n\\u00a0       std::vector<types::global_dof_index> local_stokes_dof_indices(\\n\\u00a0         stokes_fe.n_dofs_per_cell());\\n\\u00a0       std::vector<types::global_dof_index> local_temperature_dof_indices(\\n\\u00a0         temperature_fe.n_dofs_per_cell());\\n\\u00a0 \\n\\u00a0       typename DoFHandler<dim>::active_cell_iterator\\n\\u00a0         joint_cell       = joint_dof_handler.begin_active(),\\n\\u00a0         joint_endc       = joint_dof_handler.end(),\\n\\u00a0         stokes_cell      = stokes_dof_handler.begin_active(),\\n\\u00a0         temperature_cell = temperature_dof_handler.begin_active();\\n\\u00a0       for (; joint_cell != joint_endc;\\n\\u00a0            ++joint_cell, ++stokes_cell, ++temperature_cell)\\n\\u00a0         if (joint_cell->is_locally_owned())\\n\\u00a0           {\\n\\u00a0             joint_cell->get_dof_indices(local_joint_dof_indices);\\n\\u00a0             stokes_cell->get_dof_indices(local_stokes_dof_indices);\\n\\u00a0             temperature_cell->get_dof_indices(local_temperature_dof_indices);\\n\\u00a0 \\n\\u00a0             for (unsigned int i = 0; i < joint_fe.n_dofs_per_cell(); ++i)\\n\\u00a0               if (joint_fe.system_to_base_index(i).first.first == 0)\\n\\u00a0                 {\\n\\u00a0                   Assert(joint_fe.system_to_base_index(i).second <\\n\\u00a0                            local_stokes_dof_indices.size(),\\n\\u00a0                          ExcInternalError());\\n\\u00a0 \\n\\u00a0                   joint_solution(local_joint_dof_indices[i]) = stokes_solution(\\n\\u00a0                     local_stokes_dof_indices[joint_fe.system_to_base_index(i)\\n\\u00a0                                                .second]);\\n\\u00a0                 }\\n\\u00a0               else\\n\\u00a0                 {\\n\\u00a0                   Assert(joint_fe.system_to_base_index(i).first.first == 1,\\n\\u00a0                          ExcInternalError());\\n\\u00a0                   Assert(joint_fe.system_to_base_index(i).second <\\n\\u00a0                            local_temperature_dof_indices.size(),\\n\\u00a0                          ExcInternalError());\\n\\u00a0                   joint_solution(local_joint_dof_indices[i]) =\\n\\u00a0                     temperature_solution(\\n\\u00a0                       local_temperature_dof_indices\\n\\u00a0                         [joint_fe.system_to_base_index(i).second]);\\n\\u00a0                 }\\n\\u00a0           }\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     joint_solution.compress(VectorOperation::insert);\\n\\u00a0 \\n\\u00a0     const IndexSet locally_relevant_joint_dofs =\\n\\u00a0       DoFTools::extract_locally_relevant_dofs(joint_dof_handler);\\n\\u00a0     TrilinosWrappers::MPI::Vector locally_relevant_joint_solution;\\n\\u00a0     locally_relevant_joint_solution.reinit(locally_relevant_joint_dofs,\\n\\u00a0                                            MPI_COMM_WORLD);\\n\\u00a0     locally_relevant_joint_solution = joint_solution;\\n\\u00a0 \\n\\u00a0     Postprocessor postprocessor(Utilities::MPI::this_mpi_process(\\n\\u00a0                                   MPI_COMM_WORLD),\\n\\u00a0                                 stokes_solution.block(1).min());\\n\\u00a0 \\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0     data_out.attach_dof_handler(joint_dof_handler);\\n\\u00a0     data_out.add_data_vector(locally_relevant_joint_solution, postprocessor);\\n\\u00a0     data_out.build_patches();\\n\\u00a0 \\n\\u00a0     static int out_index = 0;\\n\\u00a0     data_out.write_vtu_with_pvtu_record(\\n\\u00a0       \\\"./\\\", \\\"solution\\\", out_index, MPI_COMM_WORLD, 5);\\n\\u00a0 \\n\\u00a0     ++out_index;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOutDefinition data_out.h:147\\nTrilinosWrappers::MPI::Vector::reinitvoid reinit(const Vector &v, const bool omit_zeroing_entries=false, const bool allow_different_maps=false)Definition trilinos_vector.cc:204\\nVectorOperation::insert@ insertDefinition vector_operation.h:49\\n BoussinesqFlowProblem::refine_mesh\\nThis function isn't really new either. Since the setup_dofs function that we call in the middle has its own timer section, we split timing this function into two sections. It will also allow us to easily identify which of the two is more expensive.\\nOne thing of note, however, is that we only want to compute error indicators on the locally owned subdomain. In order to achieve this, we pass one additional argument to the KellyErrorEstimator::estimate function. Note that the vector for error estimates is resized to the number of active cells present on the current process, which is less than the total number of active cells on all processors (but more than the number of locally owned active cells); each processor only has a few coarse cells around the locally owned ones, as also explained in step-40.\\nThe local error estimates are then handed to a parallel version of GridRefinement (in namespace parallel::distributed::GridRefinement, see also step-40) which looks at the errors and finds the cells that need refinement by comparing the error values across processors. As in step-31, we want to limit the maximum grid level. So in case some cells have been marked that are already at the finest level, we simply clear the refine flags.\\n\\u00a0   template <int dim>\\n\\u00a0   void\\n\\u00a0   BoussinesqFlowProblem<dim>::refine_mesh(const unsigned int max_grid_level)\\n\\u00a0   {\\n\\u00a0     parallel::distributed::SolutionTransfer<dim, TrilinosWrappers::MPI::Vector>\\n\\u00a0       temperature_trans(temperature_dof_handler);\\n\\u00a0     parallel::distributed::SolutionTransfer<dim,\\n\\u00a0                                             TrilinosWrappers::MPI::BlockVector>\\n\\u00a0       stokes_trans(stokes_dof_handler);\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope timer_section(computing_timer,\\n\\u00a0                                        \\\"Refine mesh structure, part 1\\\");\\n\\u00a0 \\n\\u00a0       Vector<float> estimated_error_per_cell(triangulation.n_active_cells());\\n\\u00a0 \\n\\u00a0       KellyErrorEstimator<dim>::estimate(\\n\\u00a0         temperature_dof_handler,\\n\\u00a0         QGauss<dim - 1>(parameters.temperature_degree + 1),\\n\\u00a0         std::map<types::boundary_id, const Function<dim> *>(),\\n\\u00a0         temperature_solution,\\n\\u00a0         estimated_error_per_cell,\\n\\u00a0         ComponentMask(),\\n\\u00a0         nullptr,\\n\\u00a0         0,\\n\\u00a0         triangulation.locally_owned_subdomain());\\n\\u00a0 \\n\\u00a0       parallel::distributed::GridRefinement::refine_and_coarsen_fixed_fraction(\\n\\u00a0         triangulation, estimated_error_per_cell, 0.3, 0.1);\\n\\u00a0 \\n\\u00a0       if (triangulation.n_levels() > max_grid_level)\\n\\u00a0         for (typename Triangulation<dim>::active_cell_iterator cell =\\n\\u00a0                triangulation.begin_active(max_grid_level);\\n\\u00a0              cell != triangulation.end();\\n\\u00a0              ++cell)\\n\\u00a0           cell->clear_refine_flag();\\n\\u00a0 \\nComponentMaskDefinition component_mask.h:81\\nKellyErrorEstimator::estimatestatic void estimate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Quadrature< dim - 1 > &quadrature, const std::map< types::boundary_id, const Function< spacedim, Number > * > &neumann_bc, const ReadVector< Number > &solution, Vector< float > &error, const ComponentMask &component_mask={}, const Function< spacedim > *coefficients=nullptr, const unsigned int n_threads=numbers::invalid_unsigned_int, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id, const types::material_id material_id=numbers::invalid_material_id, const Strategy strategy=cell_diameter_over_24)\\nTriaActiveIteratorDefinition tria_iterator.h:755\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nTriangulation::endcell_iterator end() const\\nTriangulation::begin_activeactive_cell_iterator begin_active(const unsigned int level=0) const\\nparallel::TriangulationBase::locally_owned_subdomaintypes::subdomain_id locally_owned_subdomain() const overrideDefinition tria_base.cc:345\\nparallel::distributed::SolutionTransferDefinition solution_transfer.h:224\\nparallel::distributed::GridRefinement::refine_and_coarsen_fixed_fractionvoid refine_and_coarsen_fixed_fraction(::Triangulation< dim, spacedim > &tria, const ::Vector< Number > &criteria, const double top_fraction_of_error, const double bottom_fraction_of_error, const VectorTools::NormType norm_type=VectorTools::L1_norm)Definition grid_refinement.cc:576\\nWith all flags marked as necessary, we can then tell the parallel::distributed::SolutionTransfer objects to get ready to transfer data from one mesh to the next, which they will do when notified by Triangulation as part of the execute_coarsening_and_refinement() call. The syntax is similar to the non-parallel solution transfer (with the exception that here a pointer to the vector entries is enough). The remainder of the function further down below is then concerned with setting up the data structures again after mesh refinement and restoring the solution vectors on the new mesh.\\n\\u00a0       const std::vector<const TrilinosWrappers::MPI::Vector *> x_temperature = {\\n\\u00a0         &temperature_solution, &old_temperature_solution};\\n\\u00a0       const std::vector<const TrilinosWrappers::MPI::BlockVector *> x_stokes = {\\n\\u00a0         &stokes_solution, &old_stokes_solution};\\n\\u00a0 \\n\\u00a0       triangulation.prepare_coarsening_and_refinement();\\n\\u00a0 \\n\\u00a0       temperature_trans.prepare_for_coarsening_and_refinement(x_temperature);\\n\\u00a0       stokes_trans.prepare_for_coarsening_and_refinement(x_stokes);\\n\\u00a0 \\n\\u00a0       triangulation.execute_coarsening_and_refinement();\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     setup_dofs();\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope timer_section(computing_timer,\\n\\u00a0                                        \\\"Refine mesh structure, part 2\\\");\\n\\u00a0 \\n\\u00a0       {\\n\\u00a0         TrilinosWrappers::MPI::Vector distributed_temp1(temperature_rhs);\\n\\u00a0         TrilinosWrappers::MPI::Vector distributed_temp2(temperature_rhs);\\n\\u00a0 \\n\\u00a0         std::vector<TrilinosWrappers::MPI::Vector *> tmp = {&distributed_temp1,\\n\\u00a0                                                             &distributed_temp2};\\n\\u00a0         temperature_trans.interpolate(tmp);\\n\\u00a0 \\nparallel::distributed::Triangulation::execute_coarsening_and_refinementvirtual void execute_coarsening_and_refinement() overrideDefinition tria.cc:3320\\nparallel::distributed::Triangulation::prepare_coarsening_and_refinementvirtual bool prepare_coarsening_and_refinement() overrideDefinition tria.cc:2805\\nenforce constraints to make the interpolated solution conforming on the new mesh:\\n\\u00a0         temperature_constraints.distribute(distributed_temp1);\\n\\u00a0         temperature_constraints.distribute(distributed_temp2);\\n\\u00a0 \\n\\u00a0         temperature_solution     = distributed_temp1;\\n\\u00a0         old_temperature_solution = distributed_temp2;\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0       {\\n\\u00a0         TrilinosWrappers::MPI::BlockVector distributed_stokes(stokes_rhs);\\n\\u00a0         TrilinosWrappers::MPI::BlockVector old_distributed_stokes(stokes_rhs);\\n\\u00a0 \\n\\u00a0         std::vector<TrilinosWrappers::MPI::BlockVector *> stokes_tmp = {\\n\\u00a0           &distributed_stokes, &old_distributed_stokes};\\n\\u00a0 \\n\\u00a0         stokes_trans.interpolate(stokes_tmp);\\n\\u00a0 \\nenforce constraints to make the interpolated solution conforming on the new mesh:\\n\\u00a0         stokes_constraints.distribute(distributed_stokes);\\n\\u00a0         stokes_constraints.distribute(old_distributed_stokes);\\n\\u00a0 \\n\\u00a0         stokes_solution     = distributed_stokes;\\n\\u00a0         old_stokes_solution = old_distributed_stokes;\\n\\u00a0       }\\n\\u00a0     }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n BoussinesqFlowProblem::run\\nThis is the final and controlling function in this class. It, in fact, runs the entire rest of the program and is, once more, very similar to step-31. The only substantial difference is that we use a different mesh now (a GridGenerator::hyper_shell instead of a simple cube geometry).\\n\\u00a0   template <int dim>\\n\\u00a0   void BoussinesqFlowProblem<dim>::run()\\n\\u00a0   {\\n\\u00a0     GridGenerator::hyper_shell(triangulation,\\n\\u00a0                                Point<dim>(),\\n\\u00a0                                EquationData::R0,\\n\\u00a0                                EquationData::R1,\\n\\u00a0                                (dim == 3) ? 96 : 12,\\n\\u00a0                                true);\\n\\u00a0 \\n\\u00a0     global_Omega_diameter = GridTools::diameter(triangulation);\\n\\u00a0 \\n\\u00a0     triangulation.refine_global(parameters.initial_global_refinement);\\n\\u00a0 \\n\\u00a0     setup_dofs();\\n\\u00a0 \\n\\u00a0     unsigned int pre_refinement_step = 0;\\n\\u00a0 \\n\\u00a0   start_time_iteration:\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       TrilinosWrappers::MPI::Vector solution(\\n\\u00a0         temperature_dof_handler.locally_owned_dofs());\\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nGridGenerator::hyper_shellvoid hyper_shell(Triangulation< dim, spacedim > &tria, const Point< spacedim > &center, const double inner_radius, const double outer_radius, const unsigned int n_cells=0, bool colorize=false)\\nGridTools::diameterdouble diameter(const Triangulation< dim, spacedim > &tria)Definition grid_tools_geometry.cc:43\\nVectorTools::project supports parallel vector classes with most standard finite elements via deal.II's own native MatrixFree framework: since we use standard Lagrange elements of moderate order this function works well here.\\n\\u00a0       VectorTools::project(temperature_dof_handler,\\n\\u00a0                            temperature_constraints,\\n\\u00a0                            QGauss<dim>(parameters.temperature_degree + 2),\\n\\u00a0                            EquationData::TemperatureInitialValues<dim>(),\\n\\u00a0                            solution);\\nVectorTools::projectvoid project(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const AffineConstraints< typename VectorType::value_type > &constraints, const Quadrature< dim > &quadrature, const Function< spacedim, typename VectorType::value_type > &function, VectorType &vec, const bool enforce_zero_boundary=false, const Quadrature< dim - 1 > &q_boundary=(dim > 1 ? QGauss< dim - 1 >(2) :Quadrature< dim - 1 >()), const bool project_to_boundary_first=false)\\nHaving so computed the current temperature field, let us set the member variable that holds the temperature nodes. Strictly speaking, we really only need to set old_temperature_solution since the first thing we will do is to compute the Stokes solution that only requires the previous time step's temperature field. That said, nothing good can come from not initializing the other vectors as well (especially since it's a relatively cheap operation and we only have to do it once at the beginning of the program) if we ever want to extend our numerical method or physical model, and so we initialize old_temperature_solution and old_old_temperature_solution as well. The assignment makes sure that the vectors on the left hand side (which where initialized to contain ghost elements as well) also get the correct ghost elements. In other words, the assignment here requires communication between processors:\\n\\u00a0       temperature_solution         = solution;\\n\\u00a0       old_temperature_solution     = solution;\\n\\u00a0       old_old_temperature_solution = solution;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     timestep_number = 0;\\n\\u00a0     time_step = old_time_step = 0;\\n\\u00a0 \\n\\u00a0     double time = 0;\\n\\u00a0 \\n\\u00a0     do\\n\\u00a0       {\\n\\u00a0         pcout << \\\"Timestep \\\" << timestep_number\\n\\u00a0               << \\\":  t=\\\" << time / EquationData::year_in_seconds << \\\" years\\\"\\n\\u00a0               << std::endl;\\n\\u00a0 \\n\\u00a0         assemble_stokes_system();\\n\\u00a0         build_stokes_preconditioner();\\n\\u00a0         assemble_temperature_matrix();\\n\\u00a0 \\n\\u00a0         solve();\\n\\u00a0 \\n\\u00a0         pcout << std::endl;\\n\\u00a0 \\n\\u00a0         if ((timestep_number == 0) &&\\n\\u00a0             (pre_refinement_step < parameters.initial_adaptive_refinement))\\n\\u00a0           {\\n\\u00a0             refine_mesh(parameters.initial_global_refinement +\\n\\u00a0                         parameters.initial_adaptive_refinement);\\n\\u00a0             ++pre_refinement_step;\\n\\u00a0             goto start_time_iteration;\\n\\u00a0           }\\n\\u00a0         else if ((timestep_number > 0) &&\\n\\u00a0                  (timestep_number % parameters.adaptive_refinement_interval ==\\n\\u00a0                   0))\\n\\u00a0           refine_mesh(parameters.initial_global_refinement +\\n\\u00a0                       parameters.initial_adaptive_refinement);\\n\\u00a0 \\n\\u00a0         if ((parameters.generate_graphical_output == true) &&\\n\\u00a0             (timestep_number % parameters.graphical_output_interval == 0))\\n\\u00a0           output_results();\\n\\u00a0 \\nIn order to speed up linear solvers, we extrapolate the solutions from the old time levels to the new one. This gives a very good initial guess, cutting the number of iterations needed in solvers by more than one half. We do not need to extrapolate in the last iteration, so if we reached the final time, we stop here.\\nAs the last thing during a time step (before actually bumping up the number of the time step), we check whether the current time step number is divisible by 100, and if so we let the computing timer print a summary of CPU times spent so far.\\n\\u00a0         if (time > parameters.end_time * EquationData::year_in_seconds)\\n\\u00a0           break;\\n\\u00a0 \\n\\u00a0         TrilinosWrappers::MPI::BlockVector old_old_stokes_solution;\\n\\u00a0         old_old_stokes_solution      = old_stokes_solution;\\n\\u00a0         old_stokes_solution          = stokes_solution;\\n\\u00a0         old_old_temperature_solution = old_temperature_solution;\\n\\u00a0         old_temperature_solution     = temperature_solution;\\n\\u00a0         if (old_time_step > 0)\\n\\u00a0           {\\nTrilinos sadd does not like ghost vectors even as input. Copy into distributed vectors for now:\\n\\u00a0             {\\n\\u00a0               TrilinosWrappers::MPI::BlockVector distr_solution(stokes_rhs);\\n\\u00a0               distr_solution = stokes_solution;\\n\\u00a0               TrilinosWrappers::MPI::BlockVector distr_old_solution(stokes_rhs);\\n\\u00a0               distr_old_solution = old_old_stokes_solution;\\n\\u00a0               distr_solution.sadd(1. + time_step / old_time_step,\\n\\u00a0                                   -time_step / old_time_step,\\n\\u00a0                                   distr_old_solution);\\n\\u00a0               stokes_solution = distr_solution;\\n\\u00a0             }\\n\\u00a0             {\\n\\u00a0               TrilinosWrappers::MPI::Vector distr_solution(temperature_rhs);\\n\\u00a0               distr_solution = temperature_solution;\\n\\u00a0               TrilinosWrappers::MPI::Vector distr_old_solution(temperature_rhs);\\n\\u00a0               distr_old_solution = old_old_temperature_solution;\\n\\u00a0               distr_solution.sadd(1. + time_step / old_time_step,\\n\\u00a0                                   -time_step / old_time_step,\\n\\u00a0                                   distr_old_solution);\\n\\u00a0               temperature_solution = distr_solution;\\n\\u00a0             }\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         if ((timestep_number > 0) && (timestep_number % 100 == 0))\\n\\u00a0           computing_timer.print_summary();\\n\\u00a0 \\n\\u00a0         time += time_step;\\n\\u00a0         ++timestep_number;\\n\\u00a0       }\\n\\u00a0     while (true);\\n\\u00a0 \\nIf we are generating graphical output, do so also for the last time step unless we had just done so before we left the do-while loop\\n\\u00a0     if ((parameters.generate_graphical_output == true) &&\\n\\u00a0         !((timestep_number - 1) % parameters.graphical_output_interval == 0))\\n\\u00a0       output_results();\\n\\u00a0   }\\n\\u00a0 } // namespace Step32\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n The main function\\nThe main function is short as usual and very similar to the one in step-31. Since we use a parameter file which is specified as an argument in the command line, we have to read it in here and pass it on to the Parameters class for parsing. If no filename is given in the command line, we simply use the step-32.prm file which is distributed together with the program.\\nBecause 3d computations are simply very slow unless you throw a lot of processors at them, the program defaults to 2d. You can get the 3d version by changing the constant dimension below to 3.\\n\\u00a0 int main(int argc, char *argv[])\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       using namespace Step32;\\n\\u00a0       using namespace dealii;\\n\\u00a0 \\n\\u00a0       Utilities::MPI::MPI_InitFinalize mpi_initialization(\\n\\u00a0         argc, argv, numbers::invalid_unsigned_int);\\n\\u00a0 \\n\\u00a0       std::string parameter_filename;\\n\\u00a0       if (argc >= 2)\\n\\u00a0         parameter_filename = argv[1];\\n\\u00a0       else\\n\\u00a0         parameter_filename = \\\"step-32.prm\\\";\\n\\u00a0 \\n\\u00a0       const int                              dim = 2;\\n\\u00a0       BoussinesqFlowProblem<dim>::Parameters parameters(parameter_filename);\\n\\u00a0       BoussinesqFlowProblem<dim>             flow_problem(parameters);\\n\\u00a0       flow_problem.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0 \\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return 0;\\n\\u00a0 }\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\nnumbers::invalid_unsigned_intstatic const unsigned int invalid_unsigned_intDefinition types.h:220\\n Results\\nWhen run, the program simulates convection in 3d in much the same way as step-31 did, though with an entirely different testcase.\\nComparison of results with step-31\\nBefore we go to this testcase, however, let us show a few results from a slightly earlier version of this program that was solving exactly the testcase we used in step-31, just that we now solve it in parallel and with much higher resolution. We show these results mainly for comparison.\\nHere are two images that show this higher resolution if we choose a 3d computation in main() and if we set initial_refinement=3 and n_pre_refinement_steps=4. At the time steps shown, the meshes had around 72,000 and 236,000 cells, for a total of 2,680,000 and 8,250,000 degrees of freedom, respectively, more than an order of magnitude more than we had available in step-31:\\n\\n\\n \\n\\n \\n\\nThe computation was done on a subset of 50 processors of the Brazos cluster at Texas A&M University.\\nResults for a 2d circular shell testcase\\nNext, we will run step-32 with the parameter file in the directory with one change: we increase the final time to 1e9. Here we are using 16 processors. The command to launch is (note that step-32.prm is the default):\\n \\n$ mpirun -np 16 ./step-32\\n \\nNote that running a job on a cluster typically requires going through a job scheduler, which we won't discuss here. The output will look roughly like this:\\n \\n$ mpirun -np 16 ./step-32\\nNumber of active cells: 12,288 (on 6 levels)\\nNumber of degrees of freedom: 186,624 (99,840+36,864+49,920)\\n\\nTimestep 0:  t=0 years\\n\\n   Rebuilding Stokes preconditioner...\\n   Solving Stokes system... 41 iterations.\\n   Maximal velocity: 60.4935 cm/year\\n   Time step: 18166.9 years\\n   17 CG iterations for temperature\\n   Temperature range: 973 4273.16\\n\\nNumber of active cells: 15,921 (on 7 levels)\\nNumber of degrees of freedom: 252,723 (136,640+47,763+68,320)\\n\\nTimestep 0:  t=0 years\\n\\n   Rebuilding Stokes preconditioner...\\n   Solving Stokes system... 50 iterations.\\n   Maximal velocity: 60.3223 cm/year\\n   Time step: 10557.6 years\\n   19 CG iterations for temperature\\n   Temperature range: 973 4273.16\\n\\nNumber of active cells: 19,926 (on 8 levels)\\nNumber of degrees of freedom: 321,246 (174,312+59,778+87,156)\\n\\nTimestep 0:  t=0 years\\n\\n   Rebuilding Stokes preconditioner...\\n   Solving Stokes system... 50 iterations.\\n   Maximal velocity: 57.8396 cm/year\\n   Time step: 5453.78 years\\n   18 CG iterations for temperature\\n   Temperature range: 973 4273.16\\n\\nTimestep 1:  t=5453.78 years\\n\\n   Solving Stokes system... 49 iterations.\\n   Maximal velocity: 59.0231 cm/year\\n   Time step: 5345.86 years\\n   18 CG iterations for temperature\\n   Temperature range: 973 4273.16\\n\\nTimestep 2:  t=10799.6 years\\n\\n   Solving Stokes system... 24 iterations.\\n   Maximal velocity: 60.2139 cm/year\\n   Time step: 5241.51 years\\n   17 CG iterations for temperature\\n   Temperature range: 973 4273.16\\n\\n[...]\\n\\nTimestep 100:  t=272151 years\\n\\n   Solving Stokes system... 21 iterations.\\n   Maximal velocity: 161.546 cm/year\\n   Time step: 1672.96 years\\n   17 CG iterations for temperature\\n   Temperature range: 973 4282.57\\n\\nNumber of active cells: 56,085 (on 8 levels)\\nNumber of degrees of freedom: 903,408 (490,102+168,255+245,051)\\n\\n\\n\\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |       115s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| Assemble Stokes system          |       103 |      2.82s |       2.5% |\\n| Assemble temperature matrices   |        12 |     0.452s |      0.39% |\\n| Assemble temperature rhs        |       103 |      11.5s |        10% |\\n| Build Stokes preconditioner     |        12 |      2.09s |       1.8% |\\n| Solve Stokes system             |       103 |      90.4s |        79% |\\n| Solve temperature system        |       103 |      1.53s |       1.3% |\\n| Postprocessing                  |         3 |     0.532s |      0.46% |\\n| Refine mesh structure, part 1   |        12 |      0.93s |      0.81% |\\n| Refine mesh structure, part 2   |        12 |     0.384s |      0.33% |\\n| Setup dof systems               |        13 |      2.96s |       2.6% |\\n+---------------------------------+-----------+------------+------------+\\n\\n[...]\\n\\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |  9.14e+04s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| Assemble Stokes system          |     47045 |  2.05e+03s |       2.2% |\\n| Assemble temperature matrices   |      4707 |       310s |      0.34% |\\n| Assemble temperature rhs        |     47045 |   8.7e+03s |       9.5% |\\n| Build Stokes preconditioner     |      4707 |  1.48e+03s |       1.6% |\\n| Solve Stokes system             |     47045 |  7.34e+04s |        80% |\\n| Solve temperature system        |     47045 |  1.46e+03s |       1.6% |\\n| Postprocessing                  |      1883 |       222s |      0.24% |\\n| Refine mesh structure, part 1   |      4706 |       641s |       0.7% |\\n| Refine mesh structure, part 2   |      4706 |       259s |      0.28% |\\n| Setup dof systems               |      4707 |  1.86e+03s |         2% |\\n+---------------------------------+-----------+------------+------------+\\n \\nThe simulation terminates when the time reaches the 1 billion years selected in the input file. You can extrapolate from this how long a simulation would take for a different final time (the time step size ultimately settles on somewhere around 20,000 years, so computing for two billion years will take 100,000 time steps, give or take 20%). As can be seen here, we spend most of the compute time in assembling linear systems and \\u2014 above all \\u2014 in solving Stokes systems.\\nTo demonstrate the output we show the output from every 1250th time step here: \\n\\n   \\n\\n   \\n\\n   \\n\\n   \\n\\n   \\n\\nThe last two images show the grid as well as the partitioning of the mesh for the same computation with 16 subdomains and 16 processors. The full dynamics of this simulation are really only visible by looking at an animation, for example the one shown on this site. This image is well worth watching due to its artistic quality and entrancing depiction of the evolution of the magma plumes.\\nIf you watch the movie, you'll see that the convection pattern goes through several stages: First, it gets rid of the instable temperature layering with the hot material overlain by the dense cold material. After this great driver is removed and we have a sort of stable situation, a few blobs start to separate from the hot boundary layer at the inner ring and rise up, with a few cold fingers also dropping down from the outer boundary layer. During this phase, the solution remains mostly symmetric, reflecting the 12-fold symmetry of the original mesh. In a final phase, the fluid enters vigorous chaotic stirring in which all symmetries are lost. This is a pattern that then continues to dominate flow.\\nThese different phases can also be identified if we look at the maximal velocity as a function of time in the simulation:\\n\\nHere, the velocity (shown in centimeters per year) becomes very large, to the order of several meters per year) at the beginning when the temperature layering is instable. It then calms down to relatively small values before picking up again in the chaotic stirring regime. There, it remains in the range of 10-40 centimeters per year, quite within the physically expected region.\\nResults for a 3d spherical shell testcase\\n3d computations are very expensive computationally. Furthermore, as seen above, interesting behavior only starts after quite a long time requiring more CPU hours than is available on a typical cluster. Consequently, rather than showing a complete simulation here, let us simply show a couple of pictures we have obtained using the successor to this program, called ASPECT (short for Advanced Solver for Problems in Earth's ConvecTion), that is being developed independently of deal.II and that already incorporates some of the extensions discussed below. The following two pictures show isocontours of the temperature and the partition of the domain (along with the mesh) onto 512 processors:\\n\\n\\n \\n Possibilities for extensions\\nThere are many directions in which this program could be extended. As mentioned at the end of the introduction, most of these are under active development in the ASPECT (short for Advanced Solver for Problems in Earth's ConvecTion) code at the time this tutorial program is being finished. Specifically, the following are certainly topics that one should address to make the program more useful:\\n\\n\\nAdiabatic heating/cooling: The temperature field we get in our simulations after a while is mostly constant with boundary layers at the inner and outer boundary, and streamers of cold and hot material mixing everything. Yet, this doesn't match our expectation that things closer to the earth core should be hotter than closer to the surface. The reason is that the energy equation we have used does not include a term that describes adiabatic cooling and heating: rock, like gas, heats up as you compress it. Consequently, material that rises up cools adiabatically, and cold material that sinks down heats adiabatically. The correct temperature equation would therefore look somewhat like this:     \\n\\\\begin{eqnarray*}\\n    \\\\frac{D T}{Dt}\\n    -\\n    \\\\nabla \\\\cdot \\\\kappa \\\\nabla T &=& \\\\gamma + \\\\tau\\\\frac{Dp}{Dt},\\n  \\\\end{eqnarray*}\\n\\n or, expanding the advected derivative  \\\\(\\\\frac{D}{Dt} =\\n  \\\\frac{\\\\partial}{\\\\partial t} + \\\\mathbf u \\\\cdot \\\\nabla\\\\):         \\n\\\\begin{eqnarray*}\\n    \\\\frac{\\\\partial T}{\\\\partial t}\\n    +\\n    {\\\\mathbf u} \\\\cdot \\\\nabla T\\n    -\\n    \\\\nabla \\\\cdot \\\\kappa \\\\nabla T &=& \\\\gamma +\\n    \\\\tau\\\\left\\\\{\\\\frac{\\\\partial\\n    p}{\\\\partial t} + \\\\mathbf u \\\\cdot \\\\nabla p \\\\right\\\\}.\\n  \\\\end{eqnarray*}\\n\\n In other words, as pressure increases in a rock volume ( \\\\(\\\\frac{Dp}{Dt}>0\\\\)) we get an additional heat source, and vice versa.\\nThe time derivative of the pressure is a bit awkward to implement. If necessary, one could approximate using the fact outlined in the introduction that the pressure can be decomposed into a dynamic component due to temperature differences and the resulting flow, and a static component that results solely from the static pressure of the overlying rock. Since the latter is much bigger, one may approximate  \\\\(p\\\\approx p_{\\\\text{static}}=-\\\\rho_{\\\\text{ref}}\\n  [1+\\\\beta T_{\\\\text{ref}}] \\\\varphi\\\\), and consequently   \\\\(\\\\frac{Dp}{Dt} \\\\approx \\\\left\\\\{- \\\\mathbf u \\\\cdot \\\\nabla \\\\rho_{\\\\text{ref}}\\n  [1+\\\\beta T_{\\\\text{ref}}]\\\\varphi\\\\right\\\\} = \\\\rho_{\\\\text{ref}}\\n  [1+\\\\beta T_{\\\\text{ref}}] \\\\mathbf u \\\\cdot \\\\mathbf g\\\\). In other words, if the fluid is moving in the direction of gravity (downward) it will be compressed and because in that case  \\\\(\\\\mathbf u\\n  \\\\cdot \\\\mathbf g > 0\\\\) we get a positive heat source. Conversely, the fluid will cool down if it moves against the direction of gravity.\\n\\n\\n\\nCompressibility: As already hinted at in the temperature model above, mantle rocks are not incompressible. Rather, given the enormous pressures in the earth mantle (at the core-mantle boundary, the pressure is approximately 140 GPa, equivalent to 1,400,000 times atmospheric pressure), rock actually does compress to something around 1.5 times the density it would have at surface pressure. Modeling this presents any number of difficulties. Primarily, the mass conservation equation is no longer \\\\(\\\\textrm{div}\\\\;\\\\mathbf u=0\\\\) but should read \\\\(\\\\textrm{div}(\\\\rho\\\\mathbf u)=0\\\\) where the density \\\\(\\\\rho\\\\) is now no longer spatially constant but depends on temperature and pressure. A consequence is that the model is now no longer linear; a linearized version of the Stokes equation is also no longer symmetric requiring us to rethink preconditioners and, possibly, even the discretization. We won't go into detail here as to how this can be resolved.\\n\\n\\n\\nNonlinear material models: As already hinted at in various places, material parameters such as the density, the viscosity, and the various thermal parameters are not constant throughout the earth mantle. Rather, they nonlinearly depend on the pressure and temperature, and in the case of the viscosity on the strain rate \\\\(\\\\varepsilon(\\\\mathbf u)\\\\). For complicated models, the only way to solve such models accurately may be to actually iterate this dependence out in each time step, rather than simply freezing coefficients at values extrapolated from the previous time step(s).\\n\\n\\n\\nCheckpoint/restart: Running this program in 2d on a number of processors allows solving realistic models in a day or two. However, in 3d, compute times are so large that one runs into two typical problems: (i) On most compute clusters, the queuing system limits run times for individual jobs are to 2 or 3 days; (ii) losing the results of a computation due to hardware failures, misconfigurations, or power outages is a shame when running on hundreds of processors for a couple of days. Both of these problems can be addressed by periodically saving the state of the program and, if necessary, restarting the program at this point. This technique is commonly called checkpoint/restart and it requires that the entire state of the program is written to a permanent storage location (e.g. a hard drive). Given the complexity of the data structures of this program, this is not entirely trivial (it may also involve writing gigabytes or more of data), but it can be made easier by realizing that one can save the state between two time steps where it essentially only consists of the mesh and solution vectors; during restart one would then first re-enumerate degrees of freedom in the same way as done before and then re-assemble matrices. Nevertheless, given the distributed nature of the data structures involved here, saving and restoring the state of a program is not trivial. An additional complexity is introduced by the fact that one may want to change the number of processors between runs, for example because one may wish to continue computing on a mesh that is finer than the one used to precompute a starting temperature field at an intermediate time.\\n\\n\\n\\nPredictive postprocessing: The point of computations like this is not simply to solve the equations. Rather, it is typically the exploration of different physical models and their comparison with things that we can measure at the earth surface, in order to find which models are realistic and which are contradicted by reality. To this end, we need to compute quantities from our solution vectors that are related to what we can observe. Among these are, for example, heatfluxes at the surface of the earth, as well as seismic velocities throughout the mantle as these affect earthquake waves that are recorded by seismographs.\\n\\n\\n\\nBetter refinement criteria: As can be seen above for the 3d case, the mesh in 3d is primarily refined along the inner boundary. This is because the boundary layer there is stronger than any other transition in the domain, leading us to refine there almost exclusively and basically not at all following the plumes. One certainly needs better refinement criteria to track the parts of the solution we are really interested in better than the criterion used here, namely the KellyErrorEstimator applied to the temperature, is able to. \\n\\nThere are many other ways to extend the current program. However, rather than discussing them here, let us point to the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that constitutes the further development of step-32 and that already includes many such possible extensions.\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2008 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Authors: Martin Kronbichler, Uppsala University,\\n *          Wolfgang Bangerth, Texas A&M University,\\n *          Timo Heister, University of Goettingen, 2008-2011\\n */\\n \\n \\n \\n#include <deal.II/base/quadrature_lib.h>\\n#include <deal.II/base/function.h>\\n#include <deal.II/base/utilities.h>\\n#include <deal.II/base/conditional_ostream.h>\\n#include <deal.II/base/work_stream.h>\\n#include <deal.II/base/timer.h>\\n#include <deal.II/base/parameter_handler.h>\\n \\n#include <deal.II/lac/full_matrix.h>\\n#include <deal.II/lac/solver_bicgstab.h>\\n#include <deal.II/lac/solver_cg.h>\\n#include <deal.II/lac/solver_gmres.h>\\n#include <deal.II/lac/affine_constraints.h>\\n#include <deal.II/lac/block_sparsity_pattern.h>\\n#include <deal.II/lac/trilinos_parallel_block_vector.h>\\n#include <deal.II/lac/trilinos_sparse_matrix.h>\\n#include <deal.II/lac/trilinos_block_sparse_matrix.h>\\n#include <deal.II/lac/trilinos_precondition.h>\\n#include <deal.II/lac/trilinos_solver.h>\\n \\n#include <deal.II/grid/tria.h>\\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/filtered_iterator.h>\\n#include <deal.II/grid/manifold_lib.h>\\n#include <deal.II/grid/grid_tools.h>\\n#include <deal.II/grid/grid_refinement.h>\\n \\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/dofs/dof_renumbering.h>\\n#include <deal.II/dofs/dof_tools.h>\\n \\n#include <deal.II/fe/fe_q.h>\\n#include <deal.II/fe/fe_dgq.h>\\n#include <deal.II/fe/fe_dgp.h>\\n#include <deal.II/fe/fe_system.h>\\n#include <deal.II/fe/fe_values.h>\\n#include <deal.II/fe/mapping_q.h>\\n \\n#include <deal.II/numerics/vector_tools.h>\\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/error_estimator.h>\\n#include <deal.II/numerics/solution_transfer.h>\\n \\n#include <fstream>\\n#include <iostream>\\n#include <limits>\\n#include <locale>\\n#include <string>\\n \\n#include <deal.II/distributed/solution_transfer.h>\\n \\n#include <deal.II/base/index_set.h>\\n#include <deal.II/distributed/tria.h>\\n#include <deal.II/distributed/grid_refinement.h>\\n \\n \\nnamespace Step32\\n{\\n using namespace dealii;\\n \\n \\n namespace EquationData\\n  {\\n constexpr double eta                   = 1e21;    /* Pa s       */\\n constexpr double kappa                 = 1e-6;    /* m^2 / s    */\\n constexpr double reference_density     = 3300;    /* kg / m^3   */\\n constexpr double reference_temperature = 293;     /* K          */\\n constexpr double expansion_coefficient = 2e-5;    /* 1/K        */\\n constexpr double specific_heat         = 1250;    /* J / K / kg */\\n constexpr double radiogenic_heating    = 7.4e-12; /* W / kg     */\\n \\n \\n constexpr double R0 = 6371000. - 2890000.; /* m          */\\n constexpr double R1 = 6371000. - 35000.;   /* m          */\\n \\n constexpr double T0 = 4000 + 273; /* K          */\\n constexpr double T1 = 700 + 273;  /* K          */\\n \\n \\n double density(const double temperature)\\n    {\\n return (\\n        reference_density *\\n        (1 - expansion_coefficient * (temperature - reference_temperature)));\\n    }\\n \\n \\n template <int dim>\\n Tensor<1, dim> gravity_vector(const Point<dim> &p)\\n    {\\n const double r = p.norm();\\n return -(1.245e-6 * r + 7.714e13 / r / r) * p / r;\\n    }\\n \\n \\n \\n template <int dim>\\n class TemperatureInitialValues : public Function<dim>\\n    {\\n public:\\n      TemperatureInitialValues()\\n        : Function<dim>(1)\\n      {}\\n \\n virtual double value(const Point<dim>  &p,\\n const unsigned int component = 0) const override;\\n \\n virtual void vector_value(const Point<dim> &p,\\n Vector<double>   &value) const override;\\n    };\\n \\n \\n \\n template <int dim>\\n double TemperatureInitialValues<dim>::value(const Point<dim> &p,\\n const unsigned int) const\\n {\\n const double r = p.norm();\\n const double h = R1 - R0;\\n \\n const double s = (r - R0) / h;\\n const double q =\\n        (dim == 3) ? std::max(0.0, cos(numbers::PI * abs(p[2] / R1))) : 1.0;\\n const double phi = std::atan2(p[0], p[1]);\\n const double tau = s + 0.2 * s * (1 - s) * std::sin(6 * phi) * q;\\n \\n return T0 * (1.0 - tau) + T1 * tau;\\n    }\\n \\n \\n template <int dim>\\n void\\n    TemperatureInitialValues<dim>::vector_value(const Point<dim> &p,\\n Vector<double>   &values) const\\n {\\n for (unsigned int c = 0; c < this->n_components; ++c)\\n        values(c) = TemperatureInitialValues<dim>::value(p, c);\\n    }\\n \\n \\n constexpr double pressure_scaling = eta / 10000;\\n \\n const double year_in_seconds = 60 * 60 * 24 * 365.2425;\\n \\n  } // namespace EquationData\\n \\n \\n \\n \\n namespace LinearSolvers\\n  {\\n template <class PreconditionerTypeA, class PreconditionerTypeMp>\\n class BlockSchurPreconditioner : public Subscriptor\\n    {\\n public:\\n      BlockSchurPreconditioner(const TrilinosWrappers::BlockSparseMatrix &S,\\n const TrilinosWrappers::BlockSparseMatrix &Spre,\\n const PreconditionerTypeMp &Mppreconditioner,\\n const PreconditionerTypeA  &Apreconditioner,\\n const bool                  do_solve_A)\\n        : stokes_matrix(&S)\\n        , stokes_preconditioner_matrix(&Spre)\\n        , mp_preconditioner(Mppreconditioner)\\n        , a_preconditioner(Apreconditioner)\\n        , do_solve_A(do_solve_A)\\n      {}\\n \\n void vmult(TrilinosWrappers::MPI::BlockVector       &dst,\\n const TrilinosWrappers::MPI::BlockVector &src) const\\n {\\n TrilinosWrappers::MPI::Vector utmp(src.block(0));\\n \\n        {\\n SolverControl solver_control(5000, 1e-6 * src.block(1).l2_norm());\\n \\n SolverCG<TrilinosWrappers::MPI::Vector> solver(solver_control);\\n \\n          solver.solve(stokes_preconditioner_matrix->block(1, 1),\\n                       dst.block(1),\\n                       src.block(1),\\n                       mp_preconditioner);\\n \\n          dst.block(1) *= -1.0;\\n        }\\n \\n        {\\n          stokes_matrix->block(0, 1).vmult(utmp, dst.block(1));\\n          utmp *= -1.0;\\n          utmp.add(src.block(0));\\n        }\\n \\n if (do_solve_A == true)\\n          {\\n SolverControl solver_control(5000, utmp.l2_norm() * 1e-2);\\n TrilinosWrappers::SolverCG solver(solver_control);\\n            solver.solve(stokes_matrix->block(0, 0),\\n                         dst.block(0),\\n                         utmp,\\n                         a_preconditioner);\\n          }\\n else\\n          a_preconditioner.vmult(dst.block(0), utmp);\\n      }\\n \\n private:\\n const SmartPointer<const TrilinosWrappers::BlockSparseMatrix>\\n        stokes_matrix;\\n const SmartPointer<const TrilinosWrappers::BlockSparseMatrix>\\n                                  stokes_preconditioner_matrix;\\n const PreconditionerTypeMp &mp_preconditioner;\\n const PreconditionerTypeA  &a_preconditioner;\\n const bool                  do_solve_A;\\n    };\\n  } // namespace LinearSolvers\\n \\n \\n \\n namespace Assembly\\n  {\\n namespace Scratch\\n    {\\n template <int dim>\\n struct StokesPreconditioner\\n      {\\n        StokesPreconditioner(const FiniteElement<dim> &stokes_fe,\\n const Quadrature<dim>    &stokes_quadrature,\\n const Mapping<dim>       &mapping,\\n const UpdateFlags         update_flags);\\n \\n        StokesPreconditioner(const StokesPreconditioner &data);\\n \\n \\n FEValues<dim> stokes_fe_values;\\n \\n        std::vector<Tensor<2, dim>> grad_phi_u;\\n        std::vector<double>         phi_p;\\n      };\\n \\n template <int dim>\\n      StokesPreconditioner<dim>::StokesPreconditioner(\\n const FiniteElement<dim> &stokes_fe,\\n const Quadrature<dim>    &stokes_quadrature,\\n const Mapping<dim>       &mapping,\\n const UpdateFlags         update_flags)\\n        : stokes_fe_values(mapping, stokes_fe, stokes_quadrature, update_flags)\\n        , grad_phi_u(stokes_fe.n_dofs_per_cell())\\n        , phi_p(stokes_fe.n_dofs_per_cell())\\n      {}\\n \\n \\n \\n template <int dim>\\n      StokesPreconditioner<dim>::StokesPreconditioner(\\n const StokesPreconditioner &scratch)\\n        : stokes_fe_values(scratch.stokes_fe_values.get_mapping(),\\n                           scratch.stokes_fe_values.get_fe(),\\n                           scratch.stokes_fe_values.get_quadrature(),\\n                           scratch.stokes_fe_values.get_update_flags())\\n        , grad_phi_u(scratch.grad_phi_u)\\n        , phi_p(scratch.phi_p)\\n      {}\\n \\n \\n \\n template <int dim>\\n struct StokesSystem : public StokesPreconditioner<dim>\\n      {\\n        StokesSystem(const FiniteElement<dim> &stokes_fe,\\n const Mapping<dim>       &mapping,\\n const Quadrature<dim>    &stokes_quadrature,\\n const UpdateFlags         stokes_update_flags,\\n const FiniteElement<dim> &temperature_fe,\\n const UpdateFlags         temperature_update_flags);\\n \\n        StokesSystem(const StokesSystem<dim> &data);\\n \\n \\n FEValues<dim> temperature_fe_values;\\n \\n        std::vector<Tensor<1, dim>>          phi_u;\\n        std::vector<SymmetricTensor<2, dim>> grads_phi_u;\\n        std::vector<double>                  div_phi_u;\\n \\n        std::vector<double> old_temperature_values;\\n      };\\n \\n \\n template <int dim>\\n      StokesSystem<dim>::StokesSystem(\\n const FiniteElement<dim> &stokes_fe,\\n const Mapping<dim>       &mapping,\\n const Quadrature<dim>    &stokes_quadrature,\\n const UpdateFlags         stokes_update_flags,\\n const FiniteElement<dim> &temperature_fe,\\n const UpdateFlags         temperature_update_flags)\\n        : StokesPreconditioner<dim>(stokes_fe,\\n                                    stokes_quadrature,\\n                                    mapping,\\n                                    stokes_update_flags)\\n        , temperature_fe_values(mapping,\\n                                temperature_fe,\\n                                stokes_quadrature,\\n                                temperature_update_flags)\\n        , phi_u(stokes_fe.n_dofs_per_cell())\\n        , grads_phi_u(stokes_fe.n_dofs_per_cell())\\n        , div_phi_u(stokes_fe.n_dofs_per_cell())\\n        , old_temperature_values(stokes_quadrature.size())\\n      {}\\n \\n \\n template <int dim>\\n      StokesSystem<dim>::StokesSystem(const StokesSystem<dim> &scratch)\\n        : StokesPreconditioner<dim>(scratch)\\n        , temperature_fe_values(\\n            scratch.temperature_fe_values.get_mapping(),\\n            scratch.temperature_fe_values.get_fe(),\\n            scratch.temperature_fe_values.get_quadrature(),\\n            scratch.temperature_fe_values.get_update_flags())\\n        , phi_u(scratch.phi_u)\\n        , grads_phi_u(scratch.grads_phi_u)\\n        , div_phi_u(scratch.div_phi_u)\\n        , old_temperature_values(scratch.old_temperature_values)\\n      {}\\n \\n \\n template <int dim>\\n struct TemperatureMatrix\\n      {\\n        TemperatureMatrix(const FiniteElement<dim> &temperature_fe,\\n const Mapping<dim>       &mapping,\\n const Quadrature<dim>    &temperature_quadrature);\\n \\n        TemperatureMatrix(const TemperatureMatrix &data);\\n \\n \\n FEValues<dim> temperature_fe_values;\\n \\n        std::vector<double>         phi_T;\\n        std::vector<Tensor<1, dim>> grad_phi_T;\\n      };\\n \\n \\n template <int dim>\\n      TemperatureMatrix<dim>::TemperatureMatrix(\\n const FiniteElement<dim> &temperature_fe,\\n const Mapping<dim>       &mapping,\\n const Quadrature<dim>    &temperature_quadrature)\\n        : temperature_fe_values(mapping,\\n                                temperature_fe,\\n                                temperature_quadrature,\\n update_values | update_gradients |\\n update_JxW_values)\\n        , phi_T(temperature_fe.n_dofs_per_cell())\\n        , grad_phi_T(temperature_fe.n_dofs_per_cell())\\n      {}\\n \\n \\n template <int dim>\\n      TemperatureMatrix<dim>::TemperatureMatrix(\\n const TemperatureMatrix &scratch)\\n        : temperature_fe_values(\\n            scratch.temperature_fe_values.get_mapping(),\\n            scratch.temperature_fe_values.get_fe(),\\n            scratch.temperature_fe_values.get_quadrature(),\\n            scratch.temperature_fe_values.get_update_flags())\\n        , phi_T(scratch.phi_T)\\n        , grad_phi_T(scratch.grad_phi_T)\\n      {}\\n \\n \\n template <int dim>\\n struct TemperatureRHS\\n      {\\n        TemperatureRHS(const FiniteElement<dim> &temperature_fe,\\n const FiniteElement<dim> &stokes_fe,\\n const Mapping<dim>       &mapping,\\n const Quadrature<dim>    &quadrature);\\n \\n        TemperatureRHS(const TemperatureRHS &data);\\n \\n \\n FEValues<dim> temperature_fe_values;\\n FEValues<dim> stokes_fe_values;\\n \\n        std::vector<double>         phi_T;\\n        std::vector<Tensor<1, dim>> grad_phi_T;\\n \\n        std::vector<Tensor<1, dim>> old_velocity_values;\\n        std::vector<Tensor<1, dim>> old_old_velocity_values;\\n \\n        std::vector<SymmetricTensor<2, dim>> old_strain_rates;\\n        std::vector<SymmetricTensor<2, dim>> old_old_strain_rates;\\n \\n        std::vector<double>         old_temperature_values;\\n        std::vector<double>         old_old_temperature_values;\\n        std::vector<Tensor<1, dim>> old_temperature_grads;\\n        std::vector<Tensor<1, dim>> old_old_temperature_grads;\\n        std::vector<double>         old_temperature_laplacians;\\n        std::vector<double>         old_old_temperature_laplacians;\\n      };\\n \\n \\n template <int dim>\\n      TemperatureRHS<dim>::TemperatureRHS(\\n const FiniteElement<dim> &temperature_fe,\\n const FiniteElement<dim> &stokes_fe,\\n const Mapping<dim>       &mapping,\\n const Quadrature<dim>    &quadrature)\\n        : temperature_fe_values(mapping,\\n                                temperature_fe,\\n                                quadrature,\\n update_values | update_gradients |\\n update_hessians | update_quadrature_points |\\n update_JxW_values)\\n        , stokes_fe_values(mapping,\\n                           stokes_fe,\\n                           quadrature,\\n update_values | update_gradients)\\n        , phi_T(temperature_fe.n_dofs_per_cell())\\n        , grad_phi_T(temperature_fe.n_dofs_per_cell())\\n        ,\\n \\n        old_velocity_values(quadrature.size())\\n        , old_old_velocity_values(quadrature.size())\\n        , old_strain_rates(quadrature.size())\\n        , old_old_strain_rates(quadrature.size())\\n        ,\\n \\n        old_temperature_values(quadrature.size())\\n        , old_old_temperature_values(quadrature.size())\\n        , old_temperature_grads(quadrature.size())\\n        , old_old_temperature_grads(quadrature.size())\\n        , old_temperature_laplacians(quadrature.size())\\n        , old_old_temperature_laplacians(quadrature.size())\\n      {}\\n \\n \\n template <int dim>\\n      TemperatureRHS<dim>::TemperatureRHS(const TemperatureRHS &scratch)\\n        : temperature_fe_values(\\n            scratch.temperature_fe_values.get_mapping(),\\n            scratch.temperature_fe_values.get_fe(),\\n            scratch.temperature_fe_values.get_quadrature(),\\n            scratch.temperature_fe_values.get_update_flags())\\n        , stokes_fe_values(scratch.stokes_fe_values.get_mapping(),\\n                           scratch.stokes_fe_values.get_fe(),\\n                           scratch.stokes_fe_values.get_quadrature(),\\n                           scratch.stokes_fe_values.get_update_flags())\\n        , phi_T(scratch.phi_T)\\n        , grad_phi_T(scratch.grad_phi_T)\\n        ,\\n \\n        old_velocity_values(scratch.old_velocity_values)\\n        , old_old_velocity_values(scratch.old_old_velocity_values)\\n        , old_strain_rates(scratch.old_strain_rates)\\n        , old_old_strain_rates(scratch.old_old_strain_rates)\\n        ,\\n \\n        old_temperature_values(scratch.old_temperature_values)\\n        , old_old_temperature_values(scratch.old_old_temperature_values)\\n        , old_temperature_grads(scratch.old_temperature_grads)\\n        , old_old_temperature_grads(scratch.old_old_temperature_grads)\\n        , old_temperature_laplacians(scratch.old_temperature_laplacians)\\n        , old_old_temperature_laplacians(scratch.old_old_temperature_laplacians)\\n      {}\\n    } // namespace Scratch\\n \\n \\n namespace CopyData\\n    {\\n template <int dim>\\n struct StokesPreconditioner\\n      {\\n        StokesPreconditioner(const FiniteElement<dim> &stokes_fe);\\n        StokesPreconditioner(const StokesPreconditioner &data);\\n        StokesPreconditioner &operator=(const StokesPreconditioner &) = default;\\n \\n FullMatrix<double>                   local_matrix;\\n        std::vector<types::global_dof_index> local_dof_indices;\\n      };\\n \\n template <int dim>\\n      StokesPreconditioner<dim>::StokesPreconditioner(\\n const FiniteElement<dim> &stokes_fe)\\n        : local_matrix(stokes_fe.n_dofs_per_cell(), stokes_fe.n_dofs_per_cell())\\n        , local_dof_indices(stokes_fe.n_dofs_per_cell())\\n      {}\\n \\n template <int dim>\\n      StokesPreconditioner<dim>::StokesPreconditioner(\\n const StokesPreconditioner &data)\\n        : local_matrix(data.local_matrix)\\n        , local_dof_indices(data.local_dof_indices)\\n      {}\\n \\n \\n \\n template <int dim>\\n struct StokesSystem : public StokesPreconditioner<dim>\\n      {\\n        StokesSystem(const FiniteElement<dim> &stokes_fe);\\n \\n Vector<double> local_rhs;\\n      };\\n \\n template <int dim>\\n      StokesSystem<dim>::StokesSystem(const FiniteElement<dim> &stokes_fe)\\n        : StokesPreconditioner<dim>(stokes_fe)\\n        , local_rhs(stokes_fe.n_dofs_per_cell())\\n      {}\\n \\n \\n \\n template <int dim>\\n struct TemperatureMatrix\\n      {\\n        TemperatureMatrix(const FiniteElement<dim> &temperature_fe);\\n \\n FullMatrix<double>                   local_mass_matrix;\\n FullMatrix<double>                   local_stiffness_matrix;\\n        std::vector<types::global_dof_index> local_dof_indices;\\n      };\\n \\n template <int dim>\\n      TemperatureMatrix<dim>::TemperatureMatrix(\\n const FiniteElement<dim> &temperature_fe)\\n        : local_mass_matrix(temperature_fe.n_dofs_per_cell(),\\n                            temperature_fe.n_dofs_per_cell())\\n        , local_stiffness_matrix(temperature_fe.n_dofs_per_cell(),\\n                                 temperature_fe.n_dofs_per_cell())\\n        , local_dof_indices(temperature_fe.n_dofs_per_cell())\\n      {}\\n \\n \\n \\n template <int dim>\\n struct TemperatureRHS\\n      {\\n        TemperatureRHS(const FiniteElement<dim> &temperature_fe);\\n \\n Vector<double>                       local_rhs;\\n        std::vector<types::global_dof_index> local_dof_indices;\\n FullMatrix<double>                   matrix_for_bc;\\n      };\\n \\n template <int dim>\\n      TemperatureRHS<dim>::TemperatureRHS(\\n const FiniteElement<dim> &temperature_fe)\\n        : local_rhs(temperature_fe.n_dofs_per_cell())\\n        , local_dof_indices(temperature_fe.n_dofs_per_cell())\\n        , matrix_for_bc(temperature_fe.n_dofs_per_cell(),\\n                        temperature_fe.n_dofs_per_cell())\\n      {}\\n    } // namespace CopyData\\n  }   // namespace Assembly\\n \\n \\n \\n template <int dim>\\n class BoussinesqFlowProblem\\n  {\\n public:\\n struct Parameters;\\n    BoussinesqFlowProblem(Parameters &parameters);\\n void run();\\n \\n private:\\n void   setup_dofs();\\n void   assemble_stokes_preconditioner();\\n void   build_stokes_preconditioner();\\n void   assemble_stokes_system();\\n void   assemble_temperature_matrix();\\n void   assemble_temperature_system(const double maximal_velocity);\\n double get_maximal_velocity() const;\\n double get_cfl_number() const;\\n double get_entropy_variation(const double average_temperature) const;\\n    std::pair<double, double> get_extrapolated_temperature_range() const;\\n void                      solve();\\n void                      output_results();\\n void                      refine_mesh(const unsigned int max_grid_level);\\n \\n double compute_viscosity(\\n const std::vector<double>         &old_temperature,\\n const std::vector<double>         &old_old_temperature,\\n const std::vector<Tensor<1, dim>> &old_temperature_grads,\\n const std::vector<Tensor<1, dim>> &old_old_temperature_grads,\\n const std::vector<double>         &old_temperature_laplacians,\\n const std::vector<double>         &old_old_temperature_laplacians,\\n const std::vector<Tensor<1, dim>> &old_velocity_values,\\n const std::vector<Tensor<1, dim>> &old_old_velocity_values,\\n const std::vector<SymmetricTensor<2, dim>> &old_strain_rates,\\n const std::vector<SymmetricTensor<2, dim>> &old_old_strain_rates,\\n const double                                global_u_infty,\\n const double                                global_T_variation,\\n const double                                average_temperature,\\n const double                                global_entropy_variation,\\n const double                                cell_diameter) const;\\n \\n public:\\n struct Parameters\\n    {\\n      Parameters(const std::string &parameter_filename);\\n \\n static void declare_parameters(ParameterHandler &prm);\\n void        parse_parameters(ParameterHandler &prm);\\n \\n double end_time;\\n \\n unsigned int initial_global_refinement;\\n unsigned int initial_adaptive_refinement;\\n \\n bool         generate_graphical_output;\\n unsigned int graphical_output_interval;\\n \\n unsigned int adaptive_refinement_interval;\\n \\n double stabilization_alpha;\\n double stabilization_c_R;\\n double stabilization_beta;\\n \\n unsigned int stokes_velocity_degree;\\n bool         use_locally_conservative_discretization;\\n \\n unsigned int temperature_degree;\\n    };\\n \\n private:\\n    Parameters &parameters;\\n \\n ConditionalOStream pcout;\\n \\n parallel::distributed::Triangulation<dim> triangulation;\\n double                                    global_Omega_diameter;\\n \\n const MappingQ<dim> mapping;\\n \\n const FESystem<dim>       stokes_fe;\\n DoFHandler<dim>           stokes_dof_handler;\\n AffineConstraints<double> stokes_constraints;\\n \\n TrilinosWrappers::BlockSparseMatrix stokes_matrix;\\n TrilinosWrappers::BlockSparseMatrix stokes_preconditioner_matrix;\\n \\n TrilinosWrappers::MPI::BlockVector stokes_solution;\\n TrilinosWrappers::MPI::BlockVector old_stokes_solution;\\n TrilinosWrappers::MPI::BlockVector stokes_rhs;\\n \\n \\n const FE_Q<dim>           temperature_fe;\\n DoFHandler<dim>           temperature_dof_handler;\\n AffineConstraints<double> temperature_constraints;\\n \\n TrilinosWrappers::SparseMatrix temperature_mass_matrix;\\n TrilinosWrappers::SparseMatrix temperature_stiffness_matrix;\\n TrilinosWrappers::SparseMatrix temperature_matrix;\\n \\n TrilinosWrappers::MPI::Vector temperature_solution;\\n TrilinosWrappers::MPI::Vector old_temperature_solution;\\n TrilinosWrappers::MPI::Vector old_old_temperature_solution;\\n TrilinosWrappers::MPI::Vector temperature_rhs;\\n \\n \\n double       time_step;\\n double       old_time_step;\\n unsigned int timestep_number;\\n \\n    std::shared_ptr<TrilinosWrappers::PreconditionAMG>    Amg_preconditioner;\\n    std::shared_ptr<TrilinosWrappers::PreconditionJacobi> Mp_preconditioner;\\n    std::shared_ptr<TrilinosWrappers::PreconditionJacobi> T_preconditioner;\\n \\n bool rebuild_stokes_matrix;\\n bool rebuild_stokes_preconditioner;\\n bool rebuild_temperature_matrices;\\n bool rebuild_temperature_preconditioner;\\n \\n TimerOutput computing_timer;\\n \\n void setup_stokes_matrix(\\n const std::vector<IndexSet> &stokes_partitioning,\\n const std::vector<IndexSet> &stokes_relevant_partitioning);\\n void setup_stokes_preconditioner(\\n const std::vector<IndexSet> &stokes_partitioning,\\n const std::vector<IndexSet> &stokes_relevant_partitioning);\\n void setup_temperature_matrices(\\n const IndexSet &temperature_partitioning,\\n const IndexSet &temperature_relevant_partitioning);\\n \\n \\n void local_assemble_stokes_preconditioner(\\n const typename DoFHandler<dim>::active_cell_iterator &cell,\\n      Assembly::Scratch::StokesPreconditioner<dim>         &scratch,\\n      Assembly::CopyData::StokesPreconditioner<dim>        &data);\\n \\n void copy_local_to_global_stokes_preconditioner(\\n const Assembly::CopyData::StokesPreconditioner<dim> &data);\\n \\n \\n void local_assemble_stokes_system(\\n const typename DoFHandler<dim>::active_cell_iterator &cell,\\n      Assembly::Scratch::StokesSystem<dim>                 &scratch,\\n      Assembly::CopyData::StokesSystem<dim>                &data);\\n \\n void copy_local_to_global_stokes_system(\\n const Assembly::CopyData::StokesSystem<dim> &data);\\n \\n \\n void local_assemble_temperature_matrix(\\n const typename DoFHandler<dim>::active_cell_iterator &cell,\\n      Assembly::Scratch::TemperatureMatrix<dim>            &scratch,\\n      Assembly::CopyData::TemperatureMatrix<dim>           &data);\\n \\n void copy_local_to_global_temperature_matrix(\\n const Assembly::CopyData::TemperatureMatrix<dim> &data);\\n \\n \\n \\n void local_assemble_temperature_rhs(\\n const std::pair<double, double> global_T_range,\\n const double                    global_max_velocity,\\n const double                    global_entropy_variation,\\n const typename DoFHandler<dim>::active_cell_iterator &cell,\\n      Assembly::Scratch::TemperatureRHS<dim>               &scratch,\\n      Assembly::CopyData::TemperatureRHS<dim>              &data);\\n \\n void copy_local_to_global_temperature_rhs(\\n const Assembly::CopyData::TemperatureRHS<dim> &data);\\n \\n class Postprocessor;\\n  };\\n \\n \\n \\n template <int dim>\\n  BoussinesqFlowProblem<dim>::Parameters::Parameters(\\n const std::string &parameter_filename)\\n    : end_time(1e8)\\n    , initial_global_refinement(2)\\n    , initial_adaptive_refinement(2)\\n    , adaptive_refinement_interval(10)\\n    , stabilization_alpha(2)\\n    , stabilization_c_R(0.11)\\n    , stabilization_beta(0.078)\\n    , stokes_velocity_degree(2)\\n    , use_locally_conservative_discretization(true)\\n    , temperature_degree(2)\\n  {\\n ParameterHandler prm;\\n    BoussinesqFlowProblem<dim>::Parameters::declare_parameters(prm);\\n \\n    std::ifstream parameter_file(parameter_filename);\\n \\n if (!parameter_file)\\n      {\\n        parameter_file.close();\\n \\n        std::ofstream parameter_out(parameter_filename);\\n        prm.print_parameters(parameter_out, ParameterHandler::Text);\\n \\n AssertThrow(\\n false,\\n          ExcMessage(\\n \\\"Input parameter file <\\\" + parameter_filename +\\n \\\"> not found. Creating a template file of the same name.\\\"));\\n      }\\n \\n    prm.parse_input(parameter_file);\\n    parse_parameters(prm);\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::Parameters::declare_parameters(\\n ParameterHandler &prm)\\n  {\\n    prm.declare_entry(\\\"End time\\\",\\n \\\"1e8\\\",\\n Patterns::Double(0),\\n \\\"The end time of the simulation in years.\\\");\\n    prm.declare_entry(\\\"Initial global refinement\\\",\\n \\\"2\\\",\\n Patterns::Integer(0),\\n \\\"The number of global refinement steps performed on \\\"\\n \\\"the initial coarse mesh, before the problem is first \\\"\\n \\\"solved there.\\\");\\n    prm.declare_entry(\\\"Initial adaptive refinement\\\",\\n \\\"2\\\",\\n Patterns::Integer(0),\\n \\\"The number of adaptive refinement steps performed after \\\"\\n \\\"initial global refinement.\\\");\\n    prm.declare_entry(\\\"Time steps between mesh refinement\\\",\\n \\\"10\\\",\\n Patterns::Integer(1),\\n \\\"The number of time steps after which the mesh is to be \\\"\\n \\\"adapted based on computed error indicators.\\\");\\n    prm.declare_entry(\\\"Generate graphical output\\\",\\n \\\"false\\\",\\n Patterns::Bool(),\\n \\\"Whether graphical output is to be generated or not. \\\"\\n \\\"You may not want to get graphical output if the number \\\"\\n \\\"of processors is large.\\\");\\n    prm.declare_entry(\\\"Time steps between graphical output\\\",\\n \\\"50\\\",\\n Patterns::Integer(1),\\n \\\"The number of time steps between each generation of \\\"\\n \\\"graphical output files.\\\");\\n \\n    prm.enter_subsection(\\\"Stabilization parameters\\\");\\n    {\\n      prm.declare_entry(\\\"alpha\\\",\\n \\\"2\\\",\\n Patterns::Double(1, 2),\\n \\\"The exponent in the entropy viscosity stabilization.\\\");\\n      prm.declare_entry(\\\"c_R\\\",\\n \\\"0.11\\\",\\n Patterns::Double(0),\\n \\\"The c_R factor in the entropy viscosity \\\"\\n \\\"stabilization.\\\");\\n      prm.declare_entry(\\\"beta\\\",\\n \\\"0.078\\\",\\n Patterns::Double(0),\\n \\\"The beta factor in the artificial viscosity \\\"\\n \\\"stabilization. An appropriate value for 2d is 0.052 \\\"\\n \\\"and 0.078 for 3d.\\\");\\n    }\\n    prm.leave_subsection();\\n \\n    prm.enter_subsection(\\\"Discretization\\\");\\n    {\\n      prm.declare_entry(\\n \\\"Stokes velocity polynomial degree\\\",\\n \\\"2\\\",\\n Patterns::Integer(1),\\n \\\"The polynomial degree to use for the velocity variables \\\"\\n \\\"in the Stokes system.\\\");\\n      prm.declare_entry(\\n \\\"Temperature polynomial degree\\\",\\n \\\"2\\\",\\n Patterns::Integer(1),\\n \\\"The polynomial degree to use for the temperature variable.\\\");\\n      prm.declare_entry(\\n \\\"Use locally conservative discretization\\\",\\n \\\"true\\\",\\n Patterns::Bool(),\\n \\\"Whether to use a Stokes discretization that is locally \\\"\\n \\\"conservative at the expense of a larger number of degrees \\\"\\n \\\"of freedom, or to go with a cheaper discretization \\\"\\n \\\"that does not locally conserve mass (although it is \\\"\\n \\\"globally conservative.\\\");\\n    }\\n    prm.leave_subsection();\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::Parameters::parse_parameters(\\n ParameterHandler &prm)\\n  {\\n    end_time                  = prm.get_double(\\\"End time\\\");\\n    initial_global_refinement = prm.get_integer(\\\"Initial global refinement\\\");\\n    initial_adaptive_refinement =\\n      prm.get_integer(\\\"Initial adaptive refinement\\\");\\n \\n    adaptive_refinement_interval =\\n      prm.get_integer(\\\"Time steps between mesh refinement\\\");\\n \\n    generate_graphical_output = prm.get_bool(\\\"Generate graphical output\\\");\\n    graphical_output_interval =\\n      prm.get_integer(\\\"Time steps between graphical output\\\");\\n \\n    prm.enter_subsection(\\\"Stabilization parameters\\\");\\n    {\\n      stabilization_alpha = prm.get_double(\\\"alpha\\\");\\n      stabilization_c_R   = prm.get_double(\\\"c_R\\\");\\n      stabilization_beta  = prm.get_double(\\\"beta\\\");\\n    }\\n    prm.leave_subsection();\\n \\n    prm.enter_subsection(\\\"Discretization\\\");\\n    {\\n      stokes_velocity_degree =\\n        prm.get_integer(\\\"Stokes velocity polynomial degree\\\");\\n      temperature_degree = prm.get_integer(\\\"Temperature polynomial degree\\\");\\n      use_locally_conservative_discretization =\\n        prm.get_bool(\\\"Use locally conservative discretization\\\");\\n    }\\n    prm.leave_subsection();\\n  }\\n \\n \\n \\n template <int dim>\\n  BoussinesqFlowProblem<dim>::BoussinesqFlowProblem(Parameters &parameters_)\\n    : parameters(parameters_)\\n    , pcout(std::cout, (Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0))\\n    ,\\n \\n triangulation(MPI_COMM_WORLD,\\n                  typename Triangulation<dim>::MeshSmoothing(\\n Triangulation<dim>::smoothing_on_refinement |\\n Triangulation<dim>::smoothing_on_coarsening))\\n    ,\\n \\n    global_Omega_diameter(0.)\\n    ,\\n \\n    mapping(4)\\n    ,\\n \\n    stokes_fe(FE_Q<dim>(parameters.stokes_velocity_degree) ^ dim,\\n              (parameters.use_locally_conservative_discretization ?\\n                 static_cast<const FiniteElement<dim> &>(\\n FE_DGP<dim>(parameters.stokes_velocity_degree - 1)) :\\n                 static_cast<const FiniteElement<dim> &>(\\n FE_Q<dim>(parameters.stokes_velocity_degree - 1))))\\n    ,\\n \\n    stokes_dof_handler(triangulation)\\n    ,\\n \\n    temperature_fe(parameters.temperature_degree)\\n    , temperature_dof_handler(triangulation)\\n    ,\\n \\n    time_step(0)\\n    , old_time_step(0)\\n    , timestep_number(0)\\n    , rebuild_stokes_matrix(true)\\n    , rebuild_stokes_preconditioner(true)\\n    , rebuild_temperature_matrices(true)\\n    , rebuild_temperature_preconditioner(true)\\n    ,\\n \\n    computing_timer(MPI_COMM_WORLD,\\n                    pcout,\\n TimerOutput::summary,\\n TimerOutput::wall_times)\\n  {}\\n \\n \\n \\n \\n template <int dim>\\n double BoussinesqFlowProblem<dim>::get_maximal_velocity() const\\n {\\n const QIterated<dim> quadrature_formula(QTrapezoid<1>(),\\n                                            parameters.stokes_velocity_degree);\\n const unsigned int   n_q_points = quadrature_formula.size();\\n \\n FEValues<dim>               fe_values(mapping,\\n                            stokes_fe,\\n                            quadrature_formula,\\n update_values);\\n    std::vector<Tensor<1, dim>> velocity_values(n_q_points);\\n \\n const FEValuesExtractors::Vector velocities(0);\\n \\n double max_local_velocity = 0;\\n \\n for (const auto &cell : stokes_dof_handler.active_cell_iterators())\\n      if (cell->is_locally_owned())\\n        {\\n          fe_values.reinit(cell);\\n          fe_values[velocities].get_function_values(stokes_solution,\\n                                                    velocity_values);\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n            max_local_velocity =\\n std::max(max_local_velocity, velocity_values[q].norm());\\n        }\\n \\n return Utilities::MPI::max(max_local_velocity, MPI_COMM_WORLD);\\n  }\\n \\n \\n \\n template <int dim>\\n double BoussinesqFlowProblem<dim>::get_cfl_number() const\\n {\\n const QIterated<dim> quadrature_formula(QTrapezoid<1>(),\\n                                            parameters.stokes_velocity_degree);\\n const unsigned int   n_q_points = quadrature_formula.size();\\n \\n FEValues<dim>               fe_values(mapping,\\n                            stokes_fe,\\n                            quadrature_formula,\\n update_values);\\n    std::vector<Tensor<1, dim>> velocity_values(n_q_points);\\n \\n const FEValuesExtractors::Vector velocities(0);\\n \\n double max_local_cfl = 0;\\n \\n for (const auto &cell : stokes_dof_handler.active_cell_iterators())\\n      if (cell->is_locally_owned())\\n        {\\n          fe_values.reinit(cell);\\n          fe_values[velocities].get_function_values(stokes_solution,\\n                                                    velocity_values);\\n \\n double max_local_velocity = 1e-10;\\n for (unsigned int q = 0; q < n_q_points; ++q)\\n            max_local_velocity =\\n std::max(max_local_velocity, velocity_values[q].norm());\\n          max_local_cfl =\\n std::max(max_local_cfl, max_local_velocity / cell->diameter());\\n        }\\n \\n return Utilities::MPI::max(max_local_cfl, MPI_COMM_WORLD);\\n  }\\n \\n \\n \\n template <int dim>\\n double BoussinesqFlowProblem<dim>::get_entropy_variation(\\n const double average_temperature) const\\n {\\n if (parameters.stabilization_alpha != 2)\\n return 1.;\\n \\n const QGauss<dim>  quadrature_formula(parameters.temperature_degree + 1);\\n const unsigned int n_q_points = quadrature_formula.size();\\n \\n FEValues<dim>       fe_values(temperature_fe,\\n                            quadrature_formula,\\n update_values | update_JxW_values);\\n    std::vector<double> old_temperature_values(n_q_points);\\n    std::vector<double> old_old_temperature_values(n_q_points);\\n \\n double min_entropy = std::numeric_limits<double>::max(),\\n           max_entropy = -std::numeric_limits<double>::max(), area = 0,\\n           entropy_integrated = 0;\\n \\n for (const auto &cell : temperature_dof_handler.active_cell_iterators())\\n      if (cell->is_locally_owned())\\n        {\\n          fe_values.reinit(cell);\\n          fe_values.get_function_values(old_temperature_solution,\\n                                        old_temperature_values);\\n          fe_values.get_function_values(old_old_temperature_solution,\\n                                        old_old_temperature_values);\\n for (unsigned int q = 0; q < n_q_points; ++q)\\n            {\\n const double T =\\n                (old_temperature_values[q] + old_old_temperature_values[q]) / 2;\\n const double entropy =\\n                ((T - average_temperature) * (T - average_temperature));\\n \\n              min_entropy = std::min(min_entropy, entropy);\\n              max_entropy = std::max(max_entropy, entropy);\\n              area += fe_values.JxW(q);\\n              entropy_integrated += fe_values.JxW(q) * entropy;\\n            }\\n        }\\n \\n const double local_sums[2]   = {entropy_integrated, area},\\n                 local_maxima[2] = {-min_entropy, max_entropy};\\n double global_sums[2], global_maxima[2];\\n \\n Utilities::MPI::sum(local_sums, MPI_COMM_WORLD, global_sums);\\n Utilities::MPI::max(local_maxima, MPI_COMM_WORLD, global_maxima);\\n \\n const double average_entropy = global_sums[0] / global_sums[1];\\n const double entropy_diff    = std::max(global_maxima[1] - average_entropy,\\n                                         average_entropy - (-global_maxima[0]));\\n return entropy_diff;\\n  }\\n \\n \\n \\n \\n template <int dim>\\n  std::pair<double, double>\\n  BoussinesqFlowProblem<dim>::get_extrapolated_temperature_range() const\\n {\\n const QIterated<dim> quadrature_formula(QTrapezoid<1>(),\\n                                            parameters.temperature_degree);\\n const unsigned int   n_q_points = quadrature_formula.size();\\n \\n FEValues<dim>       fe_values(mapping,\\n                            temperature_fe,\\n                            quadrature_formula,\\n update_values);\\n    std::vector<double> old_temperature_values(n_q_points);\\n    std::vector<double> old_old_temperature_values(n_q_points);\\n \\n double min_local_temperature = std::numeric_limits<double>::max(),\\n           max_local_temperature = -std::numeric_limits<double>::max();\\n \\n if (timestep_number != 0)\\n      {\\n for (const auto &cell : temperature_dof_handler.active_cell_iterators())\\n          if (cell->is_locally_owned())\\n            {\\n              fe_values.reinit(cell);\\n              fe_values.get_function_values(old_temperature_solution,\\n                                            old_temperature_values);\\n              fe_values.get_function_values(old_old_temperature_solution,\\n                                            old_old_temperature_values);\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n                {\\n const double temperature =\\n                    (1. + time_step / old_time_step) *\\n                      old_temperature_values[q] -\\n                    time_step / old_time_step * old_old_temperature_values[q];\\n \\n                  min_local_temperature =\\n std::min(min_local_temperature, temperature);\\n                  max_local_temperature =\\n std::max(max_local_temperature, temperature);\\n                }\\n            }\\n      }\\n else\\n      {\\n for (const auto &cell : temperature_dof_handler.active_cell_iterators())\\n          if (cell->is_locally_owned())\\n            {\\n              fe_values.reinit(cell);\\n              fe_values.get_function_values(old_temperature_solution,\\n                                            old_temperature_values);\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n                {\\n const double temperature = old_temperature_values[q];\\n \\n                  min_local_temperature =\\n std::min(min_local_temperature, temperature);\\n                  max_local_temperature =\\n std::max(max_local_temperature, temperature);\\n                }\\n            }\\n      }\\n \\n double local_extrema[2] = {-min_local_temperature, max_local_temperature};\\n double global_extrema[2];\\n Utilities::MPI::max(local_extrema, MPI_COMM_WORLD, global_extrema);\\n \\n return std::make_pair(-global_extrema[0], global_extrema[1]);\\n  }\\n \\n \\n \\n template <int dim>\\n double BoussinesqFlowProblem<dim>::compute_viscosity(\\n const std::vector<double>                  &old_temperature,\\n const std::vector<double>                  &old_old_temperature,\\n const std::vector<Tensor<1, dim>>          &old_temperature_grads,\\n const std::vector<Tensor<1, dim>>          &old_old_temperature_grads,\\n const std::vector<double>                  &old_temperature_laplacians,\\n const std::vector<double>                  &old_old_temperature_laplacians,\\n const std::vector<Tensor<1, dim>>          &old_velocity_values,\\n const std::vector<Tensor<1, dim>>          &old_old_velocity_values,\\n const std::vector<SymmetricTensor<2, dim>> &old_strain_rates,\\n const std::vector<SymmetricTensor<2, dim>> &old_old_strain_rates,\\n const double                                global_u_infty,\\n const double                                global_T_variation,\\n const double                                average_temperature,\\n const double                                global_entropy_variation,\\n const double                                cell_diameter) const\\n {\\n if (global_u_infty == 0)\\n return 5e-3 * cell_diameter;\\n \\n const unsigned int n_q_points = old_temperature.size();\\n \\n double max_residual = 0;\\n double max_velocity = 0;\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n      {\\n const Tensor<1, dim> u =\\n          (old_velocity_values[q] + old_old_velocity_values[q]) / 2;\\n \\n const SymmetricTensor<2, dim> strain_rate =\\n          (old_strain_rates[q] + old_old_strain_rates[q]) / 2;\\n \\n const double T = (old_temperature[q] + old_old_temperature[q]) / 2;\\n const double dT_dt =\\n          (old_temperature[q] - old_old_temperature[q]) / old_time_step;\\n const double u_grad_T =\\n          u * (old_temperature_grads[q] + old_old_temperature_grads[q]) / 2;\\n \\n const double kappa_Delta_T =\\n          EquationData::kappa *\\n          (old_temperature_laplacians[q] + old_old_temperature_laplacians[q]) /\\n          2;\\n const double gamma =\\n          ((EquationData::radiogenic_heating * EquationData::density(T) +\\n            2 * EquationData::eta * strain_rate * strain_rate) /\\n           (EquationData::density(T) * EquationData::specific_heat));\\n \\n double residual = std::abs(dT_dt + u_grad_T - kappa_Delta_T - gamma);\\n if (parameters.stabilization_alpha == 2)\\n          residual *= std::abs(T - average_temperature);\\n \\n        max_residual = std::max(residual, max_residual);\\n        max_velocity = std::max(std::sqrt(u * u), max_velocity);\\n      }\\n \\n const double max_viscosity =\\n      (parameters.stabilization_beta * max_velocity * cell_diameter);\\n if (timestep_number == 0)\\n return max_viscosity;\\n else\\n      {\\n Assert(old_time_step > 0, ExcInternalError());\\n \\n double entropy_viscosity;\\n if (parameters.stabilization_alpha == 2)\\n          entropy_viscosity =\\n            (parameters.stabilization_c_R * cell_diameter * cell_diameter *\\n             max_residual / global_entropy_variation);\\n else\\n          entropy_viscosity =\\n            (parameters.stabilization_c_R * cell_diameter *\\n             global_Omega_diameter * max_velocity * max_residual /\\n             (global_u_infty * global_T_variation));\\n \\n return std::min(max_viscosity, entropy_viscosity);\\n      }\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::setup_stokes_matrix(\\n const std::vector<IndexSet> &stokes_partitioning,\\n const std::vector<IndexSet> &stokes_relevant_partitioning)\\n  {\\n    stokes_matrix.clear();\\n \\n TrilinosWrappers::BlockSparsityPattern sp(stokes_partitioning,\\n                                              stokes_partitioning,\\n                                              stokes_relevant_partitioning,\\n                                              MPI_COMM_WORLD);\\n \\n Table<2, DoFTools::Coupling> coupling(dim + 1, dim + 1);\\n for (unsigned int c = 0; c < dim + 1; ++c)\\n for (unsigned int d = 0; d < dim + 1; ++d)\\n if (!((c == dim) && (d == dim)))\\n          coupling[c][d] = DoFTools::always;\\n else\\n          coupling[c][d] = DoFTools::none;\\n \\n DoFTools::make_sparsity_pattern(stokes_dof_handler,\\n                                    coupling,\\n                                    sp,\\n                                    stokes_constraints,\\n false,\\n Utilities::MPI::this_mpi_process(\\n                                      MPI_COMM_WORLD));\\n    sp.compress();\\n \\n    stokes_matrix.reinit(sp);\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::setup_stokes_preconditioner(\\n const std::vector<IndexSet> &stokes_partitioning,\\n const std::vector<IndexSet> &stokes_relevant_partitioning)\\n  {\\n    Amg_preconditioner.reset();\\n    Mp_preconditioner.reset();\\n \\n    stokes_preconditioner_matrix.clear();\\n \\n TrilinosWrappers::BlockSparsityPattern sp(stokes_partitioning,\\n                                              stokes_partitioning,\\n                                              stokes_relevant_partitioning,\\n                                              MPI_COMM_WORLD);\\n \\n Table<2, DoFTools::Coupling> coupling(dim + 1, dim + 1);\\n for (unsigned int c = 0; c < dim + 1; ++c)\\n for (unsigned int d = 0; d < dim + 1; ++d)\\n if (c == d)\\n          coupling[c][d] = DoFTools::always;\\n else\\n          coupling[c][d] = DoFTools::none;\\n \\n DoFTools::make_sparsity_pattern(stokes_dof_handler,\\n                                    coupling,\\n                                    sp,\\n                                    stokes_constraints,\\n false,\\n Utilities::MPI::this_mpi_process(\\n                                      MPI_COMM_WORLD));\\n    sp.compress();\\n \\n    stokes_preconditioner_matrix.reinit(sp);\\n  }\\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::setup_temperature_matrices(\\n const IndexSet &temperature_partitioner,\\n const IndexSet &temperature_relevant_partitioner)\\n  {\\n    T_preconditioner.reset();\\n    temperature_mass_matrix.clear();\\n    temperature_stiffness_matrix.clear();\\n    temperature_matrix.clear();\\n \\n TrilinosWrappers::SparsityPattern sp(temperature_partitioner,\\n                                         temperature_partitioner,\\n                                         temperature_relevant_partitioner,\\n                                         MPI_COMM_WORLD);\\n DoFTools::make_sparsity_pattern(temperature_dof_handler,\\n                                    sp,\\n                                    temperature_constraints,\\n false,\\n Utilities::MPI::this_mpi_process(\\n                                      MPI_COMM_WORLD));\\n    sp.compress();\\n \\n    temperature_matrix.reinit(sp);\\n    temperature_mass_matrix.reinit(sp);\\n    temperature_stiffness_matrix.reinit(sp);\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::setup_dofs()\\n  {\\n TimerOutput::Scope timing_section(computing_timer, \\\"Setup dof systems\\\");\\n \\n    stokes_dof_handler.distribute_dofs(stokes_fe);\\n \\n    std::vector<unsigned int> stokes_sub_blocks(dim + 1, 0);\\n    stokes_sub_blocks[dim] = 1;\\n DoFRenumbering::component_wise(stokes_dof_handler, stokes_sub_blocks);\\n \\n    temperature_dof_handler.distribute_dofs(temperature_fe);\\n \\n const std::vector<types::global_dof_index> stokes_dofs_per_block =\\n DoFTools::count_dofs_per_fe_block(stokes_dof_handler, stokes_sub_blocks);\\n \\n const types::global_dof_index n_u = stokes_dofs_per_block[0],\\n                                  n_p = stokes_dofs_per_block[1],\\n                                  n_T = temperature_dof_handler.n_dofs();\\n \\n    std::locale s = pcout.get_stream().getloc();\\n    pcout.get_stream().imbue(std::locale(\\\"\\\"));\\n    pcout << \\\"Number of active cells: \\\" << triangulation.n_global_active_cells()\\n          << \\\" (on \\\" << triangulation.n_levels() << \\\" levels)\\\" << std::endl\\n          << \\\"Number of degrees of freedom: \\\" << n_u + n_p + n_T << \\\" (\\\" << n_u\\n          << '+' << n_p << '+' << n_T << ')' << std::endl\\n          << std::endl;\\n    pcout.get_stream().imbue(s);\\n \\n \\n \\n const IndexSet &stokes_locally_owned_index_set =\\n      stokes_dof_handler.locally_owned_dofs();\\n const IndexSet stokes_locally_relevant_set =\\n DoFTools::extract_locally_relevant_dofs(stokes_dof_handler);\\n \\n    std::vector<IndexSet> stokes_partitioning;\\n    stokes_partitioning.push_back(\\n      stokes_locally_owned_index_set.get_view(0, n_u));\\n    stokes_partitioning.push_back(\\n      stokes_locally_owned_index_set.get_view(n_u, n_u + n_p));\\n \\n    std::vector<IndexSet> stokes_relevant_partitioning;\\n    stokes_relevant_partitioning.push_back(\\n      stokes_locally_relevant_set.get_view(0, n_u));\\n    stokes_relevant_partitioning.push_back(\\n      stokes_locally_relevant_set.get_view(n_u, n_u + n_p));\\n \\n const IndexSet temperature_partitioning =\\n      temperature_dof_handler.locally_owned_dofs();\\n const IndexSet temperature_relevant_partitioning =\\n DoFTools::extract_locally_relevant_dofs(temperature_dof_handler);\\n \\n    {\\n      stokes_constraints.clear();\\n      stokes_constraints.reinit(stokes_locally_owned_index_set,\\n                                stokes_locally_relevant_set);\\n \\n DoFTools::make_hanging_node_constraints(stokes_dof_handler,\\n                                              stokes_constraints);\\n \\n const FEValuesExtractors::Vector velocity_components(0);\\n VectorTools::interpolate_boundary_values(\\n        stokes_dof_handler,\\n        0,\\n Functions::ZeroFunction<dim>(dim + 1),\\n        stokes_constraints,\\n        stokes_fe.component_mask(velocity_components));\\n \\n      std::set<types::boundary_id> no_normal_flux_boundaries;\\n      no_normal_flux_boundaries.insert(1);\\n VectorTools::compute_no_normal_flux_constraints(stokes_dof_handler,\\n                                                      0,\\n                                                      no_normal_flux_boundaries,\\n                                                      stokes_constraints,\\n                                                      mapping);\\n      stokes_constraints.close();\\n    }\\n    {\\n      temperature_constraints.clear();\\n      temperature_constraints.reinit(temperature_partitioning,\\n                                     temperature_relevant_partitioning);\\n \\n DoFTools::make_hanging_node_constraints(temperature_dof_handler,\\n                                              temperature_constraints);\\n VectorTools::interpolate_boundary_values(\\n        temperature_dof_handler,\\n        0,\\n        EquationData::TemperatureInitialValues<dim>(),\\n        temperature_constraints);\\n VectorTools::interpolate_boundary_values(\\n        temperature_dof_handler,\\n        1,\\n        EquationData::TemperatureInitialValues<dim>(),\\n        temperature_constraints);\\n      temperature_constraints.close();\\n    }\\n \\n    setup_stokes_matrix(stokes_partitioning, stokes_relevant_partitioning);\\n    setup_stokes_preconditioner(stokes_partitioning,\\n                                stokes_relevant_partitioning);\\n    setup_temperature_matrices(temperature_partitioning,\\n                               temperature_relevant_partitioning);\\n \\n    stokes_rhs.reinit(stokes_partitioning,\\n                      stokes_relevant_partitioning,\\n                      MPI_COMM_WORLD,\\n true);\\n    stokes_solution.reinit(stokes_relevant_partitioning, MPI_COMM_WORLD);\\n    old_stokes_solution.reinit(stokes_solution);\\n \\n    temperature_rhs.reinit(temperature_partitioning,\\n                           temperature_relevant_partitioning,\\n                           MPI_COMM_WORLD,\\n true);\\n    temperature_solution.reinit(temperature_relevant_partitioning,\\n                                MPI_COMM_WORLD);\\n    old_temperature_solution.reinit(temperature_solution);\\n    old_old_temperature_solution.reinit(temperature_solution);\\n \\n    rebuild_stokes_matrix              = true;\\n    rebuild_stokes_preconditioner      = true;\\n    rebuild_temperature_matrices       = true;\\n    rebuild_temperature_preconditioner = true;\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::local_assemble_stokes_preconditioner(\\n const typename DoFHandler<dim>::active_cell_iterator &cell,\\n    Assembly::Scratch::StokesPreconditioner<dim>         &scratch,\\n    Assembly::CopyData::StokesPreconditioner<dim>        &data)\\n  {\\n const unsigned int dofs_per_cell = stokes_fe.n_dofs_per_cell();\\n const unsigned int n_q_points =\\n      scratch.stokes_fe_values.n_quadrature_points;\\n \\n const FEValuesExtractors::Vector velocities(0);\\n const FEValuesExtractors::Scalar pressure(dim);\\n \\n    scratch.stokes_fe_values.reinit(cell);\\n    cell->get_dof_indices(data.local_dof_indices);\\n \\n    data.local_matrix = 0;\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n      {\\n for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n          {\\n            scratch.grad_phi_u[k] =\\n              scratch.stokes_fe_values[velocities].gradient(k, q);\\n            scratch.phi_p[k] = scratch.stokes_fe_values[pressure].value(k, q);\\n          }\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n            data.local_matrix(i, j) +=\\n              (EquationData::eta *\\n                 scalar_product(scratch.grad_phi_u[i], scratch.grad_phi_u[j]) +\\n               (1. / EquationData::eta) * EquationData::pressure_scaling *\\n                 EquationData::pressure_scaling *\\n                 (scratch.phi_p[i] * scratch.phi_p[j])) *\\n              scratch.stokes_fe_values.JxW(q);\\n      }\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::copy_local_to_global_stokes_preconditioner(\\n const Assembly::CopyData::StokesPreconditioner<dim> &data)\\n  {\\n    stokes_constraints.distribute_local_to_global(data.local_matrix,\\n                                                  data.local_dof_indices,\\n                                                  stokes_preconditioner_matrix);\\n  }\\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::assemble_stokes_preconditioner()\\n  {\\n    stokes_preconditioner_matrix = 0;\\n \\n const QGauss<dim> quadrature_formula(parameters.stokes_velocity_degree + 1);\\n \\n using CellFilter =\\n FilteredIterator<typename DoFHandler<dim>::active_cell_iterator>;\\n \\n auto worker =\\n      [this](const typename DoFHandler<dim>::active_cell_iterator &cell,\\n             Assembly::Scratch::StokesPreconditioner<dim>         &scratch,\\n             Assembly::CopyData::StokesPreconditioner<dim>        &data) {\\n        this->local_assemble_stokes_preconditioner(cell, scratch, data);\\n      };\\n \\n auto copier =\\n      [this](const Assembly::CopyData::StokesPreconditioner<dim> &data) {\\n        this->copy_local_to_global_stokes_preconditioner(data);\\n      };\\n \\n WorkStream::run(CellFilter(IteratorFilters::LocallyOwnedCell(),\\n                               stokes_dof_handler.begin_active()),\\n                    CellFilter(IteratorFilters::LocallyOwnedCell(),\\n                               stokes_dof_handler.end()),\\n                    worker,\\n                    copier,\\n                    Assembly::Scratch::StokesPreconditioner<dim>(\\n                      stokes_fe,\\n                      quadrature_formula,\\n                      mapping,\\n update_JxW_values | update_values | update_gradients),\\n                    Assembly::CopyData::StokesPreconditioner<dim>(stokes_fe));\\n \\n    stokes_preconditioner_matrix.compress(VectorOperation::add);\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::build_stokes_preconditioner()\\n  {\\n if (rebuild_stokes_preconditioner == false)\\n return;\\n \\n TimerOutput::Scope timer_section(computing_timer,\\n \\\"   Build Stokes preconditioner\\\");\\n    pcout << \\\"   Rebuilding Stokes preconditioner...\\\" << std::flush;\\n \\n    assemble_stokes_preconditioner();\\n \\n    std::vector<std::vector<bool>>   constant_modes;\\n const FEValuesExtractors::Vector velocity_components(0);\\n DoFTools::extract_constant_modes(stokes_dof_handler,\\n                                     stokes_fe.component_mask(\\n                                       velocity_components),\\n                                     constant_modes);\\n \\n    Mp_preconditioner =\\n      std::make_shared<TrilinosWrappers::PreconditionJacobi>();\\n    Amg_preconditioner = std::make_shared<TrilinosWrappers::PreconditionAMG>();\\n \\n TrilinosWrappers::PreconditionAMG::AdditionalData Amg_data;\\n    Amg_data.constant_modes        = constant_modes;\\n    Amg_data.elliptic              = true;\\n    Amg_data.higher_order_elements = true;\\n    Amg_data.smoother_sweeps       = 2;\\n    Amg_data.aggregation_threshold = 0.02;\\n \\n    Mp_preconditioner->initialize(stokes_preconditioner_matrix.block(1, 1));\\n    Amg_preconditioner->initialize(stokes_preconditioner_matrix.block(0, 0),\\n                                   Amg_data);\\n \\n    rebuild_stokes_preconditioner = false;\\n \\n    pcout << std::endl;\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::local_assemble_stokes_system(\\n const typename DoFHandler<dim>::active_cell_iterator &cell,\\n    Assembly::Scratch::StokesSystem<dim>                 &scratch,\\n    Assembly::CopyData::StokesSystem<dim>                &data)\\n  {\\n const unsigned int dofs_per_cell =\\n      scratch.stokes_fe_values.get_fe().n_dofs_per_cell();\\n const unsigned int n_q_points =\\n      scratch.stokes_fe_values.n_quadrature_points;\\n \\n const FEValuesExtractors::Vector velocities(0);\\n const FEValuesExtractors::Scalar pressure(dim);\\n \\n    scratch.stokes_fe_values.reinit(cell);\\n \\n const typename DoFHandler<dim>::active_cell_iterator temperature_cell =\\n      cell->as_dof_handler_iterator(temperature_dof_handler);\\n    scratch.temperature_fe_values.reinit(temperature_cell);\\n \\n if (rebuild_stokes_matrix)\\n      data.local_matrix = 0;\\n    data.local_rhs = 0;\\n \\n    scratch.temperature_fe_values.get_function_values(\\n      old_temperature_solution, scratch.old_temperature_values);\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n      {\\n const double old_temperature = scratch.old_temperature_values[q];\\n \\n for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n          {\\n            scratch.phi_u[k] = scratch.stokes_fe_values[velocities].value(k, q);\\n if (rebuild_stokes_matrix)\\n              {\\n                scratch.grads_phi_u[k] =\\n                  scratch.stokes_fe_values[velocities].symmetric_gradient(k, q);\\n                scratch.div_phi_u[k] =\\n                  scratch.stokes_fe_values[velocities].divergence(k, q);\\n                scratch.phi_p[k] =\\n                  scratch.stokes_fe_values[pressure].value(k, q);\\n              }\\n          }\\n \\n if (rebuild_stokes_matrix == true)\\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n              data.local_matrix(i, j) +=\\n                (EquationData::eta * 2 *\\n                   (scratch.grads_phi_u[i] * scratch.grads_phi_u[j]) -\\n                 (EquationData::pressure_scaling * scratch.div_phi_u[i] *\\n                  scratch.phi_p[j]) -\\n                 (EquationData::pressure_scaling * scratch.phi_p[i] *\\n                  scratch.div_phi_u[j])) *\\n                scratch.stokes_fe_values.JxW(q);\\n \\n const Tensor<1, dim> gravity = EquationData::gravity_vector(\\n          scratch.stokes_fe_values.quadrature_point(q));\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n          data.local_rhs(i) += (EquationData::density(old_temperature) *\\n                                gravity * scratch.phi_u[i]) *\\n                               scratch.stokes_fe_values.JxW(q);\\n      }\\n \\n    cell->get_dof_indices(data.local_dof_indices);\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::copy_local_to_global_stokes_system(\\n const Assembly::CopyData::StokesSystem<dim> &data)\\n  {\\n if (rebuild_stokes_matrix == true)\\n      stokes_constraints.distribute_local_to_global(data.local_matrix,\\n                                                    data.local_rhs,\\n                                                    data.local_dof_indices,\\n                                                    stokes_matrix,\\n                                                    stokes_rhs);\\n else\\n      stokes_constraints.distribute_local_to_global(data.local_rhs,\\n                                                    data.local_dof_indices,\\n                                                    stokes_rhs);\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::assemble_stokes_system()\\n  {\\n TimerOutput::Scope timer_section(computing_timer,\\n \\\"   Assemble Stokes system\\\");\\n \\n if (rebuild_stokes_matrix == true)\\n      stokes_matrix = 0;\\n \\n    stokes_rhs = 0;\\n \\n const QGauss<dim> quadrature_formula(parameters.stokes_velocity_degree + 1);\\n \\n using CellFilter =\\n FilteredIterator<typename DoFHandler<2>::active_cell_iterator>;\\n \\n WorkStream::run(\\n      CellFilter(IteratorFilters::LocallyOwnedCell(),\\n                 stokes_dof_handler.begin_active()),\\n      CellFilter(IteratorFilters::LocallyOwnedCell(), stokes_dof_handler.end()),\\n      [this](const typename DoFHandler<dim>::active_cell_iterator &cell,\\n             Assembly::Scratch::StokesSystem<dim>                 &scratch,\\n             Assembly::CopyData::StokesSystem<dim>                &data) {\\n        this->local_assemble_stokes_system(cell, scratch, data);\\n      },\\n      [this](const Assembly::CopyData::StokesSystem<dim> &data) {\\n        this->copy_local_to_global_stokes_system(data);\\n      },\\n      Assembly::Scratch::StokesSystem<dim>(\\n        stokes_fe,\\n        mapping,\\n        quadrature_formula,\\n        (update_values | update_quadrature_points | update_JxW_values |\\n         (rebuild_stokes_matrix == true ? update_gradients : UpdateFlags(0))),\\n        temperature_fe,\\n update_values),\\n      Assembly::CopyData::StokesSystem<dim>(stokes_fe));\\n \\n if (rebuild_stokes_matrix == true)\\n      stokes_matrix.compress(VectorOperation::add);\\n    stokes_rhs.compress(VectorOperation::add);\\n \\n    rebuild_stokes_matrix = false;\\n \\n    pcout << std::endl;\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::local_assemble_temperature_matrix(\\n const typename DoFHandler<dim>::active_cell_iterator &cell,\\n    Assembly::Scratch::TemperatureMatrix<dim>            &scratch,\\n    Assembly::CopyData::TemperatureMatrix<dim>           &data)\\n  {\\n const unsigned int dofs_per_cell =\\n      scratch.temperature_fe_values.get_fe().n_dofs_per_cell();\\n const unsigned int n_q_points =\\n      scratch.temperature_fe_values.n_quadrature_points;\\n \\n    scratch.temperature_fe_values.reinit(cell);\\n    cell->get_dof_indices(data.local_dof_indices);\\n \\n    data.local_mass_matrix      = 0;\\n    data.local_stiffness_matrix = 0;\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n      {\\n for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n          {\\n            scratch.grad_phi_T[k] =\\n              scratch.temperature_fe_values.shape_grad(k, q);\\n            scratch.phi_T[k] = scratch.temperature_fe_values.shape_value(k, q);\\n          }\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n            {\\n              data.local_mass_matrix(i, j) +=\\n                (scratch.phi_T[i] * scratch.phi_T[j] *\\n                 scratch.temperature_fe_values.JxW(q));\\n              data.local_stiffness_matrix(i, j) +=\\n                (EquationData::kappa * scratch.grad_phi_T[i] *\\n                 scratch.grad_phi_T[j] * scratch.temperature_fe_values.JxW(q));\\n            }\\n      }\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::copy_local_to_global_temperature_matrix(\\n const Assembly::CopyData::TemperatureMatrix<dim> &data)\\n  {\\n    temperature_constraints.distribute_local_to_global(data.local_mass_matrix,\\n                                                       data.local_dof_indices,\\n                                                       temperature_mass_matrix);\\n    temperature_constraints.distribute_local_to_global(\\n      data.local_stiffness_matrix,\\n      data.local_dof_indices,\\n      temperature_stiffness_matrix);\\n  }\\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::assemble_temperature_matrix()\\n  {\\n if (rebuild_temperature_matrices == false)\\n return;\\n \\n TimerOutput::Scope timer_section(computing_timer,\\n \\\"   Assemble temperature matrices\\\");\\n    temperature_mass_matrix      = 0;\\n    temperature_stiffness_matrix = 0;\\n \\n const QGauss<dim> quadrature_formula(parameters.temperature_degree + 2);\\n \\n using CellFilter =\\n FilteredIterator<typename DoFHandler<2>::active_cell_iterator>;\\n \\n WorkStream::run(\\n      CellFilter(IteratorFilters::LocallyOwnedCell(),\\n                 temperature_dof_handler.begin_active()),\\n      CellFilter(IteratorFilters::LocallyOwnedCell(),\\n                 temperature_dof_handler.end()),\\n      [this](const typename DoFHandler<dim>::active_cell_iterator &cell,\\n             Assembly::Scratch::TemperatureMatrix<dim>            &scratch,\\n             Assembly::CopyData::TemperatureMatrix<dim>           &data) {\\n        this->local_assemble_temperature_matrix(cell, scratch, data);\\n      },\\n      [this](const Assembly::CopyData::TemperatureMatrix<dim> &data) {\\n        this->copy_local_to_global_temperature_matrix(data);\\n      },\\n      Assembly::Scratch::TemperatureMatrix<dim>(temperature_fe,\\n                                                mapping,\\n                                                quadrature_formula),\\n      Assembly::CopyData::TemperatureMatrix<dim>(temperature_fe));\\n \\n    temperature_mass_matrix.compress(VectorOperation::add);\\n    temperature_stiffness_matrix.compress(VectorOperation::add);\\n \\n    rebuild_temperature_matrices       = false;\\n    rebuild_temperature_preconditioner = true;\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::local_assemble_temperature_rhs(\\n const std::pair<double, double> global_T_range,\\n const double                    global_max_velocity,\\n const double                    global_entropy_variation,\\n const typename DoFHandler<dim>::active_cell_iterator &cell,\\n    Assembly::Scratch::TemperatureRHS<dim>               &scratch,\\n    Assembly::CopyData::TemperatureRHS<dim>              &data)\\n  {\\n const bool use_bdf2_scheme = (timestep_number != 0);\\n \\n const unsigned int dofs_per_cell =\\n      scratch.temperature_fe_values.get_fe().n_dofs_per_cell();\\n const unsigned int n_q_points =\\n      scratch.temperature_fe_values.n_quadrature_points;\\n \\n const FEValuesExtractors::Vector velocities(0);\\n \\n    data.local_rhs     = 0;\\n    data.matrix_for_bc = 0;\\n    cell->get_dof_indices(data.local_dof_indices);\\n \\n    scratch.temperature_fe_values.reinit(cell);\\n \\n typename DoFHandler<dim>::active_cell_iterator stokes_cell =\\n      cell->as_dof_handler_iterator(stokes_dof_handler);\\n    scratch.stokes_fe_values.reinit(stokes_cell);\\n \\n    scratch.temperature_fe_values.get_function_values(\\n      old_temperature_solution, scratch.old_temperature_values);\\n    scratch.temperature_fe_values.get_function_values(\\n      old_old_temperature_solution, scratch.old_old_temperature_values);\\n \\n    scratch.temperature_fe_values.get_function_gradients(\\n      old_temperature_solution, scratch.old_temperature_grads);\\n    scratch.temperature_fe_values.get_function_gradients(\\n      old_old_temperature_solution, scratch.old_old_temperature_grads);\\n \\n    scratch.temperature_fe_values.get_function_laplacians(\\n      old_temperature_solution, scratch.old_temperature_laplacians);\\n    scratch.temperature_fe_values.get_function_laplacians(\\n      old_old_temperature_solution, scratch.old_old_temperature_laplacians);\\n \\n    scratch.stokes_fe_values[velocities].get_function_values(\\n      stokes_solution, scratch.old_velocity_values);\\n    scratch.stokes_fe_values[velocities].get_function_values(\\n      old_stokes_solution, scratch.old_old_velocity_values);\\n    scratch.stokes_fe_values[velocities].get_function_symmetric_gradients(\\n      stokes_solution, scratch.old_strain_rates);\\n    scratch.stokes_fe_values[velocities].get_function_symmetric_gradients(\\n      old_stokes_solution, scratch.old_old_strain_rates);\\n \\n const double nu =\\n      compute_viscosity(scratch.old_temperature_values,\\n                        scratch.old_old_temperature_values,\\n                        scratch.old_temperature_grads,\\n                        scratch.old_old_temperature_grads,\\n                        scratch.old_temperature_laplacians,\\n                        scratch.old_old_temperature_laplacians,\\n                        scratch.old_velocity_values,\\n                        scratch.old_old_velocity_values,\\n                        scratch.old_strain_rates,\\n                        scratch.old_old_strain_rates,\\n                        global_max_velocity,\\n                        global_T_range.second - global_T_range.first,\\n                        0.5 * (global_T_range.second + global_T_range.first),\\n                        global_entropy_variation,\\n                        cell->diameter());\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n      {\\n for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n          {\\n            scratch.phi_T[k] = scratch.temperature_fe_values.shape_value(k, q);\\n            scratch.grad_phi_T[k] =\\n              scratch.temperature_fe_values.shape_grad(k, q);\\n          }\\n \\n \\n const double T_term_for_rhs =\\n          (use_bdf2_scheme ?\\n             (scratch.old_temperature_values[q] *\\n                (1 + time_step / old_time_step) -\\n              scratch.old_old_temperature_values[q] * (time_step * time_step) /\\n                (old_time_step * (time_step + old_time_step))) :\\n             scratch.old_temperature_values[q]);\\n \\n const double ext_T =\\n          (use_bdf2_scheme ? (scratch.old_temperature_values[q] *\\n                                (1 + time_step / old_time_step) -\\n                              scratch.old_old_temperature_values[q] *\\n                                time_step / old_time_step) :\\n                             scratch.old_temperature_values[q]);\\n \\n const Tensor<1, dim> ext_grad_T =\\n          (use_bdf2_scheme ? (scratch.old_temperature_grads[q] *\\n                                (1 + time_step / old_time_step) -\\n                              scratch.old_old_temperature_grads[q] * time_step /\\n                                old_time_step) :\\n                             scratch.old_temperature_grads[q]);\\n \\n const Tensor<1, dim> extrapolated_u =\\n          (use_bdf2_scheme ?\\n             (scratch.old_velocity_values[q] * (1 + time_step / old_time_step) -\\n              scratch.old_old_velocity_values[q] * time_step / old_time_step) :\\n             scratch.old_velocity_values[q]);\\n \\n const SymmetricTensor<2, dim> extrapolated_strain_rate =\\n          (use_bdf2_scheme ?\\n             (scratch.old_strain_rates[q] * (1 + time_step / old_time_step) -\\n              scratch.old_old_strain_rates[q] * time_step / old_time_step) :\\n             scratch.old_strain_rates[q]);\\n \\n const double gamma =\\n          ((EquationData::radiogenic_heating * EquationData::density(ext_T) +\\n            2 * EquationData::eta * extrapolated_strain_rate *\\n              extrapolated_strain_rate) /\\n           (EquationData::density(ext_T) * EquationData::specific_heat));\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n          {\\n            data.local_rhs(i) +=\\n              (T_term_for_rhs * scratch.phi_T[i] -\\n               time_step * extrapolated_u * ext_grad_T * scratch.phi_T[i] -\\n               time_step * nu * ext_grad_T * scratch.grad_phi_T[i] +\\n               time_step * gamma * scratch.phi_T[i]) *\\n              scratch.temperature_fe_values.JxW(q);\\n \\n if (temperature_constraints.is_inhomogeneously_constrained(\\n                  data.local_dof_indices[i]))\\n              {\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n                  data.matrix_for_bc(j, i) +=\\n                    (scratch.phi_T[i] * scratch.phi_T[j] *\\n                       (use_bdf2_scheme ? ((2 * time_step + old_time_step) /\\n                                           (time_step + old_time_step)) :\\n                                          1.) +\\n                     scratch.grad_phi_T[i] * scratch.grad_phi_T[j] *\\n                       EquationData::kappa * time_step) *\\n                    scratch.temperature_fe_values.JxW(q);\\n              }\\n          }\\n      }\\n  }\\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::copy_local_to_global_temperature_rhs(\\n const Assembly::CopyData::TemperatureRHS<dim> &data)\\n  {\\n    temperature_constraints.distribute_local_to_global(data.local_rhs,\\n                                                       data.local_dof_indices,\\n                                                       temperature_rhs,\\n                                                       data.matrix_for_bc);\\n  }\\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::assemble_temperature_system(\\n const double maximal_velocity)\\n  {\\n const bool use_bdf2_scheme = (timestep_number != 0);\\n \\n if (use_bdf2_scheme == true)\\n      {\\n        temperature_matrix.copy_from(temperature_mass_matrix);\\n        temperature_matrix *=\\n          (2 * time_step + old_time_step) / (time_step + old_time_step);\\n        temperature_matrix.add(time_step, temperature_stiffness_matrix);\\n      }\\n else\\n      {\\n        temperature_matrix.copy_from(temperature_mass_matrix);\\n        temperature_matrix.add(time_step, temperature_stiffness_matrix);\\n      }\\n \\n if (rebuild_temperature_preconditioner == true)\\n      {\\n        T_preconditioner =\\n          std::make_shared<TrilinosWrappers::PreconditionJacobi>();\\n        T_preconditioner->initialize(temperature_matrix);\\n        rebuild_temperature_preconditioner = false;\\n      }\\n \\n    temperature_rhs = 0;\\n \\n const QGauss<dim> quadrature_formula(parameters.temperature_degree + 2);\\n const std::pair<double, double> global_T_range =\\n      get_extrapolated_temperature_range();\\n \\n const double average_temperature =\\n      0.5 * (global_T_range.first + global_T_range.second);\\n const double global_entropy_variation =\\n      get_entropy_variation(average_temperature);\\n \\n using CellFilter =\\n FilteredIterator<typename DoFHandler<2>::active_cell_iterator>;\\n \\n auto worker =\\n      [this, global_T_range, maximal_velocity, global_entropy_variation](\\n const typename DoFHandler<dim>::active_cell_iterator &cell,\\n        Assembly::Scratch::TemperatureRHS<dim>               &scratch,\\n        Assembly::CopyData::TemperatureRHS<dim>              &data) {\\n        this->local_assemble_temperature_rhs(global_T_range,\\n                                             maximal_velocity,\\n                                             global_entropy_variation,\\n                                             cell,\\n                                             scratch,\\n                                             data);\\n      };\\n \\n auto copier = [this](const Assembly::CopyData::TemperatureRHS<dim> &data) {\\n      this->copy_local_to_global_temperature_rhs(data);\\n    };\\n \\n WorkStream::run(CellFilter(IteratorFilters::LocallyOwnedCell(),\\n                               temperature_dof_handler.begin_active()),\\n                    CellFilter(IteratorFilters::LocallyOwnedCell(),\\n                               temperature_dof_handler.end()),\\n                    worker,\\n                    copier,\\n                    Assembly::Scratch::TemperatureRHS<dim>(\\n                      temperature_fe, stokes_fe, mapping, quadrature_formula),\\n                    Assembly::CopyData::TemperatureRHS<dim>(temperature_fe));\\n \\n    temperature_rhs.compress(VectorOperation::add);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::solve()\\n  {\\n    {\\n TimerOutput::Scope timer_section(computing_timer,\\n \\\"   Solve Stokes system\\\");\\n \\n      pcout << \\\"   Solving Stokes system... \\\" << std::flush;\\n \\n TrilinosWrappers::MPI::BlockVector distributed_stokes_solution(\\n        stokes_rhs);\\n      distributed_stokes_solution = stokes_solution;\\n \\n      distributed_stokes_solution.block(1) /= EquationData::pressure_scaling;\\n \\n const unsigned int\\n        start = (distributed_stokes_solution.block(0).size() +\\n                 distributed_stokes_solution.block(1).local_range().first),\\n        end   = (distributed_stokes_solution.block(0).size() +\\n               distributed_stokes_solution.block(1).local_range().second);\\n for (unsigned int i = start; i < end; ++i)\\n if (stokes_constraints.is_constrained(i))\\n          distributed_stokes_solution(i) = 0;\\n \\n \\n PrimitiveVectorMemory<TrilinosWrappers::MPI::BlockVector> mem;\\n \\n unsigned int  n_iterations     = 0;\\n const double  solver_tolerance = 1e-8 * stokes_rhs.l2_norm();\\n SolverControl solver_control(30, solver_tolerance);\\n \\n try\\n        {\\n const LinearSolvers::BlockSchurPreconditioner<\\n TrilinosWrappers::PreconditionAMG,\\n TrilinosWrappers::PreconditionJacobi>\\n            preconditioner(stokes_matrix,\\n                           stokes_preconditioner_matrix,\\n                           *Mp_preconditioner,\\n                           *Amg_preconditioner,\\n false);\\n \\n SolverFGMRES<TrilinosWrappers::MPI::BlockVector> solver(\\n            solver_control,\\n            mem,\\n SolverFGMRES<TrilinosWrappers::MPI::BlockVector>::AdditionalData(\\n              30));\\n          solver.solve(stokes_matrix,\\n                       distributed_stokes_solution,\\n                       stokes_rhs,\\n                       preconditioner);\\n \\n          n_iterations = solver_control.last_step();\\n        }\\n \\n catch (SolverControl::NoConvergence &)\\n        {\\n const LinearSolvers::BlockSchurPreconditioner<\\n TrilinosWrappers::PreconditionAMG,\\n TrilinosWrappers::PreconditionJacobi>\\n            preconditioner(stokes_matrix,\\n                           stokes_preconditioner_matrix,\\n                           *Mp_preconditioner,\\n                           *Amg_preconditioner,\\n true);\\n \\n SolverControl solver_control_refined(stokes_matrix.m(),\\n                                               solver_tolerance);\\n SolverFGMRES<TrilinosWrappers::MPI::BlockVector> solver(\\n            solver_control_refined,\\n            mem,\\n SolverFGMRES<TrilinosWrappers::MPI::BlockVector>::AdditionalData(\\n              50));\\n          solver.solve(stokes_matrix,\\n                       distributed_stokes_solution,\\n                       stokes_rhs,\\n                       preconditioner);\\n \\n          n_iterations =\\n            (solver_control.last_step() + solver_control_refined.last_step());\\n        }\\n \\n \\n      stokes_constraints.distribute(distributed_stokes_solution);\\n \\n      distributed_stokes_solution.block(1) *= EquationData::pressure_scaling;\\n \\n      stokes_solution = distributed_stokes_solution;\\n      pcout << n_iterations << \\\" iterations.\\\" << std::endl;\\n    }\\n \\n \\n    {\\n TimerOutput::Scope timer_section(computing_timer,\\n \\\"   Assemble temperature rhs\\\");\\n \\n      old_time_step = time_step;\\n \\n const double scaling = (dim == 3 ? 0.25 : 1.0);\\n      time_step            = (scaling / (2.1 * dim * std::sqrt(1. * dim)) /\\n                   (parameters.temperature_degree * get_cfl_number()));\\n \\n const double maximal_velocity = get_maximal_velocity();\\n      pcout << \\\"   Maximal velocity: \\\"\\n            << maximal_velocity * EquationData::year_in_seconds * 100\\n            << \\\" cm/year\\\" << std::endl;\\n      pcout << \\\"   \\\"\\n            << \\\"Time step: \\\" << time_step / EquationData::year_in_seconds\\n            << \\\" years\\\" << std::endl;\\n \\n      assemble_temperature_system(maximal_velocity);\\n    }\\n \\n    {\\n TimerOutput::Scope timer_section(computing_timer,\\n \\\"   Solve temperature system\\\");\\n \\n SolverControl solver_control(temperature_matrix.m(),\\n                                   1e-12 * temperature_rhs.l2_norm());\\n SolverCG<TrilinosWrappers::MPI::Vector> cg(solver_control);\\n \\n TrilinosWrappers::MPI::Vector distributed_temperature_solution(\\n        temperature_rhs);\\n      distributed_temperature_solution = old_temperature_solution;\\n \\n      cg.solve(temperature_matrix,\\n               distributed_temperature_solution,\\n               temperature_rhs,\\n               *T_preconditioner);\\n \\n      temperature_constraints.distribute(distributed_temperature_solution);\\n      temperature_solution = distributed_temperature_solution;\\n \\n      pcout << \\\"   \\\" << solver_control.last_step()\\n            << \\\" CG iterations for temperature\\\" << std::endl;\\n \\n double temperature[2] = {std::numeric_limits<double>::max(),\\n                               -std::numeric_limits<double>::max()};\\n double global_temperature[2];\\n \\n for (unsigned int i =\\n             distributed_temperature_solution.local_range().first;\\n           i < distributed_temperature_solution.local_range().second;\\n           ++i)\\n        {\\n          temperature[0] =\\n std::min<double>(temperature[0],\\n                             distributed_temperature_solution(i));\\n          temperature[1] =\\n std::max<double>(temperature[1],\\n                             distributed_temperature_solution(i));\\n        }\\n \\n      temperature[0] *= -1.0;\\n Utilities::MPI::max(temperature, MPI_COMM_WORLD, global_temperature);\\n      global_temperature[0] *= -1.0;\\n \\n      pcout << \\\"   Temperature range: \\\" << global_temperature[0] << ' '\\n            << global_temperature[1] << std::endl;\\n    }\\n  }\\n \\n \\n \\n template <int dim>\\n class BoussinesqFlowProblem<dim>::Postprocessor\\n    : public DataPostprocessor<dim>\\n  {\\n public:\\n    Postprocessor(const unsigned int partition, const double minimal_pressure);\\n \\n virtual void evaluate_vector_field(\\n const DataPostprocessorInputs::Vector<dim> &inputs,\\n      std::vector<Vector<double>> &computed_quantities) const override;\\n \\n virtual std::vector<std::string> get_names() const override;\\n \\n virtual std::vector<\\n DataComponentInterpretation::DataComponentInterpretation>\\n    get_data_component_interpretation() const override;\\n \\n virtual UpdateFlags get_needed_update_flags() const override;\\n \\n private:\\n const unsigned int partition;\\n const double       minimal_pressure;\\n  };\\n \\n \\n template <int dim>\\n  BoussinesqFlowProblem<dim>::Postprocessor::Postprocessor(\\n const unsigned int partition,\\n const double       minimal_pressure)\\n    : partition(partition)\\n    , minimal_pressure(minimal_pressure)\\n  {}\\n \\n \\n template <int dim>\\n  std::vector<std::string>\\n  BoussinesqFlowProblem<dim>::Postprocessor::get_names() const\\n {\\n    std::vector<std::string> solution_names(dim, \\\"velocity\\\");\\n    solution_names.emplace_back(\\\"p\\\");\\n    solution_names.emplace_back(\\\"T\\\");\\n    solution_names.emplace_back(\\\"friction_heating\\\");\\n    solution_names.emplace_back(\\\"partition\\\");\\n \\n return solution_names;\\n  }\\n \\n \\n template <int dim>\\n  std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n  BoussinesqFlowProblem<dim>::Postprocessor::get_data_component_interpretation()\\n    const\\n {\\n    std::vector<DataComponentInterpretation::DataComponentInterpretation>\\n      interpretation(dim,\\n DataComponentInterpretation::component_is_part_of_vector);\\n \\n    interpretation.push_back(DataComponentInterpretation::component_is_scalar);\\n    interpretation.push_back(DataComponentInterpretation::component_is_scalar);\\n    interpretation.push_back(DataComponentInterpretation::component_is_scalar);\\n    interpretation.push_back(DataComponentInterpretation::component_is_scalar);\\n \\n return interpretation;\\n  }\\n \\n \\n template <int dim>\\n UpdateFlags\\n  BoussinesqFlowProblem<dim>::Postprocessor::get_needed_update_flags() const\\n {\\n return update_values | update_gradients | update_quadrature_points;\\n  }\\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::Postprocessor::evaluate_vector_field(\\n const DataPostprocessorInputs::Vector<dim> &inputs,\\n    std::vector<Vector<double>>                &computed_quantities) const\\n {\\n const unsigned int n_evaluation_points = inputs.solution_values.size();\\n Assert(inputs.solution_gradients.size() == n_evaluation_points,\\n           ExcInternalError());\\n Assert(computed_quantities.size() == n_evaluation_points,\\n           ExcInternalError());\\n Assert(inputs.solution_values[0].size() == dim + 2, ExcInternalError());\\n \\n for (unsigned int p = 0; p < n_evaluation_points; ++p)\\n      {\\n for (unsigned int d = 0; d < dim; ++d)\\n          computed_quantities[p](d) = (inputs.solution_values[p](d) *\\n                                       EquationData::year_in_seconds * 100);\\n \\n const double pressure =\\n          (inputs.solution_values[p](dim) - minimal_pressure);\\n        computed_quantities[p](dim) = pressure;\\n \\n const double temperature        = inputs.solution_values[p](dim + 1);\\n        computed_quantities[p](dim + 1) = temperature;\\n \\n Tensor<2, dim> grad_u;\\n for (unsigned int d = 0; d < dim; ++d)\\n          grad_u[d] = inputs.solution_gradients[p][d];\\n const SymmetricTensor<2, dim> strain_rate = symmetrize(grad_u);\\n        computed_quantities[p](dim + 2) =\\n          2 * EquationData::eta * strain_rate * strain_rate;\\n \\n        computed_quantities[p](dim + 3) = partition;\\n      }\\n  }\\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::output_results()\\n  {\\n TimerOutput::Scope timer_section(computing_timer, \\\"Postprocessing\\\");\\n \\n const FESystem<dim> joint_fe(stokes_fe, 1, temperature_fe, 1);\\n \\n DoFHandler<dim> joint_dof_handler(triangulation);\\n    joint_dof_handler.distribute_dofs(joint_fe);\\n Assert(joint_dof_handler.n_dofs() ==\\n             stokes_dof_handler.n_dofs() + temperature_dof_handler.n_dofs(),\\n           ExcInternalError());\\n \\n TrilinosWrappers::MPI::Vector joint_solution;\\n    joint_solution.reinit(joint_dof_handler.locally_owned_dofs(),\\n                          MPI_COMM_WORLD);\\n \\n    {\\n      std::vector<types::global_dof_index> local_joint_dof_indices(\\n        joint_fe.n_dofs_per_cell());\\n      std::vector<types::global_dof_index> local_stokes_dof_indices(\\n        stokes_fe.n_dofs_per_cell());\\n      std::vector<types::global_dof_index> local_temperature_dof_indices(\\n        temperature_fe.n_dofs_per_cell());\\n \\n typename DoFHandler<dim>::active_cell_iterator\\n        joint_cell       = joint_dof_handler.begin_active(),\\n        joint_endc       = joint_dof_handler.end(),\\n        stokes_cell      = stokes_dof_handler.begin_active(),\\n        temperature_cell = temperature_dof_handler.begin_active();\\n for (; joint_cell != joint_endc;\\n           ++joint_cell, ++stokes_cell, ++temperature_cell)\\n if (joint_cell->is_locally_owned())\\n          {\\n            joint_cell->get_dof_indices(local_joint_dof_indices);\\n            stokes_cell->get_dof_indices(local_stokes_dof_indices);\\n            temperature_cell->get_dof_indices(local_temperature_dof_indices);\\n \\n for (unsigned int i = 0; i < joint_fe.n_dofs_per_cell(); ++i)\\n if (joint_fe.system_to_base_index(i).first.first == 0)\\n                {\\n Assert(joint_fe.system_to_base_index(i).second <\\n                           local_stokes_dof_indices.size(),\\n                         ExcInternalError());\\n \\n                  joint_solution(local_joint_dof_indices[i]) = stokes_solution(\\n                    local_stokes_dof_indices[joint_fe.system_to_base_index(i)\\n                                               .second]);\\n                }\\n else\\n                {\\n Assert(joint_fe.system_to_base_index(i).first.first == 1,\\n                         ExcInternalError());\\n Assert(joint_fe.system_to_base_index(i).second <\\n                           local_temperature_dof_indices.size(),\\n                         ExcInternalError());\\n                  joint_solution(local_joint_dof_indices[i]) =\\n                    temperature_solution(\\n                      local_temperature_dof_indices\\n                        [joint_fe.system_to_base_index(i).second]);\\n                }\\n          }\\n    }\\n \\n    joint_solution.compress(VectorOperation::insert);\\n \\n const IndexSet locally_relevant_joint_dofs =\\n DoFTools::extract_locally_relevant_dofs(joint_dof_handler);\\n TrilinosWrappers::MPI::Vector locally_relevant_joint_solution;\\n    locally_relevant_joint_solution.reinit(locally_relevant_joint_dofs,\\n                                           MPI_COMM_WORLD);\\n    locally_relevant_joint_solution = joint_solution;\\n \\n    Postprocessor postprocessor(Utilities::MPI::this_mpi_process(\\n                                  MPI_COMM_WORLD),\\n                                stokes_solution.block(1).min());\\n \\n DataOut<dim> data_out;\\n    data_out.attach_dof_handler(joint_dof_handler);\\n    data_out.add_data_vector(locally_relevant_joint_solution, postprocessor);\\n    data_out.build_patches();\\n \\n static int out_index = 0;\\n    data_out.write_vtu_with_pvtu_record(\\n \\\"./\\\", \\\"solution\\\", out_index, MPI_COMM_WORLD, 5);\\n \\n    ++out_index;\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void\\n  BoussinesqFlowProblem<dim>::refine_mesh(const unsigned int max_grid_level)\\n  {\\n parallel::distributed::SolutionTransfer<dim, TrilinosWrappers::MPI::Vector>\\n      temperature_trans(temperature_dof_handler);\\n parallel::distributed::SolutionTransfer<dim,\\n TrilinosWrappers::MPI::BlockVector>\\n      stokes_trans(stokes_dof_handler);\\n \\n    {\\n TimerOutput::Scope timer_section(computing_timer,\\n \\\"Refine mesh structure, part 1\\\");\\n \\n Vector<float> estimated_error_per_cell(triangulation.n_active_cells());\\n \\n KellyErrorEstimator<dim>::estimate(\\n        temperature_dof_handler,\\n QGauss<dim - 1>(parameters.temperature_degree + 1),\\n        std::map<types::boundary_id, const Function<dim> *>(),\\n        temperature_solution,\\n        estimated_error_per_cell,\\n ComponentMask(),\\n nullptr,\\n        0,\\n triangulation.locally_owned_subdomain());\\n \\n parallel::distributed::GridRefinement::refine_and_coarsen_fixed_fraction(\\n triangulation, estimated_error_per_cell, 0.3, 0.1);\\n \\n if (triangulation.n_levels() > max_grid_level)\\n for (typename Triangulation<dim>::active_cell_iterator cell =\\n triangulation.begin_active(max_grid_level);\\n             cell != triangulation.end();\\n             ++cell)\\n          cell->clear_refine_flag();\\n \\n const std::vector<const TrilinosWrappers::MPI::Vector *> x_temperature = {\\n        &temperature_solution, &old_temperature_solution};\\n const std::vector<const TrilinosWrappers::MPI::BlockVector *> x_stokes = {\\n        &stokes_solution, &old_stokes_solution};\\n \\n triangulation.prepare_coarsening_and_refinement();\\n \\n      temperature_trans.prepare_for_coarsening_and_refinement(x_temperature);\\n      stokes_trans.prepare_for_coarsening_and_refinement(x_stokes);\\n \\n triangulation.execute_coarsening_and_refinement();\\n    }\\n \\n    setup_dofs();\\n \\n    {\\n TimerOutput::Scope timer_section(computing_timer,\\n \\\"Refine mesh structure, part 2\\\");\\n \\n      {\\n TrilinosWrappers::MPI::Vector distributed_temp1(temperature_rhs);\\n TrilinosWrappers::MPI::Vector distributed_temp2(temperature_rhs);\\n \\n        std::vector<TrilinosWrappers::MPI::Vector *> tmp = {&distributed_temp1,\\n                                                            &distributed_temp2};\\n        temperature_trans.interpolate(tmp);\\n \\n        temperature_constraints.distribute(distributed_temp1);\\n        temperature_constraints.distribute(distributed_temp2);\\n \\n        temperature_solution     = distributed_temp1;\\n        old_temperature_solution = distributed_temp2;\\n      }\\n \\n      {\\n TrilinosWrappers::MPI::BlockVector distributed_stokes(stokes_rhs);\\n TrilinosWrappers::MPI::BlockVector old_distributed_stokes(stokes_rhs);\\n \\n        std::vector<TrilinosWrappers::MPI::BlockVector *> stokes_tmp = {\\n          &distributed_stokes, &old_distributed_stokes};\\n \\n        stokes_trans.interpolate(stokes_tmp);\\n \\n        stokes_constraints.distribute(distributed_stokes);\\n        stokes_constraints.distribute(old_distributed_stokes);\\n \\n        stokes_solution     = distributed_stokes;\\n        old_stokes_solution = old_distributed_stokes;\\n      }\\n    }\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void BoussinesqFlowProblem<dim>::run()\\n  {\\n GridGenerator::hyper_shell(triangulation,\\n Point<dim>(),\\n                               EquationData::R0,\\n                               EquationData::R1,\\n                               (dim == 3) ? 96 : 12,\\n                               true);\\n \\n    global_Omega_diameter = GridTools::diameter(triangulation);\\n \\n triangulation.refine_global(parameters.initial_global_refinement);\\n \\n    setup_dofs();\\n \\n unsigned int pre_refinement_step = 0;\\n \\n  start_time_iteration:\\n \\n    {\\n TrilinosWrappers::MPI::Vector solution(\\n        temperature_dof_handler.locally_owned_dofs());\\n VectorTools::project(temperature_dof_handler,\\n                           temperature_constraints,\\n QGauss<dim>(parameters.temperature_degree + 2),\\n                           EquationData::TemperatureInitialValues<dim>(),\\n                           solution);\\n      temperature_solution         = solution;\\n      old_temperature_solution     = solution;\\n      old_old_temperature_solution = solution;\\n    }\\n \\n    timestep_number = 0;\\n    time_step = old_time_step = 0;\\n \\n double time = 0;\\n \\n do\\n      {\\n        pcout << \\\"Timestep \\\" << timestep_number\\n              << \\\":  t=\\\" << time / EquationData::year_in_seconds << \\\" years\\\"\\n              << std::endl;\\n \\n        assemble_stokes_system();\\n        build_stokes_preconditioner();\\n        assemble_temperature_matrix();\\n \\n        solve();\\n \\n        pcout << std::endl;\\n \\n if ((timestep_number == 0) &&\\n            (pre_refinement_step < parameters.initial_adaptive_refinement))\\n          {\\n            refine_mesh(parameters.initial_global_refinement +\\n                        parameters.initial_adaptive_refinement);\\n            ++pre_refinement_step;\\n goto start_time_iteration;\\n          }\\n else if ((timestep_number > 0) &&\\n                 (timestep_number % parameters.adaptive_refinement_interval ==\\n                  0))\\n          refine_mesh(parameters.initial_global_refinement +\\n                      parameters.initial_adaptive_refinement);\\n \\n if ((parameters.generate_graphical_output == true) &&\\n            (timestep_number % parameters.graphical_output_interval == 0))\\n          output_results();\\n \\n if (time > parameters.end_time * EquationData::year_in_seconds)\\n break;\\n \\n TrilinosWrappers::MPI::BlockVector old_old_stokes_solution;\\n        old_old_stokes_solution      = old_stokes_solution;\\n        old_stokes_solution          = stokes_solution;\\n        old_old_temperature_solution = old_temperature_solution;\\n        old_temperature_solution     = temperature_solution;\\n if (old_time_step > 0)\\n          {\\n            {\\n TrilinosWrappers::MPI::BlockVector distr_solution(stokes_rhs);\\n              distr_solution = stokes_solution;\\n TrilinosWrappers::MPI::BlockVector distr_old_solution(stokes_rhs);\\n              distr_old_solution = old_old_stokes_solution;\\n              distr_solution.sadd(1. + time_step / old_time_step,\\n                                  -time_step / old_time_step,\\n                                  distr_old_solution);\\n              stokes_solution = distr_solution;\\n            }\\n            {\\n TrilinosWrappers::MPI::Vector distr_solution(temperature_rhs);\\n              distr_solution = temperature_solution;\\n TrilinosWrappers::MPI::Vector distr_old_solution(temperature_rhs);\\n              distr_old_solution = old_old_temperature_solution;\\n              distr_solution.sadd(1. + time_step / old_time_step,\\n                                  -time_step / old_time_step,\\n                                  distr_old_solution);\\n              temperature_solution = distr_solution;\\n            }\\n          }\\n \\n if ((timestep_number > 0) && (timestep_number % 100 == 0))\\n          computing_timer.print_summary();\\n \\n        time += time_step;\\n        ++timestep_number;\\n      }\\n while (true);\\n \\n if ((parameters.generate_graphical_output == true) &&\\n        !((timestep_number - 1) % parameters.graphical_output_interval == 0))\\n      output_results();\\n  }\\n} // namespace Step32\\n \\n \\n \\n \\nint main(int argc, char *argv[])\\n{\\n try\\n    {\\n using namespace Step32;\\n using namespace dealii;\\n \\n Utilities::MPI::MPI_InitFinalize mpi_initialization(\\n        argc, argv, numbers::invalid_unsigned_int);\\n \\n      std::string parameter_filename;\\n if (argc >= 2)\\n        parameter_filename = argv[1];\\n else\\n        parameter_filename = \\\"step-32.prm\\\";\\n \\n const int                              dim = 2;\\n      BoussinesqFlowProblem<dim>::Parameters parameters(parameter_filename);\\n      BoussinesqFlowProblem<dim>             flow_problem(parameters);\\n      flow_problem.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n \\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n \\n return 0;\\n}\\naffine_constraints.h\\nblock_sparsity_pattern.h\\nBlockVectorBase::saddvoid sadd(const value_type s, const BlockVectorBase &V)\\nBlockVectorBase::blockBlockType & block(const unsigned int i)\\nDataOutInterface::write_vtu_with_pvtu_recordstd::string write_vtu_with_pvtu_record(const std::string &directory, const std::string &filename_without_extension, const unsigned int counter, const MPI_Comm mpi_communicator, const unsigned int n_digits_for_counter=numbers::invalid_unsigned_int, const unsigned int n_groups=0) constDefinition data_out_base.cc:7854\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\nDoFHandler::reinitvoid reinit(const Triangulation< dim, spacedim > &tria)\\nDoFHandler::begin_activeactive_cell_iterator begin_active(const unsigned int level=0) const\\nIndexSet::get_viewIndexSet get_view(const size_type begin, const size_type end) constDefinition index_set.cc:273\\nParameterHandler::parse_inputvirtual void parse_input(std::istream &input, const std::string &filename=\\\"input file\\\", const std::string &last_line=\\\"\\\", const bool skip_undefined=false)Definition parameter_handler.cc:433\\nParameterHandler::enter_subsectionvoid enter_subsection(const std::string &subsection, const bool create_path_if_needed=true)Definition parameter_handler.cc:991\\nParameterHandler::print_parametersstd::ostream & print_parameters(std::ostream &out, const OutputStyle style) constDefinition parameter_handler.cc:1312\\nParameterHandler::leave_subsectionvoid leave_subsection()Definition parameter_handler.cc:1015\\nParameterHandler::get_integerlong int get_integer(const std::string &entry_string) constDefinition parameter_handler.cc:1087\\nParameterHandler::get_boolbool get_bool(const std::string &entry_name) constDefinition parameter_handler.cc:1175\\nParameterHandler::declare_entryvoid declare_entry(const std::string &entry, const std::string &default_value, const Patterns::PatternBase &pattern=Patterns::Anything(), const std::string &documentation=\\\"\\\", const bool has_to_be_set=false)Definition parameter_handler.cc:846\\nParameterHandler::get_doubledouble get_double(const std::string &entry_name) constDefinition parameter_handler.cc:1130\\nTrilinosWrappers::MPI::Vector::compressvoid compress(VectorOperation::values operation)Definition trilinos_vector.cc:608\\nconditional_ostream.h\\ngrid_refinement.h\\nsolution_transfer.h\\ntria.h\\ndof_handler.h\\ndof_renumbering.h\\ndof_tools.h\\nerror_estimator.h\\nfe_values.h\\nfe_dgp.h\\nfe_dgq.h\\nfe_q.h\\nfe_system.h\\nfiltered_iterator.h\\nfull_matrix.h\\nfunction.h\\ngrid_refinement.h\\nmanifold_lib.h\\ntria.h\\ngrid_generator.h\\ngrid_tools.h\\nutilities.h\\nindex_set.h\\nmapping_q.h\\nLocalIntegrators::Divergence::normdouble norm(const FEValuesBase< dim > &fe, const ArrayView< const std::vector< Tensor< 1, dim > > > &Du)Definition divergence.h:471\\nPhysics::Elasticity::Kinematics::eSymmetricTensor< 2, dim, Number > e(const Tensor< 2, dim, Number > &F)\\nPhysics::Elasticity::Kinematics::dSymmetricTensor< 2, dim, Number > d(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\nSparsityTools::partitionvoid partition(const SparsityPattern &sparsity_pattern, const unsigned int n_partitions, std::vector< unsigned int > &partition_indices, const Partitioner partitioner=Partitioner::metis)Definition sparsity_tools.cc:398\\nTrilinosWrappers::internal::endVectorType::value_type * end(VectorType &V)Definition trilinos_sparse_matrix.cc:64\\nWorkStream::internal::tbb_no_coloring::runvoid run(const Iterator &begin, const std_cxx20::type_identity_t< Iterator > &end, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length, const unsigned int chunk_size)Definition work_stream.h:471\\ninternal::QGaussLobatto::gammalong double gamma(const unsigned int n)Definition quadrature_lib.cc:103\\ninternal::EvaluatorQuantity::value@ value\\ndata_out.h\\nsolution_transfer.h\\nparameter_handler.h\\nquadrature_lib.h\\nsolver_bicgstab.h\\nsolver_cg.h\\nsolver_gmres.h\\nDataPostprocessorInputs::Vector::solution_valuesstd::vector<::Vector< double > > solution_valuesDefinition data_postprocessor.h:410\\nDataPostprocessorInputs::Vector::solution_gradientsstd::vector< std::vector< Tensor< 1, spacedim > > > solution_gradientsDefinition data_postprocessor.h:429\\nTrilinosWrappers::PreconditionAMG::AdditionalData::aggregation_thresholddouble aggregation_thresholdDefinition trilinos_precondition.h:1474\\nTrilinosWrappers::PreconditionAMG::AdditionalData::smoother_sweepsunsigned int smoother_sweepsDefinition trilinos_precondition.h:1512\\nTrilinosWrappers::PreconditionAMG::AdditionalData::ellipticbool ellipticDefinition trilinos_precondition.h:1445\\nTrilinosWrappers::PreconditionAMG::AdditionalData::higher_order_elementsbool higher_order_elementsDefinition trilinos_precondition.h:1451\\ntimer.h\\ntrilinos_block_sparse_matrix.h\\ntrilinos_parallel_block_vector.h\\ntrilinos_precondition.h\\ntrilinos_solver.h\\ntrilinos_sparse_matrix.h\\nvector_tools.h\\nwork_stream.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"