"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_77.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-77 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-77 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-77 tutorial program\\n\\n\\nThis tutorial depends on step-15.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\n How deal.II interfaces with KINSOL \\n Details of the implementation \\n\\n The commented program\\n\\nInclude files\\nThe MinimalSurfaceProblem class template\\nBoundary condition\\nThe MinimalSurfaceProblem class implementation\\n\\nConstructor and set up functions\\nAssembling and factorizing the Jacobian matrix\\nComputing the residual vector\\nSolving linear systems with the Jacobian matrix\\nRefining the mesh, setting boundary values, and generating graphical output\\nThe run() function and the overall logic of the program\\n\\n\\n\\n Results\\n\\n Possibilities for extensions \\n\\n Better linear solvers \\n Replacing SUNDIALS' KINSOL by PETSc's SNES \\n Replacing SUNDIALS' KINSOL by Trilinos' NOX package \\n Replacing SUNDIALS' KINSOL by a generic nonlinear solver \\n\\n\\n The plain program\\n   \\n\\n\\n This program was contributed by Wolfgang Bangerth, Colorado State University.\\nThis material is based upon work partially supported by National Science Foundation grants OAC-1835673, DMS-1821210, and EAR-1925595; and by the Computational Infrastructure in Geodynamics initiative (CIG), through the National Science Foundation under Award No. EAR-1550901 and The University of California-Davis.\\nStefano Zampini (King Abdullah University of Science and Technology) contributed the results obtained with the PETSc variant of this program discussed in the results section below.  \\n\\n Introduction\\nThe step-15 program solved the following, nonlinear equation describing the minimal surface problem:      \\n\\\\begin{align*}\\n    -\\\\nabla \\\\cdot \\\\left( \\\\frac{1}{\\\\sqrt{1+|\\\\nabla u|^{2}}}\\\\nabla u \\\\right) &= 0 \\\\qquad\\n    \\\\qquad &&\\\\textrm{in} ~ \\\\Omega\\n    \\\\\\\\\\n    u&=g \\\\qquad\\\\qquad &&\\\\textrm{on} ~ \\\\partial \\\\Omega.\\n\\\\end{align*}\\n\\n step-15 uses a Newton method, and Newton's method works by repeatedly solving a linearized problem for an update \\\\(\\\\delta u_k\\\\) \\u2013 called the \\\"search direction\\\" \\u2013, computing a \\\"step length\\\" \\\\(\\\\alpha_k\\\\), and then combining them to compute the new guess for the solution via   \\n\\\\begin{align*}\\n    u_{k+1} = u_k + \\\\alpha_k \\\\, \\\\delta u_k.\\n\\\\end{align*}\\n\\nIn the course of the discussions in step-15, we found that it is awkward to compute the step length, and so just settled for simple choice: Always choose \\\\(\\\\alpha_k=0.1\\\\). This is of course not efficient: We know that we can only realize Newton's quadratic convergence rate if we eventually are able to choose \\\\(\\\\alpha_k=1\\\\), though we may have to choose it smaller for the first few iterations where we are still too far away to use this long a step length.\\nAmong the goals of this program is therefore to address this shortcoming. Since line search algorithms are not entirely trivial to implement, one does as one should do anyway: Import complicated functionality from an external library. To this end, we will make use of the interfaces deal.II has to one of the big nonlinear solver packages, namely the KINSOL sub-package of the SUNDIALS suite. SUNDIALS is, at its heart, a package meant to solve complex ordinary differential equations (ODEs) and differential-algebraic equations (DAEs), and the deal.II interfaces allow for this via the classes in the SUNDIALS namespace: Notably the SUNDIALS::ARKode and SUNDIALS::IDA classes. But, because that is an important step in the solution of ODEs and DAEs with implicit methods, SUNDIALS also has a solver for nonlinear problems called KINSOL, and deal.II has an interface to it in the form of the SUNDIALS::KINSOL class. This is what we will use for the solution of our problem.\\nBut SUNDIALS isn't just a convenient way for us to avoid writing a line search algorithm. In general, the solution of nonlinear problems is quite expensive, and one typically wants to save as much compute time as possible. One way one can achieve this is as follows: The algorithm in step-15 discretizes the problem and then in every iteration solves a linear system of the form   \\n\\\\begin{align*}\\n  J_k \\\\, \\\\delta U_k = -F_k\\n\\\\end{align*}\\n\\n where \\\\(F_k\\\\) is the residual vector computed using the current vector of nodal values \\\\(U_k\\\\), \\\\(J_k\\\\) is its derivative (called the \\\"Jacobian\\\"), and \\\\(\\\\delta U_k\\\\) is the update vector that corresponds to the function \\\\(\\\\delta u_k\\\\) mentioned above. The construction of \\\\(J_k,F_k\\\\) has been thoroughly discussed in step-15, as has the way to solve the linear system in each Newton iteration. So let us focus on another aspect of the nonlinear solution procedure: Computing \\\\(F_k\\\\) is expensive, and assembling the matrix \\\\(J_k\\\\) even more so. Do we actually need to do that in every iteration? It turns out that in many applications, this is not actually necessary: These methods often converge even if we replace \\\\(J_k\\\\) by an approximation \\\\(\\\\tilde J_k\\\\) and solve   \\n\\\\begin{align*}\\n  \\\\tilde J_k \\\\, \\\\widetilde{\\\\delta U}_k = -F_k\\n\\\\end{align*}\\n\\n instead, then update   \\n\\\\begin{align*}\\n    U_{k+1} = U_k + \\\\alpha_k \\\\, \\\\widetilde{\\\\delta U}_k.\\n\\\\end{align*}\\n\\n This may require an iteration or two more because our update \\\\(\\\\widetilde{\\\\delta U}_k\\\\) is not quite as good as \\\\(\\\\delta U_k\\\\), but it may still be a win because we don't have to assemble \\\\(J_k\\\\) quite as often.\\nWhat kind of approximation \\\\(\\\\tilde J_k\\\\) would we like for \\\\(J_k\\\\)? Theory says that as \\\\(U_k\\\\) converges to the exact solution \\\\(U^\\\\ast\\\\), we need to ensure that \\\\(\\\\tilde J_k\\\\) needs to converge to \\\\(J^\\\\ast = \\\\nabla F(U^\\\\ast)\\\\). In particular, since \\\\(J_k\\\\rightarrow J^\\\\ast\\\\), a valid choice is \\\\(\\\\tilde J_k = J_k\\\\). But so is choosing \\\\(\\\\tilde J_k = J_k\\\\) every, say, fifth iteration \\\\(k=0,5,10,\\\\ldots\\\\) and for the other iterations, we choose \\\\(\\\\tilde J_k\\\\) equal to the last computed \\\\(J_{k'}\\\\). This is what we will do here: we will just re-use \\\\(\\\\tilde J_{k-1}\\\\) from the previous iteration, which may again be what we had used in the iteration before that, \\\\(\\\\tilde J_{k-2}\\\\).\\nThis scheme becomes even more interesting if, for the solution of the linear system with \\\\(J_k\\\\), we don't just have to assemble a matrix, but also compute a good preconditioner. For example, if we were to use a sparse LU decomposition via the SparseDirectUMFPACK class, or used a geometric or algebraic multigrid. In those cases, we would also not have to update the preconditioner, whose computation may have taken about as long or longer than the assembly of the matrix in the first place. Indeed, with this mindset, we should probably think about using the best preconditioner we can think of, even though their construction is typically quite expensive: We will hope to amortize the cost of computing this preconditioner by applying it to more than one just one linear solve.\\nThe big question is, of course: By what criterion do we decide whether we can get away with the approximation \\\\(\\\\tilde J_k\\\\) based on a previously computed Jacobian matrix \\\\(J_{k-s}\\\\) that goes back \\\\(s\\\\) steps, or whether we need to \\u2013 at least in this iteration \\u2013 actually re-compute the Jacobian \\\\(J_k\\\\) and the corresponding preconditioner? This is, like the issue with line search, one that requires a non-trivial amount of code that monitors the convergence of the overall algorithm. We could implement these sorts of things ourselves, but we probably shouldn't: KINSOL already does that for us. It will tell our code when to \\\"update\\\" the Jacobian matrix.\\nOne last consideration if we were to use an iterative solver instead of the sparse direct one mentioned above: Not only is it possible to get away with replacing \\\\(J_k\\\\) by some approximation \\\\(\\\\tilde J_k\\\\) when solving for the update \\\\(\\\\delta U_k\\\\), but one can also ask whether it is necessary to solve the linear system   \\n\\\\begin{align*}\\n  \\\\tilde J_k \\\\widetilde{\\\\delta U}_k = -F_k\\n\\\\end{align*}\\n\\n to high accuracy. The thinking goes like this: While our current solution \\\\(U_k\\\\) is still far away from \\\\(U^\\\\ast\\\\), why would we solve this linear system particularly accurately? The update \\\\(U_{k+1}=U_k + \\\\widetilde{\\\\delta U}_k\\\\) is likely still going to be far away from the exact solution, so why spend much time on solving the linear system to great accuracy? This is the kind of thinking that underlies algorithms such as the \\\"Eisenstat-Walker trick\\\" [196] in which one is given a tolerance to which the linear system above in iteration \\\\(k\\\\) has to be solved, with this tolerance dependent on the progress in the overall nonlinear solver. As before, one could try to implement this oneself, but KINSOL already provides this kind of information for us \\u2013 though we will not use it in this program since we use a direct solver that requires no solver tolerance and just solves the linear system exactly up to round-off.\\nAs a summary of all of these considerations, we could say the following: There is no need to reinvent the wheel. Just like deal.II provides a vast amount of finite-element functionality, SUNDIALS' KINSOL package provides a vast amount of nonlinear solver functionality, and we better use it.\\nNoteWhile this program uses SUNDIAL's KINSOL package as the engine to solve nonlinear problems, KINSOL is not the only option you have. deal.II also has interfaces to PETSc's SNES collection of algorithms (see the PETScWrappers::NonlinearSolver class) as well as to the Trilinos NOX package (see the TrilinosWrappers::NOXSolver class) that provide not only very similar functionality, but also a largely identical interface. If you have installed a version of deal.II that is configured to use either PETSc or Trilinos, but not SUNDIALS, then it is not too difficult to switch this program to use either of the former two packages instead: Basically everything that we say and do below will also be true and work for these other packages! (We will also come back to this point in the results section below.)\\nHow deal.II interfaces with KINSOL \\nKINSOL, like many similar packages, works in a pretty abstract way. At its core, it sees a nonlinear problem of the form   \\n\\\\begin{align*}\\n    F(U) = 0\\n\\\\end{align*}\\n\\n and constructs a sequence of iterates \\\\(U_k\\\\) which, in general, are vectors of the same length as the vector returned by the function \\\\(F\\\\). To do this, there are a few things it needs from the user:\\nA way to resize a given vector to the correct size.\\nA way to evaluate, for a given vector \\\\(U\\\\), the function \\\\(F(U)\\\\). This function is generally called the \\\"residual\\\" operation because the goal is of course to find a point \\\\(U^\\\\ast\\\\) for which \\\\(F(U^\\\\ast)=0\\\\); if \\\\(F(U)\\\\) returns a nonzero vector, then this is the \\\"residual\\\" (i.e., the \\\"rest\\\", or whatever is \\\"left over\\\"). The function that will do this is in essence the same as the computation of the right hand side vector in step-15, but with an important difference: There, the right hand side denoted the negative of the residual, so we have to switch a sign.\\nA way to compute the matrix \\\\(J_k\\\\) if that is necessary in the current iteration, along with possibly a preconditioner or other data structures (e.g., a sparse decomposition via SparseDirectUMFPACK if that's what we choose to use to solve a linear system). This operation will generally be called the \\\"setup\\\" operation.\\nA way to solve a linear system \\\\(\\\\tilde J_k x = b\\\\) with whatever matrix \\\\(\\\\tilde J_k\\\\) was last computed. This operation will generally be called the \\\"solve\\\" operation.\\n\\nAll of these operations need to be provided to KINSOL by std::function objects that take the appropriate set of arguments and that generally return an integer that indicates success (a zero return value) or failure (a nonzero return value). Specifically, the objects we will access are the SUNDIALS::KINSOL::reinit_vector, SUNDIALS::KINSOL::residual, SUNDIALS::KINSOL::setup_jacobian, and SUNDIALS::KINSOL::solve_with_jacobian member variables. (See the documentation of these variables for their details.) In our implementation, we will use lambda functions to implement these \\\"callbacks\\\" that in turn can call member functions; KINSOL will then call these callbacks whenever its internal algorithms think it is useful.\\nDetails of the implementation \\nThe majority of the code of this tutorial program is as in step-15, and we will not comment on it in much detail. There is really just one aspect one has to pay some attention to, namely how to compute \\\\(F(U)\\\\) given a vector \\\\(U\\\\) on the one hand, and \\\\(J(U)\\\\) given a vector \\\\(U\\\\) separately. At first, this seems trivial: We just take the assemble_system() function and in the one case throw out all code that deals with the matrix and in the other case with the right hand side vector. There: Problem solved.\\nBut it isn't quite as simple. That's because the two are not independent if we have nonzero Dirichlet boundary values, as we do here. The linear system we want to solve contains both interior and boundary degrees of freedom, and when eliminating those degrees of freedom from those that are truly \\\"free\\\", using for example AffineConstraints::distribute_local_to_global(), we need to know the matrix when assembling the right hand side vector.\\nOf course, this completely contravenes the original intent: To not assemble the matrix if we can get away without it. We solve this problem as follows:\\nWe set the starting guess for the solution vector, \\\\(U_0\\\\), to one where boundary degrees of freedom already have their correct values.\\nThis implies that all updates can have zero updates for these degrees of freedom, and we can build both residual vectors \\\\(F(U_k)\\\\) and Jacobian matrices \\\\(J_k\\\\) that corresponds to linear systems whose solutions are zero in these vector components. For this special case, the assembly of matrix and right hand side vectors is independent, and can be broken into separate functions.\\n\\nThere is an assumption here that whenever KINSOL asks for a linear solver with the (approximation of the) Jacobian, that this will be for an update \\\\(\\\\delta U\\\\) (which has zero boundary values), a multiple of which will be added to the solution (which already has the right boundary values). This may not be true and if so, we might have to rethink our approach. That said, it turns out that in practice this is exactly what KINSOL does when using a Newton method, and so our approach is successful.\\n The commented program\\n Include files\\nThis program starts out like most others with well known include files. Compared to the step-15 program from which most of what we do here is copied, the only difference is the include of the header files from which we import the SparseDirectUMFPACK class and the actual interface to KINSOL:\\n\\u00a0 #include <deal.II/base/quadrature_lib.h>\\n\\u00a0 #include <deal.II/base/function.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 #include <deal.II/base/utilities.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/vector.h>\\n\\u00a0 #include <deal.II/lac/full_matrix.h>\\n\\u00a0 #include <deal.II/lac/sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/dynamic_sparsity_pattern.h>\\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/lac/sparse_direct.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/grid/tria.h>\\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/grid_refinement.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/dofs/dof_accessor.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/fe/fe_values.h>\\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/error_estimator.h>\\n\\u00a0 #include <deal.II/numerics/solution_transfer.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/sundials/kinsol.h>\\n\\u00a0 \\n\\u00a0 #include <fstream>\\n\\u00a0 #include <iostream>\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 namespace Step77\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n The MinimalSurfaceProblem class template\\nSimilarly, the main class of this program is essentially a copy of the one in step-15. The class does, however, split the computation of the Jacobian (system) matrix (and its factorization using a direct solver) and residual into separate functions for the reasons outlined in the introduction. For the same reason, the class also has a pointer to a factorization of the Jacobian matrix that is reset every time we update the Jacobian matrix.\\n(If you are wondering why the program uses a direct object for the Jacobian matrix but a pointer for the factorization: Every time KINSOL requests that the Jacobian be updated, we can simply write jacobian_matrix=0; to reset it to a zero matrix that we can then fill again. On the other hand, the SparseDirectUMFPACK class does not have any way to throw away its content or to replace it with a new factorization, and so we use a pointer: We just throw away the whole object and create a new one whenever we have a new Jacobian matrix to factor.)\\nFinally, the class has a timer variable that we will use to assess how long the different parts of the program take so that we can assess whether KINSOL's tendency to not rebuild the matrix and its factorization makes sense. We will discuss this in the \\\"Results\\\" section below.\\n\\u00a0   template <int dim>\\n\\u00a0   class MinimalSurfaceProblem\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     MinimalSurfaceProblem();\\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void setup_system();\\n\\u00a0     void solve(const Vector<double> &rhs,\\n\\u00a0                Vector<double>       &solution,\\n\\u00a0                const double          tolerance);\\n\\u00a0     void refine_mesh();\\n\\u00a0     void output_results(const unsigned int refinement_cycle);\\n\\u00a0     void compute_and_factorize_jacobian(const Vector<double> &evaluation_point);\\n\\u00a0     void compute_residual(const Vector<double> &evaluation_point,\\n\\u00a0                           Vector<double>       &residual);\\n\\u00a0 \\n\\u00a0     Triangulation<dim> triangulation;\\n\\u00a0 \\n\\u00a0     DoFHandler<dim> dof_handler;\\n\\u00a0     const FE_Q<dim> fe;\\n\\u00a0 \\n\\u00a0     AffineConstraints<double> zero_constraints;\\n\\u00a0     AffineConstraints<double> nonzero_constraints;\\n\\u00a0 \\n\\u00a0     SparsityPattern                      sparsity_pattern;\\n\\u00a0     SparseMatrix<double>                 jacobian_matrix;\\n\\u00a0     std::unique_ptr<SparseDirectUMFPACK> jacobian_matrix_factorization;\\n\\u00a0 \\n\\u00a0     Vector<double> current_solution;\\n\\u00a0 \\n\\u00a0     TimerOutput computing_timer;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nDoFHandlerDefinition dof_handler.h:317\\nFE_QDefinition fe_q.h:554\\nSparseMatrixDefinition sparse_matrix.h:520\\nSparsityPatternDefinition sparsity_pattern.h:343\\nTimerOutputDefinition timer.h:549\\nTriangulationDefinition tria.h:1323\\nVectorDefinition vector.h:120\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\n Boundary condition\\nThe classes implementing boundary values are a copy from step-15:\\n\\u00a0   template <int dim>\\n\\u00a0   class BoundaryValues : public Function<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     virtual double value(const Point<dim>  &p,\\n\\u00a0                          const unsigned int component = 0) const override;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   double BoundaryValues<dim>::value(const Point<dim> &p,\\n\\u00a0                                     const unsigned int /*component*/) const\\n\\u00a0   {\\n\\u00a0     return std::sin(2 * numbers::PI * (p[0] + p[1]));\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nFunctionDefinition function.h:152\\nFunction::valuevirtual RangeNumberType value(const Point< dim > &p, const unsigned int component=0) const\\nPointDefinition point.h:111\\nnumbers::PIstatic constexpr double PIDefinition numbers.h:259\\nstd::sin::VectorizedArray< Number, width > sin(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6589\\n The MinimalSurfaceProblem class implementation\\n Constructor and set up functions\\nThe following few functions are also essentially copies of what step-15 already does, and so there is little to discuss.\\n\\u00a0   template <int dim>\\n\\u00a0   MinimalSurfaceProblem<dim>::MinimalSurfaceProblem()\\n\\u00a0     : dof_handler(triangulation)\\n\\u00a0     , fe(1)\\n\\u00a0     , computing_timer(std::cout, TimerOutput::never, TimerOutput::wall_times)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::setup_system()\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"set up\\\");\\n\\u00a0 \\n\\u00a0     dof_handler.distribute_dofs(fe);\\n\\u00a0     current_solution.reinit(dof_handler.n_dofs());\\n\\u00a0 \\n\\u00a0     zero_constraints.clear();\\n\\u00a0     VectorTools::interpolate_boundary_values(dof_handler,\\n\\u00a0                                              0,\\n\\u00a0                                              Functions::ZeroFunction<dim>(),\\n\\u00a0                                              zero_constraints);\\n\\u00a0 \\n\\u00a0     DoFTools::make_hanging_node_constraints(dof_handler, zero_constraints);\\n\\u00a0     zero_constraints.close();\\n\\u00a0 \\n\\u00a0     nonzero_constraints.clear();\\n\\u00a0     VectorTools::interpolate_boundary_values(dof_handler,\\n\\u00a0                                              0,\\n\\u00a0                                              BoundaryValues<dim>(),\\n\\u00a0                                              nonzero_constraints);\\n\\u00a0 \\n\\u00a0     DoFTools::make_hanging_node_constraints(dof_handler, nonzero_constraints);\\n\\u00a0     nonzero_constraints.close();\\n\\u00a0 \\n\\u00a0     DynamicSparsityPattern dsp(dof_handler.n_dofs());\\n\\u00a0     DoFTools::make_sparsity_pattern(dof_handler, dsp, zero_constraints);\\n\\u00a0 \\n\\u00a0     sparsity_pattern.copy_from(dsp);\\n\\u00a0     jacobian_matrix.reinit(sparsity_pattern);\\n\\u00a0     jacobian_matrix_factorization.reset();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nDynamicSparsityPatternDefinition dynamic_sparsity_pattern.h:322\\nFunctions::ZeroFunctionDefinition function.h:510\\nTimerOutput::ScopeDefinition timer.h:557\\nDoFTools::make_hanging_node_constraintsvoid make_hanging_node_constraints(const DoFHandler< dim, spacedim > &dof_handler, AffineConstraints< number > &constraints)Definition dof_tools_constraints.cc:3073\\nDoFTools::make_sparsity_patternvoid make_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, SparsityPatternBase &sparsity_pattern, const AffineConstraints< number > &constraints={}, const bool keep_constrained_dofs=true, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id)Definition dof_tools_sparsity.cc:56\\nVectorTools::interpolate_boundary_valuesvoid interpolate_boundary_values(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const std::map< types::boundary_id, const Function< spacedim, number > * > &function_map, std::map< types::global_dof_index, number > &boundary_values, const ComponentMask &component_mask={})\\nstdSTL namespace.\\n Assembling and factorizing the Jacobian matrix\\nThe following function is then responsible for assembling and factorizing the Jacobian matrix. The first half of the function is in essence the assemble_system() function of step-15, except that it does not deal with also forming a right hand side vector (i.e., the residual) since we do not always have to do these operations at the same time.\\nWe put the whole assembly functionality into a code block enclosed by curly braces so that we can use a TimerOutput::Scope variable to measure how much time is spent in this code block, excluding everything that happens in this function after the matching closing brace }.\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::compute_and_factorize_jacobian(\\n\\u00a0     const Vector<double> &evaluation_point)\\n\\u00a0   {\\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope t(computing_timer, \\\"assembling the Jacobian\\\");\\n\\u00a0 \\n\\u00a0       std::cout << \\\"  Computing Jacobian matrix\\\" << std::endl;\\n\\u00a0 \\n\\u00a0       const QGauss<dim> quadrature_formula(fe.degree + 1);\\n\\u00a0 \\n\\u00a0       jacobian_matrix = 0;\\n\\u00a0 \\n\\u00a0       FEValues<dim> fe_values(fe,\\n\\u00a0                               quadrature_formula,\\n\\u00a0                               update_gradients | update_quadrature_points |\\n\\u00a0                                 update_JxW_values);\\n\\u00a0 \\n\\u00a0       const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0       const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0       FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n\\u00a0 \\n\\u00a0       std::vector<Tensor<1, dim>> evaluation_point_gradients(n_q_points);\\n\\u00a0 \\n\\u00a0       std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0 \\n\\u00a0       for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0         {\\n\\u00a0           cell_matrix = 0;\\n\\u00a0 \\n\\u00a0           fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0           fe_values.get_function_gradients(evaluation_point,\\n\\u00a0                                            evaluation_point_gradients);\\n\\u00a0 \\n\\u00a0           for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0             {\\n\\u00a0               const double coeff =\\n\\u00a0                 1.0 / std::sqrt(1 + evaluation_point_gradients[q] *\\n\\u00a0                                       evaluation_point_gradients[q]);\\n\\u00a0 \\n\\u00a0               for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0                 {\\n\\u00a0                   for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0                     cell_matrix(i, j) +=\\n\\u00a0                       (((fe_values.shape_grad(i, q)    // ((\\\\nabla \\\\phi_i\\n\\u00a0                          * coeff                       //   * a_n\\n\\u00a0                          * fe_values.shape_grad(j, q)) //   * \\\\nabla \\\\phi_j)\\n\\u00a0                         -                              //  -\\n\\u00a0                         (fe_values.shape_grad(i, q)    //  (\\\\nabla \\\\phi_i\\n\\u00a0                          * coeff * coeff * coeff       //   * a_n^3\\n\\u00a0                          *\\n\\u00a0                          (fe_values.shape_grad(j, q)       //   * (\\\\nabla \\\\phi_j\\n\\u00a0                           * evaluation_point_gradients[q]) //      * \\\\nabla u_n)\\n\\u00a0                          * evaluation_point_gradients[q])) //   * \\\\nabla u_n)))\\n\\u00a0                        * fe_values.JxW(q));                // * dx\\n\\u00a0                 }\\n\\u00a0             }\\n\\u00a0 \\n\\u00a0           cell->get_dof_indices(local_dof_indices);\\n\\u00a0           zero_constraints.distribute_local_to_global(cell_matrix,\\n\\u00a0                                                       local_dof_indices,\\n\\u00a0                                                       jacobian_matrix);\\n\\u00a0         }\\n\\u00a0     }\\n\\u00a0 \\nFEValuesDefinition fe_values.h:63\\nFullMatrixDefinition full_matrix.h:79\\nQGaussDefinition quadrature_lib.h:40\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\nstd::sqrt::VectorizedArray< Number, width > sqrt(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6869\\nThe second half of the function then deals with factorizing the so-computed matrix. To do this, we first create a new SparseDirectUMFPACK object and by assigning it to the member variable jacobian_matrix_factorization, we also destroy whatever object that pointer previously pointed to (if any). Then we tell the object to factorize the Jacobian.\\nAs above, we enclose this block of code into curly braces and use a timer to assess how long this part of the program takes.\\n(Strictly speaking, we don't actually need the matrix any more after we are done here, and could throw the matrix object away. A code intended to be memory efficient would do this, and only create the matrix object in this function, rather than as a member variable of the surrounding class. We omit this step here because using the same coding style as in previous tutorial programs breeds familiarity with the common style and helps make these tutorial programs easier to read.)\\n\\u00a0     {\\n\\u00a0       TimerOutput::Scope t(computing_timer, \\\"factorizing the Jacobian\\\");\\n\\u00a0 \\n\\u00a0       std::cout << \\\"  Factorizing Jacobian matrix\\\" << std::endl;\\n\\u00a0 \\n\\u00a0       jacobian_matrix_factorization = std::make_unique<SparseDirectUMFPACK>();\\n\\u00a0       jacobian_matrix_factorization->factorize(jacobian_matrix);\\n\\u00a0     }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Computing the residual vector\\nThe second part of what assemble_system() used to do in step-15 is computing the residual vector, i.e., the right hand side vector of the Newton linear systems. We have broken this out of the previous function, but the following function will be easy to understand if you understood what assemble_system() in step-15 did. Importantly, however, we need to compute the residual not linearized around the current solution vector, but whatever we get from KINSOL. This is necessary for operations such as line search where we want to know what the residual  \\\\(F(U^k + \\\\alpha_k \\\\delta\\n   U^K)\\\\) is for different values of \\\\(\\\\alpha_k\\\\); KINSOL in those cases simply gives us the argument to the function \\\\(F\\\\) and we then compute the residual \\\\(F(\\\\cdot)\\\\) at this point.\\nThe function prints the norm of the so-computed residual at the end as a way for us to follow along the progress of the program.\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::compute_residual(\\n\\u00a0     const Vector<double> &evaluation_point,\\n\\u00a0     Vector<double>       &residual)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"assembling the residual\\\");\\n\\u00a0 \\n\\u00a0     std::cout << \\\"  Computing residual vector...\\\" << std::flush;\\n\\u00a0 \\n\\u00a0     residual = 0.0;\\n\\u00a0 \\n\\u00a0     const QGauss<dim> quadrature_formula(fe.degree + 1);\\n\\u00a0     FEValues<dim>     fe_values(fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_gradients | update_quadrature_points |\\n\\u00a0                               update_JxW_values);\\n\\u00a0 \\n\\u00a0     const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     Vector<double>              cell_residual(dofs_per_cell);\\n\\u00a0     std::vector<Tensor<1, dim>> evaluation_point_gradients(n_q_points);\\n\\u00a0 \\n\\u00a0     std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       {\\n\\u00a0         cell_residual = 0;\\n\\u00a0         fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0         fe_values.get_function_gradients(evaluation_point,\\n\\u00a0                                          evaluation_point_gradients);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0         for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0           {\\n\\u00a0             const double coeff =\\n\\u00a0               1.0 / std::sqrt(1 + evaluation_point_gradients[q] *\\n\\u00a0                                     evaluation_point_gradients[q]);\\n\\u00a0 \\n\\u00a0             for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0               cell_residual(i) +=\\n\\u00a0                 (fe_values.shape_grad(i, q)      // \\\\nabla \\\\phi_i\\n\\u00a0                  * coeff                         // * a_n\\n\\u00a0                  * evaluation_point_gradients[q] // * \\\\nabla u_n\\n\\u00a0                  * fe_values.JxW(q));            // * dx\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         cell->get_dof_indices(local_dof_indices);\\n\\u00a0         zero_constraints.distribute_local_to_global(cell_residual,\\n\\u00a0                                                     local_dof_indices,\\n\\u00a0                                                     residual);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     std::cout << \\\" norm=\\\" << residual.l2_norm() << std::endl;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Solving linear systems with the Jacobian matrix\\nNext up is the function that implements the solution of a linear system with the Jacobian matrix. Since we have already factored the matrix when we built the matrix, solving a linear system comes down to applying the inverse matrix to the given right hand side vector: This is what the SparseDirectUMFPACK::vmult() function does that we use here. Following this, we have to make sure that we also address the values of hanging nodes in the solution vector, and this is done using AffineConstraints::distribute().\\nThe function takes an additional, but unused, argument tolerance that indicates how accurately we have to solve the linear system. The meaning of this argument is discussed in the introduction in the context of the \\\"Eisenstat Walker trick\\\", but since we are using a direct rather than an iterative solver, we are not using this opportunity to solve linear systems only inexactly.\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::solve(const Vector<double> &rhs,\\n\\u00a0                                          Vector<double>       &solution,\\n\\u00a0                                          const double /*tolerance*/)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"linear system solve\\\");\\n\\u00a0 \\n\\u00a0     std::cout << \\\"  Solving linear system\\\" << std::endl;\\n\\u00a0 \\n\\u00a0     jacobian_matrix_factorization->vmult(solution, rhs);\\n\\u00a0     zero_constraints.distribute(solution);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Refining the mesh, setting boundary values, and generating graphical output\\nThe following three functions are again simply copies of the ones in step-15:\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::refine_mesh()\\n\\u00a0   {\\n\\u00a0     Vector<float> estimated_error_per_cell(triangulation.n_active_cells());\\n\\u00a0 \\n\\u00a0     KellyErrorEstimator<dim>::estimate(\\n\\u00a0       dof_handler,\\n\\u00a0       QGauss<dim - 1>(fe.degree + 1),\\n\\u00a0       std::map<types::boundary_id, const Function<dim> *>(),\\n\\u00a0       current_solution,\\n\\u00a0       estimated_error_per_cell);\\n\\u00a0 \\n\\u00a0     GridRefinement::refine_and_coarsen_fixed_number(triangulation,\\n\\u00a0                                                     estimated_error_per_cell,\\n\\u00a0                                                     0.3,\\n\\u00a0                                                     0.03);\\n\\u00a0 \\n\\u00a0     triangulation.prepare_coarsening_and_refinement();\\n\\u00a0 \\n\\u00a0     SolutionTransfer<dim> solution_transfer(dof_handler);\\n\\u00a0     const Vector<double>  coarse_solution = current_solution;\\n\\u00a0     solution_transfer.prepare_for_coarsening_and_refinement(coarse_solution);\\n\\u00a0 \\n\\u00a0     triangulation.execute_coarsening_and_refinement();\\n\\u00a0 \\n\\u00a0     setup_system();\\n\\u00a0 \\n\\u00a0     solution_transfer.interpolate(coarse_solution, current_solution);\\n\\u00a0 \\n\\u00a0     nonzero_constraints.distribute(current_solution);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::output_results(\\n\\u00a0     const unsigned int refinement_cycle)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"graphical output\\\");\\n\\u00a0 \\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0 \\n\\u00a0     data_out.attach_dof_handler(dof_handler);\\n\\u00a0     data_out.add_data_vector(current_solution, \\\"solution\\\");\\n\\u00a0     data_out.build_patches();\\n\\u00a0 \\n\\u00a0     const std::string filename =\\n\\u00a0       \\\"solution-\\\" + Utilities::int_to_string(refinement_cycle, 2) + \\\".vtu\\\";\\n\\u00a0     std::ofstream output(filename);\\n\\u00a0     data_out.write_vtu(output);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOutDefinition data_out.h:147\\nKellyErrorEstimator::estimatestatic void estimate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Quadrature< dim - 1 > &quadrature, const std::map< types::boundary_id, const Function< spacedim, Number > * > &neumann_bc, const ReadVector< Number > &solution, Vector< float > &error, const ComponentMask &component_mask={}, const Function< spacedim > *coefficients=nullptr, const unsigned int n_threads=numbers::invalid_unsigned_int, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id, const types::material_id material_id=numbers::invalid_material_id, const Strategy strategy=cell_diameter_over_24)\\nSolutionTransferDefinition solution_transfer.h:337\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nparallel::distributed::Triangulation::execute_coarsening_and_refinementvirtual void execute_coarsening_and_refinement() overrideDefinition tria.cc:3320\\nparallel::distributed::Triangulation::prepare_coarsening_and_refinementvirtual bool prepare_coarsening_and_refinement() overrideDefinition tria.cc:2805\\nunsigned int\\nGridRefinement::refine_and_coarsen_fixed_numbervoid refine_and_coarsen_fixed_number(Triangulation< dim, spacedim > &triangulation, const Vector< Number > &criteria, const double top_fraction_of_cells, const double bottom_fraction_of_cells, const unsigned int max_n_cells=std::numeric_limits< unsigned int >::max())Definition grid_refinement.cc:318\\nUtilities::int_to_stringstd::string int_to_string(const unsigned int value, const unsigned int digits=numbers::invalid_unsigned_int)Definition utilities.cc:470\\n The run() function and the overall logic of the program\\nThe only function that really is interesting in this program is the one that drives the overall algorithm of starting on a coarse mesh, doing some mesh refinement cycles, and on each mesh using KINSOL to find the solution of the nonlinear algebraic equation we obtain from discretization on this mesh. The refine_mesh() function above makes sure that the solution on one mesh is used as the starting guess on the next mesh. We also use a TimerOutput object to measure how much time every operation on each mesh costs, and reset the timer at the beginning of each cycle.\\nAs discussed in the introduction, it is not necessary to solve problems on coarse meshes particularly accurately since these will only solve as starting guesses for the next mesh. As a consequence, we will use a target tolerance of \\\\(\\\\tau=10^{-3} \\\\frac{1}{10^k}\\\\) for the \\\\(k\\\\)th mesh refinement cycle.\\nAll of this is encoded in the first part of this function:\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::run()\\n\\u00a0   {\\n\\u00a0     GridGenerator::hyper_ball(triangulation);\\n\\u00a0     triangulation.refine_global(2);\\n\\u00a0 \\n\\u00a0     setup_system();\\n\\u00a0     nonzero_constraints.distribute(current_solution);\\n\\u00a0 \\n\\u00a0     for (unsigned int refinement_cycle = 0; refinement_cycle < 6;\\n\\u00a0          ++refinement_cycle)\\n\\u00a0       {\\n\\u00a0         computing_timer.reset();\\n\\u00a0         std::cout << \\\"Mesh refinement step \\\" << refinement_cycle << std::endl;\\n\\u00a0 \\n\\u00a0         if (refinement_cycle != 0)\\n\\u00a0           refine_mesh();\\n\\u00a0 \\n\\u00a0         const double target_tolerance = 1e-3 * std::pow(0.1, refinement_cycle);\\n\\u00a0         std::cout << \\\"  Target_tolerance: \\\" << target_tolerance << std::endl\\n\\u00a0                   << std::endl;\\n\\u00a0 \\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nGridGenerator::hyper_ballvoid hyper_ball(Triangulation< dim > &tria, const Point< dim > &center=Point< dim >(), const double radius=1., const bool attach_spherical_manifold_on_boundary_cells=false)\\nstd::pow::VectorizedArray< Number, width > pow(const ::VectorizedArray< Number, width > &, const Number p)Definition vectorization.h:6885\\nThis is where the fun starts. At the top we create the KINSOL solver object and feed it with an object that encodes a number of additional specifics (of which we only change the nonlinear tolerance we want to reach; but you might want to look into what other members the SUNDIALS::KINSOL::AdditionalData class has and play with them).\\n\\u00a0         {\\n\\u00a0           typename SUNDIALS::KINSOL<Vector<double>>::AdditionalData\\n\\u00a0             additional_data;\\n\\u00a0           additional_data.function_tolerance = target_tolerance;\\n\\u00a0 \\n\\u00a0           SUNDIALS::KINSOL<Vector<double>> nonlinear_solver(additional_data);\\n\\u00a0 \\nSUNDIALS::KINSOLDefinition kinsol.h:184\\nThen we have to describe the operations that were already mentioned in the introduction. In essence, we have to teach KINSOL how to (i) resize a vector to the correct size, (ii) compute the residual vector, (iii) compute the Jacobian matrix (during which we also compute its factorization), and (iv) solve a linear system with the Jacobian.\\nAll four of these operations are represented by member variables of the SUNDIALS::KINSOL class that are of type std::function, i.e., they are objects to which we can assign a pointer to a function or, as we do here, a \\\"lambda function\\\" that takes the appropriate arguments and returns the appropriate information. It turns out that we can do all of this in just over 20 lines of code.\\n(If you're not familiar what \\\"lambda functions\\\" are, take a look at step-12 or at the wikipedia page on the subject. The idea of lambda functions is that one wants to define a function with a certain set of arguments, but (i) not make it a named functions because, typically, the function is used in only one place and it seems unnecessary to give it a global name; and (ii) that the function has access to some of the variables that exist at the place where it is defined, including member variables. The syntax of lambda functions is awkward, but ultimately quite useful.)\\nAt the very end of the code block we then tell KINSOL to go to work and solve our problem. The member functions called from the 'residual', 'setup_jacobian', and 'solve_with_jacobian' functions will then print output to screen that allows us to follow along with the progress of the program.\\n\\u00a0           nonlinear_solver.reinit_vector = [&](Vector<double> &x) {\\n\\u00a0             x.reinit(dof_handler.n_dofs());\\n\\u00a0           };\\n\\u00a0 \\n\\u00a0           nonlinear_solver.residual =\\n\\u00a0             [&](const Vector<double> &evaluation_point,\\n\\u00a0                 Vector<double>       &residual) {\\n\\u00a0               compute_residual(evaluation_point, residual);\\n\\u00a0             };\\n\\u00a0 \\n\\u00a0           nonlinear_solver.setup_jacobian =\\n\\u00a0             [&](const Vector<double> &current_u,\\n\\u00a0                 const Vector<double> & /*current_f*/) {\\n\\u00a0               compute_and_factorize_jacobian(current_u);\\n\\u00a0             };\\n\\u00a0 \\n\\u00a0           nonlinear_solver.solve_with_jacobian = [&](const Vector<double> &rhs,\\n\\u00a0                                                      Vector<double>       &dst,\\n\\u00a0                                                      const double tolerance) {\\n\\u00a0             solve(rhs, dst, tolerance);\\n\\u00a0           };\\n\\u00a0 \\n\\u00a0           nonlinear_solver.solve(current_solution);\\n\\u00a0         }\\n\\u00a0 \\nThe rest is then just house-keeping: Writing data to a file for visualizing, and showing a summary of the timing collected so that we can interpret how long each operation has taken, how often it was executed, etc:\\n\\u00a0         output_results(refinement_cycle);\\n\\u00a0 \\n\\u00a0         computing_timer.print_summary();\\n\\u00a0 \\n\\u00a0         std::cout << std::endl;\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 } // namespace Step77\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 int main()\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       using namespace Step77;\\n\\u00a0 \\n\\u00a0       MinimalSurfaceProblem<2> problem;\\n\\u00a0       problem.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0 \\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   return 0;\\n\\u00a0 }\\n Results\\nWhen running the program, you get output that looks like this: Mesh refinement step 0\\n  Target_tolerance: 0.001\\n \\n  Computing residual vector... norm=0.867975\\n  Computing Jacobian matrix\\n  Factorizing Jacobian matrix\\n  Solving linear system\\n  Computing residual vector... norm=0.867975\\n  Computing residual vector... norm=0.212073\\n  Solving linear system\\n  Computing residual vector... norm=0.212073\\n  Computing residual vector... norm=0.202631\\n  Solving linear system\\n  Computing residual vector... norm=0.202631\\n  Computing residual vector... norm=0.165773\\n  Solving linear system\\n  Computing residual vector... norm=0.165774\\n  Computing residual vector... norm=0.162594\\n  Solving linear system\\n  Computing residual vector... norm=0.162594\\n  Computing residual vector... norm=0.148175\\n  Solving linear system\\n  Computing residual vector... norm=0.148175\\n  Computing residual vector... norm=0.145391\\n  Solving linear system\\n  Computing residual vector... norm=0.145391\\n  Computing residual vector... norm=0.137551\\n  Solving linear system\\n  Computing residual vector... norm=0.137551\\n  Computing residual vector... norm=0.135366\\n  Solving linear system\\n  Computing residual vector... norm=0.135365\\n  Computing residual vector... norm=0.130367\\n  Solving linear system\\n  Computing residual vector... norm=0.130367\\n  Computing residual vector... norm=0.128704\\n  Computing Jacobian matrix\\n  Factorizing Jacobian matrix\\n  Solving linear system\\n  Computing residual vector... norm=0.128704\\n  Computing residual vector... norm=0.0302623\\n  Solving linear system\\n  Computing residual vector... norm=0.0302624\\n  Computing residual vector... norm=0.0126764\\n  Solving linear system\\n  Computing residual vector... norm=0.0126763\\n  Computing residual vector... norm=0.00488315\\n  Solving linear system\\n  Computing residual vector... norm=0.00488322\\n  Computing residual vector... norm=0.00195788\\n  Solving linear system\\n  Computing residual vector... norm=0.00195781\\n  Computing residual vector... norm=0.000773169\\n \\n \\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |     0.121s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| assembling the Jacobian         |         2 |    0.0151s |        12% |\\n| assembling the residual         |        31 |    0.0945s |        78% |\\n| factorizing the Jacobian        |         2 |   0.00176s |       1.5% |\\n| graphical output                |         1 |   0.00504s |       4.2% |\\n| linear system solve             |        15 |  0.000893s |      0.74% |\\n+---------------------------------+-----------+------------+------------+\\n \\n \\nMesh refinement step 1\\n  Target_tolerance: 0.0001\\n \\n  Computing residual vector... norm=0.2467\\n  Computing Jacobian matrix\\n  Factorizing Jacobian matrix\\n  Solving linear system\\n  Computing residual vector... norm=0.246699\\n  Computing residual vector... norm=0.0357783\\n  Solving linear system\\n  Computing residual vector... norm=0.0357784\\n  Computing residual vector... norm=0.0222161\\n  Solving linear system\\n  Computing residual vector... norm=0.022216\\n  Computing residual vector... norm=0.0122148\\n  Solving linear system\\n  Computing residual vector... norm=0.0122149\\n  Computing residual vector... norm=0.00750795\\n  Solving linear system\\n  Computing residual vector... norm=0.00750787\\n  Computing residual vector... norm=0.00439629\\n  Solving linear system\\n  Computing residual vector... norm=0.00439638\\n  Computing residual vector... norm=0.00265093\\n  Solving linear system\\n \\n[...]\\nThe way this should be interpreted is most easily explained by looking at the first few lines of the output on the first mesh: Mesh refinement step 0\\nMesh refinement step 0\\n  Target_tolerance: 0.001\\n \\n  Computing residual vector... norm=0.867975\\n  Computing Jacobian matrix\\n  Factorizing Jacobian matrix\\n  Solving linear system\\n  Computing residual vector... norm=0.867975\\n  Computing residual vector... norm=0.212073\\n  Solving linear system\\n  Computing residual vector... norm=0.212073\\n  Computing residual vector... norm=0.202631\\n  Solving linear system\\n  Computing residual vector... norm=0.202631\\n  Computing residual vector... norm=0.165773\\n  Solving linear system\\n  Computing residual vector... norm=0.165774\\n  Computing residual vector... norm=0.162594\\n  Solving linear system\\n  Computing residual vector... norm=0.162594\\n  Computing residual vector... norm=0.148175\\n  Solving linear system\\n  ...\\n What is happening is this:\\nIn the first residual computation, KINSOL computes the residual to see whether the desired tolerance has been reached. The answer is no, so it requests the user program to compute the Jacobian matrix (and the function then also factorizes the matrix via SparseDirectUMFPACK).\\nKINSOL then instructs us to solve a linear system of the form \\\\(J_k \\\\, \\\\delta U_k = -F_k\\\\) with this matrix and the previously computed residual vector.\\nIt is then time to determine how far we want to go in this direction, i.e., do line search. To this end, KINSOL requires us to compute the residual vector \\\\(F(U_k + \\\\alpha_k \\\\delta U_k)\\\\) for different step lengths \\\\(\\\\alpha_k\\\\). For the first step above, it finds an acceptable \\\\(\\\\alpha_k\\\\) after two tries, and that's generally what will happen in later line searches as well.\\nHaving found a suitable updated solution \\\\(U_{k+1}\\\\), the process is repeated except now KINSOL is happy with the current Jacobian matrix and does not instruct us to re-build the matrix and its factorization, instead asking us to solve a linear system with that same matrix. That will happen several times over, and only after ten solves with the same matrix are we instructed to build a matrix again, using what is by then an already substantially improved solution as linearization point.\\n\\nThe program also writes the solution to a VTU file at the end of each mesh refinement cycle, and it looks as follows: \\n\\n \\n\\nThe key takeaway messages of this program are the following:\\n\\nThe solution is the same as the one we computed in step-15, i.e., the interfaces to SUNDIALS' KINSOL package really did what they were supposed to do. This should not come as a surprise, but the important point is that we don't have to spend the time implementing the complex algorithms that underlie advanced nonlinear solvers ourselves.\\nKINSOL is able to avoid all sorts of operations such as rebuilding the Jacobian matrix when that is not actually necessary. Comparing the number of linear solves in the output above with the number of times we rebuild the Jacobian and compute its factorization should make it clear that this leads to very substantial savings in terms of compute times, without us having to implement the intricacies of algorithms that determine when we need to rebuild this information.\\n\\n Possibilities for extensions \\nBetter linear solvers \\nFor all but the small problems we consider here, a sparse direct solver requires too much time and memory \\u2013 we need an iterative solver like we use in many other programs. The trade-off between constructing an expensive preconditioner (say, a geometric or algebraic multigrid method) is different in the current case, however: Since we can re-use the same matrix for numerous linear solves, we can do the same for the preconditioner and putting more work into building a good preconditioner can more easily be justified than if we used it only for a single linear solve as one does for many other situations.\\nBut iterative solvers also afford other opportunities. For example (and as discussed briefly in the introduction), we may not need to solve to very high accuracy (small tolerances) in early nonlinear iterations as long as we are still far away from the actual solution. This was the basis of the Eisenstat-Walker trick mentioned there. (This is also the underlying reason why one can store the matrix in single precision rather than double precision, see the discussion in the \\\"Possibilities for extensions\\\" section of step-15.)\\nKINSOL provides the function that does the linear solution with a target tolerance that needs to be reached. We ignore it in the program above because the direct solver we use does not need a tolerance and instead solves the linear system exactly (up to round-off, of course), but iterative solvers could make use of this kind of information \\u2013 and, in fact, should. Indeed, the infrastructure is already there: The solve() function of this program is declared as template <int dim>\\nvoid MinimalSurfaceProblem<dim>::solve(const Vector<double> &rhs,\\n Vector<double> &      solution,\\n const double /*tolerance*/)\\n i.e., the tolerance parameter already exists, but is unused.\\nReplacing SUNDIALS' KINSOL by PETSc's SNES \\nAs mentioned in the introduction, SUNDIALS' KINSOL package is not the only player in town. Rather, very similar interfaces exist to the SNES package that is part of PETSc, and the NOX package that is part of Trilinos, via the PETScWrappers::NonlinearSolver and TrilinosWrappers::NOXSolver classes.\\nIt is not very difficult to change the program to use either of these two alternatives. Rather than show exactly what needs to be done, let us point out that a version of this program that uses SNES instead of KINSOL is available as part of the test suite, in the file tests/petsc/step-77-snes.cc. Setting up the solver for PETScWrappers::NonlinearSolver turns out to be even simpler than for the SUNDIALS::KINSOL class we use here because we don't even need the reinit lambda function \\u2013 SNES only needs us to set up the remaining three functions residual, setup_jacobian, and solve_with_jacobian. The majority of changes necessary to convert the program to use SNES are related to the fact that SNES can only deal with PETSc vectors and matrices, and these need to be set up slightly differently. On the upside, the test suite program mentioned above already works in parallel.\\nSNES also allows playing with a number of parameters about the solver, and that enables some interesting comparisons between methods. When you run the test program (or a slightly modified version that outputs information to the screen instead of a file), you get output that looks comparable to something like this: Mesh refinement step 0\\n  Target_tolerance: 0.001\\n \\n  Computing residual vector\\n0 norm=0.867975\\n  Computing Jacobian matrix\\n  Computing residual vector\\n  Computing residual vector\\n1 norm=0.212073\\n  Computing Jacobian matrix\\n  Computing residual vector\\n  Computing residual vector\\n2 norm=0.0189603\\n  Computing Jacobian matrix\\n  Computing residual vector\\n  Computing residual vector\\n3 norm=0.000314854\\n \\n[...]\\nBy default, PETSc uses a Newton solver with cubic backtracking, resampling the Jacobian matrix at each Newton step. That is, we compute and factorize the matrix once per Newton step, and then sample the residual to check for a successful line-search.\\nThe attentive reader should have noticed that in this case we are computing one more extra residual per Newton step. This is because the deal.II code is set up to use a Jacobian-free approach, and the extra residual computation pops up when computing a matrix-vector product to test the validity of the Newton solution.\\nPETSc can be configured in many interesting ways via the command line. We can visualize the details of the solver by using the command line argument -snes_view, which produces the excerpt below at the end of each solve call: Mesh refinement step 0\\n[...]\\nSNES Object: 1 MPI process\\n  type: newtonls\\n  maximum iterations=50, maximum function evaluations=10000\\n  tolerances: relative=1e-08, absolute=0.001, solution=1e-08\\n  total number of linear solver iterations=3\\n  total number of function evaluations=7\\n  norm schedule ALWAYS\\n  Jacobian is applied matrix-free with differencing\\n  Jacobian is applied matrix-free with differencing, no explicit Jacobian\\n  SNESLineSearch Object: 1 MPI process\\n    type: bt\\n      interpolation: cubic\\n      alpha=1.000000e-04\\n    maxstep=1.000000e+08, minlambda=1.000000e-12\\n    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08\\n    maximum iterations=40\\n  KSP Object: 1 MPI process\\n    type: preonly\\n    maximum iterations=10000, initial guess is zero\\n    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\\n    left preconditioning\\n using NONE norm type for convergence test\\n  PC Object: 1 MPI process\\n    type: shell\\n      deal.II user solve\\n    linear system matrix followed by preconditioner matrix:\\n    Mat Object: 1 MPI process\\n      type: mffd\\n      rows=89, cols=89\\n        Matrix-free approximation:\\n          err=1.49012e-08 (relative error in function evaluation)\\n          Using wp compute h routine\\n              Does not compute normU\\n    Mat Object: 1 MPI process\\n      type: seqaij\\n      rows=89, cols=89\\n      total: nonzeros=745, allocated nonzeros=745\\n      total number of mallocs used during MatSetValues calls=0\\n        not using I-node routines\\n[...]\\nInitializeLibrary::MPI@ MPI\\n From the above details, we see that we are using the \\\"newtonls\\\" solver type (\\\"Newton line search\\\"), with \\\"bt\\\" (\\\"backtracting\\\") line search.\\nFrom the output of -snes_view we can also get information about the linear solver details; specifically, when using the solve_with_jacobian interface, the deal.II interface internally uses a custom solver configuration within a \\\"shell\\\" preconditioner, that wraps the action of solve_with_jacobian.\\nWe can also see the details of the type of matrices used within the solve: \\\"mffd\\\" (matrix-free finite-differencing) for the action of the linearized operator and \\\"seqaij\\\" for the assembled Jacobian we have used to construct the preconditioner.\\nDiagnostics for the line search procedure can be turned on using the command line -snes_linesearch_monitor, producing the excerpt below: Mesh refinement step 0\\n  Target_tolerance: 0.001\\n \\n  Computing residual vector\\n0 norm=0.867975\\n  Computing Jacobian matrix\\n  Computing residual vector\\n  Computing residual vector\\n      Line search: Using full step: fnorm 8.679748230595e-01 gnorm 2.120728179320e-01\\n1 norm=0.212073\\n  Computing Jacobian matrix\\n  Computing residual vector\\n  Computing residual vector\\n      Line search: Using full step: fnorm 2.120728179320e-01 gnorm 1.896033864659e-02\\n2 norm=0.0189603\\n  Computing Jacobian matrix\\n  Computing residual vector\\n  Computing residual vector\\n      Line search: Using full step: fnorm 1.896033864659e-02 gnorm 3.148542199408e-04\\n3 norm=0.000314854\\n \\n[...]\\nWithin the run, the Jacobian matrix is assembled (and factored) 29 times: ./step-77-snes | grep \\\"Computing Jacobian\\\" | wc -l\\n29\\nKINSOL internally decided when it was necessary to update the Jacobian matrix (which is when it would call setup_jacobian). SNES can do something similar: We can compute the explicit sparse Jacobian matrix only once per refinement step (and reuse the initial factorization) by using the command line -snes_lag_jacobian -2, producing: ./step-77-snes -snes_lag_jacobian -2 | grep \\\"Computing Jacobian\\\" | wc -l\\n6\\n In other words, this dramatically reduces the number of times we have to build the Jacobian matrix, though at a cost to the number of nonlinear steps we have to take.\\nThe lagging period can also be decided automatically. For example, if we want to recompute the Jacobian at every other step: ./step-77-snes -snes_lag_jacobian 2 | grep \\\"Computing Jacobian\\\" | wc -l\\n25\\n Note, however, that we didn't exactly halve the number of Jacobian computations. In this case the solution process will require many more nonlinear iterations since the accuracy of the linear system solve is not enough.\\nIf we switch to using the preconditioned conjugate gradient method as a linear solve, still using our initial factorization as preconditioner, we get: ./step-77-snes -snes_lag_jacobian 2 -ksp_type cg | grep \\\"Computing Jacobian\\\" | wc -l\\n17\\n Note that in this case we use an approximate preconditioner (the LU factorization of the initial approximation) but we use a matrix-free operator for the action of our Jacobian matrix, thus solving for the correct linear system.\\nWe can switch to a quasi-Newton method by using the command line -snes_type qn -snes_qn_scale_type jacobian, and we can see that our Jacobian is sampled and factored only when needed, at the cost of an increase of the number of steps: Mesh refinement step 0\\n  Target_tolerance: 0.001\\n \\n  Computing residual vector\\n0 norm=0.867975\\n  Computing Jacobian matrix\\n  Computing residual vector\\n  Computing residual vector\\n1 norm=0.166391\\n  Computing residual vector\\n  Computing residual vector\\n2 norm=0.0507703\\n  Computing residual vector\\n  Computing residual vector\\n3 norm=0.0160007\\n  Computing residual vector\\n  Computing residual vector\\n  Computing residual vector\\n4 norm=0.00172425\\n  Computing residual vector\\n  Computing residual vector\\n  Computing residual vector\\n5 norm=0.000460486\\n[...]\\nNonlinear preconditioning can also be used. For example, we can run a right-preconditioned nonlinear GMRES, using one Newton step as a preconditioner, with the command: ./step-77-snes -snes_type ngmres -npc_snes_type newtonls -snes_monitor -npc_snes_monitor | grep SNES\\n  0 SNES Function norm 8.679748230595e-01\\n    0 SNES Function norm 8.679748230595e-01\\n    1 SNES Function norm 2.120738413585e-01\\n  1 SNES Function norm 1.284613424341e-01\\n    0 SNES Function norm 1.284613424341e-01\\n    1 SNES Function norm 6.539358995036e-03\\n  2 SNES Function norm 5.148828618635e-03\\n    0 SNES Function norm 5.148828618635e-03\\n    1 SNES Function norm 6.048613313899e-06\\n  3 SNES Function norm 3.199913594705e-06\\n  0 SNES Function norm 2.464793634583e-01\\n    0 SNES Function norm 2.464793634583e-01\\n    1 SNES Function norm 3.591625291931e-02\\n  1 SNES Function norm 3.235827289342e-02\\n    0 SNES Function norm 3.235827289342e-02\\n    1 SNES Function norm 1.249214136060e-03\\n  2 SNES Function norm 5.302288687547e-04\\n    0 SNES Function norm 5.302288687547e-04\\n    1 SNES Function norm 1.490247730530e-07\\n  3 SNES Function norm 1.436531309822e-07\\n  0 SNES Function norm 5.044203686086e-01\\n    0 SNES Function norm 5.044203686086e-01\\n    1 SNES Function norm 1.716855756535e-01\\n  1 SNES Function norm 7.770484434662e-02\\n    0 SNES Function norm 7.770484434662e-02\\n    1 SNES Function norm 2.462422395554e-02\\n  2 SNES Function norm 1.438187947066e-02\\n    0 SNES Function norm 1.438187947066e-02\\n    1 SNES Function norm 9.214168343848e-04\\n  3 SNES Function norm 2.268378169625e-04\\n    0 SNES Function norm 2.268378169625e-04\\n    1 SNES Function norm 3.463704776158e-07\\n  4 SNES Function norm 9.964533647277e-08\\n  0 SNES Function norm 1.942213246154e-01\\n    0 SNES Function norm 1.942213246154e-01\\n    1 SNES Function norm 1.125558372384e-01\\n  1 SNES Function norm 1.309880643103e-01\\n    0 SNES Function norm 1.309880643103e-01\\n    1 SNES Function norm 2.595634741967e-02\\n  2 SNES Function norm 1.149616419685e-02\\n    0 SNES Function norm 1.149616419685e-02\\n    1 SNES Function norm 7.204904831783e-04\\n  3 SNES Function norm 6.743539224973e-04\\n    0 SNES Function norm 6.743539224973e-04\\n    1 SNES Function norm 1.521290969181e-05\\n  4 SNES Function norm 8.121151857453e-06\\n    0 SNES Function norm 8.121151857453e-06\\n    1 SNES Function norm 1.460470903719e-09\\n  5 SNES Function norm 9.982794797188e-10\\n  0 SNES Function norm 1.225979459424e-01\\n    0 SNES Function norm 1.225979459424e-01\\n    1 SNES Function norm 4.946412992249e-02\\n  1 SNES Function norm 2.466574163571e-02\\n    0 SNES Function norm 2.466574163571e-02\\n    1 SNES Function norm 8.537739703503e-03\\n  2 SNES Function norm 5.935412895618e-03\\n    0 SNES Function norm 5.935412895618e-03\\n    1 SNES Function norm 3.699307476482e-04\\n  3 SNES Function norm 2.188768476656e-04\\n    0 SNES Function norm 2.188768476656e-04\\n    1 SNES Function norm 9.478344390128e-07\\n  4 SNES Function norm 4.559224590570e-07\\n    0 SNES Function norm 4.559224590570e-07\\n    1 SNES Function norm 1.317127376721e-11\\n  5 SNES Function norm 1.311046524394e-11\\n  0 SNES Function norm 1.011637873732e-01\\n    0 SNES Function norm 1.011637873732e-01\\n    1 SNES Function norm 1.072720108836e-02\\n  1 SNES Function norm 8.985302820531e-03\\n    0 SNES Function norm 8.985302820531e-03\\n    1 SNES Function norm 5.807781788861e-04\\n  2 SNES Function norm 5.594756759727e-04\\n    0 SNES Function norm 5.594756759727e-04\\n    1 SNES Function norm 1.834638371641e-05\\n  3 SNES Function norm 1.408280767367e-05\\n    0 SNES Function norm 1.408280767367e-05\\n    1 SNES Function norm 5.763656314185e-08\\n  4 SNES Function norm 1.702747382189e-08\\n    0 SNES Function norm 1.702747382189e-08\\n    1 SNES Function norm 1.452722802538e-12\\n  5 SNES Function norm 1.444478767837e-12\\nAs also discussed for the KINSOL use above, optimal preconditioners should be used instead of the LU factorization used here by default. This is already possible within this tutorial by playing with the command line options. For example, algebraic multigrid can be used by simply specifying -pc_type gamg. When using iterative linear solvers, the \\\"Eisenstat-Walker trick\\\" [196] can be also requested at command line via -snes_ksp_ew. Using these options, we can see that the number of nonlinear iterations used by the solver increases as the mesh is refined, and that the number of linear iterations increases as the Newton solver is entering the second-order ball of convergence: ./step-77-snes -pc_type gamg -ksp_type cg -ksp_converged_reason -snes_converged_reason -snes_ksp_ew | grep CONVERGED\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 2\\n  Linear solve converged due to CONVERGED_RTOL iterations 3\\nNonlinear solve converged due to CONVERGED_FNORM_ABS iterations 3\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 2\\nNonlinear solve converged due to CONVERGED_FNORM_ABS iterations 3\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 2\\n  Linear solve converged due to CONVERGED_RTOL iterations 2\\n  Linear solve converged due to CONVERGED_RTOL iterations 2\\n  Linear solve converged due to CONVERGED_RTOL iterations 3\\n  Linear solve converged due to CONVERGED_RTOL iterations 4\\nNonlinear solve converged due to CONVERGED_FNORM_ABS iterations 6\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 2\\n  Linear solve converged due to CONVERGED_RTOL iterations 4\\n  Linear solve converged due to CONVERGED_RTOL iterations 7\\nNonlinear solve converged due to CONVERGED_FNORM_ABS iterations 12\\n  Linear solve converged due to CONVERGED_RTOL iterations 1\\n  Linear solve converged due to CONVERGED_RTOL iterations 2\\n  Linear solve converged due to CONVERGED_RTOL iterations 3\\n  Linear solve converged due to CONVERGED_RTOL iterations 4\\n  Linear solve converged due to CONVERGED_RTOL iterations 7\\nNonlinear solve converged due to CONVERGED_FNORM_ABS iterations 5\\n  Linear solve converged due to CONVERGED_RTOL iterations 2\\n  Linear solve converged due to CONVERGED_RTOL iterations 3\\n  Linear solve converged due to CONVERGED_RTOL iterations 7\\n  Linear solve converged due to CONVERGED_RTOL iterations 6\\n  Linear solve converged due to CONVERGED_RTOL iterations 7\\n  Linear solve converged due to CONVERGED_RTOL iterations 12\\nNonlinear solve converged due to CONVERGED_FNORM_ABS iterations 6\\nFinally we describe how to get some diagnostic on the correctness of the computed Jacobian. Deriving the correct linearization is sometimes difficult: It took a page or two in the introduction to derive the exact bilinear form for the Jacobian matrix, and it would be quite nice compute it automatically from the residual of which it is the derivative. (This is what step-72 does!) But if one is set on doing things by hand, it would at least be nice if we had a way to check the correctness of the derivation. SNES allows us to do this: we can use the options -snes_test_jacobian -snes_test_jacobian_view: Mesh refinement step 0\\n  Target_tolerance: 0.001\\n \\n  Computing residual vector\\n0 norm=0.867975\\n  Computing Jacobian matrix\\n  ---------- Testing Jacobian -------------\\n  Testing hand-coded Jacobian, if (for double precision runs) ||J - Jfd||_F/||J||_F is\\n    O(1.e-8), the hand-coded Jacobian is probably correct.\\n[...]\\n  ||J - Jfd||_F/||J||_F = 0.0196815, ||J - Jfd||_F = 0.503436\\n[...]\\n  Hand-coded minus finite-difference Jacobian with tolerance 1e-05 ----------\\nMat Object: 1 MPI process\\n  type: seqaij\\nrow 0: (0, 0.125859)\\nrow 1: (1, 0.0437112)\\nrow 2:\\nrow 3:\\nrow 4: (4, 0.902232)\\nrow 5:\\nrow 6:\\nrow 7:\\nrow 8:\\nrow 9: (9, 0.537306)\\nrow 10:\\nrow 11: (11, 1.38157)\\nrow 12:\\n[...]\\n showing that the only errors we commit in assembling the Jacobian are on the boundary dofs. As discussed in the tutorial, those errors are harmless.\\nThe key take-away messages of this modification of the tutorial program are therefore basically the same of what we already found using KINSOL:\\n\\nThe solution is the same as the one we computed in step-15, i.e., the interfaces to PETSc SNES package really did what they were supposed to do. This should not come as a surprise, but the important point is that we don't have to spend the time implementing the complex algorithms that underlie advanced nonlinear solvers ourselves.\\nSNES offers a wide variety of solvers and line search techniques, not only Newton. It also allows us to control Jacobian setups; however, differently from KINSOL, this is not automatically decided within the library by looking at the residual vector but it needs to be specified by the user.\\n\\nReplacing SUNDIALS' KINSOL by Trilinos' NOX package \\nBesides KINSOL and SNES, the third option you have is to use the NOX package. As before, rather than showing in detail how that needs to happen, let us simply point out that the test suite program tests/trilinos/step-77-with-nox.cc does this. The modifications necessary to use NOX instead of KINSOL are quite minimal; in particular, NOX (unlike SNES) is happy to work with deal.II's own vector and matrix classes.\\nReplacing SUNDIALS' KINSOL by a generic nonlinear solver \\nHaving to choose which of these three frameworks (KINSOL, SNES, or NOX) to use at compile time is cumbersome when wanting to compare things. It would be nicer if one could decide the package to use at run time, assuming that one has a copy of deal.II installed that is compiled against all three of these dependencies. It turns out that this is possible, using the class NonlinearSolverSelector that presents a common interface to all three of these solvers, along with the ability to choose which one to use based on run-time parameters.\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2021 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Author: Wolfgang Bangerth, Colorado State University, 2021.\\n * Based on @ref step_15 \\\"step-15\\\" by Sven Wetterauer, University of Heidelberg, 2012.\\n */\\n \\n \\n \\n \\n#include <deal.II/base/quadrature_lib.h>\\n#include <deal.II/base/function.h>\\n#include <deal.II/base/timer.h>\\n#include <deal.II/base/utilities.h>\\n \\n#include <deal.II/lac/vector.h>\\n#include <deal.II/lac/full_matrix.h>\\n#include <deal.II/lac/sparse_matrix.h>\\n#include <deal.II/lac/dynamic_sparsity_pattern.h>\\n#include <deal.II/lac/affine_constraints.h>\\n#include <deal.II/lac/sparse_direct.h>\\n \\n#include <deal.II/grid/tria.h>\\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/grid_refinement.h>\\n \\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/dofs/dof_accessor.h>\\n#include <deal.II/dofs/dof_tools.h>\\n \\n#include <deal.II/fe/fe_values.h>\\n#include <deal.II/fe/fe_q.h>\\n \\n#include <deal.II/numerics/vector_tools.h>\\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/error_estimator.h>\\n#include <deal.II/numerics/solution_transfer.h>\\n \\n#include <deal.II/sundials/kinsol.h>\\n \\n#include <fstream>\\n#include <iostream>\\n \\n \\nnamespace Step77\\n{\\n using namespace dealii;\\n \\n \\n \\n template <int dim>\\n class MinimalSurfaceProblem\\n  {\\n public:\\n    MinimalSurfaceProblem();\\n void run();\\n \\n private:\\n void setup_system();\\n void solve(const Vector<double> &rhs,\\n Vector<double>       &solution,\\n const double          tolerance);\\n void refine_mesh();\\n void output_results(const unsigned int refinement_cycle);\\n void compute_and_factorize_jacobian(const Vector<double> &evaluation_point);\\n void compute_residual(const Vector<double> &evaluation_point,\\n Vector<double>       &residual);\\n \\n Triangulation<dim> triangulation;\\n \\n DoFHandler<dim> dof_handler;\\n const FE_Q<dim> fe;\\n \\n AffineConstraints<double> zero_constraints;\\n AffineConstraints<double> nonzero_constraints;\\n \\n SparsityPattern                      sparsity_pattern;\\n SparseMatrix<double>                 jacobian_matrix;\\n    std::unique_ptr<SparseDirectUMFPACK> jacobian_matrix_factorization;\\n \\n Vector<double> current_solution;\\n \\n TimerOutput computing_timer;\\n  };\\n \\n \\n \\n \\n template <int dim>\\n class BoundaryValues : public Function<dim>\\n  {\\n public:\\n virtual double value(const Point<dim>  &p,\\n const unsigned int component = 0) const override;\\n  };\\n \\n \\n template <int dim>\\n double BoundaryValues<dim>::value(const Point<dim> &p,\\n const unsigned int /*component*/) const\\n {\\n return std::sin(2 * numbers::PI * (p[0] + p[1]));\\n  }\\n \\n \\n \\n \\n template <int dim>\\n  MinimalSurfaceProblem<dim>::MinimalSurfaceProblem()\\n    : dof_handler(triangulation)\\n    , fe(1)\\n    , computing_timer(std::cout, TimerOutput::never, TimerOutput::wall_times)\\n  {}\\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::setup_system()\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"set up\\\");\\n \\n    dof_handler.distribute_dofs(fe);\\n    current_solution.reinit(dof_handler.n_dofs());\\n \\n    zero_constraints.clear();\\n VectorTools::interpolate_boundary_values(dof_handler,\\n                                             0,\\n Functions::ZeroFunction<dim>(),\\n                                             zero_constraints);\\n \\n DoFTools::make_hanging_node_constraints(dof_handler, zero_constraints);\\n    zero_constraints.close();\\n \\n    nonzero_constraints.clear();\\n VectorTools::interpolate_boundary_values(dof_handler,\\n                                             0,\\n                                             BoundaryValues<dim>(),\\n                                             nonzero_constraints);\\n \\n DoFTools::make_hanging_node_constraints(dof_handler, nonzero_constraints);\\n    nonzero_constraints.close();\\n \\n DynamicSparsityPattern dsp(dof_handler.n_dofs());\\n DoFTools::make_sparsity_pattern(dof_handler, dsp, zero_constraints);\\n \\n    sparsity_pattern.copy_from(dsp);\\n    jacobian_matrix.reinit(sparsity_pattern);\\n    jacobian_matrix_factorization.reset();\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::compute_and_factorize_jacobian(\\n const Vector<double> &evaluation_point)\\n  {\\n    {\\n TimerOutput::Scope t(computing_timer, \\\"assembling the Jacobian\\\");\\n \\n      std::cout << \\\"  Computing Jacobian matrix\\\" << std::endl;\\n \\n const QGauss<dim> quadrature_formula(fe.degree + 1);\\n \\n      jacobian_matrix = 0;\\n \\n FEValues<dim> fe_values(fe,\\n                              quadrature_formula,\\n update_gradients | update_quadrature_points |\\n update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n \\n      std::vector<Tensor<1, dim>> evaluation_point_gradients(n_q_points);\\n \\n      std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n \\n for (const auto &cell : dof_handler.active_cell_iterators())\\n        {\\n cell_matrix = 0;\\n \\n          fe_values.reinit(cell);\\n \\n          fe_values.get_function_gradients(evaluation_point,\\n                                           evaluation_point_gradients);\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n            {\\n const double coeff =\\n                1.0 / std::sqrt(1 + evaluation_point_gradients[q] *\\n                                      evaluation_point_gradients[q]);\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n                {\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n cell_matrix(i, j) +=\\n                      (((fe_values.shape_grad(i, q)    // ((\\\\nabla \\\\phi_i\\n                         * coeff                       //   * a_n\\n                         * fe_values.shape_grad(j, q)) //   * \\\\nabla \\\\phi_j)\\n                        -                              //  -\\n                        (fe_values.shape_grad(i, q)    //  (\\\\nabla \\\\phi_i\\n                         * coeff * coeff * coeff       //   * a_n^3\\n                         *\\n                         (fe_values.shape_grad(j, q)       //   * (\\\\nabla \\\\phi_j\\n                          * evaluation_point_gradients[q]) //      * \\\\nabla u_n)\\n                         * evaluation_point_gradients[q])) //   * \\\\nabla u_n)))\\n                       * fe_values.JxW(q));                // * dx\\n                }\\n            }\\n \\n          cell->get_dof_indices(local_dof_indices);\\n          zero_constraints.distribute_local_to_global(cell_matrix,\\n                                                      local_dof_indices,\\n                                                      jacobian_matrix);\\n        }\\n    }\\n \\n    {\\n TimerOutput::Scope t(computing_timer, \\\"factorizing the Jacobian\\\");\\n \\n      std::cout << \\\"  Factorizing Jacobian matrix\\\" << std::endl;\\n \\n      jacobian_matrix_factorization = std::make_unique<SparseDirectUMFPACK>();\\n      jacobian_matrix_factorization->factorize(jacobian_matrix);\\n    }\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::compute_residual(\\n const Vector<double> &evaluation_point,\\n Vector<double>       &residual)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"assembling the residual\\\");\\n \\n    std::cout << \\\"  Computing residual vector...\\\" << std::flush;\\n \\n    residual = 0.0;\\n \\n const QGauss<dim> quadrature_formula(fe.degree + 1);\\n FEValues<dim>     fe_values(fe,\\n                            quadrature_formula,\\n update_gradients | update_quadrature_points |\\n update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n Vector<double> cell_residual(dofs_per_cell);\\n    std::vector<Tensor<1, dim>> evaluation_point_gradients(n_q_points);\\n \\n    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n \\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      {\\n cell_residual = 0;\\n        fe_values.reinit(cell);\\n \\n        fe_values.get_function_gradients(evaluation_point,\\n                                         evaluation_point_gradients);\\n \\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n          {\\n const double coeff =\\n              1.0 / std::sqrt(1 + evaluation_point_gradients[q] *\\n                                    evaluation_point_gradients[q]);\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n cell_residual(i) +=\\n                (fe_values.shape_grad(i, q)      // \\\\nabla \\\\phi_i\\n                 * coeff                         // * a_n\\n                 * evaluation_point_gradients[q] // * \\\\nabla u_n\\n                 * fe_values.JxW(q));            // * dx\\n          }\\n \\n        cell->get_dof_indices(local_dof_indices);\\n        zero_constraints.distribute_local_to_global(cell_residual,\\n                                                    local_dof_indices,\\n                                                    residual);\\n      }\\n \\n    std::cout << \\\" norm=\\\" << residual.l2_norm() << std::endl;\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::solve(const Vector<double> &rhs,\\n Vector<double>       &solution,\\n const double /*tolerance*/)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"linear system solve\\\");\\n \\n    std::cout << \\\"  Solving linear system\\\" << std::endl;\\n \\n    jacobian_matrix_factorization->vmult(solution, rhs);\\n    zero_constraints.distribute(solution);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::refine_mesh()\\n  {\\n Vector<float> estimated_error_per_cell(triangulation.n_active_cells());\\n \\n KellyErrorEstimator<dim>::estimate(\\n      dof_handler,\\n QGauss<dim - 1>(fe.degree + 1),\\n      std::map<types::boundary_id, const Function<dim> *>(),\\n      current_solution,\\n      estimated_error_per_cell);\\n \\n GridRefinement::refine_and_coarsen_fixed_number(triangulation,\\n                                                    estimated_error_per_cell,\\n                                                    0.3,\\n                                                    0.03);\\n \\n triangulation.prepare_coarsening_and_refinement();\\n \\n SolutionTransfer<dim> solution_transfer(dof_handler);\\n const Vector<double>  coarse_solution = current_solution;\\n    solution_transfer.prepare_for_coarsening_and_refinement(coarse_solution);\\n \\n triangulation.execute_coarsening_and_refinement();\\n \\n    setup_system();\\n \\n    solution_transfer.interpolate(coarse_solution, current_solution);\\n \\n    nonzero_constraints.distribute(current_solution);\\n  }\\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::output_results(\\n const unsigned int refinement_cycle)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"graphical output\\\");\\n \\n DataOut<dim> data_out;\\n \\n    data_out.attach_dof_handler(dof_handler);\\n    data_out.add_data_vector(current_solution, \\\"solution\\\");\\n    data_out.build_patches();\\n \\n const std::string filename =\\n \\\"solution-\\\" + Utilities::int_to_string(refinement_cycle, 2) + \\\".vtu\\\";\\n    std::ofstream output(filename);\\n    data_out.write_vtu(output);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::run()\\n  {\\n GridGenerator::hyper_ball(triangulation);\\n triangulation.refine_global(2);\\n \\n    setup_system();\\n    nonzero_constraints.distribute(current_solution);\\n \\n for (unsigned int refinement_cycle = 0; refinement_cycle < 6;\\n         ++refinement_cycle)\\n      {\\n        computing_timer.reset();\\n        std::cout << \\\"Mesh refinement step \\\" << refinement_cycle << std::endl;\\n \\n if (refinement_cycle != 0)\\n          refine_mesh();\\n \\n const double target_tolerance = 1e-3 * std::pow(0.1, refinement_cycle);\\n        std::cout << \\\"  Target_tolerance: \\\" << target_tolerance << std::endl\\n                  << std::endl;\\n \\n        {\\n typename SUNDIALS::KINSOL<Vector<double>>::AdditionalData\\n            additional_data;\\n          additional_data.function_tolerance = target_tolerance;\\n \\n SUNDIALS::KINSOL<Vector<double>> nonlinear_solver(additional_data);\\n \\n          nonlinear_solver.reinit_vector = [&](Vector<double> &x) {\\n            x.reinit(dof_handler.n_dofs());\\n          };\\n \\n          nonlinear_solver.residual =\\n            [&](const Vector<double> &evaluation_point,\\n Vector<double>       &residual) {\\n              compute_residual(evaluation_point, residual);\\n            };\\n \\n          nonlinear_solver.setup_jacobian =\\n            [&](const Vector<double> &current_u,\\n const Vector<double> & /*current_f*/) {\\n              compute_and_factorize_jacobian(current_u);\\n            };\\n \\n          nonlinear_solver.solve_with_jacobian = [&](const Vector<double> &rhs,\\n Vector<double>       &dst,\\n const double tolerance) {\\n            solve(rhs, dst, tolerance);\\n          };\\n \\n          nonlinear_solver.solve(current_solution);\\n        }\\n \\n        output_results(refinement_cycle);\\n \\n        computing_timer.print_summary();\\n \\n        std::cout << std::endl;\\n      }\\n  }\\n} // namespace Step77\\n \\n \\nint main()\\n{\\n try\\n    {\\n using namespace Step77;\\n \\n      MinimalSurfaceProblem<2> problem;\\n      problem.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n \\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n return 0;\\n}\\naffine_constraints.h\\nDataOutInterface::write_vtuvoid write_vtu(std::ostream &out) constDefinition data_out_base.cc:7692\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\nVector::l2_normreal_type l2_norm() const\\ndof_accessor.h\\ndof_handler.h\\ndof_tools.h\\ndynamic_sparsity_pattern.h\\nerror_estimator.h\\nfe_values.h\\nfe_q.h\\nfull_matrix.h\\nfunction.h\\ngrid_refinement.h\\ntria.h\\ngrid_generator.h\\nutilities.h\\nkinsol.h\\nLocalIntegrators::Advection::cell_matrixvoid cell_matrix(FullMatrix< double > &M, const FEValuesBase< dim > &fe, const FEValuesBase< dim > &fetest, const ArrayView< const std::vector< double > > &velocity, const double factor=1.)Definition advection.h:74\\nLocalIntegrators::Advection::cell_residualvoid cell_residual(Vector< double > &result, const FEValuesBase< dim > &fe, const std::vector< Tensor< 1, dim > > &input, const ArrayView< const std::vector< double > > &velocity, double factor=1.)Definition advection.h:130\\nPhysics::Elasticity::Kinematics::eSymmetricTensor< 2, dim, Number > e(const Tensor< 2, dim, Number > &F)\\ndata_out.h\\nsolution_transfer.h\\nquadrature_lib.h\\nsparse_direct.h\\nsparse_matrix.h\\ntimer.h\\nvector.h\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"