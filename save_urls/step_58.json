"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_58.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-58 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-58 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-58 tutorial program\\n\\n\\nThis tutorial depends on step-26, step-29.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\nA note about the character of the equations\\nThe general idea of operator splitting\\nOperator splitting: the \\\"Lie splitting\\\" approach\\nOperator splitting: the \\\"Strang splitting\\\" approach\\nTime discretization\\nSpatial discretization and dealing with complex variables\\nLinear solvers\\nDefinition of the test case\\n\\n The commented program\\n\\nInclude files\\nThe NonlinearSchroedingerEquation class\\nEquation data\\nImplementation of the NonlinearSchroedingerEquation class\\n\\nSetting up data structures and assembling matrices\\nImplementing the Strang splitting steps\\nCreating graphical output\\nRunning the simulation\\nThe main() function\\n\\n\\n\\n Results\\n\\nVisualizing the solution\\nPossibilities for extensions\\n\\n Better linear solvers \\n Boundary conditions \\n Adaptive meshes \\n\\n\\n The plain program\\n   \\n\\n\\nThis program was contributed by Wolfgang Bangerth (Colorado State University) and Yong-Yong Cai (Beijing Computational Science Research Center, CSRC) and is the result of the first author's time as a visitor at CSRC.\\nThis material is based upon work partially supported by National Science Foundation grants OCI-1148116, OAC-1835673, DMS-1821210, and EAR-1925595; and by the Computational Infrastructure in Geodynamics initiative (CIG), through the National Science Foundation under Award No. EAR-1550901 and The University of California-Davis. \\n Introduction\\nThe Nonlinear Schr\\u00f6dinger Equation (NLSE) for a function  \\\\(\\\\psi=\\\\psi(\\\\mathbf\\nx,t)\\\\) and a potential \\\\(V=V(\\\\mathbf x)\\\\) is a model often used in quantum mechanics and nonlinear optics. If one measures in appropriate quantities (so that \\\\(\\\\hbar=1\\\\)), then it reads as follows:                  \\n\\\\begin{align*}\\n  - i \\\\frac{\\\\partial \\\\psi}{\\\\partial t}\\n  - \\\\frac 12 \\\\Delta \\\\psi\\n  + V \\\\psi\\n  + \\\\kappa |\\\\psi|^2 \\\\psi\\n  &= 0\\n  \\\\qquad\\\\qquad\\n  &\\n  \\\\text{in}\\\\; \\\\Omega\\\\times (0,T),\\n  \\\\\\\\\\n  \\\\psi(\\\\mathbf x,0) &= \\\\psi_0(\\\\mathbf x)\\n  &\\n  \\\\text{in}\\\\; \\\\Omega,\\n  \\\\\\\\\\n  \\\\psi(\\\\mathbf x,t) &= 0\\n  &\\n  \\\\text{on}\\\\; \\\\partial\\\\Omega\\\\times (0,T).\\n\\\\end{align*}\\n\\n If there is no potential, i.e. \\\\(V(\\\\mathbf x)=0\\\\), then it can be used to describe the propagation of light in optical fibers. If  \\\\(V(\\\\mathbf\\nx)\\\\neq 0\\\\), the equation is also sometimes called the Gross-Pitaevskii equation and can be used to model the time dependent behavior of Bose-Einstein condensates.\\nFor this particular tutorial program, the physical interpretation of the equation is not of much concern to us. Rather, we want to use it as a model that allows us to explain two aspects:\\nIt is a complex-valued equation for  \\\\(\\\\psi \\\\in H^1(\\\\Omega,{\\\\mathbb\\n  C})\\\\). We have previously seen complex-valued equations in step-29, but there have opted to split the equations into real and imaginary parts and consequently ended up solving a system of two real-valued equations. In contrast, the goal here is to show how to solve problems in which we keep everything as complex numbers.\\nThe equation is a nice model problem to explain how operator splitting methods work. This is because it has terms with fundamentally different character: on the one hand,  \\\\(- \\\\frac 12\\n  \\\\Delta \\\\psi\\\\) is a regular spatial operator in the way we have seen many times before; on the other hand,  \\\\(\\\\kappa |\\\\psi(\\\\mathbf x,t)|^2\\n  \\\\psi\\\\) has no spatial or temporal derivatives, i.e., it is a purely local operator. It turns out that we have efficient methods for each of these terms (in particular, we have analytic solutions for the latter), and that we may be better off treating these terms differently and separately. We will explain this in more detail below.\\n\\nA note about the character of the equations\\nAt first glance, the equations appear to be parabolic and similar to the heat equation (see step-26) as there is only a single time derivative and two spatial derivatives. But this is misleading. Indeed, that this is not the correct interpretation is more easily seen if we assume for a moment that the potential \\\\(V=0\\\\) and \\\\(\\\\kappa=0\\\\). Then we have the equation     \\n\\\\begin{align*}\\n  - i \\\\frac{\\\\partial \\\\psi}{\\\\partial t}\\n  - \\\\frac 12 \\\\Delta \\\\psi\\n  &= 0.\\n\\\\end{align*}\\n\\n If we separate the solution into real and imaginary parts, \\\\(\\\\psi=v+iw\\\\), with \\\\(v=\\\\textrm{Re}\\\\;\\\\psi,\\\\; w=\\\\textrm{Im}\\\\;\\\\psi\\\\), then we can split the one equation into its real and imaginary parts in the same way as we did in step-29:         \\n\\\\begin{align*}\\n  \\\\frac{\\\\partial w}{\\\\partial t}\\n  - \\\\frac 12 \\\\Delta v\\n  &= 0,\\n  \\\\\\\\\\n  -\\\\frac{\\\\partial v}{\\\\partial t}\\n  - \\\\frac 12 \\\\Delta w\\n  &= 0.\\n\\\\end{align*}\\n\\n Not surprisingly, the factor \\\\(i\\\\) in front of the time derivative couples the real and imaginary parts of the equation. If we want to understand this equation further, take the time derivative of one of the equations, say     \\n\\\\begin{align*}\\n  \\\\frac{\\\\partial^2 w}{\\\\partial t^2}\\n  - \\\\frac 12 \\\\Delta \\\\frac{\\\\partial v}{\\\\partial t}\\n  &= 0,\\n\\\\end{align*}\\n\\n (where we have assumed that, at least in some formal sense, we can commute the spatial and temporal derivatives), and then insert the other equation into it:     \\n\\\\begin{align*}\\n  \\\\frac{\\\\partial^2 w}{\\\\partial t^2}\\n  + \\\\frac 14 \\\\Delta^2 w\\n  &= 0.\\n\\\\end{align*}\\n\\n This equation is hyperbolic and similar in character to the wave equation. (This will also be obvious if you look at the video in the \\\"Results\\\" section of this program.) Furthermore, we could have arrived at the same equation for \\\\(v\\\\) as well. Consequently, a better assumption for the NLSE is to think of it as a hyperbolic, wave-propagation equation than as a diffusion equation such as the heat equation. (You may wonder whether it is correct that the operator \\\\(\\\\Delta^2\\\\) appears with a positive sign whereas in the wave equation, \\\\(\\\\Delta\\\\) has a negative sign. This is indeed correct: After multiplying by a test function and integrating by parts, we want to come out with a positive (semi-)definite form. So, from \\\\(-\\\\Delta u\\\\) we obtain \\\\(+(\\\\nabla v,\\\\nabla u)\\\\). Likewise, after integrating by parts twice, we obtain from \\\\(+\\\\Delta^2 u\\\\) the form \\\\(+(\\\\Delta v,\\\\Delta u)\\\\). In both cases do we get the desired positive sign.)\\nThe real NLSE, of course, also has the terms \\\\(V\\\\psi\\\\) and \\\\(\\\\kappa|\\\\psi|^2\\\\psi\\\\). However, these are of lower order in the spatial derivatives, and while they are obviously important, they do not change the character of the equation.\\nIn any case, the purpose of this discussion is to figure out what time stepping scheme might be appropriate for the equation. The conclusions is that, as a hyperbolic-kind of equation, we need to choose a time step that satisfies a CFL-type condition. If we were to use an explicit method (which we will not), we would have to investigate the eigenvalues of the matrix that corresponds to the spatial operator. If you followed the discussions of the video lectures (See also video lecture 26, video lecture 27, video lecture 28.) then you will remember that the pattern is that one needs to make sure that \\\\(k^s \\\\propto h^t\\\\) where \\\\(k\\\\) is the time step, \\\\(h\\\\) the mesh width, and \\\\(s,t\\\\) are the orders of temporal and spatial derivatives. Whether you take the original equation ( \\\\(s=1,t=2\\\\)) or the reformulation for only the real or imaginary part, the outcome is that we would need to choose \\\\(k \\\\propto h^2\\\\) if we were to use an explicit time stepping method. This is not feasible for the same reasons as in step-26 for the heat equation: It would yield impractically small time steps for even only modestly refined meshes. Rather, we have to use an implicit time stepping method and can then choose a more balanced \\\\(k \\\\propto h\\\\). Indeed, we will use the implicit Crank-Nicolson method as we have already done in step-23 before for the regular wave equation.\\nThe general idea of operator splitting\\nNoteThe material presented here is also discussed in video lecture 30.25. (All video lectures are also available here.)\\nIf one thought of the NLSE as an ordinary differential equation in which the right hand side happens to have spatial derivatives, i.e., write it as            \\n\\\\begin{align*}\\n  \\\\frac{d\\\\psi}{dt}\\n  &=\\n  i\\\\frac 12 \\\\Delta \\\\psi\\n  -i V \\\\psi\\n  -i\\\\kappa |\\\\psi|^2 \\\\psi,\\n  \\\\qquad\\\\qquad\\n  &\\n  \\\\text{for}\\\\; t \\\\in (0,T),\\n  \\\\\\\\\\n  \\\\psi(0) &= \\\\psi_0,\\n\\\\end{align*}\\n\\n one may be tempted to \\\"formally solve\\\" it by integrating both sides over a time interval \\\\([t_{n},t_{n+1}]\\\\) and obtain              \\n\\\\begin{align*}\\n  \\\\psi(t_{n+1})\\n  &=\\n  \\\\psi(t_n)\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  i\\\\frac 12 \\\\Delta \\\\psi(t)\\n  -i V \\\\psi(t)\\n  -i\\\\kappa |\\\\psi(t)|^2 \\\\psi(t)\\n  \\\\right)\\n  \\\\;\\n  dt.\\n\\\\end{align*}\\n\\n Of course, it's not that simple: the \\\\(\\\\psi(t)\\\\) in the integrand is still changing over time in accordance with the differential equation, so we cannot just evaluate the integral (or approximate it easily via quadrature) because we don't know \\\\(\\\\psi(t)\\\\). But we can write this with separate contributions as follows, and this will allow us to deal with different terms separately:                          \\n\\\\begin{align*}\\n  \\\\psi(t_{n+1})\\n  &=\\n  \\\\psi(t_n)\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  i\\\\frac 12 \\\\Delta \\\\psi(t)\\n  \\\\right)\\n  \\\\;\\n  dt\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  -i V \\\\psi(t)\\n  \\\\right)\\n  \\\\;\\n  dt\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  -i\\\\kappa |\\\\psi(t)|^2 \\\\,\\\\psi(t)\\n  \\\\right)\\n  \\\\;\\n  dt.\\n\\\\end{align*}\\n\\n The way this equation can now be read is as follows: For each time interval \\\\([t_{n},t_{n+1}]\\\\), the change \\\\(\\\\psi(t_{n+1})-\\\\psi(t_{n})\\\\) in the solution consists of three contributions:\\nThe contribution of the Laplace operator.\\nThe contribution of the potential \\\\(V\\\\).\\nThe contribution of the \\\"phase\\\" term \\\\(-i\\\\kappa |\\\\psi(t)|^2\\\\,\\\\psi(t)\\\\).\\n\\nOperator splitting is now an approximation technique that allows us to treat each of these contributions separately. (If we want: In practice, we will treat the first two together, and the last one separate. But that is a detail, conceptually we could treat all of them differently.) To this end, let us introduce three separate \\\"solutions\\\":                                  \\n\\\\begin{align*}\\n  \\\\psi^{(1)}(t_{n+1})\\n  &=\\n  \\\\psi(t_n)\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  i\\\\frac 12 \\\\Delta \\\\psi^{(1)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt,\\n\\\\\\\\\\n  \\\\psi^{(2)}(t_{n+1})\\n  &=\\n  \\\\psi(t_n)\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  -i V \\\\psi^{(2)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt,\\n\\\\\\\\\\n  \\\\psi^{(3)}(t_{n+1})\\n  &=\\n  \\\\psi(t_n)\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  -i\\\\kappa |\\\\psi^{(3)}(t)|^2 \\\\,\\\\psi^{(3)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt.\\n\\\\end{align*}\\n\\nThese three \\\"solutions\\\" can be thought of as satisfying the following differential equations:                          \\n\\\\begin{align*}\\n  \\\\frac{d\\\\psi^{(1)}}{dt}\\n  &=\\n  i\\\\frac 12 \\\\Delta \\\\psi^{(1)},\\n  \\\\qquad\\n  &\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(1)}(t_n) &= \\\\psi(t_n),\\n\\\\\\\\\\n  \\\\frac{d\\\\psi^{(2)}}{dt}\\n  &=\\n  -i V \\\\psi^{(2)},\\n  &\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(2)}(t_n) &= \\\\psi(t_n),\\n\\\\\\\\\\n  \\\\frac{d\\\\psi^{(3)}}{dt}\\n  &=\\n  -i\\\\kappa |\\\\psi^{(3)}|^2 \\\\,\\\\psi^{(3)},\\n  &\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(3)}(t_n) &= \\\\psi(t_n).\\n\\\\end{align*}\\n\\n In other words, they are all trajectories \\\\(\\\\psi^{(k)}\\\\) that start at \\\\(\\\\psi(t_n)\\\\) and integrate up the effects of exactly one of the three terms. The increments resulting from each of these terms over our time interval are then \\\\(I^{(1)}=\\\\psi^{(1)}(t_{n+1})-\\\\psi(t_n)\\\\), \\\\(I^{(2)}=\\\\psi^{(2)}(t_{n+1})-\\\\psi(t_n)\\\\), and \\\\(I^{(3)}=\\\\psi^{(3)}(t_{n+1})-\\\\psi(t_n)\\\\).\\nIt is now reasonable to assume (this is an approximation!) that the change due to all three of the effects in question is well approximated by the sum of the three separate increments:     \\n\\\\begin{align*}\\n \\\\psi(t_{n+1})-\\\\psi(t_n)\\n \\\\approx\\n I^{(1)} + I^{(2)} + I^{(3)}.\\n\\\\end{align*}\\n\\n This intuition is indeed correct, though the approximation is not exact: the difference between the exact left hand side and the term \\\\(I^{(1)}+I^{(2)}+I^{(3)}\\\\) (i.e., the difference between the exact increment for the exact solution \\\\(\\\\psi(t)\\\\) when moving from \\\\(t_n\\\\) to \\\\(t_{n+1}\\\\), and the increment composed of the three parts on the right hand side), is proportional to \\\\(\\\\Delta t=t_{n+1}-t_{n}\\\\). In other words, this approach introduces an error of size \\\\({\\\\cal O}(\\\\Delta t)\\\\). Nothing we have done so far has discretized anything in time or space, so the overall error is going to be \\\\({\\\\cal O}(\\\\Delta t)\\\\) plus whatever error we commit when approximating the integrals (the temporal discretization error) plus whatever error we commit when approximating the spatial dependencies of \\\\(\\\\psi\\\\) (the spatial error).\\nBefore we continue with discussions about operator splitting, let us talk about why one would even want to go this way? The answer is simple: For some of the separate equations for the \\\\(\\\\psi^{(k)}\\\\), we may have ways to solve them more efficiently than if we throw everything together and try to solve it at once. For example, and particularly pertinent in the current case: The equation for \\\\(\\\\psi^{(3)}\\\\), i.e.,          \\n\\\\begin{align*}\\n  \\\\frac{d\\\\psi^{(3)}}{dt}\\n  &=\\n  -i\\\\kappa |\\\\psi^{(3)}|^2 \\\\,\\\\psi^{(3)},\\n  \\\\qquad\\\\qquad\\n  &\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(3)}(t_n) &= \\\\psi(t_n),\\n\\\\end{align*}\\n\\n or equivalently,            \\n\\\\begin{align*}\\n  \\\\psi^{(3)}(t_{n+1})\\n  &=\\n  \\\\psi(t_n)\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  -i\\\\kappa |\\\\psi^{(3)}(t)|^2 \\\\,\\\\psi^{(3)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt,\\n\\\\end{align*}\\n\\n can be solved exactly: the equation is solved by   \\n\\\\begin{align*}\\n  \\\\psi^{(3)}(t) = e^{-i\\\\kappa|\\\\psi(t_n)|^2 (t-t_{n})} \\\\psi(t_n).\\n\\\\end{align*}\\n\\n This is easy to see if (i) you plug this solution into the differential equation, and (ii) realize that the magnitude \\\\(|\\\\psi^{(3)}|\\\\) is constant, i.e., the term \\\\(|\\\\psi(t_n)|^2\\\\) in the exponent is in fact equal to \\\\(|\\\\psi^{(3)}(t)|^2\\\\). In other words, the solution of the ODE for \\\\(\\\\psi^{(3)}(t)\\\\) only changes its phase, but the magnitude of the complex-valued function \\\\(\\\\psi^{(3)}(t)\\\\) remains constant. This makes computing \\\\(I^{(3)}\\\\) particularly convenient: we don't actually need to solve any ODE, we can write the solution down by hand. Using the operator splitting approach, none of the methods to compute \\\\(I^{(1)},I^{(2)}\\\\) therefore have to deal with the nonlinear term and all of the associated unpleasantries: we can get away with solving only linear problems, as long as we allow ourselves the luxury of using an operator splitting approach.\\nSecondly, one often uses operator splitting if the different physical effects described by the different terms have different time scales. Imagine, for example, a case where we really did have some sort of diffusion equation. Diffusion acts slowly, but if \\\\(\\\\kappa\\\\) is large, then the \\\"phase rotation\\\" by the term  \\\\(-i\\\\kappa\\n|\\\\psi^{(3)}(t)|^2 \\\\,\\\\psi^{(3)}(t)\\\\) acts quickly. If we treated everything together, this would imply having to take rather small time steps. But with operator splitting, we can take large time steps \\\\(\\\\Delta t=t_{n+1}-t_{n}\\\\) for the diffusion, and (assuming we didn't have an analytic solution) use an ODE solver with many small time steps to integrate the \\\"phase rotation\\\" equation for \\\\(\\\\psi^{(3)}\\\\) from \\\\(t_n\\\\) to \\\\(t_{n+1}\\\\). In other words, operator splitting allows us to decouple slow and fast time scales and treat them differently, with methods adjusted to each case.\\nOperator splitting: the \\\"Lie splitting\\\" approach\\nWhile the method above allows to compute the three contributions \\\\(I^{(k)}\\\\) in parallel, if we want, the method can be made slightly more accurate and easy to implement if we don't let the trajectories for the \\\\(\\\\psi^{(k)}\\\\) start all at \\\\(\\\\psi(t_n)\\\\), but instead let the trajectory for \\\\(\\\\psi^{(2)}\\\\) start at the end point of the trajectory for \\\\(\\\\psi^{(1)}\\\\), namely \\\\(\\\\psi^{(1)}(t_{n+1})\\\\); similarly, we will start the trajectory for \\\\(\\\\psi^{(3)}\\\\) start at the end point of the trajectory for \\\\(\\\\psi^{(2)}\\\\), namely \\\\(\\\\psi^{(2)}(t_{n+1})\\\\). This method is then called \\\"Lie splitting\\\" and has the same order of error as the method above, i.e., the splitting error is  \\\\({\\\\cal O}(\\\\Delta\\nt)\\\\).\\nThis variation of operator splitting can be written as follows (carefully compare the initial conditions to the ones above):                          \\n\\\\begin{align*}\\n  \\\\frac{d\\\\psi^{(1)}}{dt}\\n  &=\\n  i\\\\frac 12 \\\\Delta \\\\psi^{(1)},\\n  \\\\qquad\\n  &\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(1)}(t_n) &= \\\\psi(t_n),\\n\\\\\\\\\\n  \\\\frac{d\\\\psi^{(2)}}{dt}\\n  &=\\n  -i V \\\\psi^{(2)},\\n  &\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(2)}(t_n) &= \\\\psi^{(1)}(t_{n+1}),\\n\\\\\\\\\\n  \\\\frac{d\\\\psi^{(3)}}{dt}\\n  &=\\n  -i\\\\kappa |\\\\psi^{(3)}|^2 \\\\,\\\\psi^{(3)},\\n  &\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(3)}(t_n) &= \\\\psi^{(2)}(t_{n+1}).\\n\\\\end{align*}\\n\\n (Obviously, while the formulas above imply that we should solve these problems in this particular order, it is equally valid to first solve for trajectory 3, then 2, then 1, or any other permutation.)\\nThe integrated forms of these equations are then                                  \\n\\\\begin{align*}\\n  \\\\psi^{(1)}(t_{n+1})\\n  &=\\n  \\\\psi(t_n)\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  i\\\\frac 12 \\\\Delta \\\\psi^{(1)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt,\\n\\\\\\\\\\n  \\\\psi^{(2)}(t_{n+1})\\n  &=\\n  \\\\psi^{(1)}(t_{n+1})\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  -i V \\\\psi^{(2)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt,\\n\\\\\\\\\\n  \\\\psi^{(3)}(t_{n+1})\\n  &=\\n  \\\\psi^{(2)}(t_{n+1})\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  -i\\\\kappa |\\\\psi^{(3)}(t)|^2 \\\\,\\\\psi^{(3)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt.\\n\\\\end{align*}\\n\\n From a practical perspective, this has the advantage that we need to keep around fewer solution vectors: Once \\\\(\\\\psi^{(1)}(t_n)\\\\) has been computed, we don't need \\\\(\\\\psi(t_n)\\\\) any more; once \\\\(\\\\psi^{(2)}(t_n)\\\\) has been computed, we don't need \\\\(\\\\psi^{(1)}(t_n)\\\\) any more. And once \\\\(\\\\psi^{(3)}(t_n)\\\\) has been computed, we can just call it \\\\(\\\\psi(t_{n+1})\\\\) because, if you insert the first into the second, and then into the third equation, you see that the right hand side of \\\\(\\\\psi^{(3)}(t_n)\\\\) now contains the contributions of all three physical effects:                         \\n\\\\begin{align*}\\n  \\\\psi^{(3)}(t_{n+1})\\n  &=\\n  \\\\psi(t_n)\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  i\\\\frac 12 \\\\Delta \\\\psi^{(1)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt\\n  +\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  -i V \\\\psi^{(2)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt+\\n  \\\\int_{t_n}^{t_{n+1}}\\n  \\\\left(\\n  -i\\\\kappa |\\\\psi^{(3)}(t)|^2 \\\\,\\\\psi^{(3)}(t)\\n  \\\\right)\\n  \\\\;\\n  dt.\\n\\\\end{align*}\\n\\n (Compare this again with the \\\"exact\\\" computation of \\\\(\\\\psi(t_{n+1})\\\\): It only differs in how we approximate \\\\(\\\\psi(t)\\\\) in each of the three integrals.) In other words, Lie splitting is a lot simpler to implement that the original method outlined above because data handling is so much simpler.\\nOperator splitting: the \\\"Strang splitting\\\" approach\\nAs mentioned above, Lie splitting is only \\\\({\\\\cal O}(\\\\Delta t)\\\\) accurate. This is acceptable if we were to use a first order time discretization, for example using the explicit or implicit Euler methods to solve the differential equations for \\\\(\\\\psi^{(k)}\\\\). This is because these time integration methods introduce an error proportional to \\\\(\\\\Delta t\\\\) themselves, and so the splitting error is proportional to an error that we would introduce anyway, and does not diminish the overall convergence order.\\nBut we typically want to use something higher order \\u2013 say, a Crank-Nicolson or BDF2 method \\u2013 since these are often not more expensive than a simple Euler method. It would be a shame if we were to use a time stepping method that is \\\\({\\\\cal O}(\\\\Delta t^2)\\\\), but then lose the accuracy again through the operator splitting.\\nThis is where the Strang splitting method comes in. It is easier to explain if we had only two parts, and so let us combine the effects of the Laplace operator and of the potential into one, and the phase rotation into a second effect. (Indeed, this is what we will do in the code since solving the equation with the Laplace equation with or without the potential costs the same \\u2013 so we merge these two steps.) The Lie splitting method from above would then do the following: It computes solutions of the following two ODEs,                  \\n\\\\begin{align*}\\n  \\\\frac{d\\\\psi^{(1)}}{dt}\\n  &=\\n  i\\\\frac 12 \\\\Delta \\\\psi^{(1)} -i V \\\\psi^{(1)},\\n  \\\\qquad\\n  &\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(1)}(t_n) &= \\\\psi(t_n),\\n\\\\\\\\\\n  \\\\frac{d\\\\psi^{(2)}}{dt}\\n  &=\\n  -i\\\\kappa |\\\\psi^{(2)}|^2 \\\\,\\\\psi^{(2)},\\n  &\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(2)}(t_n) &= \\\\psi^{(1)}(t_{n+1}),\\n\\\\end{align*}\\n\\n and then uses the approximation  \\\\(\\\\psi(t_{n+1}) \\\\approx\\n\\\\psi^{(2)}(t_{n+1})\\\\). In other words, we first make one full time step for physical effect one, then one full time step for physical effect two. The solution at the end of the time step is simply the sum of the increments due to each of these physical effects separately.\\nIn contrast, Gil Strang (one of the titans of numerical analysis starting in the mid-20th century) figured out that it is more accurate to first do one half-step for one physical effect, then a full time step for the other physical effect, and then another half step for the first. Which one is which does not matter, but because it is so simple to do the phase rotation, we will use this effect for the half steps and then only need to do one spatial solve with the Laplace operator plus potential. This operator splitting method is now  \\\\({\\\\cal O}(\\\\Delta\\nt^2)\\\\) accurate. Written in formulas, this yields the following sequence of steps:                          \\n\\\\begin{align*}\\n  \\\\frac{d\\\\psi^{(1)}}{dt}\\n  &=\\n  -i\\\\kappa |\\\\psi^{(1)}|^2 \\\\,\\\\psi^{(1)},\\n  &&\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_n+\\\\tfrac 12\\\\Delta t),\\n  \\\\qquad\\\\qquad&\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(1)}(t_n) &= \\\\psi(t_n),\\n\\\\\\\\\\n  \\\\frac{d\\\\psi^{(2)}}{dt}\\n  &=\\n  i\\\\frac 12 \\\\Delta \\\\psi^{(2)} -i V \\\\psi^{(2)},\\n  \\\\qquad\\n  &&\\n  \\\\text{for}\\\\; t \\\\in (t_n,t_{n+1}),\\n  \\\\qquad\\\\qquad&\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(2)}(t_n) &= \\\\psi^{(1)}(t_n+\\\\tfrac 12\\\\Delta t),\\n\\\\\\\\\\n  \\\\frac{d\\\\psi^{(3)}}{dt}\\n  &=\\n  -i\\\\kappa |\\\\psi^{(3)}|^2 \\\\,\\\\psi^{(3)},\\n  &&\\n  \\\\text{for}\\\\; t \\\\in (t_n+\\\\tfrac 12\\\\Delta t,t_{n+1}),\\n  \\\\qquad\\\\qquad&\\\\text{with initial condition}\\\\;\\n  \\\\psi^{(3)}(t_n) &= \\\\psi^{(2)}(t_{n+1}).\\n\\\\end{align*}\\n\\n As before, the first and third step can be computed exactly for this particular equation, yielding       \\n\\\\begin{align*}\\n  \\\\psi^{(1)}(t_n+\\\\tfrac 12\\\\Delta t) &= e^{-i\\\\kappa|\\\\psi(t_n)|^2 \\\\tfrac\\n  12\\\\Delta t} \\\\; \\\\psi(t_n),\\n  \\\\\\\\\\n  \\\\psi^{(3)}(t_{n+1}) &= e^{-i\\\\kappa|\\\\psi^{(2)}(t_{n+1})|^2 \\\\tfrac\\n  12\\\\Delta t} \\\\; \\\\psi^{(2)}(t_{n+1}).\\n\\\\end{align*}\\n\\nThis is then how we are going to implement things in this program: In each time step, we execute three steps, namely\\nUpdate the solution value at each node by analytically integrating the phase rotation equation by one half time step;\\nSolving the space-time equation that corresponds to the full step for \\\\(\\\\psi^{(2)}\\\\), namely   \\\\(-i\\\\frac{\\\\partial\\\\psi^{(2)}}{\\\\partial t}\\n  -\\n  \\\\frac 12 \\\\Delta \\\\psi^{(2)} + V \\\\psi^{(2)} = 0\\\\), with initial conditions equal to the solution of the first half step above.\\nUpdate the solution value at each node by analytically integrating the phase rotation equation by another half time step.\\n\\nThis structure will be reflected in an obvious way in the main time loop of the program.\\nTime discretization\\nFrom the discussion above, it should have become clear that the only partial differential equation we have to solve in each time step is     \\n\\\\begin{align*}\\n  -i\\\\frac{\\\\partial\\\\psi^{(2)}}{\\\\partial t}\\n  -\\n  \\\\frac 12 \\\\Delta \\\\psi^{(2)} + V \\\\psi^{(2)} = 0.\\n\\\\end{align*}\\n\\n This equation is linear. Furthermore, we only have to solve it from \\\\(t_n\\\\) to \\\\(t_{n+1}\\\\), i.e., for exactly one time step.\\nTo do this, we will apply the second order accurate Crank-Nicolson scheme that we have already used in some of the other time dependent codes (specifically: step-23 and step-26). It reads as follows:        \\n\\\\begin{align*}\\n  -i\\\\frac{\\\\psi^{(n,2)}-\\\\psi^{(n,1)}}{k_{n+1}}\\n  -\\n  \\\\frac 12 \\\\Delta \\\\left[\\\\frac 12\\n  \\\\left(\\\\psi^{(n,2)}+\\\\psi^{(n,1)}\\\\right)\\\\right]\\n  +\\n  V \\\\left[\\\\frac 12 \\\\left(\\\\psi^{(n,2)}+\\\\psi^{(n,1)}\\\\right)\\\\right] = 0.\\n\\\\end{align*}\\n\\n Here, the \\\"previous\\\" solution \\\\(\\\\psi^{(n,1)}\\\\) (or the \\\"initial\\ncondition\\\" for this part of the time step) is the output of the first phase rotation half-step; the output of the current step will be denoted by \\\\(\\\\psi^{(n,2)}\\\\). \\\\(k_{n+1}=t_{n+1}-t_n\\\\) is the length of the time step. (One could argue whether \\\\(\\\\psi^{(n,1)}\\\\) and \\\\(\\\\psi^{(n,1)}\\\\) live at time step \\\\(n\\\\) or \\\\(n+1\\\\) and what their upper indices should be. This is a philosophical discussion without practical impact, and one might think of \\\\(\\\\psi^{(n,1)}\\\\) as something like \\\\(\\\\psi^{(n+\\\\tfrac 13)}\\\\), and \\\\(\\\\psi^{(n,2)}\\\\) as \\\\(\\\\psi^{(n+\\\\tfrac 23)}\\\\) if that helps clarify things \\u2013 though, again \\\\(n+\\\\frac 13\\\\) is not to be understood as \\\"one third time step after\\n \\\\_form#339\\\" but more like \\\"we've already done one third of the work necessary\\nfor time step \\\\_form#3447\\\".)\\nIf we multiply the whole equation with \\\\(k_{n+1}\\\\) and sort terms with the unknown \\\\(\\\\psi^{(n+1,2)}\\\\) to the left and those with the known \\\\(\\\\psi^{(n,2)}\\\\) to the right, then we obtain the following (spatial) partial differential equation that needs to be solved in each time step:             \\n\\\\begin{align*}\\n  -i\\\\psi^{(n,2)}\\n  -\\n  \\\\frac 14 k_{n+1} \\\\Delta \\\\psi^{(n,2)}\\n  +\\n  \\\\frac 12 k_{n+1} V \\\\psi^{(n,2)}\\n  =\\n  -i\\\\psi^{(n,1)}\\n  +\\n  \\\\frac 14 k_{n+1} \\\\Delta \\\\psi^{(n,1)}\\n  -\\n  \\\\frac 12 k_{n+1} V \\\\psi^{(n,1)}.\\n\\\\end{align*}\\n\\nSpatial discretization and dealing with complex variables\\nAs mentioned above, the previous tutorial program dealing with complex-valued solutions (namely, step-29) separated real and imaginary parts of the solution. It thus reduced everything to real arithmetic. In contrast, we here want to keep things complex-valued.\\nThe first part of this is that we need to define the discretized solution as  \\\\(\\\\psi_h^n(\\\\mathbf x)=\\\\sum_j \\\\Psi^n_j \\\\varphi_j(\\\\mathbf\\nx) \\\\approx \\\\psi(\\\\mathbf x,t_n)\\\\) where the \\\\(\\\\varphi_j\\\\) are the usual shape functions (which are real valued) but the expansion coefficients \\\\(\\\\Psi^n_j\\\\) at time step \\\\(n\\\\) are now complex-valued. This is easily done in deal.II: We just have to use Vector<std::complex<double>> instead of Vector<double> to store these coefficients.\\nOf more interest is how to build and solve the linear system. Obviously, this will only be necessary for the second step of the Strang splitting discussed above, with the time discretization of the previous subsection. We obtain the fully discrete version through straightforward substitution of \\\\(\\\\psi^n\\\\) by \\\\(\\\\psi^n_h\\\\) and multiplication by a test function:             \\n\\\\begin{align*}\\n  -iM\\\\Psi^{(n,2)}\\n  +\\n  \\\\frac 14 k_{n+1} A \\\\Psi^{(n,2)}\\n  +\\n  \\\\frac 12 k_{n+1} W \\\\Psi^{(n,2)}\\n  =\\n  -iM\\\\Psi^{(n+1,1)}\\n  -\\n  \\\\frac 14 k_{n+1} A \\\\Psi^{(n,1)}\\n  -\\n  \\\\frac 12 k_{n+1} W \\\\Psi^{(n,1)},\\n\\\\end{align*}\\n\\n or written in a more compact way:                 \\n\\\\begin{align*}\\n  \\\\left[\\n    -iM\\n    +\\n    \\\\frac 14 k_{n+1} A\\n    +\\n    \\\\frac 12 k_{n+1} W\\n  \\\\right] \\\\Psi^{(n,2)}\\n  =\\n  \\\\left[\\n    -iM\\n    -\\n    \\\\frac 14 k_{n+1} A\\n    -\\n   \\\\frac 12 k_{n+1} W\\n  \\\\right] \\\\Psi^{(n,1)}.\\n\\\\end{align*}\\n\\n Here, the matrices are defined in their obvious ways:     \\n\\\\begin{align*}\\n  M_{ij} &= (\\\\varphi_i,\\\\varphi_j), \\\\\\\\\\n  A_{ij} &= (\\\\nabla\\\\varphi_i,\\\\nabla\\\\varphi_j), \\\\\\\\\\n  W_{ij} &= (\\\\varphi_i,V \\\\varphi_j).\\n\\\\end{align*}\\n\\n Note that all matrices individually are in fact symmetric, real-valued, and at least positive semidefinite, though the same is obviously not true for the system matrix \\\\(C = -iM + \\\\frac 14 k_{n+1} A + \\\\frac 12 k_{n+1} W\\\\) and the corresponding matrix \\\\(R = -iM - \\\\frac 14 k_{n+1} A - \\\\frac 12 k_{n+1} W\\\\) on the right hand side.\\nLinear solvers\\nNoteThe material presented here is also discussed in video lecture 34. (All video lectures are also available here.)\\nThe only remaining important question about the solution procedure is how to solve the complex-valued linear system     \\n\\\\begin{align*}\\n  C \\\\Psi^{(n+1,2)}\\n  =\\n  R \\\\Psi^{(n+1,1)},\\n\\\\end{align*}\\n\\n with the matrix  \\\\(C = -iM + \\\\frac 14 k_{n+1} A + \\\\frac 12 k_{n+1}\\nW\\\\) and a right hand side that is easily computed as the product of a known matrix and the previous part-step's solution. As usual, this comes down to the question of what properties the matrix \\\\(C\\\\) has. If it is symmetric and positive definite, then we can for example use the Conjugate Gradient method.\\nUnfortunately, the matrix's only useful property is that it is complex symmetric, i.e., \\\\(C_{ij}=C_{ji}\\\\), as is easy to see by recalling that \\\\(M,A,W\\\\) are all symmetric. It is not, however, Hermitian, which would require that \\\\(C_{ij}=\\\\bar C_{ji}\\\\) where the bar indicates complex conjugation.\\nComplex symmetry can be exploited for iterative solvers as a quick literature search indicates. We will here not try to become too sophisticated (and indeed leave this to the Possibilities for extensions section below) and instead simply go with the good old standby for problems without properties: A direct solver. That's not optimal, especially for large problems, but it shall suffice for the purposes of a tutorial program. Fortunately, the SparseDirectUMFPACK class allows solving complex-valued problems.\\nDefinition of the test case\\nInitial conditions for the NLSE are typically chosen to represent particular physical situations. This is beyond the scope of this program, but suffice it to say that these initial conditions are (i) often superpositions of the wave functions of particles located at different points, and that (ii) because \\\\(|\\\\psi(\\\\mathbf x,t)|^2\\\\) corresponds to a particle density function, the integral   \\n\\\\[\\n  N(t) = \\\\int_\\\\Omega |\\\\psi(\\\\mathbf x,t)|^2\\n\\\\]\\n\\n corresponds to the number of particles in the system. (Clearly, if one were to be physically correct, \\\\(N(t)\\\\) better be a constant if the system is closed, or \\\\(\\\\frac{dN}{dt}<0\\\\) if one has absorbing boundary conditions.) The important point is that one should choose initial conditions so that   \\n\\\\[\\n  N(0) = \\\\int_\\\\Omega |\\\\psi_0(\\\\mathbf x)|^2\\n\\\\]\\n\\n makes sense.\\nWhat we will use here, primarily because it makes for good graphics, is the following:   \\n\\\\[\\n  \\\\psi_0(\\\\mathbf x) = \\\\sqrt{\\\\sum_{k=1}^4 \\\\alpha_k e^{-\\\\frac{r_k^2}{R^2}}},\\n\\\\]\\n\\n where \\\\(r_k = |\\\\mathbf x-\\\\mathbf x_k|\\\\) is the distance from the (fixed) locations \\\\(\\\\mathbf x_k\\\\), and \\\\(\\\\alpha_k\\\\) are chosen so that each of the Gaussians that we are adding up adds an integer number of particles to \\\\(N(0)\\\\). We achieve this by making sure that   \\n\\\\[\\n  \\\\int_\\\\Omega \\\\alpha_k e^{-\\\\frac{r_k^2}{R^2}}\\n\\\\]\\n\\n is a positive integer. In other words, we need to choose \\\\(\\\\alpha\\\\) as an integer multiple of     \\n\\\\[\\n  \\\\left(\\\\int_\\\\Omega e^{-\\\\frac{r_k^2}{R^2}}\\\\right)^{-1}\\n  =\\n  \\\\left(R^d\\\\sqrt{\\\\pi^d}\\\\right)^{-1},\\n\\\\]\\n\\n assuming for the moment that \\\\(\\\\Omega={\\\\mathbb R}^d\\\\) \\u2013 which is of course not the case, but we'll ignore the small difference in integral.\\nThus, we choose \\\\(\\\\alpha_k=\\\\left(R^d\\\\sqrt{\\\\pi^d}\\\\right)^{-1}\\\\) for all, and \\\\(R=0.1\\\\). This \\\\(R\\\\) is small enough that the difference between the exact (infinite) integral and the integral over \\\\(\\\\Omega\\\\) should not be too concerning. We choose the four points \\\\(\\\\mathbf x_k\\\\) as  \\\\((\\\\pm 0.3, 0), (0, \\\\pm\\n0.3)\\\\) \\u2013 also far enough away from the boundary of \\\\(\\\\Omega\\\\) to keep ourselves on the safe side.\\nFor simplicity, we pose the problem on the square \\\\([-1,1]^2\\\\). For boundary conditions, we will use time-independent Neumann conditions of the form   \\n\\\\[\\n  \\\\nabla\\\\psi(\\\\mathbf x,t)\\\\cdot \\\\mathbf n=0 \\\\qquad\\\\qquad \\\\forall \\\\mathbf x\\\\in\\\\partial\\\\Omega.\\n\\\\]\\n\\n This is not a realistic choice of boundary conditions but sufficient for what we want to demonstrate here. We will comment further on this in the Possibilities for extensions section below.\\nFinally, we choose \\\\(\\\\kappa=1\\\\), and the potential as        \\n\\\\[\\n  V(\\\\mathbf x)\\n  =\\n  \\\\begin{cases} 0 & \\\\text{if}\\\\; |\\\\mathbf x|<0.7\\n                \\\\\\\\\\n                1000 & \\\\text{otherwise}.\\n  \\\\end{cases}\\n\\\\]\\n\\n Using a large potential makes sure that the wave function \\\\(\\\\psi\\\\) remains small outside the circle of radius 0.7. All of the Gaussians that make up the initial conditions are within this circle, and the solution will mostly oscillate within it, with a small amount of energy radiating into the outside. The use of a large potential also makes sure that the nonphysical boundary condition does not have too large an effect.\\n The commented program\\n Include files\\nThe program starts with the usual include files, all of which you should have seen before by now:\\n\\u00a0 #include <deal.II/lac/vector.h>\\n\\u00a0 #include <deal.II/lac/full_matrix.h>\\n\\u00a0 #include <deal.II/lac/dynamic_sparsity_pattern.h>\\n\\u00a0 #include <deal.II/lac/sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/block_sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/block_vector.h>\\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/lac/sparse_direct.h>\\n\\u00a0 #include <deal.II/grid/tria.h>\\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/grid_refinement.h>\\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 #include <deal.II/fe/fe_values.h>\\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 #include <deal.II/numerics/error_estimator.h>\\n\\u00a0 \\n\\u00a0 #include <fstream>\\n\\u00a0 #include <iostream>\\n\\u00a0 \\n\\u00a0 \\nThen the usual placing of all content of this program into a namespace and the importation of the deal.II namespace into the one we will work in:\\n\\u00a0 namespace Step58\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n The NonlinearSchroedingerEquation class\\nThen the main class. It looks very much like the corresponding classes in step-4 or step-6, with the only exception that the matrices and vectors and everything else related to the linear system are now storing elements of type std::complex<double> instead of just double.\\n\\u00a0   template <int dim>\\n\\u00a0   class NonlinearSchroedingerEquation\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     NonlinearSchroedingerEquation();\\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void setup_system();\\n\\u00a0     void assemble_matrices();\\n\\u00a0     void do_half_phase_step();\\n\\u00a0     void do_full_spatial_step();\\n\\u00a0     void output_results() const;\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     Triangulation<dim> triangulation;\\n\\u00a0     const FE_Q<dim>    fe;\\n\\u00a0     DoFHandler<dim>    dof_handler;\\n\\u00a0 \\n\\u00a0     AffineConstraints<std::complex<double>> constraints;\\n\\u00a0 \\n\\u00a0     SparsityPattern                    sparsity_pattern;\\n\\u00a0     SparseMatrix<std::complex<double>> system_matrix;\\n\\u00a0     SparseMatrix<std::complex<double>> rhs_matrix;\\n\\u00a0 \\n\\u00a0     Vector<std::complex<double>> solution;\\n\\u00a0     Vector<std::complex<double>> system_rhs;\\n\\u00a0 \\n\\u00a0     double       time;\\n\\u00a0     double       time_step;\\n\\u00a0     unsigned int timestep_number;\\n\\u00a0 \\n\\u00a0     double kappa;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nDoFHandlerDefinition dof_handler.h:317\\nFE_QDefinition fe_q.h:554\\nSparseMatrixDefinition sparse_matrix.h:520\\nSparsityPatternDefinition sparsity_pattern.h:343\\nTriangulationDefinition tria.h:1323\\nVectorDefinition vector.h:120\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\n Equation data\\nBefore we go on filling in the details of the main class, let us define the equation data corresponding to the problem, i.e. initial values, as well as a right hand side class. (We will reuse the initial conditions also for the boundary values, which we simply keep constant.) We do so using classes derived from the Function class template that has been used many times before, so the following should not look surprising. The only point of interest is that we here have a complex-valued problem, so we have to provide the second template argument of the Function class (which would otherwise default to double). Furthermore, the return type of the value() functions is then of course also complex.\\nWhat precisely these functions return has been discussed at the end of the Introduction section.\\n\\u00a0   template <int dim>\\n\\u00a0   class InitialValues : public Function<dim, std::complex<double>>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     InitialValues()\\n\\u00a0       : Function<dim, std::complex<double>>(1)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0     virtual std::complex<double>\\n\\u00a0     value(const Point<dim> &p, const unsigned int component = 0) const override;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   std::complex<double>\\n\\u00a0   InitialValues<dim>::value(const Point<dim>  &p,\\n\\u00a0                             const unsigned int component) const\\n\\u00a0   {\\n\\u00a0     static_assert(dim == 2, \\\"This initial condition only works in 2d.\\\");\\n\\u00a0 \\n\\u00a0     (void)component;\\n\\u00a0     Assert(component == 0, ExcIndexRange(component, 0, 1));\\n\\u00a0 \\n\\u00a0     const std::vector<Point<dim>> vortex_centers = {{0, -0.3},\\n\\u00a0                                                     {0, +0.3},\\n\\u00a0                                                     {+0.3, 0},\\n\\u00a0                                                     {-0.3, 0}};\\n\\u00a0 \\n\\u00a0     const double R = 0.1;\\n\\u00a0     const double alpha =\\n\\u00a0       1. / (std::pow(R, dim) * std::pow(numbers::PI, dim / 2.));\\n\\u00a0 \\n\\u00a0     double sum = 0;\\n\\u00a0     for (const auto &vortex_center : vortex_centers)\\n\\u00a0       {\\n\\u00a0         const Tensor<1, dim> distance = p - vortex_center;\\n\\u00a0         const double         r        = distance.norm();\\n\\u00a0 \\n\\u00a0         sum += alpha * std::exp(-(r * r) / (R * R));\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     return {std::sqrt(sum), 0.};\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   class Potential : public Function<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     Potential() = default;\\n\\u00a0     virtual double value(const Point<dim>  &p,\\n\\u00a0                          const unsigned int component = 0) const override;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   double Potential<dim>::value(const Point<dim>  &p,\\n\\u00a0                                const unsigned int component) const\\n\\u00a0   {\\n\\u00a0     (void)component;\\n\\u00a0     Assert(component == 0, ExcIndexRange(component, 0, 1));\\n\\u00a0 \\n\\u00a0     return (Point<dim>().distance(p) > 0.7 ? 1000 : 0);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFunctionDefinition function.h:152\\nFunction::valuevirtual RangeNumberType value(const Point< dim > &p, const unsigned int component=0) const\\nPointDefinition point.h:111\\nTensorDefinition tensor.h:471\\nTensor::normnumbers::NumberTraits< Number >::real_type norm() const\\nAssert#define Assert(cond, exc)Definition exceptions.h:1638\\nUtilities::MPI::sumT sum(const T &t, const MPI_Comm mpi_communicator)\\nnumbers::PIstatic constexpr double PIDefinition numbers.h:259\\nstdSTL namespace.\\nstd::exp::VectorizedArray< Number, width > exp(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6829\\nstd::sqrt::VectorizedArray< Number, width > sqrt(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6869\\nstd::pow::VectorizedArray< Number, width > pow(const ::VectorizedArray< Number, width > &, const Number p)Definition vectorization.h:6885\\n Implementation of the NonlinearSchroedingerEquation class\\nWe start by specifying the implementation of the constructor of the class. There is nothing of surprise to see here except perhaps that we choose quadratic ( \\\\(Q_2\\\\)) Lagrange elements \\u2013 the solution is expected to be smooth, so we choose a higher polynomial degree than the bare minimum.\\n\\u00a0   template <int dim>\\n\\u00a0   NonlinearSchroedingerEquation<dim>::NonlinearSchroedingerEquation()\\n\\u00a0     : fe(2)\\n\\u00a0     , dof_handler(triangulation)\\n\\u00a0     , time(0)\\n\\u00a0     , time_step(1. / 128)\\n\\u00a0     , timestep_number(0)\\n\\u00a0     , kappa(1)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\n Setting up data structures and assembling matrices\\nThe next function is the one that sets up the mesh, DoFHandler, and matrices and vectors at the beginning of the program, i.e. before the first time step. The first few lines are pretty much standard if you've read through the tutorial programs at least up to step-6:\\n\\u00a0   template <int dim>\\n\\u00a0   void NonlinearSchroedingerEquation<dim>::setup_system()\\n\\u00a0   {\\n\\u00a0     GridGenerator::hyper_cube(triangulation, -1, 1);\\n\\u00a0     triangulation.refine_global(6);\\n\\u00a0 \\n\\u00a0     std::cout << \\\"Number of active cells: \\\" << triangulation.n_active_cells()\\n\\u00a0               << std::endl;\\n\\u00a0 \\n\\u00a0     dof_handler.distribute_dofs(fe);\\n\\u00a0 \\n\\u00a0     std::cout << \\\"Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n\\u00a0               << std::endl\\n\\u00a0               << std::endl;\\n\\u00a0 \\n\\u00a0     DynamicSparsityPattern dsp(dof_handler.n_dofs(), dof_handler.n_dofs());\\n\\u00a0     DoFTools::make_sparsity_pattern(dof_handler, dsp);\\n\\u00a0     sparsity_pattern.copy_from(dsp);\\n\\u00a0 \\n\\u00a0     system_matrix.reinit(sparsity_pattern);\\n\\u00a0     rhs_matrix.reinit(sparsity_pattern);\\n\\u00a0 \\n\\u00a0     solution.reinit(dof_handler.n_dofs());\\n\\u00a0     system_rhs.reinit(dof_handler.n_dofs());\\n\\u00a0 \\n\\u00a0     constraints.close();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nDynamicSparsityPatternDefinition dynamic_sparsity_pattern.h:322\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nDoFTools::make_sparsity_patternvoid make_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, SparsityPatternBase &sparsity_pattern, const AffineConstraints< number > &constraints={}, const bool keep_constrained_dofs=true, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id)Definition dof_tools_sparsity.cc:56\\nGridGenerator::hyper_cubevoid hyper_cube(Triangulation< dim, spacedim > &tria, const double left=0., const double right=1., const bool colorize=false)\\nNext, we assemble the relevant matrices. The way we have written the Crank-Nicolson discretization of the spatial step of the Strang splitting (i.e., the second of the three partial steps in each time step), we were led to the linear system     \\\\(\\\\left[ -iM  +  \\\\frac 14 k_{n+1} A + \\\\frac 12 k_{n+1} W \\\\right]\\n   \\\\Psi^{(n,2)}\\n   =\\n   \\\\left[ -iM  -  \\\\frac 14 k_{n+1} A - \\\\frac 12 k_{n+1} W \\\\right]\\n   \\\\Psi^{(n,1)}\\\\). In other words, there are two matrices in play here \\u2013 one for the left and one for the right hand side. We build these matrices separately. (One could avoid building the right hand side matrix and instead just form the action of the matrix on \\\\(\\\\Psi^{(n,1)}\\\\) in each time step. This may or may not be more efficient, but efficiency is not foremost on our minds for this program.)\\n\\u00a0   template <int dim>\\n\\u00a0   void NonlinearSchroedingerEquation<dim>::assemble_matrices()\\n\\u00a0   {\\n\\u00a0     const QGauss<dim> quadrature_formula(fe.degree + 1);\\n\\u00a0 \\n\\u00a0     FEValues<dim> fe_values(fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_values | update_gradients |\\n\\u00a0                               update_quadrature_points | update_JxW_values);\\n\\u00a0 \\n\\u00a0     const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     FullMatrix<std::complex<double>> cell_matrix_lhs(dofs_per_cell,\\n\\u00a0                                                      dofs_per_cell);\\n\\u00a0     FullMatrix<std::complex<double>> cell_matrix_rhs(dofs_per_cell,\\n\\u00a0                                                      dofs_per_cell);\\n\\u00a0 \\n\\u00a0     std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0     std::vector<double>                  potential_values(n_q_points);\\n\\u00a0     const Potential<dim>                 potential;\\n\\u00a0 \\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       {\\n\\u00a0         cell_matrix_lhs = std::complex<double>(0.);\\n\\u00a0         cell_matrix_rhs = std::complex<double>(0.);\\n\\u00a0 \\n\\u00a0         fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0         potential.value_list(fe_values.get_quadrature_points(),\\n\\u00a0                              potential_values);\\n\\u00a0 \\n\\u00a0         for (unsigned int q_index = 0; q_index < n_q_points; ++q_index)\\n\\u00a0           {\\n\\u00a0             for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n\\u00a0               {\\n\\u00a0                 for (unsigned int l = 0; l < dofs_per_cell; ++l)\\n\\u00a0                   {\\n\\u00a0                     const std::complex<double> i = {0, 1};\\n\\u00a0 \\n\\u00a0                     cell_matrix_lhs(k, l) +=\\n\\u00a0                       (-i * fe_values.shape_value(k, q_index) *\\n\\u00a0                          fe_values.shape_value(l, q_index) +\\n\\u00a0                        time_step / 4 * fe_values.shape_grad(k, q_index) *\\n\\u00a0                          fe_values.shape_grad(l, q_index) +\\n\\u00a0                        time_step / 2 * potential_values[q_index] *\\n\\u00a0                          fe_values.shape_value(k, q_index) *\\n\\u00a0                          fe_values.shape_value(l, q_index)) *\\n\\u00a0                       fe_values.JxW(q_index);\\n\\u00a0 \\n\\u00a0                     cell_matrix_rhs(k, l) +=\\n\\u00a0                       (-i * fe_values.shape_value(k, q_index) *\\n\\u00a0                          fe_values.shape_value(l, q_index) -\\n\\u00a0                        time_step / 4 * fe_values.shape_grad(k, q_index) *\\n\\u00a0                          fe_values.shape_grad(l, q_index) -\\n\\u00a0                        time_step / 2 * potential_values[q_index] *\\n\\u00a0                          fe_values.shape_value(k, q_index) *\\n\\u00a0                          fe_values.shape_value(l, q_index)) *\\n\\u00a0                       fe_values.JxW(q_index);\\n\\u00a0                   }\\n\\u00a0               }\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         cell->get_dof_indices(local_dof_indices);\\n\\u00a0         constraints.distribute_local_to_global(cell_matrix_lhs,\\n\\u00a0                                                local_dof_indices,\\n\\u00a0                                                system_matrix);\\n\\u00a0         constraints.distribute_local_to_global(cell_matrix_rhs,\\n\\u00a0                                                local_dof_indices,\\n\\u00a0                                                rhs_matrix);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nFEValuesDefinition fe_values.h:63\\nFullMatrixDefinition full_matrix.h:79\\nQGaussDefinition quadrature_lib.h:40\\nupdate_values@ update_valuesShape function values.Definition fe_update_flags.h:75\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\n Implementing the Strang splitting steps\\nHaving set up all data structures above, we are now in a position to implement the partial steps that form the Strang splitting scheme. We start with the half-step to advance the phase, and that is used as the first and last part of each time step.\\nTo this end, recall that for the first half step, we needed to compute  \\\\(\\\\psi^{(n,1)} = e^{-i\\\\kappa|\\\\psi^{(n,0)}|^2 \\\\tfrac\\n   12\\\\Delta t} \\\\; \\\\psi^{(n,0)}\\\\). Here, \\\\(\\\\psi^{(n,0)}=\\\\psi^{(n)}\\\\) and \\\\(\\\\psi^{(n,1)}\\\\) are functions of space and correspond to the output of the previous complete time step and the result of the first of the three part steps, respectively. A corresponding solution must be computed for the third of the part steps, i.e.  \\\\(\\\\psi^{(n,3)} = e^{-i\\\\kappa|\\\\psi^{(n,2)}|^2 \\\\tfrac\\n   12\\\\Delta t} \\\\; \\\\psi^{(n,2)}\\\\), where \\\\(\\\\psi^{(n,3)}=\\\\psi^{(n+1)}\\\\) is the result of the time step as a whole, and its input \\\\(\\\\psi^{(n,2)}\\\\) is the result of the spatial step of the Strang splitting.\\nAn important realization is that while \\\\(\\\\psi^{(n,0)}(\\\\mathbf x)\\\\) may be a finite element function (i.e., is piecewise polynomial), this may not necessarily be the case for the \\\"rotated\\\" function in which we have updated the phase using the exponential factor (recall that the amplitude of that function remains constant as part of that step). In other words, we could compute \\\\(\\\\psi^{(n,1)}(\\\\mathbf x)\\\\) at every point \\\\(\\\\mathbf x\\\\in\\\\Omega\\\\), but we can't represent it on a mesh because it is not a piecewise polynomial function. The best we can do in a discrete setting is to compute a projection or interpolation. In other words, we can compute   \\\\(\\\\psi_h^{(n,1)}(\\\\mathbf x) = \\\\Pi_h\\n   \\\\left(e^{-i\\\\kappa|\\\\psi_h^{(n,0)}(\\\\mathbf x)|^2 \\\\tfrac 12\\\\Delta t}\\n   \\\\; \\\\psi_h^{(n,0)}(\\\\mathbf x) \\\\right)\\\\) where \\\\(\\\\Pi_h\\\\) is a projection or interpolation operator. The situation is particularly simple if we choose the interpolation: Then, all we need to compute is the value of the right hand side at the node points and use these as nodal values for the vector \\\\(\\\\Psi^{(n,1)}\\\\) of degrees of freedom. This is easily done because evaluating the right hand side at node points for a Lagrange finite element as used here requires us to only look at a single (complex-valued) entry of the node vector. In other words, what we need to do is to compute  \\\\(\\\\Psi^{(n,1)}_j = e^{-i\\\\kappa|\\\\Psi^{(n,0)}_j|^2 \\\\tfrac\\n   12\\\\Delta t} \\\\; \\\\Psi^{(n,0)}_j\\\\) where \\\\(j\\\\) loops over all of the entries of our solution vector. This is what the function below does \\u2013 in fact, it doesn't even use separate vectors for \\\\(\\\\Psi^{(n,0)}\\\\) and \\\\(\\\\Psi^{(n,1)}\\\\), but just updates the same vector as appropriate.\\n\\u00a0   template <int dim>\\n\\u00a0   void NonlinearSchroedingerEquation<dim>::do_half_phase_step()\\n\\u00a0   {\\n\\u00a0     for (auto &value : solution)\\n\\u00a0       {\\n\\u00a0         const std::complex<double> i         = {0, 1};\\n\\u00a0         const double               magnitude = std::abs(value);\\n\\u00a0 \\n\\u00a0         value = std::exp(-i * kappa * magnitude * magnitude * (time_step / 2)) *\\n\\u00a0                 value;\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nstd::abs::VectorizedArray< Number, width > abs(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6927\\nThe next step is to solve for the linear system in each time step, i.e., the second half step of the Strang splitting we use. Recall that it had the form \\\\(C\\\\Psi^{(n,2)} = R\\\\Psi^{(n,1)}\\\\) where \\\\(C\\\\) and \\\\(R\\\\) are the matrices we assembled earlier.\\nThe way we solve this here is using a direct solver. We first form the right hand side \\\\(r=R\\\\Psi^{(n,1)}\\\\) using the SparseMatrix::vmult() function and put the result into the system_rhs variable. We then call SparseDirectUMFPACK::solver() which takes as argument the matrix \\\\(C\\\\) and the right hand side vector and returns the solution in the same vector system_rhs. The final step is then to put the solution so computed back into the solution variable.\\n\\u00a0   template <int dim>\\n\\u00a0   void NonlinearSchroedingerEquation<dim>::do_full_spatial_step()\\n\\u00a0   {\\n\\u00a0     rhs_matrix.vmult(system_rhs, solution);\\n\\u00a0 \\n\\u00a0     SparseDirectUMFPACK direct_solver;\\n\\u00a0     direct_solver.solve(system_matrix, system_rhs);\\n\\u00a0 \\n\\u00a0     solution = system_rhs;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nSparseDirectUMFPACKDefinition sparse_direct.h:92\\nSparseDirectUMFPACK::solvevoid solve(Vector< double > &rhs_and_solution, const bool transpose=false) constDefinition sparse_direct.cc:377\\n Creating graphical output\\nThe last of the helper functions and classes we ought to discuss are the ones that create graphical output. The result of running the half and full steps for the local and spatial parts of the Strang splitting is that we have updated the solution vector \\\\(\\\\Psi^n\\\\) to the correct value at the end of each time step. Its entries contain complex numbers for the solution at the nodes of the finite element mesh.\\nComplex numbers are not easily visualized. We can output their real and imaginary parts, i.e., the fields \\\\(\\\\text{Re}(\\\\psi_h^{(n)}(\\\\mathbf x))\\\\) and \\\\(\\\\text{Im}(\\\\psi_h^{(n)}(\\\\mathbf x))\\\\), and that is exactly what the DataOut class does when one attaches as complex-valued vector via DataOut::add_data_vector() and then calls DataOut::build_patches(). That is indeed what we do below.\\nBut oftentimes we are not particularly interested in real and imaginary parts of the solution vector, but instead in derived quantities such as the magnitude \\\\(|\\\\psi|\\\\) and phase angle \\\\(\\\\text{arg}(\\\\psi)\\\\) of the solution. In the context of quantum systems such as here, the magnitude itself is not so interesting, but instead it is the \\\"amplitude\\\", \\\\(|\\\\psi|^2\\\\) that is a physical property: it corresponds to the probability density of finding a particle in a particular place of state. The way to put computed quantities into output files for visualization \\u2013 as used in numerous previous tutorial programs \\u2013 is to use the facilities of the DataPostprocessor and derived classes. Specifically, both the amplitude of a complex number and its phase angles are scalar quantities, and so the DataPostprocessorScalar class is the right tool to base what we want to do on.\\nConsequently, what we do here is to implement two classes ComplexAmplitude and ComplexPhase that compute for each point at which DataOut decides to generate output, the amplitudes \\\\(|\\\\psi_h|^2\\\\) and phases \\\\(\\\\text{arg}(\\\\psi_h)\\\\) of the solution for visualization. There is a fair amount of boiler-plate code below, with the only interesting parts of the first of these two classes being how its evaluate_vector_field() function computes the computed_quantities object.\\n(There is also the rather awkward fact that the std::norm() function does not compute what one would naively imagine, namely \\\\(|\\\\psi|\\\\), but returns \\\\(|\\\\psi|^2\\\\) instead. It's certainly quite confusing to have a standard function mis-named in such a way...)\\n\\u00a0   namespace DataPostprocessors\\n\\u00a0   {\\n\\u00a0     template <int dim>\\n\\u00a0     class ComplexAmplitude : public DataPostprocessorScalar<dim>\\n\\u00a0     {\\n\\u00a0     public:\\n\\u00a0       ComplexAmplitude();\\n\\u00a0 \\n\\u00a0       virtual void evaluate_vector_field(\\n\\u00a0         const DataPostprocessorInputs::Vector<dim> &inputs,\\n\\u00a0         std::vector<Vector<double>> &computed_quantities) const override;\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <int dim>\\n\\u00a0     ComplexAmplitude<dim>::ComplexAmplitude()\\n\\u00a0       : DataPostprocessorScalar<dim>(\\\"Amplitude\\\", update_values)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <int dim>\\n\\u00a0     void ComplexAmplitude<dim>::evaluate_vector_field(\\n\\u00a0       const DataPostprocessorInputs::Vector<dim> &inputs,\\n\\u00a0       std::vector<Vector<double>>                &computed_quantities) const\\n\\u00a0     {\\n\\u00a0       AssertDimension(computed_quantities.size(),\\n\\u00a0                       inputs.solution_values.size());\\n\\u00a0 \\n\\u00a0       for (unsigned int p = 0; p < computed_quantities.size(); ++p)\\n\\u00a0         {\\n\\u00a0           AssertDimension(computed_quantities[p].size(), 1);\\n\\u00a0           AssertDimension(inputs.solution_values[p].size(), 2);\\n\\u00a0 \\n\\u00a0           const std::complex<double> psi(inputs.solution_values[p](0),\\n\\u00a0                                          inputs.solution_values[p](1));\\n\\u00a0           computed_quantities[p](0) = std::norm(psi);\\n\\u00a0         }\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nDataPostprocessorScalarDefinition data_postprocessor.h:716\\nAssertDimension#define AssertDimension(dim1, dim2)Definition exceptions.h:1985\\nDataPostprocessorsDefinition data_postprocessor.h:1315\\nDataPostprocessorInputs::VectorDefinition data_postprocessor.h:400\\nThe second of these postprocessor classes computes the phase angle of the complex-valued solution at each point. In other words, if we represent \\\\(\\\\psi(\\\\mathbf x,t)=r(\\\\mathbf x,t) e^{i\\\\varphi(\\\\mathbf x,t)}\\\\), then this class computes \\\\(\\\\varphi(\\\\mathbf x,t)\\\\). The function std::arg does this for us, and returns the angle as a real number between \\\\(-\\\\pi\\\\) and \\\\(+\\\\pi\\\\).\\nFor reasons that we will explain in detail in the results section, we do not actually output this value at each location where output is generated. Rather, we take the maximum over all evaluation points of the phase and then fill each evaluation point's output field with this maximum \\u2013 in essence, we output the phase angle as a piecewise constant field, where each cell has its own constant value. The reasons for this will become clear once you read through the discussion further down below.\\n\\u00a0     template <int dim>\\n\\u00a0     class ComplexPhase : public DataPostprocessorScalar<dim>\\n\\u00a0     {\\n\\u00a0     public:\\n\\u00a0       ComplexPhase();\\n\\u00a0 \\n\\u00a0       virtual void evaluate_vector_field(\\n\\u00a0         const DataPostprocessorInputs::Vector<dim> &inputs,\\n\\u00a0         std::vector<Vector<double>> &computed_quantities) const override;\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <int dim>\\n\\u00a0     ComplexPhase<dim>::ComplexPhase()\\n\\u00a0       : DataPostprocessorScalar<dim>(\\\"Phase\\\", update_values)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     template <int dim>\\n\\u00a0     void ComplexPhase<dim>::evaluate_vector_field(\\n\\u00a0       const DataPostprocessorInputs::Vector<dim> &inputs,\\n\\u00a0       std::vector<Vector<double>>                &computed_quantities) const\\n\\u00a0     {\\n\\u00a0       AssertDimension(computed_quantities.size(),\\n\\u00a0                       inputs.solution_values.size());\\n\\u00a0 \\n\\u00a0       double max_phase = -numbers::PI;\\n\\u00a0       for (unsigned int p = 0; p < computed_quantities.size(); ++p)\\n\\u00a0         {\\n\\u00a0           AssertDimension(computed_quantities[p].size(), 1);\\n\\u00a0           AssertDimension(inputs.solution_values[p].size(), 2);\\n\\u00a0 \\n\\u00a0           max_phase =\\n\\u00a0             std::max(max_phase,\\n\\u00a0                      std::arg(\\n\\u00a0                        std::complex<double>(inputs.solution_values[p](0),\\n\\u00a0                                             inputs.solution_values[p](1))));\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       for (auto &output : computed_quantities)\\n\\u00a0         output(0) = max_phase;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   } // namespace DataPostprocessors\\n\\u00a0 \\n\\u00a0 \\nDataPostprocessor::evaluate_vector_fieldvirtual void evaluate_vector_field(const DataPostprocessorInputs::Vector< dim > &input_data, std::vector< Vector< double > > &computed_quantities) constDefinition data_postprocessor.cc:59\\nstd::max::VectorizedArray< Number, width > max(const ::VectorizedArray< Number, width > &, const ::VectorizedArray< Number, width > &)Definition vectorization.h:6943\\nHaving so implemented these post-processors, we create output as we always do. As in many other time-dependent tutorial programs, we attach flags to DataOut that indicate the number of the time step and the current simulation time.\\n\\u00a0   template <int dim>\\n\\u00a0   void NonlinearSchroedingerEquation<dim>::output_results() const\\n\\u00a0   {\\n\\u00a0     const DataPostprocessors::ComplexAmplitude<dim> complex_magnitude;\\n\\u00a0     const DataPostprocessors::ComplexPhase<dim>     complex_phase;\\n\\u00a0 \\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0 \\n\\u00a0     data_out.attach_dof_handler(dof_handler);\\n\\u00a0     data_out.add_data_vector(solution, \\\"Psi\\\");\\n\\u00a0     data_out.add_data_vector(solution, complex_magnitude);\\n\\u00a0     data_out.add_data_vector(solution, complex_phase);\\n\\u00a0     data_out.build_patches();\\n\\u00a0 \\n\\u00a0     data_out.set_flags(DataOutBase::VtkFlags(time, timestep_number));\\n\\u00a0 \\n\\u00a0     const std::string filename =\\n\\u00a0       \\\"solution-\\\" + Utilities::int_to_string(timestep_number, 3) + \\\".vtu\\\";\\n\\u00a0     std::ofstream output(filename);\\n\\u00a0     data_out.write_vtu(output);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOutDefinition data_out.h:147\\nUtilities::int_to_stringstd::string int_to_string(const unsigned int value, const unsigned int digits=numbers::invalid_unsigned_int)Definition utilities.cc:470\\nDataOutBase::VtkFlagsDefinition data_out_base.h:1127\\n Running the simulation\\nThe remaining step is how we set up the overall logic for this program. It's really relatively simple: Set up the data structures; interpolate the initial conditions onto finite element space; then iterate over all time steps, and on each time step perform the three parts of the Strang splitting method. Every tenth time step, we generate graphical output. That's it.\\n\\u00a0   template <int dim>\\n\\u00a0   void NonlinearSchroedingerEquation<dim>::run()\\n\\u00a0   {\\n\\u00a0     setup_system();\\n\\u00a0     assemble_matrices();\\n\\u00a0 \\n\\u00a0     time = 0;\\n\\u00a0     VectorTools::interpolate(dof_handler, InitialValues<dim>(), solution);\\n\\u00a0     output_results();\\n\\u00a0 \\n\\u00a0     const double end_time = 1;\\n\\u00a0     for (; time <= end_time; time += time_step)\\n\\u00a0       {\\n\\u00a0         ++timestep_number;\\n\\u00a0 \\n\\u00a0         std::cout << \\\"Time step \\\" << timestep_number << \\\" at t=\\\" << time\\n\\u00a0                   << std::endl;\\n\\u00a0 \\n\\u00a0         do_half_phase_step();\\n\\u00a0         do_full_spatial_step();\\n\\u00a0         do_half_phase_step();\\n\\u00a0 \\n\\u00a0         if (timestep_number % 1 == 0)\\n\\u00a0           output_results();\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 } // namespace Step58\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nVectorTools::interpolatevoid interpolate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Function< spacedim, typename VectorType::value_type > &function, VectorType &vec, const ComponentMask &component_mask={})\\n The main() function\\nThe rest is again boiler plate and exactly as in almost all of the previous tutorial programs:\\n\\u00a0 int main()\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       using namespace Step58;\\n\\u00a0 \\n\\u00a0       NonlinearSchroedingerEquation<2> nse;\\n\\u00a0       nse.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   return 0;\\n\\u00a0 }\\n Results\\nRunning the code results in screen output like the following: Number of active cells: 4096\\nNumber of degrees of freedom: 16641\\n \\nTime step 1 at t=0\\nTime step 2 at t=0.00390625\\nTime step 3 at t=0.0078125\\nTime step 4 at t=0.0117188\\n[...]\\n Running the program also yields a good number of output files that we will visualize in the following.\\nVisualizing the solution\\nThe output_results() function of this program generates output files that consist of a number of variables: The solution (split into its real and imaginary parts), the amplitude, and the phase. If we visualize these four fields, we get images like the following after a few time steps (at time \\\\(t=0.242\\\\), to be precise:\\n             While the real and imaginary parts of the solution shown above are not particularly interesting (because, from a physical perspective, the global offset of the phase and therefore the balance between real and imaginary components, is meaningless), it is much more interesting to visualize the amplitude \\\\(|\\\\psi(\\\\mathbf x,t)|^2\\\\) and phase \\\\(\\\\text{arg}(\\\\psi(\\\\mathbf x,t))\\\\) of the solution and, in particular, their evolution. This leads to pictures like the following:\\nThe phase picture shown here clearly has some flaws:\\nFirst, phase is a \\\"cyclic quantity\\\", but the color scale uses a fundamentally different color for values close to \\\\(-\\\\pi\\\\) than for values close to \\\\(+\\\\pi\\\\). This is a nuisance \\u2013 what we need is a \\\"cyclic color map\\\" that uses the same colors for the two extremes of the range of the phase. Such color maps exist, see this blog post of Nicol\\u00e1s Guar\\u00edn-Zapata or this StackExchange post, for example. The problem is that the author's favorite one of the two big visualization packages, VisIt, does not have any of these color maps built in. In an act of desperation, I therefore had to resort to using Paraview given that it has several of the color maps mentioned in the post above implemented. The picture below uses the nic_Edge map in which both of the extreme values are shown as black.\\nThere is a problem on cells in which the phase wraps around. If at some evaluation point of the cell the phase value is close to \\\\(-\\\\pi\\\\) and at another evaluation point it is close to \\\\(+\\\\pi\\\\), then what we would really like to happen is for the entire cell to have a color close to the extremes. But, instead, visualization programs produce a linear interpolation in which the values within the cell, i.e., between the evaluation points, is linearly interpolated between these two values, covering essentially the entire range of possible phase values and, consequently, cycling through the entire rainbow of colors from dark red to dark green over the course of one cell. The solution to this problem is to just output the phase value on each cell as a piecewise constant. Because averaging values close to the \\\\(-\\\\pi\\\\) and \\\\(+\\\\pi\\\\) is going to result in an average that has nothing to do with the actual phase angle, the ComplexPhase class just uses the maximal phase angle encountered on each cell.\\n\\nWith these modifications, the phase plot now looks as follows:\\n\\n \\nFinally, we can generate a movie out of this. (To be precise, the video uses two more global refinement cycles and a time step half the size of what is used in the program above.) The author of these lines made the movie with VisIt, because that's what he's more familiar with, and using a hacked color map that is also cyclic \\u2013 though this color map lacks all of the skill employed by the people who wrote the posts mentioned in the links above. It does, however, show the character of the solution as a wave equation if you look at the shaded part of the domain outside the circle of radius 0.7 in which the potential is zero \\u2013 you can see how every time one of the bumps (showing the amplitude \\\\(|\\\\psi_h(\\\\mathbf x,t)|^2\\\\)) bumps into the area where the potential is large: a wave travels outbound from there. Take a look at the video:\\n\\n\\n\\n\\n\\nSo why did I end up shading the area where the potential \\\\(V(\\\\mathbf x)\\\\) is large? In that outside region, the solution is relatively small. It is also relatively smooth. As a consequence, to some approximate degree, the equation in that region simplifies to     \\n\\\\[\\n  - i \\\\frac{\\\\partial \\\\psi}{\\\\partial t}\\n  + V \\\\psi\\n  \\\\approx 0,\\n\\\\]\\n\\n or maybe easier to read:    \\n\\\\[\\n  \\\\frac{\\\\partial \\\\psi}{\\\\partial t}\\n  \\\\approx - i V \\\\psi.\\n\\\\]\\n\\n To the degree to which this approximation is valid (which, among other things, eliminates the traveling waves you can see in the video), this equation has a solution   \\n\\\\[\\n  \\\\psi(\\\\mathbf x, t) = \\\\psi(\\\\mathbf x, 0) e^{-i V t}.\\n\\\\]\\n\\n Because \\\\(V\\\\) is large, this means that the phase rotates quite rapidly. If you focus on the semi-transparent outer part of the domain, you can see that. If one colors this region in the same way as the inner part of the domain, this rapidly flashing outer part may be psychedelic, but is also distracting of what's happening on the inside; it's also quite hard to actually see the radiating waves that are easy to see at the beginning of the video.\\n Possibilities for extensions\\nBetter linear solvers \\nThe solver chosen here is just too simple. It is also not efficient. What we do here is give the matrix to a sparse direct solver in every time step and let it find the solution of the linear system. But we know that we could do far better:\\n\\nFirst, we should make use of the fact that the matrix doesn't actually change from time step to time step. This is an artifact of the fact that we here have constant boundary values and that we don't change the time step size \\u2013 two assumptions that might not be true in actual applications. But at least in cases where this does happen to be the case, it would make sense to only factorize the matrix once (i.e., compute \\\\(L\\\\) and \\\\(U\\\\) factors once) and then use these factors for all following time steps until the matrix \\\\(C\\\\) changes and requires a new factorization. The interface of the SparseDirectUMFPACK class allows for this.\\nUltimately, however, sparse direct solvers are only efficient for relatively small problems, say up to a few 100,000 unknowns. Beyond this, one needs iterative solvers such as the Conjugate Gradient method (for symmetric and positive definite problems) or GMRES. We have used many of these in other tutorial programs. In all cases, they need to be accompanied by good preconditioners. For the current case, one could in principle use GMRES \\u2013 a method that does not require any specific properties of the matrix \\u2013 as the outer solver but at least at the time of writing this sentence (in 2022), the SolverGMRES class can only handle real-valued linear systems. This can be overcome by implementing a variation of GMRES that can deal with complex-valued matrices and vectors, see for example [89] . Even better would be to implement an iterative scheme that exploits the one structural feature we know is true for this problem: That the matrix is complex-symmetric (albeit not Hermitian), for which a literature search would probably find schemes as well.\\nA different strategy towards iterative solvers would be to break the linear system into a \\\\(2\\\\times 2\\\\) block system of real and imaginary components, like we did in step-29. This would then enable using real-valued iterative solvers on the outer level (e.g., the existing GMRES implementation), but one would have to come up with preconditioners that exploit the block structure. There is, again, literature on the topic, of which we simply point out a non-representative sample: [7] , [71] , [143] .\\n\\nBoundary conditions \\nIn order to be usable for actual, realistic problems, solvers for the nonlinear Schr\\u00f6dinger equation need to utilize boundary conditions that make sense for the problem at hand. We have here restricted ourselves to simple Neumann boundary conditions \\u2013 but these do not actually make sense for the problem. Indeed, the equations are generally posed on an infinite domain. But, since we can't compute on infinite domains, we need to truncate it somewhere and instead pose boundary conditions that make sense for this artificially small domain. The approach widely used is to use the Perfectly Matched Layer method that corresponds to a particular kind of attenuation. It is, in a different context, also used in step-62.\\nAdaptive meshes \\nFinally, we know from experience and many other tutorial programs that it is worthwhile to use adaptively refined meshes, rather than the uniform meshes used here. It would, in fact, not be very difficult to add this here: It just requires periodic remeshing and transfer of the solution from one mesh to the next. step-26 will be a good guide for how this could be implemented.\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2020 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Authors: Wolfgang Bangerth, Colorado State University\\n *          Yong-Yong Cai, Beijing Computational Science Research Center\\n */\\n \\n#include <deal.II/lac/vector.h>\\n#include <deal.II/lac/full_matrix.h>\\n#include <deal.II/lac/dynamic_sparsity_pattern.h>\\n#include <deal.II/lac/sparse_matrix.h>\\n#include <deal.II/lac/block_sparse_matrix.h>\\n#include <deal.II/lac/block_vector.h>\\n#include <deal.II/lac/affine_constraints.h>\\n#include <deal.II/lac/sparse_direct.h>\\n#include <deal.II/grid/tria.h>\\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/grid_refinement.h>\\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/dofs/dof_tools.h>\\n#include <deal.II/fe/fe_q.h>\\n#include <deal.II/fe/fe_values.h>\\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/vector_tools.h>\\n#include <deal.II/numerics/error_estimator.h>\\n \\n#include <fstream>\\n#include <iostream>\\n \\n \\nnamespace Step58\\n{\\n using namespace dealii;\\n \\n template <int dim>\\n class NonlinearSchroedingerEquation\\n  {\\n public:\\n    NonlinearSchroedingerEquation();\\n void run();\\n \\n private:\\n void setup_system();\\n void assemble_matrices();\\n void do_half_phase_step();\\n void do_full_spatial_step();\\n void output_results() const;\\n \\n \\n Triangulation<dim> triangulation;\\n const FE_Q<dim>    fe;\\n DoFHandler<dim>    dof_handler;\\n \\n AffineConstraints<std::complex<double>> constraints;\\n \\n SparsityPattern                    sparsity_pattern;\\n SparseMatrix<std::complex<double>> system_matrix;\\n SparseMatrix<std::complex<double>> rhs_matrix;\\n \\n Vector<std::complex<double>> solution;\\n Vector<std::complex<double>> system_rhs;\\n \\n double       time;\\n double       time_step;\\n unsigned int timestep_number;\\n \\n double kappa;\\n  };\\n \\n \\n \\n \\n template <int dim>\\n class InitialValues : public Function<dim, std::complex<double>>\\n  {\\n public:\\n    InitialValues()\\n      : Function<dim, std::complex<double>>(1)\\n    {}\\n \\n virtual std::complex<double>\\n value(const Point<dim> &p, const unsigned int component = 0) const override;\\n  };\\n \\n \\n \\n template <int dim>\\n  std::complex<double>\\n  InitialValues<dim>::value(const Point<dim>  &p,\\n const unsigned int component) const\\n {\\n static_assert(dim == 2, \\\"This initial condition only works in 2d.\\\");\\n \\n    (void)component;\\n Assert(component == 0, ExcIndexRange(component, 0, 1));\\n \\n const std::vector<Point<dim>> vortex_centers = {{0, -0.3},\\n                                                    {0, +0.3},\\n                                                    {+0.3, 0},\\n                                                    {-0.3, 0}};\\n \\n const double R = 0.1;\\n const double alpha =\\n      1. / (std::pow(R, dim) * std::pow(numbers::PI, dim / 2.));\\n \\n double sum = 0;\\n for (const auto &vortex_center : vortex_centers)\\n      {\\n const Tensor<1, dim> distance = p - vortex_center;\\n const double         r        = distance.norm();\\n \\n sum += alpha * std::exp(-(r * r) / (R * R));\\n      }\\n \\n return {std::sqrt(sum), 0.};\\n  }\\n \\n \\n \\n template <int dim>\\n class Potential : public Function<dim>\\n  {\\n public:\\n    Potential() = default;\\n virtual double value(const Point<dim>  &p,\\n const unsigned int component = 0) const override;\\n  };\\n \\n \\n \\n template <int dim>\\n double Potential<dim>::value(const Point<dim>  &p,\\n const unsigned int component) const\\n {\\n    (void)component;\\n Assert(component == 0, ExcIndexRange(component, 0, 1));\\n \\n return (Point<dim>().distance(p) > 0.7 ? 1000 : 0);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n  NonlinearSchroedingerEquation<dim>::NonlinearSchroedingerEquation()\\n    : fe(2)\\n    , dof_handler(triangulation)\\n    , time(0)\\n    , time_step(1. / 128)\\n    , timestep_number(0)\\n    , kappa(1)\\n  {}\\n \\n \\n \\n template <int dim>\\n void NonlinearSchroedingerEquation<dim>::setup_system()\\n  {\\n GridGenerator::hyper_cube(triangulation, -1, 1);\\n triangulation.refine_global(6);\\n \\n    std::cout << \\\"Number of active cells: \\\" << triangulation.n_active_cells()\\n              << std::endl;\\n \\n    dof_handler.distribute_dofs(fe);\\n \\n    std::cout << \\\"Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n              << std::endl\\n              << std::endl;\\n \\n DynamicSparsityPattern dsp(dof_handler.n_dofs(), dof_handler.n_dofs());\\n DoFTools::make_sparsity_pattern(dof_handler, dsp);\\n    sparsity_pattern.copy_from(dsp);\\n \\n    system_matrix.reinit(sparsity_pattern);\\n    rhs_matrix.reinit(sparsity_pattern);\\n \\n    solution.reinit(dof_handler.n_dofs());\\n    system_rhs.reinit(dof_handler.n_dofs());\\n \\n    constraints.close();\\n  }\\n \\n \\n \\n template <int dim>\\n void NonlinearSchroedingerEquation<dim>::assemble_matrices()\\n  {\\n const QGauss<dim> quadrature_formula(fe.degree + 1);\\n \\n FEValues<dim> fe_values(fe,\\n                            quadrature_formula,\\n update_values | update_gradients |\\n update_quadrature_points | update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n FullMatrix<std::complex<double>> cell_matrix_lhs(dofs_per_cell,\\n                                                     dofs_per_cell);\\n FullMatrix<std::complex<double>> cell_matrix_rhs(dofs_per_cell,\\n                                                     dofs_per_cell);\\n \\n    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n    std::vector<double>                  potential_values(n_q_points);\\n const Potential<dim>                 potential;\\n \\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      {\\n        cell_matrix_lhs = std::complex<double>(0.);\\n        cell_matrix_rhs = std::complex<double>(0.);\\n \\n        fe_values.reinit(cell);\\n \\n        potential.value_list(fe_values.get_quadrature_points(),\\n                             potential_values);\\n \\n for (unsigned int q_index = 0; q_index < n_q_points; ++q_index)\\n          {\\n for (unsigned int k = 0; k < dofs_per_cell; ++k)\\n              {\\n for (unsigned int l = 0; l < dofs_per_cell; ++l)\\n                  {\\n const std::complex<double> i = {0, 1};\\n \\n                    cell_matrix_lhs(k, l) +=\\n                      (-i * fe_values.shape_value(k, q_index) *\\n                         fe_values.shape_value(l, q_index) +\\n                       time_step / 4 * fe_values.shape_grad(k, q_index) *\\n                         fe_values.shape_grad(l, q_index) +\\n                       time_step / 2 * potential_values[q_index] *\\n                         fe_values.shape_value(k, q_index) *\\n                         fe_values.shape_value(l, q_index)) *\\n                      fe_values.JxW(q_index);\\n \\n                    cell_matrix_rhs(k, l) +=\\n                      (-i * fe_values.shape_value(k, q_index) *\\n                         fe_values.shape_value(l, q_index) -\\n                       time_step / 4 * fe_values.shape_grad(k, q_index) *\\n                         fe_values.shape_grad(l, q_index) -\\n                       time_step / 2 * potential_values[q_index] *\\n                         fe_values.shape_value(k, q_index) *\\n                         fe_values.shape_value(l, q_index)) *\\n                      fe_values.JxW(q_index);\\n                  }\\n              }\\n          }\\n \\n        cell->get_dof_indices(local_dof_indices);\\n        constraints.distribute_local_to_global(cell_matrix_lhs,\\n                                               local_dof_indices,\\n                                               system_matrix);\\n        constraints.distribute_local_to_global(cell_matrix_rhs,\\n                                               local_dof_indices,\\n                                               rhs_matrix);\\n      }\\n  }\\n \\n \\n \\n template <int dim>\\n void NonlinearSchroedingerEquation<dim>::do_half_phase_step()\\n  {\\n for (auto &value : solution)\\n      {\\n const std::complex<double> i         = {0, 1};\\n const double               magnitude = std::abs(value);\\n \\n value = std::exp(-i * kappa * magnitude * magnitude * (time_step / 2)) *\\n value;\\n      }\\n  }\\n \\n \\n \\n template <int dim>\\n void NonlinearSchroedingerEquation<dim>::do_full_spatial_step()\\n  {\\n    rhs_matrix.vmult(system_rhs, solution);\\n \\n SparseDirectUMFPACK direct_solver;\\n    direct_solver.solve(system_matrix, system_rhs);\\n \\n    solution = system_rhs;\\n  }\\n \\n \\n \\n \\n \\n namespace DataPostprocessors\\n  {\\n template <int dim>\\n class ComplexAmplitude : public DataPostprocessorScalar<dim>\\n    {\\n public:\\n      ComplexAmplitude();\\n \\n virtual void evaluate_vector_field(\\n const DataPostprocessorInputs::Vector<dim> &inputs,\\n        std::vector<Vector<double>> &computed_quantities) const override;\\n    };\\n \\n \\n template <int dim>\\n    ComplexAmplitude<dim>::ComplexAmplitude()\\n      : DataPostprocessorScalar<dim>(\\\"Amplitude\\\", update_values)\\n    {}\\n \\n \\n template <int dim>\\n void ComplexAmplitude<dim>::evaluate_vector_field(\\n const DataPostprocessorInputs::Vector<dim> &inputs,\\n      std::vector<Vector<double>>                &computed_quantities) const\\n {\\n AssertDimension(computed_quantities.size(),\\n                      inputs.solution_values.size());\\n \\n for (unsigned int p = 0; p < computed_quantities.size(); ++p)\\n        {\\n AssertDimension(computed_quantities[p].size(), 1);\\n AssertDimension(inputs.solution_values[p].size(), 2);\\n \\n const std::complex<double> psi(inputs.solution_values[p](0),\\n                                         inputs.solution_values[p](1));\\n          computed_quantities[p](0) = std::norm(psi);\\n        }\\n    }\\n \\n \\n \\n template <int dim>\\n class ComplexPhase : public DataPostprocessorScalar<dim>\\n    {\\n public:\\n      ComplexPhase();\\n \\n virtual void evaluate_vector_field(\\n const DataPostprocessorInputs::Vector<dim> &inputs,\\n        std::vector<Vector<double>> &computed_quantities) const override;\\n    };\\n \\n \\n template <int dim>\\n    ComplexPhase<dim>::ComplexPhase()\\n      : DataPostprocessorScalar<dim>(\\\"Phase\\\", update_values)\\n    {}\\n \\n \\n template <int dim>\\n void ComplexPhase<dim>::evaluate_vector_field(\\n const DataPostprocessorInputs::Vector<dim> &inputs,\\n      std::vector<Vector<double>>                &computed_quantities) const\\n {\\n AssertDimension(computed_quantities.size(),\\n                      inputs.solution_values.size());\\n \\n double max_phase = -numbers::PI;\\n for (unsigned int p = 0; p < computed_quantities.size(); ++p)\\n        {\\n AssertDimension(computed_quantities[p].size(), 1);\\n AssertDimension(inputs.solution_values[p].size(), 2);\\n \\n          max_phase =\\n std::max(max_phase,\\n                     std::arg(\\n                       std::complex<double>(inputs.solution_values[p](0),\\n                                            inputs.solution_values[p](1))));\\n        }\\n \\n for (auto &output : computed_quantities)\\n        output(0) = max_phase;\\n    }\\n \\n  } // namespace DataPostprocessors\\n \\n \\n template <int dim>\\n void NonlinearSchroedingerEquation<dim>::output_results() const\\n {\\n const DataPostprocessors::ComplexAmplitude<dim> complex_magnitude;\\n const DataPostprocessors::ComplexPhase<dim>     complex_phase;\\n \\n DataOut<dim> data_out;\\n \\n    data_out.attach_dof_handler(dof_handler);\\n    data_out.add_data_vector(solution, \\\"Psi\\\");\\n    data_out.add_data_vector(solution, complex_magnitude);\\n    data_out.add_data_vector(solution, complex_phase);\\n    data_out.build_patches();\\n \\n    data_out.set_flags(DataOutBase::VtkFlags(time, timestep_number));\\n \\n const std::string filename =\\n \\\"solution-\\\" + Utilities::int_to_string(timestep_number, 3) + \\\".vtu\\\";\\n    std::ofstream output(filename);\\n    data_out.write_vtu(output);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void NonlinearSchroedingerEquation<dim>::run()\\n  {\\n    setup_system();\\n    assemble_matrices();\\n \\n    time = 0;\\n VectorTools::interpolate(dof_handler, InitialValues<dim>(), solution);\\n    output_results();\\n \\n const double end_time = 1;\\n for (; time <= end_time; time += time_step)\\n      {\\n        ++timestep_number;\\n \\n        std::cout << \\\"Time step \\\" << timestep_number << \\\" at t=\\\" << time\\n                  << std::endl;\\n \\n        do_half_phase_step();\\n        do_full_spatial_step();\\n        do_half_phase_step();\\n \\n if (timestep_number % 1 == 0)\\n          output_results();\\n      }\\n  }\\n} // namespace Step58\\n \\n \\n \\nint main()\\n{\\n try\\n    {\\n using namespace Step58;\\n \\n      NonlinearSchroedingerEquation<2> nse;\\n      nse.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n return 0;\\n}\\naffine_constraints.h\\nblock_sparse_matrix.h\\nblock_vector.h\\nDataOutInterface::write_vtuvoid write_vtu(std::ostream &out) constDefinition data_out_base.cc:7692\\nDataOutInterface::set_flagsvoid set_flags(const FlagType &flags)Definition data_out_base.cc:8863\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\ndof_handler.h\\ndof_tools.h\\ndynamic_sparsity_pattern.h\\nerror_estimator.h\\nfe_values.h\\nfe_q.h\\nfull_matrix.h\\ngrid_refinement.h\\ntria.h\\ngrid_generator.h\\nPhysics::Elasticity::Kinematics::lTensor< 2, dim, Number > l(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\ninternal::EvaluatorQuantity::value@ value\\ndata_out.h\\nsparse_direct.h\\nsparse_matrix.h\\nDataPostprocessorInputs::Vector::solution_valuesstd::vector<::Vector< double > > solution_valuesDefinition data_postprocessor.h:410\\nvector.h\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"