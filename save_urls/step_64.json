"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_64.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-64 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-64 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-64 tutorial program\\n\\n\\nThis tutorial depends on step-7, step-37.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\nThe test case\\nMoving data to and from the device\\nMatrix-vector product implementation\\n\\n The commented program\\n\\nClass VaryingCoefficientFunctor\\nClass HelmholtzOperatorQuad\\nClass LocalHelmholtzOperator\\nClass HelmholtzOperator\\nClass HelmholtzProblem\\nThe main() function\\n\\n\\n Results\\n\\n Possibilities for extensions \\n\\n The plain program\\n   \\n\\n\\n This program was contributed by Bruno Turcksin and Daniel Arndt, Oak Ridge National Laboratory. \\nIntroduction\\nThis example shows how to implement a matrix-free method on the GPU using CUDA for the Helmholtz equation with variable coefficients on a hypercube. The linear system will be solved using the conjugate gradient method and is parallelized with MPI.\\nIn the last few years, heterogeneous computing in general and GPUs in particular have gained a lot of popularity. This is because GPUs offer better computing capabilities and memory bandwidth than CPUs for a given power budget. Among the architectures available in early 2019, GPUs are about 2x-3x as power efficient than server CPUs with wide SIMD for PDE-related tasks. GPUs are also the most popular architecture for machine learning. On the other hand, GPUs are not easy to program. This program explores the deal.II capabilities to see how efficiently such a program can be implemented.\\nWhile we have tried for the interface of the matrix-free classes for the CPU and the GPU to be as close as possible, there are a few differences. When using the matrix-free framework on a GPU, one must write some CUDA code. However, the amount is fairly small and the use of CUDA is limited to a few keywords.\\nThe test case\\nIn this example, we consider the Helmholtz problem  \\n\\\\begin{eqnarray*} - \\\\nabla \\\\cdot\\n\\\\nabla u + a(\\\\mathbf x) u &=&1,\\\\\\\\ u &=& 0 \\\\quad \\\\text{on } \\\\partial \\\\Omega \\\\end{eqnarray*}\\n\\n where \\\\(a(\\\\mathbf x)\\\\) is a variable coefficient.\\nWe choose as domain \\\\(\\\\Omega=[0,1]^3\\\\) and  \\\\(a(\\\\mathbf x)=\\\\frac{10}{0.05 +\\n2\\\\|\\\\mathbf x\\\\|^2}\\\\). Since the coefficient is symmetric around the origin but the domain is not, we will end up with a non-symmetric solution.\\nIf you've made it this far into the tutorial, you will know how the weak formulation of this problem looks like and how, in principle, one assembles linear systems for it. Of course, in this program we will in fact not actually form the matrix, but rather only represent its action when one multiplies with it.\\nMoving data to and from the device\\nGPUs (we will use the term \\\"device\\\" from now on to refer to the GPU) have their own memory that is separate from the memory accessible to the CPU (we will use the term \\\"host\\\" from now on). A normal calculation on the device can be divided in three separate steps:\\nthe data is moved from the host to the device,\\nthe computation is done on the device,\\nthe result is moved back from the device to the host\\n\\nThe data movements can either be done explicitly by the user code or done automatically using UVM (Unified Virtual Memory). In deal.II, only the first method is supported. While it means an extra burden for the user, this allows for better control of data movement and more importantly it avoids to mistakenly run important kernels on the host instead of the device.\\nThe data movement in deal.II is done using LinearAlgebra::ReadWriteVector. These vectors can be seen as buffers on the host that are used to either store data received from the device or to send data to the device. There are two types of vectors that can be used on the device:\\nLinearAlgebra::CUDAWrappers::Vector, which is similar to the more common Vector<Number>, and\\nLinearAlgebra::distributed::Vector<Number, MemorySpace::CUDA>, which is a regular LinearAlgebra::distributed::Vector where we have specified which memory space to use.\\n\\nIf no memory space is specified, the default is MemorySpace::Host.\\nNext, we show how to move data to/from the device using LinearAlgebra::CUDAWrappers::Vector: unsigned int size = 10;\\nLinearAlgebra::ReadWriteVector<double> rw_vector(size);\\n \\n...do something with the rw_vector...\\n \\n// Move the data to the device:\\nLinearAlgebra::CUDAWrappers::Vector<double> vector_dev(size);\\nvector_dev.import_elements(rw_vector, VectorOperations::insert);\\n \\n...do some computations on the device...\\n \\n// Move the data back to the host:\\nrw_vector.import_elements(vector_dev, VectorOperations::insert);\\nLinearAlgebra::ReadWriteVectorDefinition trilinos_epetra_vector.h:42\\n Both of the vector classes used here only work on a single machine, i.e., one memory space on a host and one on a device.\\nBut there are cases where one wants to run a parallel computation between multiple MPI processes on a number of machines, each of which is equipped with GPUs. In that case, one wants to use LinearAlgebra::distributed::Vector<Number,MemorySpace::CUDA>, which is similar but the import() stage may involve MPI communication: IndexSet locally_owned_dofs, locally_relevant_dofs;\\n...fill the two IndexSet objects...\\n \\n// Create the ReadWriteVector using an IndexSet instead of the size\\nLinearAlgebra::ReadWriteVector<double> owned_rw_vector(locally_owned_dofs);\\n \\n...do something with the rw_vector...\\n \\n// Move the data to the device:\\nLinearAlgebra::distributed::Vector<double, MemorySpace::CUDA>\\n  distributed_vector_dev(locally_owned_dofs, MPI_COMM_WORLD);\\ndistributed_vector_dev.import_elements(owned_rw_vector, VectorOperations::insert);\\n \\n...do something with the dev_vector...\\n \\n// Create a ReadWriteVector with a different IndexSet:\\nLinearAlgebra::ReadWriteVector<double>\\n  relevant_rw_vector(locally_relevant_dofs);\\n \\n// Move the data to the host, possibly using MPI communication:\\nrelevant_rw_vector.import_elements(distributed_vector_dev, VectorOperations::insert);\\nIndexSetDefinition index_set.h:70\\nMemorySpace::DefaultDefinition memory_space.h:44\\n The relevant_rw_vector is an object that stores a subset of all elements of the vector. Typically, these are the locally relevant DoFs, which implies that they overlap between different MPI processes. Consequently, the elements stored in that vector on one machine may not coincide with the ones stored by the GPU on that machine, requiring MPI communication to import them.\\nIn all of these cases, while importing a vector, values can either be inserted (using VectorOperation::insert) or added to prior content of the vector (using VectorOperation::add).\\nMatrix-vector product implementation\\nThe code necessary to evaluate the matrix-free operator on the device is very similar to the one on the host. However, there are a few differences, the main ones being that the local_apply() function in step-37 and the loop over quadrature points both need to be encapsulated in their own functors.\\n The commented program\\nFirst include the necessary files from the deal.II library known from the previous tutorials.\\n\\u00a0 #include <deal.II/base/conditional_ostream.h>\\n\\u00a0 #include <deal.II/base/quadrature_lib.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/tria.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/lac/la_parallel_vector.h>\\n\\u00a0 #include <deal.II/lac/precondition.h>\\n\\u00a0 #include <deal.II/lac/solver_cg.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 \\nThe following ones include the data structures for the implementation of matrix-free methods on GPU:\\n\\u00a0 #include <deal.II/matrix_free/portable_fe_evaluation.h>\\n\\u00a0 #include <deal.II/matrix_free/portable_matrix_free.h>\\n\\u00a0 #include <deal.II/matrix_free/operators.h>\\n\\u00a0 \\n\\u00a0 #include <fstream>\\n\\u00a0 \\n\\u00a0 \\nAs usual, we enclose everything into a namespace of its own:\\n\\u00a0 namespace Step64\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n Class VaryingCoefficientFunctor\\nNext, we define a class that implements the varying coefficients we want to use in the Helmholtz operator. Later, we want to pass an object of this type to a Portable::MatrixFree object that expects the class to have an operator() that fills the values provided in the constructor for a given cell. This operator needs to run on the device, so it needs to be marked as DEAL_II_HOST_DEVICE for the compiler.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   class VaryingCoefficientFunctor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     VaryingCoefficientFunctor(double *coefficient)\\n\\u00a0       : coef(coefficient)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0     DEAL_II_HOST_DEVICE void\\n\\u00a0     operator()(const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n\\u00a0                const unsigned int                                      cell,\\n\\u00a0                const unsigned int                                      q) const;\\n\\u00a0 \\nMatrixFreeDefinition matrix_free.h:113\\nint\\nPortableDefinition portable_fe_evaluation.h:38\\nDEAL_II_HOST_DEVICE#define DEAL_II_HOST_DEVICEDefinition numbers.h:34\\nSince Portable::MatrixFree::Data doesn't know about the size of its arrays, we need to store the number of quadrature points and the number of degrees of freedom in this class to do necessary index conversions.\\n\\u00a0     static const unsigned int n_dofs_1d    = fe_degree + 1;\\n\\u00a0     static const unsigned int n_local_dofs = Utilities::pow(n_dofs_1d, dim);\\n\\u00a0     static const unsigned int n_q_points   = Utilities::pow(n_dofs_1d, dim);\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     double *coef;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nUtilities::powconstexpr T pow(const T base, const int iexp)Definition utilities.h:966\\nThe following function implements this coefficient. Recall from the introduction that we have defined it as  \\\\(a(\\\\mathbf\\n   x)=\\\\frac{10}{0.05 + 2\\\\|\\\\mathbf x\\\\|^2}\\\\)\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   DEAL_II_HOST_DEVICE void\\n\\u00a0   VaryingCoefficientFunctor<dim, fe_degree>::operator()(\\n\\u00a0     const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n\\u00a0     const unsigned int                                      cell,\\n\\u00a0     const unsigned int                                      q) const\\n\\u00a0   {\\n\\u00a0     const unsigned int pos = gpu_data->local_q_point_id(cell, n_q_points, q);\\n\\u00a0     const Point<dim>   q_point = gpu_data->get_quadrature_point(cell, q);\\n\\u00a0 \\n\\u00a0     double p_square = 0.;\\n\\u00a0     for (unsigned int i = 0; i < dim; ++i)\\n\\u00a0       {\\n\\u00a0         const double coord = q_point[i];\\n\\u00a0         p_square += coord * coord;\\n\\u00a0       }\\n\\u00a0     coef[pos] = 10. / (0.05 + 2. * p_square);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nPointDefinition point.h:111\\nPortable::MatrixFree::DataDefinition portable_matrix_free.h:160\\n Class HelmholtzOperatorQuad\\nThe class HelmholtzOperatorQuad implements the evaluation of the Helmholtz operator at each quadrature point. It uses a similar mechanism as the MatrixFree framework introduced in step-37. In contrast to there, the actual quadrature point index is treated implicitly by converting the current thread index. As before, the functions of this class need to run on the device, so need to be marked as DEAL_II_HOST_DEVICE for the compiler.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   class HelmholtzOperatorQuad\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     DEAL_II_HOST_DEVICE HelmholtzOperatorQuad(\\n\\u00a0       const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n\\u00a0       double                                                 *coef,\\n\\u00a0       int                                                     cell)\\n\\u00a0       : gpu_data(gpu_data)\\n\\u00a0       , coef(coef)\\n\\u00a0       , cell(cell)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0     DEAL_II_HOST_DEVICE void operator()(\\n\\u00a0       Portable::FEEvaluation<dim, fe_degree, fe_degree + 1, 1, double> *fe_eval,\\n\\u00a0       const int q_point) const;\\n\\u00a0 \\n\\u00a0     static const unsigned int n_q_points =\\n\\u00a0       ::Utilities::pow(fe_degree + 1, dim);\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     const typename Portable::MatrixFree<dim, double>::Data *gpu_data;\\n\\u00a0     double                                                 *coef;\\n\\u00a0     int                                                     cell;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\nFEEvaluationDefinition fe_evaluation.h:1355\\nThe Helmholtz problem we want to solve here reads in weak form as follows:   \\n\\\\begin{eqnarray*}\\n   (\\\\nabla v, \\\\nabla u)+ (v, a(\\\\mathbf x) u) &=&(v,1) \\\\quad \\\\forall v.\\n   \\\\end{eqnarray*}\\n\\n If you have seen step-37, then it will be obvious that the two terms on the left-hand side correspond to the two function calls here:\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   DEAL_II_HOST_DEVICE void HelmholtzOperatorQuad<dim, fe_degree>::operator()(\\n\\u00a0     Portable::FEEvaluation<dim, fe_degree, fe_degree + 1, 1, double> *fe_eval,\\n\\u00a0     const int q_point) const\\n\\u00a0   {\\n\\u00a0     const unsigned int pos =\\n\\u00a0       gpu_data->local_q_point_id(cell, n_q_points, q_point);\\n\\u00a0 \\n\\u00a0     fe_eval->submit_value(coef[pos] * fe_eval->get_value(q_point), q_point);\\n\\u00a0     fe_eval->submit_gradient(fe_eval->get_gradient(q_point), q_point);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nPortable::FEEvaluationDefinition portable_fe_evaluation.h:70\\n Class LocalHelmholtzOperator\\nFinally, we need to define a class that implements the whole operator evaluation that corresponds to a matrix-vector product in matrix-based approaches.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   class LocalHelmholtzOperator\\n\\u00a0   {\\n\\u00a0   public:\\nAgain, the Portable::MatrixFree object doesn't know about the number of degrees of freedom and the number of quadrature points so we need to store these for index calculations in the call operator.\\n\\u00a0     static constexpr unsigned int n_dofs_1d = fe_degree + 1;\\n\\u00a0     static constexpr unsigned int n_local_dofs =\\n\\u00a0       Utilities::pow(fe_degree + 1, dim);\\n\\u00a0     static constexpr unsigned int n_q_points =\\n\\u00a0       Utilities::pow(fe_degree + 1, dim);\\n\\u00a0 \\n\\u00a0     LocalHelmholtzOperator(double *coefficient)\\n\\u00a0       : coef(coefficient)\\n\\u00a0     {}\\n\\u00a0 \\n\\u00a0     DEAL_II_HOST_DEVICE void\\n\\u00a0     operator()(const unsigned int                                      cell,\\n\\u00a0                const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n\\u00a0                Portable::SharedData<dim, double> *shared_data,\\n\\u00a0                const double                      *src,\\n\\u00a0                double                            *dst) const;\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     double *coef;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\nThis is the call operator that performs the Helmholtz operator evaluation on a given cell similar to the MatrixFree framework on the CPU. In particular, we need access to both values and gradients of the source vector and we write value and gradient information to the destination vector.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   DEAL_II_HOST_DEVICE void LocalHelmholtzOperator<dim, fe_degree>::operator()(\\n\\u00a0     const unsigned int                                      cell,\\n\\u00a0     const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n\\u00a0     Portable::SharedData<dim, double>                      *shared_data,\\n\\u00a0     const double                                           *src,\\n\\u00a0     double                                                 *dst) const\\n\\u00a0   {\\n\\u00a0     Portable::FEEvaluation<dim, fe_degree, fe_degree + 1, 1, double> fe_eval(\\n\\u00a0       gpu_data, shared_data);\\n\\u00a0     fe_eval.read_dof_values(src);\\n\\u00a0     fe_eval.evaluate(EvaluationFlags::values | EvaluationFlags::gradients);\\n\\u00a0     fe_eval.apply_for_each_quad_point(\\n\\u00a0       HelmholtzOperatorQuad<dim, fe_degree>(gpu_data, coef, cell));\\n\\u00a0     fe_eval.integrate(EvaluationFlags::values | EvaluationFlags::gradients);\\n\\u00a0     fe_eval.distribute_local_to_global(dst);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nEvaluationFlags::gradients@ gradientsDefinition evaluation_flags.h:54\\nEvaluationFlags::values@ valuesDefinition evaluation_flags.h:50\\nPortable::SharedDataDefinition portable_matrix_free.h:633\\n Class HelmholtzOperator\\nThe HelmholtzOperator class acts as wrapper for LocalHelmholtzOperator defining an interface that can be used with linear solvers like SolverCG. In particular, like every class that implements the interface of a linear operator, it needs to have a vmult() function that performs the action of the linear operator on a source vector.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   class HelmholtzOperator\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     HelmholtzOperator(const DoFHandler<dim>           &dof_handler,\\n\\u00a0                       const AffineConstraints<double> &constraints);\\n\\u00a0 \\n\\u00a0     void\\n\\u00a0     vmult(LinearAlgebra::distributed::Vector<double, MemorySpace::Default> &dst,\\n\\u00a0           const LinearAlgebra::distributed::Vector<double, MemorySpace::Default>\\n\\u00a0             &src) const;\\n\\u00a0 \\n\\u00a0     void initialize_dof_vector(\\n\\u00a0       LinearAlgebra::distributed::Vector<double, MemorySpace::Default> &vec)\\n\\u00a0       const;\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     Portable::MatrixFree<dim, double>                                mf_data;\\n\\u00a0     LinearAlgebra::distributed::Vector<double, MemorySpace::Default> coef;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nDoFHandlerDefinition dof_handler.h:317\\nLinearAlgebra::distributed::VectorDefinition la_parallel_vector.h:250\\nPortable::MatrixFreeDefinition portable_matrix_free.h:94\\nThe following is the implementation of the constructor of this class. In the first part, we initialize the mf_data member variable that is going to provide us with the necessary information when evaluating the operator.\\nIn the second half, we need to store the value of the coefficient for each quadrature point in every active, locally owned cell. We can ask the parallel triangulation for the number of active, locally owned cells but only have a DoFHandler object at hand. Since DoFHandler::get_triangulation() returns a Triangulation object, not a parallel::TriangulationBase object, we have to downcast the return value. This is safe to do here because we know that the triangulation is a parallel::distributed::Triangulation object in fact.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   HelmholtzOperator<dim, fe_degree>::HelmholtzOperator(\\n\\u00a0     const DoFHandler<dim>           &dof_handler,\\n\\u00a0     const AffineConstraints<double> &constraints)\\n\\u00a0   {\\n\\u00a0     MappingQ<dim> mapping(fe_degree);\\n\\u00a0     typename Portable::MatrixFree<dim, double>::AdditionalData additional_data;\\n\\u00a0     additional_data.mapping_update_flags = update_values | update_gradients |\\n\\u00a0                                            update_JxW_values |\\n\\u00a0                                            update_quadrature_points;\\n\\u00a0     const QGauss<1> quad(fe_degree + 1);\\n\\u00a0     mf_data.reinit(mapping, dof_handler, constraints, quad, additional_data);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     const unsigned int n_owned_cells =\\n\\u00a0       dynamic_cast<const parallel::TriangulationBase<dim> *>(\\n\\u00a0         &dof_handler.get_triangulation())\\n\\u00a0         ->n_locally_owned_active_cells();\\n\\u00a0     coef.reinit(Utilities::pow(fe_degree + 1, dim) * n_owned_cells);\\n\\u00a0 \\n\\u00a0     const VaryingCoefficientFunctor<dim, fe_degree> functor(coef.get_values());\\n\\u00a0     mf_data.evaluate_coefficients(functor);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nMappingQDefinition mapping_q.h:110\\nQGaussDefinition quadrature_lib.h:40\\nparallel::TriangulationBaseDefinition tria_base.h:80\\nupdate_values@ update_valuesShape function values.Definition fe_update_flags.h:75\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\nPortable::MatrixFree::AdditionalDataDefinition portable_matrix_free.h:105\\nPortable::MatrixFree::AdditionalData::mapping_update_flagsUpdateFlags mapping_update_flagsDefinition portable_matrix_free.h:139\\nThe key step then is to use all of the previous classes to loop over all cells to perform the matrix-vector product. We implement this in the next function.\\nWhen applying the Helmholtz operator, we have to be careful to handle boundary conditions correctly. Since the local operator doesn't know about constraints, we have to copy the correct values from the source to the destination vector afterwards.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void HelmholtzOperator<dim, fe_degree>::vmult(\\n\\u00a0     LinearAlgebra::distributed::Vector<double, MemorySpace::Default>       &dst,\\n\\u00a0     const LinearAlgebra::distributed::Vector<double, MemorySpace::Default> &src)\\n\\u00a0     const\\n\\u00a0   {\\n\\u00a0     dst = 0.;\\n\\u00a0     LocalHelmholtzOperator<dim, fe_degree> helmholtz_operator(\\n\\u00a0       coef.get_values());\\n\\u00a0     mf_data.cell_loop(helmholtz_operator, src, dst);\\n\\u00a0     mf_data.copy_constrained_values(src, dst);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void HelmholtzOperator<dim, fe_degree>::initialize_dof_vector(\\n\\u00a0     LinearAlgebra::distributed::Vector<double, MemorySpace::Default> &vec) const\\n\\u00a0   {\\n\\u00a0     mf_data.initialize_dof_vector(vec);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n Class HelmholtzProblem\\nThis is the main class of this program. It defines the usual framework we use for tutorial programs. The only point worth commenting on is the solve() function and the choice of vector types.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   class HelmholtzProblem\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     HelmholtzProblem();\\n\\u00a0 \\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void setup_system();\\n\\u00a0 \\n\\u00a0     void assemble_rhs();\\n\\u00a0 \\n\\u00a0     void solve();\\n\\u00a0 \\n\\u00a0     void output_results(const unsigned int cycle) const;\\n\\u00a0 \\n\\u00a0     MPI_Comm mpi_communicator;\\n\\u00a0 \\n\\u00a0     parallel::distributed::Triangulation<dim> triangulation;\\n\\u00a0 \\n\\u00a0     const FE_Q<dim> fe;\\n\\u00a0     DoFHandler<dim> dof_handler;\\n\\u00a0 \\n\\u00a0     IndexSet locally_owned_dofs;\\n\\u00a0     IndexSet locally_relevant_dofs;\\n\\u00a0 \\n\\u00a0     AffineConstraints<double>                          constraints;\\n\\u00a0     std::unique_ptr<HelmholtzOperator<dim, fe_degree>> system_matrix_dev;\\n\\u00a0 \\nFE_QDefinition fe_q.h:554\\nMPI_Comm\\nparallel::distributed::TriangulationDefinition tria.h:268\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\nSince all the operations in the solve() function are executed on the graphics card, it is necessary for the vectors used to store their values on the GPU as well. LinearAlgebra::distributed::Vector can be told which memory space to use. There is also LinearAlgebra::CUDAWrappers::Vector that always uses GPU memory storage but doesn't work with MPI. It might be worth noticing that the communication between different MPI processes can be improved if the MPI implementation is GPU-aware and the configure flag DEAL_II_MPI_WITH_DEVICE_SUPPORT is enabled. (The value of this flag needs to be set at the time you call cmake when installing deal.II.)\\nIn addition, we also keep a solution vector with CPU storage such that we can view and display the solution as usual.\\n\\u00a0     LinearAlgebra::distributed::Vector<double, MemorySpace::Host>\\n\\u00a0       ghost_solution_host;\\n\\u00a0     LinearAlgebra::distributed::Vector<double, MemorySpace::Default>\\n\\u00a0       solution_dev;\\n\\u00a0     LinearAlgebra::distributed::Vector<double, MemorySpace::Default>\\n\\u00a0       system_rhs_dev;\\n\\u00a0 \\n\\u00a0     ConditionalOStream pcout;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\nConditionalOStreamDefinition conditional_ostream.h:80\\nThe implementation of all the remaining functions of this class apart from Helmholtzproblem::solve() doesn't contain anything new and we won't further comment much on the overall approach.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   HelmholtzProblem<dim, fe_degree>::HelmholtzProblem()\\n\\u00a0     : mpi_communicator(MPI_COMM_WORLD)\\n\\u00a0     , triangulation(mpi_communicator)\\n\\u00a0     , fe(fe_degree)\\n\\u00a0     , dof_handler(triangulation)\\n\\u00a0     , pcout(std::cout, Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void HelmholtzProblem<dim, fe_degree>::setup_system()\\n\\u00a0   {\\n\\u00a0     dof_handler.distribute_dofs(fe);\\n\\u00a0 \\n\\u00a0     locally_owned_dofs = dof_handler.locally_owned_dofs();\\n\\u00a0     locally_relevant_dofs =\\n\\u00a0       DoFTools::extract_locally_relevant_dofs(dof_handler);\\n\\u00a0     system_rhs_dev.reinit(locally_owned_dofs, mpi_communicator);\\n\\u00a0 \\n\\u00a0     constraints.clear();\\n\\u00a0     constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n\\u00a0     DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n\\u00a0     VectorTools::interpolate_boundary_values(dof_handler,\\n\\u00a0                                              0,\\n\\u00a0                                              Functions::ZeroFunction<dim>(),\\n\\u00a0                                              constraints);\\n\\u00a0     constraints.close();\\n\\u00a0 \\n\\u00a0     system_matrix_dev.reset(\\n\\u00a0       new HelmholtzOperator<dim, fe_degree>(dof_handler, constraints));\\n\\u00a0 \\n\\u00a0     ghost_solution_host.reinit(locally_owned_dofs,\\n\\u00a0                                locally_relevant_dofs,\\n\\u00a0                                mpi_communicator);\\n\\u00a0     system_matrix_dev->initialize_dof_vector(solution_dev);\\n\\u00a0     system_rhs_dev.reinit(solution_dev);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFunctions::ZeroFunctionDefinition function.h:510\\nDoFTools::make_hanging_node_constraintsvoid make_hanging_node_constraints(const DoFHandler< dim, spacedim > &dof_handler, AffineConstraints< number > &constraints)Definition dof_tools_constraints.cc:3073\\nInitializeLibrary::MPI@ MPI\\nDoFTools::extract_locally_relevant_dofsIndexSet extract_locally_relevant_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1164\\nUtilitiesDefinition communication_pattern_base.h:30\\nVectorTools::interpolate_boundary_valuesvoid interpolate_boundary_values(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const std::map< types::boundary_id, const Function< spacedim, number > * > &function_map, std::map< types::global_dof_index, number > &boundary_values, const ComponentMask &component_mask={})\\nstdSTL namespace.\\nUnlike programs such as step-4 or step-6, we will not have to assemble the whole linear system but only the right hand side vector. This looks in essence like we did in step-4, for example, but we have to pay attention to using the right constraints object when copying local contributions into the global vector. In particular, we need to make sure the entries that correspond to boundary nodes are properly zeroed out. This is necessary for CG to converge. (Another solution would be to modify the vmult() function above in such a way that we pretend the source vector has zero entries by just not taking them into account in matrix-vector products. But the approach used here is simpler.)\\nAt the end of the function, we can't directly copy the values from the host to the device but need to use an intermediate object of type LinearAlgebra::ReadWriteVector to construct the correct communication pattern necessary.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void HelmholtzProblem<dim, fe_degree>::assemble_rhs()\\n\\u00a0   {\\n\\u00a0     LinearAlgebra::distributed::Vector<double, MemorySpace::Host>\\n\\u00a0                       system_rhs_host(locally_owned_dofs,\\n\\u00a0                       locally_relevant_dofs,\\n\\u00a0                       mpi_communicator);\\n\\u00a0     const QGauss<dim> quadrature_formula(fe_degree + 1);\\n\\u00a0 \\n\\u00a0     FEValues<dim> fe_values(fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_values | update_quadrature_points |\\n\\u00a0                               update_JxW_values);\\n\\u00a0 \\n\\u00a0     const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     Vector<double> cell_rhs(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       if (cell->is_locally_owned())\\n\\u00a0         {\\n\\u00a0           cell_rhs = 0;\\n\\u00a0 \\n\\u00a0           fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0           for (unsigned int q_index = 0; q_index < n_q_points; ++q_index)\\n\\u00a0             {\\n\\u00a0               for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0                 cell_rhs(i) += (fe_values.shape_value(i, q_index) * 1.0 *\\n\\u00a0                                 fe_values.JxW(q_index));\\n\\u00a0             }\\n\\u00a0 \\n\\u00a0           cell->get_dof_indices(local_dof_indices);\\n\\u00a0           constraints.distribute_local_to_global(cell_rhs,\\n\\u00a0                                                  local_dof_indices,\\n\\u00a0                                                  system_rhs_host);\\n\\u00a0         }\\n\\u00a0     system_rhs_host.compress(VectorOperation::add);\\n\\u00a0 \\n\\u00a0     LinearAlgebra::ReadWriteVector<double> rw_vector(locally_owned_dofs);\\n\\u00a0     rw_vector.import_elements(system_rhs_host, VectorOperation::insert);\\n\\u00a0     system_rhs_dev.import_elements(rw_vector, VectorOperation::insert);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFEValuesDefinition fe_values.h:63\\nVectorDefinition vector.h:120\\nVectorOperation::add@ addDefinition vector_operation.h:53\\nVectorOperation::insert@ insertDefinition vector_operation.h:49\\nThis solve() function finally contains the calls to the new classes previously discussed. Here we don't use any preconditioner, i.e., precondition by the identity matrix, to focus just on the peculiarities of the Portable::MatrixFree framework. Of course, in a real application the choice of a suitable preconditioner is crucial but we have at least the same restrictions as in step-37 since matrix entries are computed on the fly and not stored.\\nAfter solving the linear system in the first part of the function, we copy the solution from the device to the host to be able to view its values and display it in output_results(). This transfer works the same as at the end of the previous function.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void HelmholtzProblem<dim, fe_degree>::solve()\\n\\u00a0   {\\n\\u00a0     PreconditionIdentity preconditioner;\\n\\u00a0 \\n\\u00a0     SolverControl solver_control(system_rhs_dev.size(),\\n\\u00a0                                  1e-12 * system_rhs_dev.l2_norm());\\n\\u00a0     SolverCG<LinearAlgebra::distributed::Vector<double, MemorySpace::Default>>\\n\\u00a0       cg(solver_control);\\n\\u00a0     cg.solve(*system_matrix_dev, solution_dev, system_rhs_dev, preconditioner);\\n\\u00a0 \\n\\u00a0     pcout << \\\"  Solved in \\\" << solver_control.last_step() << \\\" iterations.\\\"\\n\\u00a0           << std::endl;\\n\\u00a0 \\n\\u00a0     LinearAlgebra::ReadWriteVector<double> rw_vector(locally_owned_dofs);\\n\\u00a0     rw_vector.import_elements(solution_dev, VectorOperation::insert);\\n\\u00a0     ghost_solution_host.import_elements(rw_vector, VectorOperation::insert);\\n\\u00a0 \\n\\u00a0     constraints.distribute(ghost_solution_host);\\n\\u00a0 \\n\\u00a0     ghost_solution_host.update_ghost_values();\\n\\u00a0   }\\n\\u00a0 \\nPreconditionIdentityDefinition precondition.h:220\\nSolverCGDefinition solver_cg.h:179\\nSolverControlDefinition solver_control.h:67\\nThe output results function is as usual since we have already copied the values back from the GPU to the CPU.\\nWhile we're already doing something with the function, we might as well compute the \\\\(L_2\\\\) norm of the solution. We do this by calling VectorTools::integrate_difference(). That function is meant to compute the error by evaluating the difference between the numerical solution (given by a vector of values for the degrees of freedom) and an object representing the exact solution. But we can easily compute the \\\\(L_2\\\\) norm of the solution by passing in a zero function instead. That is, instead of evaluating the error \\\\(\\\\|u_h-u\\\\|_{L_2(\\\\Omega)}\\\\), we are just evaluating \\\\(\\\\|u_h-0\\\\|_{L_2(\\\\Omega)}=\\\\|u_h\\\\|_{L_2(\\\\Omega)}\\\\) instead.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void HelmholtzProblem<dim, fe_degree>::output_results(\\n\\u00a0     const unsigned int cycle) const\\n\\u00a0   {\\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0 \\n\\u00a0     data_out.attach_dof_handler(dof_handler);\\n\\u00a0     data_out.add_data_vector(ghost_solution_host, \\\"solution\\\");\\n\\u00a0     data_out.build_patches();\\n\\u00a0 \\n\\u00a0     DataOutBase::VtkFlags flags;\\n\\u00a0     flags.compression_level = DataOutBase::CompressionLevel::best_speed;\\n\\u00a0     data_out.set_flags(flags);\\n\\u00a0     data_out.write_vtu_with_pvtu_record(\\n\\u00a0       \\\"./\\\", \\\"solution\\\", cycle, mpi_communicator, 2);\\n\\u00a0 \\n\\u00a0     Vector<float> cellwise_norm(triangulation.n_active_cells());\\n\\u00a0     VectorTools::integrate_difference(dof_handler,\\n\\u00a0                                       ghost_solution_host,\\n\\u00a0                                       Functions::ZeroFunction<dim>(),\\n\\u00a0                                       cellwise_norm,\\n\\u00a0                                       QGauss<dim>(fe.degree + 2),\\n\\u00a0                                       VectorTools::L2_norm);\\n\\u00a0     const double global_norm =\\n\\u00a0       VectorTools::compute_global_error(triangulation,\\n\\u00a0                                         cellwise_norm,\\n\\u00a0                                         VectorTools::L2_norm);\\n\\u00a0     pcout << \\\"  solution norm: \\\" << global_norm << std::endl;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOutDefinition data_out.h:147\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nDataOutBase::CompressionLevel::best_speed@ best_speed\\nVectorTools::compute_global_errordouble compute_global_error(const Triangulation< dim, spacedim > &tria, const InVector &cellwise_error, const NormType &norm, const double exponent=2.)\\nVectorTools::L2_norm@ L2_normDefinition vector_tools_common.h:112\\nVectorTools::integrate_differencevoid integrate_difference(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const ReadVector< Number > &fe_function, const Function< spacedim, Number > &exact_solution, OutVector &difference, const Quadrature< dim > &q, const NormType &norm, const Function< spacedim, double > *weight=nullptr, const double exponent=2.)\\nDataOutBase::VtkFlagsDefinition data_out_base.h:1127\\nDataOutBase::VtkFlags::compression_levelDataOutBase::CompressionLevel compression_levelDefinition data_out_base.h:1182\\nThere is nothing surprising in the run() function either. We simply compute the solution on a series of (globally) refined meshes.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void HelmholtzProblem<dim, fe_degree>::run()\\n\\u00a0   {\\n\\u00a0     for (unsigned int cycle = 0; cycle < 7 - dim; ++cycle)\\n\\u00a0       {\\n\\u00a0         pcout << \\\"Cycle \\\" << cycle << std::endl;\\n\\u00a0 \\n\\u00a0         if (cycle == 0)\\n\\u00a0           GridGenerator::hyper_cube(triangulation, 0., 1.);\\n\\u00a0         triangulation.refine_global(1);\\n\\u00a0 \\n\\u00a0         setup_system();\\n\\u00a0 \\n\\u00a0         pcout << \\\"   Number of active cells:       \\\"\\n\\u00a0               << triangulation.n_global_active_cells() << std::endl\\n\\u00a0               << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n\\u00a0               << std::endl;\\n\\u00a0 \\n\\u00a0         assemble_rhs();\\n\\u00a0         solve();\\n\\u00a0         output_results(cycle);\\n\\u00a0         pcout << std::endl;\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 } // namespace Step64\\n\\u00a0 \\n\\u00a0 \\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nparallel::TriangulationBase::n_global_active_cellsvirtual types::global_cell_index n_global_active_cells() const overrideDefinition tria_base.cc:151\\nGridGenerator::hyper_cubevoid hyper_cube(Triangulation< dim, spacedim > &tria, const double left=0., const double right=1., const bool colorize=false)\\n The main() function\\nFinally for the main() function. By default, all the MPI ranks will try to access the device with number 0, which we assume to be the GPU device associated with the CPU on which a particular MPI rank runs. This works, but if we are running with MPI support it may be that multiple MPI processes are running on the same machine (for example, one per CPU core) and then they would all want to access the same GPU on that machine. If there is only one GPU in the machine, there is nothing we can do about it: All MPI ranks on that machine need to share it. But if there are more than one GPU, then it is better to address different graphic cards for different processes. The choice below is based on the MPI process id by assigning GPUs round robin to GPU ranks. (To work correctly, this scheme assumes that the MPI ranks on one machine are consecutive. If that were not the case, then the rank-GPU association may just not be optimal.) To make this work, MPI needs to be initialized before using this function.\\n\\u00a0 int main(int argc, char *argv[])\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       using namespace Step64;\\n\\u00a0 \\n\\u00a0       Utilities::MPI::MPI_InitFinalize mpi_init(argc, argv, 1);\\n\\u00a0 \\n\\u00a0       HelmholtzProblem<3, 3> helmholtz_problem;\\n\\u00a0       helmholtz_problem.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return 0;\\n\\u00a0 }\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\n Results\\nSince the main purpose of this tutorial is to demonstrate how to use the Portable::MatrixFree interface, not to compute anything useful in itself, we just show the expected output here: Cycle 0\\n   Number of active cells:       8\\n   Number of degrees of freedom: 343\\n  Solved in 27 iterations.\\n  solution norm: 0.0205439\\n \\nCycle 1\\n   Number of active cells:       64\\n   Number of degrees of freedom: 2197\\n  Solved in 60 iterations.\\n  solution norm: 0.0205269\\n \\nCycle 2\\n   Number of active cells:       512\\n   Number of degrees of freedom: 15625\\n  Solved in 114 iterations.\\n  solution norm: 0.0205261\\n \\nCycle 3\\n   Number of active cells:       4096\\n   Number of degrees of freedom: 117649\\n  Solved in 227 iterations.\\n  solution norm: 0.0205261\\nOne can make two observations here: First, the norm of the numerical solution converges, presumably to the norm of the exact (but unknown) solution. And second, the number of iterations roughly doubles with each refinement of the mesh. (This is in keeping with the expectation that the number of CG iterations grows with the square root of the condition number of the matrix; and that we know that the condition number of the matrix of a second-order differential operation grows like \\\\({\\\\cal O}(h^{-2})\\\\).) This is of course rather inefficient, as an optimal solver would have a number of iterations that is independent of the size of the problem. But having such a solver would require using a better preconditioner than the identity matrix we have used here.\\n Possibilities for extensions \\nCurrently, this program uses no preconditioner at all. This is mainly since constructing an efficient matrix-free preconditioner is non-trivial. However, simple choices just requiring the diagonal of the corresponding matrix are good candidates and these can be computed in a matrix-free way as well. Alternatively, and maybe even better, one could extend the tutorial to use multigrid with Chebyshev smoothers similar to step-37.\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2019 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Authors: Bruno Turcksin, Daniel Arndt, Oak Ridge National Laboratory, 2019\\n */\\n \\n#include <deal.II/base/conditional_ostream.h>\\n#include <deal.II/base/quadrature_lib.h>\\n \\n#include <deal.II/dofs/dof_tools.h>\\n \\n#include <deal.II/fe/fe_q.h>\\n \\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/tria.h>\\n \\n#include <deal.II/lac/affine_constraints.h>\\n#include <deal.II/lac/la_parallel_vector.h>\\n#include <deal.II/lac/precondition.h>\\n#include <deal.II/lac/solver_cg.h>\\n \\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/vector_tools.h>\\n \\n#include <deal.II/matrix_free/portable_fe_evaluation.h>\\n#include <deal.II/matrix_free/portable_matrix_free.h>\\n#include <deal.II/matrix_free/operators.h>\\n \\n#include <fstream>\\n \\n \\nnamespace Step64\\n{\\n using namespace dealii;\\n \\n \\n \\n template <int dim, int fe_degree>\\n class VaryingCoefficientFunctor\\n  {\\n public:\\n    VaryingCoefficientFunctor(double *coefficient)\\n      : coef(coefficient)\\n    {}\\n \\n DEAL_II_HOST_DEVICE void\\n    operator()(const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n const unsigned int                                      cell,\\n const unsigned int                                      q) const;\\n \\n static const unsigned int n_dofs_1d    = fe_degree + 1;\\n static const unsigned int n_local_dofs = Utilities::pow(n_dofs_1d, dim);\\n static const unsigned int n_q_points   = Utilities::pow(n_dofs_1d, dim);\\n \\n private:\\n double *coef;\\n  };\\n \\n \\n \\n template <int dim, int fe_degree>\\n DEAL_II_HOST_DEVICE void\\n  VaryingCoefficientFunctor<dim, fe_degree>::operator()(\\n const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n const unsigned int                                      cell,\\n const unsigned int                                      q) const\\n {\\n const unsigned int pos = gpu_data->local_q_point_id(cell, n_q_points, q);\\n const Point<dim>   q_point = gpu_data->get_quadrature_point(cell, q);\\n \\n double p_square = 0.;\\n for (unsigned int i = 0; i < dim; ++i)\\n      {\\n const double coord = q_point[i];\\n        p_square += coord * coord;\\n      }\\n    coef[pos] = 10. / (0.05 + 2. * p_square);\\n  }\\n \\n \\n \\n template <int dim, int fe_degree>\\n class HelmholtzOperatorQuad\\n  {\\n public:\\n DEAL_II_HOST_DEVICE HelmholtzOperatorQuad(\\n const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n double                                                 *coef,\\n int                                                     cell)\\n      : gpu_data(gpu_data)\\n      , coef(coef)\\n      , cell(cell)\\n    {}\\n \\n DEAL_II_HOST_DEVICE void operator()(\\n Portable::FEEvaluation<dim, fe_degree, fe_degree + 1, 1, double> *fe_eval,\\n const int q_point) const;\\n \\n static const unsigned int n_q_points =\\n ::Utilities::pow(fe_degree + 1, dim);\\n \\n private:\\n const typename Portable::MatrixFree<dim, double>::Data *gpu_data;\\n double                                                 *coef;\\n int                                                     cell;\\n  };\\n \\n \\n template <int dim, int fe_degree>\\n DEAL_II_HOST_DEVICE void HelmholtzOperatorQuad<dim, fe_degree>::operator()(\\n Portable::FEEvaluation<dim, fe_degree, fe_degree + 1, 1, double> *fe_eval,\\n const int q_point) const\\n {\\n const unsigned int pos =\\n      gpu_data->local_q_point_id(cell, n_q_points, q_point);\\n \\n    fe_eval->submit_value(coef[pos] * fe_eval->get_value(q_point), q_point);\\n    fe_eval->submit_gradient(fe_eval->get_gradient(q_point), q_point);\\n  }\\n \\n \\n \\n template <int dim, int fe_degree>\\n class LocalHelmholtzOperator\\n  {\\n public:\\n static constexpr unsigned int n_dofs_1d = fe_degree + 1;\\n static constexpr unsigned int n_local_dofs =\\n Utilities::pow(fe_degree + 1, dim);\\n static constexpr unsigned int n_q_points =\\n Utilities::pow(fe_degree + 1, dim);\\n \\n    LocalHelmholtzOperator(double *coefficient)\\n      : coef(coefficient)\\n    {}\\n \\n DEAL_II_HOST_DEVICE void\\n    operator()(const unsigned int                                      cell,\\n const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n Portable::SharedData<dim, double> *shared_data,\\n const double                      *src,\\n double                            *dst) const;\\n \\n private:\\n double *coef;\\n  };\\n \\n \\n template <int dim, int fe_degree>\\n DEAL_II_HOST_DEVICE void LocalHelmholtzOperator<dim, fe_degree>::operator()(\\n const unsigned int                                      cell,\\n const typename Portable::MatrixFree<dim, double>::Data *gpu_data,\\n Portable::SharedData<dim, double>                      *shared_data,\\n const double                                           *src,\\n double                                                 *dst) const\\n {\\n Portable::FEEvaluation<dim, fe_degree, fe_degree + 1, 1, double> fe_eval(\\n      gpu_data, shared_data);\\n    fe_eval.read_dof_values(src);\\n    fe_eval.evaluate(EvaluationFlags::values | EvaluationFlags::gradients);\\n    fe_eval.apply_for_each_quad_point(\\n      HelmholtzOperatorQuad<dim, fe_degree>(gpu_data, coef, cell));\\n    fe_eval.integrate(EvaluationFlags::values | EvaluationFlags::gradients);\\n    fe_eval.distribute_local_to_global(dst);\\n  }\\n \\n \\n \\n template <int dim, int fe_degree>\\n class HelmholtzOperator\\n  {\\n public:\\n    HelmholtzOperator(const DoFHandler<dim>           &dof_handler,\\n const AffineConstraints<double> &constraints);\\n \\n void\\n    vmult(LinearAlgebra::distributed::Vector<double, MemorySpace::Default> &dst,\\n const LinearAlgebra::distributed::Vector<double, MemorySpace::Default>\\n            &src) const;\\n \\n void initialize_dof_vector(\\n LinearAlgebra::distributed::Vector<double, MemorySpace::Default> &vec)\\n const;\\n \\n private:\\n Portable::MatrixFree<dim, double>                                mf_data;\\n LinearAlgebra::distributed::Vector<double, MemorySpace::Default> coef;\\n  };\\n \\n \\n \\n template <int dim, int fe_degree>\\n  HelmholtzOperator<dim, fe_degree>::HelmholtzOperator(\\n const DoFHandler<dim>           &dof_handler,\\n const AffineConstraints<double> &constraints)\\n  {\\n MappingQ<dim> mapping(fe_degree);\\n typename Portable::MatrixFree<dim, double>::AdditionalData additional_data;\\n    additional_data.mapping_update_flags = update_values | update_gradients |\\n update_JxW_values |\\n update_quadrature_points;\\n const QGauss<1> quad(fe_degree + 1);\\n    mf_data.reinit(mapping, dof_handler, constraints, quad, additional_data);\\n \\n \\n const unsigned int n_owned_cells =\\n dynamic_cast<const parallel::TriangulationBase<dim> *>(\\n        &dof_handler.get_triangulation())\\n        ->n_locally_owned_active_cells();\\n    coef.reinit(Utilities::pow(fe_degree + 1, dim) * n_owned_cells);\\n \\n const VaryingCoefficientFunctor<dim, fe_degree> functor(coef.get_values());\\n    mf_data.evaluate_coefficients(functor);\\n  }\\n \\n \\n template <int dim, int fe_degree>\\n void HelmholtzOperator<dim, fe_degree>::vmult(\\n LinearAlgebra::distributed::Vector<double, MemorySpace::Default>       &dst,\\n const LinearAlgebra::distributed::Vector<double, MemorySpace::Default> &src)\\n    const\\n {\\n    dst = 0.;\\n    LocalHelmholtzOperator<dim, fe_degree> helmholtz_operator(\\n      coef.get_values());\\n    mf_data.cell_loop(helmholtz_operator, src, dst);\\n    mf_data.copy_constrained_values(src, dst);\\n  }\\n \\n \\n \\n template <int dim, int fe_degree>\\n void HelmholtzOperator<dim, fe_degree>::initialize_dof_vector(\\n LinearAlgebra::distributed::Vector<double, MemorySpace::Default> &vec) const\\n {\\n    mf_data.initialize_dof_vector(vec);\\n  }\\n \\n \\n \\n template <int dim, int fe_degree>\\n class HelmholtzProblem\\n  {\\n public:\\n    HelmholtzProblem();\\n \\n void run();\\n \\n private:\\n void setup_system();\\n \\n void assemble_rhs();\\n \\n void solve();\\n \\n void output_results(const unsigned int cycle) const;\\n \\n MPI_Comm mpi_communicator;\\n \\n parallel::distributed::Triangulation<dim> triangulation;\\n \\n const FE_Q<dim> fe;\\n DoFHandler<dim> dof_handler;\\n \\n IndexSet locally_owned_dofs;\\n IndexSet locally_relevant_dofs;\\n \\n AffineConstraints<double>                          constraints;\\n    std::unique_ptr<HelmholtzOperator<dim, fe_degree>> system_matrix_dev;\\n \\n LinearAlgebra::distributed::Vector<double, MemorySpace::Host>\\n      ghost_solution_host;\\n LinearAlgebra::distributed::Vector<double, MemorySpace::Default>\\n      solution_dev;\\n LinearAlgebra::distributed::Vector<double, MemorySpace::Default>\\n      system_rhs_dev;\\n \\n ConditionalOStream pcout;\\n  };\\n \\n \\n template <int dim, int fe_degree>\\n  HelmholtzProblem<dim, fe_degree>::HelmholtzProblem()\\n    : mpi_communicator(MPI_COMM_WORLD)\\n    , triangulation(mpi_communicator)\\n    , fe(fe_degree)\\n    , dof_handler(triangulation)\\n    , pcout(std::cout, Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n  {}\\n \\n \\n \\n template <int dim, int fe_degree>\\n void HelmholtzProblem<dim, fe_degree>::setup_system()\\n  {\\n    dof_handler.distribute_dofs(fe);\\n \\n    locally_owned_dofs = dof_handler.locally_owned_dofs();\\n    locally_relevant_dofs =\\n DoFTools::extract_locally_relevant_dofs(dof_handler);\\n    system_rhs_dev.reinit(locally_owned_dofs, mpi_communicator);\\n \\n    constraints.clear();\\n    constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n VectorTools::interpolate_boundary_values(dof_handler,\\n                                             0,\\n Functions::ZeroFunction<dim>(),\\n                                             constraints);\\n    constraints.close();\\n \\n    system_matrix_dev.reset(\\n new HelmholtzOperator<dim, fe_degree>(dof_handler, constraints));\\n \\n    ghost_solution_host.reinit(locally_owned_dofs,\\n                               locally_relevant_dofs,\\n                               mpi_communicator);\\n    system_matrix_dev->initialize_dof_vector(solution_dev);\\n    system_rhs_dev.reinit(solution_dev);\\n  }\\n \\n \\n \\n template <int dim, int fe_degree>\\n void HelmholtzProblem<dim, fe_degree>::assemble_rhs()\\n  {\\n LinearAlgebra::distributed::Vector<double, MemorySpace::Host>\\n                      system_rhs_host(locally_owned_dofs,\\n                      locally_relevant_dofs,\\n                      mpi_communicator);\\n const QGauss<dim> quadrature_formula(fe_degree + 1);\\n \\n FEValues<dim> fe_values(fe,\\n                            quadrature_formula,\\n update_values | update_quadrature_points |\\n update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n Vector<double> cell_rhs(dofs_per_cell);\\n \\n    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n \\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      if (cell->is_locally_owned())\\n        {\\n          cell_rhs = 0;\\n \\n          fe_values.reinit(cell);\\n \\n for (unsigned int q_index = 0; q_index < n_q_points; ++q_index)\\n            {\\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n                cell_rhs(i) += (fe_values.shape_value(i, q_index) * 1.0 *\\n                                fe_values.JxW(q_index));\\n            }\\n \\n          cell->get_dof_indices(local_dof_indices);\\n          constraints.distribute_local_to_global(cell_rhs,\\n                                                 local_dof_indices,\\n                                                 system_rhs_host);\\n        }\\n    system_rhs_host.compress(VectorOperation::add);\\n \\n LinearAlgebra::ReadWriteVector<double> rw_vector(locally_owned_dofs);\\n    rw_vector.import_elements(system_rhs_host, VectorOperation::insert);\\n    system_rhs_dev.import_elements(rw_vector, VectorOperation::insert);\\n  }\\n \\n \\n \\n template <int dim, int fe_degree>\\n void HelmholtzProblem<dim, fe_degree>::solve()\\n  {\\n PreconditionIdentity preconditioner;\\n \\n SolverControl solver_control(system_rhs_dev.size(),\\n                                 1e-12 * system_rhs_dev.l2_norm());\\n SolverCG<LinearAlgebra::distributed::Vector<double, MemorySpace::Default>>\\n      cg(solver_control);\\n    cg.solve(*system_matrix_dev, solution_dev, system_rhs_dev, preconditioner);\\n \\n    pcout << \\\"  Solved in \\\" << solver_control.last_step() << \\\" iterations.\\\"\\n          << std::endl;\\n \\n LinearAlgebra::ReadWriteVector<double> rw_vector(locally_owned_dofs);\\n    rw_vector.import_elements(solution_dev, VectorOperation::insert);\\n    ghost_solution_host.import_elements(rw_vector, VectorOperation::insert);\\n \\n    constraints.distribute(ghost_solution_host);\\n \\n    ghost_solution_host.update_ghost_values();\\n  }\\n \\n template <int dim, int fe_degree>\\n void HelmholtzProblem<dim, fe_degree>::output_results(\\n const unsigned int cycle) const\\n {\\n DataOut<dim> data_out;\\n \\n    data_out.attach_dof_handler(dof_handler);\\n    data_out.add_data_vector(ghost_solution_host, \\\"solution\\\");\\n    data_out.build_patches();\\n \\n DataOutBase::VtkFlags flags;\\n    flags.compression_level = DataOutBase::CompressionLevel::best_speed;\\n    data_out.set_flags(flags);\\n    data_out.write_vtu_with_pvtu_record(\\n \\\"./\\\", \\\"solution\\\", cycle, mpi_communicator, 2);\\n \\n Vector<float> cellwise_norm(triangulation.n_active_cells());\\n VectorTools::integrate_difference(dof_handler,\\n                                      ghost_solution_host,\\n Functions::ZeroFunction<dim>(),\\n                                      cellwise_norm,\\n QGauss<dim>(fe.degree + 2),\\n VectorTools::L2_norm);\\n const double global_norm =\\n VectorTools::compute_global_error(triangulation,\\n                                        cellwise_norm,\\n VectorTools::L2_norm);\\n    pcout << \\\"  solution norm: \\\" << global_norm << std::endl;\\n  }\\n \\n \\n template <int dim, int fe_degree>\\n void HelmholtzProblem<dim, fe_degree>::run()\\n  {\\n for (unsigned int cycle = 0; cycle < 7 - dim; ++cycle)\\n      {\\n        pcout << \\\"Cycle \\\" << cycle << std::endl;\\n \\n if (cycle == 0)\\n GridGenerator::hyper_cube(triangulation, 0., 1.);\\n triangulation.refine_global(1);\\n \\n        setup_system();\\n \\n        pcout << \\\"   Number of active cells:       \\\"\\n              << triangulation.n_global_active_cells() << std::endl\\n              << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n              << std::endl;\\n \\n        assemble_rhs();\\n        solve();\\n        output_results(cycle);\\n        pcout << std::endl;\\n      }\\n  }\\n} // namespace Step64\\n \\n \\n \\nint main(int argc, char *argv[])\\n{\\n try\\n    {\\n using namespace Step64;\\n \\n Utilities::MPI::MPI_InitFinalize mpi_init(argc, argv, 1);\\n \\n      HelmholtzProblem<3, 3> helmholtz_problem;\\n      helmholtz_problem.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n \\n return 0;\\n}\\naffine_constraints.h\\nAffineConstraints::closevoid close()\\nAffineConstraints::distribute_local_to_globalvoid distribute_local_to_global(const InVector &local_vector, const std::vector< size_type > &local_dof_indices, OutVector &global_vector) constDefinition affine_constraints.h:2651\\nAffineConstraints::distributevoid distribute(VectorType &vec) const\\nAffineConstraints::reinitvoid reinit()\\nAffineConstraints::clearvoid clear()\\nDataOutInterface::set_flagsvoid set_flags(const FlagType &flags)Definition data_out_base.cc:8863\\nDataOutInterface::write_vtu_with_pvtu_recordstd::string write_vtu_with_pvtu_record(const std::string &directory, const std::string &filename_without_extension, const unsigned int counter, const MPI_Comm mpi_communicator, const unsigned int n_digits_for_counter=numbers::invalid_unsigned_int, const unsigned int n_groups=0) constDefinition data_out_base.cc:7854\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\nDoFHandler::distribute_dofsvoid distribute_dofs(const FiniteElement< dim, spacedim > &fe)\\nDoFHandler::get_triangulationconst Triangulation< dim, spacedim > & get_triangulation() const\\nDoFHandler::locally_owned_dofsconst IndexSet & locally_owned_dofs() const\\nDoFHandler::n_dofstypes::global_dof_index n_dofs() const\\nPortable::FEEvaluation::integratevoid integrate(const EvaluationFlags::EvaluationFlags integration_flag)Definition portable_fe_evaluation.h:398\\nPortable::FEEvaluation::get_valuevalue_type get_value(int q_point) constDefinition portable_fe_evaluation.h:460\\nPortable::FEEvaluation::evaluatevoid evaluate(const EvaluationFlags::EvaluationFlags evaluate_flag)Definition portable_fe_evaluation.h:337\\nPortable::FEEvaluation::distribute_local_to_globalvoid distribute_local_to_global(Number *dst) constDefinition portable_fe_evaluation.h:298\\nPortable::FEEvaluation::get_gradientgradient_type get_gradient(int q_point) constDefinition portable_fe_evaluation.h:525\\nPortable::FEEvaluation::read_dof_valuesvoid read_dof_values(const Number *src)Definition portable_fe_evaluation.h:269\\nPortable::FEEvaluation::submit_valuevoid submit_value(const value_type &val_in, int q_point)Definition portable_fe_evaluation.h:493\\nPortable::FEEvaluation::submit_gradientvoid submit_gradient(const gradient_type &grad_in, int q_point)Definition portable_fe_evaluation.h:552\\nPortable::FEEvaluation::apply_for_each_quad_pointvoid apply_for_each_quad_point(const Functor &func)Definition portable_fe_evaluation.h:574\\nconditional_ostream.h\\ndof_tools.h\\nfe_q.h\\ntria.h\\ngrid_generator.h\\nla_parallel_vector.h\\nUtilities::MPI::this_mpi_processunsigned int this_mpi_process(const MPI_Comm mpi_communicator)Definition mpi.cc:107\\nWorkStream::internal::tbb_no_coloring::runvoid run(const Iterator &begin, const std_cxx20::type_identity_t< Iterator > &end, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length, const unsigned int chunk_size)Definition work_stream.h:471\\ndata_out.h\\noperators.h\\nportable_fe_evaluation.h\\nportable_matrix_free.h\\nprecondition.h\\nquadrature_lib.h\\nsolver_cg.h\\nPortable::MatrixFree::Data::local_q_point_idunsigned int local_q_point_id(const unsigned int cell, const unsigned int n_q_points, const unsigned int q_point) constDefinition portable_matrix_free.h:241\\nPortable::MatrixFree::Data::get_quadrature_pointPortable::MatrixFree< dim, Number >::point_type & get_quadrature_point(const unsigned int cell, const unsigned int q_point) constDefinition portable_matrix_free.h:254\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"