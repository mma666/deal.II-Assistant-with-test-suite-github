"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_15.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-15 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-15 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-15 tutorial program\\n\\n\\nThis tutorial depends on step-6.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\nForeword\\nClassical formulation\\nWeak formulation of the problem\\n Questions about the appropriate solver \\n Choice of step length and globalization \\n Summary of the algorithm and testcase \\n\\n The commented program\\n\\nInclude files\\nThe MinimalSurfaceProblem class template\\nBoundary condition\\nThe MinimalSurfaceProblem class implementation\\n\\nMinimalSurfaceProblem::MinimalSurfaceProblem\\nMinimalSurfaceProblem::setup_system\\nMinimalSurfaceProblem::assemble_system\\nMinimalSurfaceProblem::solve\\nMinimalSurfaceProblem::refine_mesh\\nMinimalSurfaceProblem::compute_residual\\nMinimalSurfaceProblem::determine_step_length\\nMinimalSurfaceProblem::output_results\\nMinimalSurfaceProblem::run\\nThe main function\\n\\n\\n\\n Results\\n\\nPossibilities for extensions\\n\\n Step length control \\n Integrating mesh refinement and nonlinear and linear solvers \\n Using automatic differentiation to compute the Jacobian matrix \\n Storing the Jacobian matrix in lower-precision floating point variables \\n\\n\\n The plain program\\n   \\n\\n\\n This program grew out of a student project by Sven Wetterauer at the University of Heidelberg, Germany. Most of the work for this program is by him.  \\n\\n Introduction\\nForeword\\nThis program deals with an example of a non-linear elliptic partial differential equation, the minimal surface equation. You can imagine the solution of this equation to describe the surface spanned by a soap film that is enclosed by a closed wire loop. We imagine the wire to not just be a planar loop, but in fact curved. The surface tension of the soap film will then reduce the surface to have minimal surface. The solution of the minimal surface equation describes this shape with the wire's vertical displacement as a boundary condition. For simplicity, we will here assume that the surface can be written as a graph \\\\(u=u(x,y)\\\\) although it is clear that it is not very hard to construct cases where the wire is bent in such a way that the surface can only locally be constructed as a graph but not globally.\\nBecause the equation is non-linear, we can't solve it directly. Rather, we have to use Newton's method to compute the solution iteratively.\\nNoteThe material presented here is also discussed in video lecture 31.5, video lecture 31.55, video lecture 31.6. (All video lectures are also available here.) (See also video lecture 31.65, video lecture 31.7.)\\nClassical formulation\\nIn a classical sense, the problem is given in the following form:\\n\\n\\\\begin{align*}\\n    -\\\\nabla \\\\cdot \\\\left( \\\\frac{1}{\\\\sqrt{1+|\\\\nabla u|^{2}}}\\\\nabla u \\\\right) &= 0 \\\\qquad\\n    \\\\qquad &&\\\\textrm{in} ~ \\\\Omega\\n    \\\\\\\\\\n    u&=g \\\\qquad\\\\qquad &&\\\\textrm{on} ~ \\\\partial \\\\Omega.\\n  \\\\end{align*}\\n\\n\\\\(\\\\Omega\\\\) is the domain we get by projecting the wire's positions into \\\\(x-y\\\\) space. In this example, we choose \\\\(\\\\Omega\\\\) as the unit disk.\\nAs described above, we solve this equation using Newton's method in which we compute the \\\\(n\\\\)th approximate solution from the \\\\((n-1)\\\\)th one, and use a damping parameter \\\\(\\\\alpha^n\\\\) to get better global convergence behavior:     \\n\\\\begin{align*}\\n    F'(u^{n},\\\\delta u^{n})&=- F(u^{n})\\n    \\\\\\\\\\n    u^{n+1}&=u^{n}+\\\\alpha^n \\\\delta u^{n}\\n  \\\\end{align*}\\n\\n with   \\n\\\\[\\n    F(u) \\\\dealcoloneq -\\\\nabla \\\\cdot \\\\left( \\\\frac{1}{\\\\sqrt{1+|\\\\nabla u|^{2}}}\\\\nabla u \\\\right)\\n  \\\\]\\n\\n and \\\\(F'(u,\\\\delta u)\\\\) the derivative of F in direction of \\\\(\\\\delta u\\\\):    \\n\\\\[\\n  F'(u,\\\\delta u)=\\\\lim \\\\limits_{\\\\epsilon \\\\rightarrow 0}{\\\\frac{F(u+\\\\epsilon \\\\delta u)-\\n  F(u)}{\\\\epsilon}}.\\n\\\\]\\n\\nGoing through the motions to find out what \\\\(F'(u,\\\\delta u)\\\\) is, we find that we have to solve a linear elliptic PDE in every Newton step, with \\\\(\\\\delta u^n\\\\) as the solution of:\\n\\n\\\\[\\n  - \\\\nabla \\\\cdot \\\\left( \\\\frac{1}{\\\\left(1+|\\\\nabla u^{n}|^{2}\\\\right)^{\\\\frac{1}{2}}}\\\\nabla\\n  \\\\delta u^{n} \\\\right) +\\n  \\\\nabla \\\\cdot \\\\left( \\\\frac{\\\\nabla u^{n} \\\\cdot\\n  \\\\nabla \\\\delta u^{n}}{\\\\left(1+|\\\\nabla u^{n}|^{2}\\\\right)^{\\\\frac{3}{2}}} \\\\nabla u^{n}\\n  \\\\right)  =\\n  -\\\\left( - \\\\nabla \\\\cdot \\\\left( \\\\frac{1}{\\\\left(1+|\\\\nabla u^{n}|^{2}\\\\right)^{\\\\frac{1}{2}}}\\n  \\\\nabla u^{n} \\\\right) \\\\right)\\n  \\\\]\\n\\nIn order to solve the minimal surface equation, we have to solve this equation repeatedly, once per Newton step. To solve this, we have to take a look at the boundary condition of this problem. Assuming that \\\\(u^{n}\\\\) already has the right boundary values, the Newton update \\\\(\\\\delta u^{n}\\\\) should have zero boundary conditions, in order to have the right boundary condition after adding both. In the first Newton step, we are starting with the solution \\\\(u^{0}\\\\equiv 0\\\\), the Newton update still has to deliver the right boundary condition to the solution \\\\(u^{1}\\\\).\\nSumming up, we have to solve the PDE above with the boundary condition  \\\\(\\\\delta\\nu^{0}=g\\\\) in the first step and with \\\\(\\\\delta u^{n}=0\\\\) in all the following steps.\\nNoteIn some sense, one may argue that if the program already implements \\\\(F(u)\\\\), it is duplicative to also have to implement \\\\(F'(u,\\\\delta)\\\\). As always, duplication tempts bugs and we would like to avoid it. While we do not explore this issue in this program, we will come back to it at the end of the Possibilities for extensions section below, and specifically in step-72.\\nWeak formulation of the problem\\nStarting with the strong formulation above, we get the weak formulation by multiplying both sides of the PDE with a test function \\\\(\\\\varphi\\\\) and integrating by parts on both sides:       \\n\\\\[\\n  \\\\left( \\\\nabla \\\\varphi , \\\\frac{1}{\\\\left(1+|\\\\nabla u^{n}|^{2}\\\\right)^{\\\\frac{1}{2}}}\\\\nabla\\n  \\\\delta u^{n} \\\\right)-\\\\left(\\\\nabla \\\\varphi ,\\\\frac{\\\\nabla u^{n} \\\\cdot \\\\nabla\\n  \\\\delta u^{n}}{\\\\left(1+|\\\\nabla u^{n}|^{2}\\\\right)^{\\\\frac{3}{2}}}\\\\nabla u^{n}  \\\\right)\\n  = -\\\\left(\\\\nabla \\\\varphi , \\\\frac{1}{\\\\left(1+|\\\\nabla u^{n}|^{2}\\\\right)^{\\\\frac{1}{2}}} \\\\nabla u^{n}\\n   \\\\right).\\n  \\\\]\\n\\n Here the solution \\\\(\\\\delta u^{n}\\\\) is a function in \\\\(H^{1}(\\\\Omega)\\\\), subject to the boundary conditions discussed above. Reducing this space to a finite dimensional space with basis  \\\\(\\\\left\\\\{\\n\\\\varphi_{0},\\\\dots , \\\\varphi_{N-1}\\\\right\\\\}\\\\), we can write the solution:\\n\\n\\\\[\\n  \\\\delta u^{n}=\\\\sum_{j=0}^{N-1} \\\\delta U_{j} \\\\varphi_{j}.\\n\\\\]\\n\\nUsing the basis functions as test functions and defining  \\\\(a_{n} \\\\dealcoloneq \\\\frac{1}\\n{\\\\sqrt{1+|\\\\nabla u^{n}|^{2}}}\\\\), we can rewrite the weak formulation:\\n\\n\\\\[\\n  \\\\sum_{j=0}^{N-1}\\\\left[ \\\\left( \\\\nabla \\\\varphi_{i} , a_{n} \\\\nabla \\\\varphi_{j} \\\\right) -\\n  \\\\left(\\\\nabla u^{n}\\\\cdot \\\\nabla \\\\varphi_{i} , a_{n}^{3} \\\\nabla u^{n} \\\\cdot \\\\nabla\\n  \\\\varphi_{j} \\\\right) \\\\right] \\\\cdot \\\\delta U_{j}=-\\\\left( \\\\nabla \\\\varphi_{i} , a_{n}\\n  \\\\nabla u^{n}\\\\right) \\\\qquad \\\\forall i=0,\\\\dots ,N-1,\\n\\\\]\\n\\nwhere the solution \\\\(\\\\delta u^{n}\\\\) is given by the coefficients \\\\(\\\\delta U^{n}_{j}\\\\). This linear system of equations can be rewritten as:\\n\\n\\\\[\\n  A^{n}\\\\; \\\\delta U^{n}=b^{n},\\n\\\\]\\n\\nwhere the entries of the matrix \\\\(A^{n}\\\\) are given by:\\n\\n\\\\[\\n  A^{n}_{ij} \\\\dealcoloneq \\\\left( \\\\nabla \\\\varphi_{i} , a_{n} \\\\nabla \\\\varphi_{j} \\\\right) -\\n  \\\\left(\\\\nabla u^{n}\\\\cdot \\\\nabla \\\\varphi_{i} , a_{n}^{3} \\\\nabla u^{n} \\\\cdot \\\\nabla\\n  \\\\varphi_{j} \\\\right),\\n\\\\]\\n\\nand the right hand side \\\\(b^{n}\\\\) is given by:\\n\\n\\\\[\\n  b^{n}_{i} \\\\dealcoloneq -\\\\left( \\\\nabla \\\\varphi_{i} , a_{n} \\\\nabla u^{n}\\\\right).\\n\\\\]\\n\\nQuestions about the appropriate solver \\nThe matrix that corresponds to the Newton step above can be reformulated to show its structure a bit better. Rewriting it slightly, we get that it has the form         \\n\\\\[\\n  A_{ij}\\n  =\\n  \\\\left(\\n    \\\\nabla \\\\varphi_i,\\n    B\\n    \\\\nabla \\\\varphi_j\\n  \\\\right),\\n\\\\]\\n\\n where the matrix \\\\(B\\\\) (of size \\\\(d \\\\times d\\\\) in \\\\(d\\\\) space dimensions) is given by the following expression:                \\n\\\\[\\n  B\\n  =\\n  a_n \\\\left\\\\{\\n   \\\\mathbf I\\n   -\\n   a_n^2 [\\\\nabla u_n] \\\\otimes [\\\\nabla u_n]\\n  \\\\right\\\\}\\n  =\\n  a_n \\\\left\\\\{\\n   \\\\mathbf I\\n   -\\n  \\\\frac{\\\\nabla u_n}{\\\\sqrt{1+|\\\\nabla u^{n}|^{2}}} \\\\otimes\\n  \\\\frac{\\\\nabla u_n}{\\\\sqrt{1+|\\\\nabla u^{n}|^{2}}}\\n  \\\\right\\\\}.\\n\\\\]\\n\\n From this expression, it is obvious that \\\\(B\\\\) is symmetric, and so \\\\(A\\\\) is symmetric as well. On the other hand, \\\\(B\\\\) is also positive definite, which confers the same property onto \\\\(A\\\\). This can be seen by noting that the vector  \\\\(v_1 =\\n\\\\frac{\\\\nabla u^n}{|\\\\nabla u^n|}\\\\) is an eigenvector of \\\\(B\\\\) with eigenvalue \\\\(\\\\lambda_1=a_n \\\\left(1-\\\\frac{|\\\\nabla u^n|^2}{1+|\\\\nabla u^n|^2}\\\\right) > 0\\\\) while all vectors \\\\(v_2\\\\ldots v_d\\\\) that are perpendicular to \\\\(v_1\\\\) and each other are eigenvectors with eigenvalue \\\\(a_n\\\\). Since all eigenvalues are positive, \\\\(B\\\\) is positive definite and so is \\\\(A\\\\). We can thus use the CG method for solving the Newton steps. (The fact that the matrix \\\\(A\\\\) is symmetric and positive definite should not come as a surprise. It results from taking the derivative of an operator that results from taking the derivative of an energy functional: the minimal surface equation simply minimizes some non-quadratic energy. Consequently, the Newton matrix, as the matrix of second derivatives of a scalar energy, must be symmetric since the derivative with regard to the \\\\(i\\\\)th and \\\\(j\\\\)th degree of freedom should clearly commute. Likewise, if the energy functional is convex, then the matrix of second derivatives must be positive definite, and the direct calculation above simply reaffirms this.)\\nIt is worth noting, however, that the positive definiteness degenerates for problems where \\\\(\\\\nabla u\\\\) becomes large. In other words, if we simply multiply all boundary values by 2, then to first order \\\\(u\\\\) and \\\\(\\\\nabla u\\\\) will also be multiplied by two, but as a consequence the smallest eigenvalue of \\\\(B\\\\) will become smaller and the matrix will become more ill-conditioned. (More specifically, for \\\\(|\\\\nabla u^n|\\\\rightarrow\\\\infty\\\\) we have that \\\\(\\\\lambda_1 \\\\propto a_n \\\\frac{1}{|\\\\nabla u^n|^2}\\\\) whereas \\\\(\\\\lambda_2\\\\ldots \\\\lambda_d=a_n\\\\); thus, the condition number of \\\\(B\\\\), which is a multiplicative factor in the condition number of \\\\(A\\\\) grows like \\\\({\\\\cal O}(|\\\\nabla u^n|^2)\\\\).) It is simple to verify with the current program that indeed multiplying the boundary values used in the current program by larger and larger values results in a problem that will ultimately no longer be solvable using the simple preconditioned CG method we use here.\\nChoice of step length and globalization \\nAs stated above, Newton's method works by computing a direction \\\\(\\\\delta u^n\\\\) and then performing the update  \\\\(u^{n+1} = u^{n}+\\\\alpha^n\\n\\\\delta u^{n}\\\\) with a step length \\\\(0 < \\\\alpha^n \\\\le 1\\\\). It is a common observation that for strongly nonlinear models, Newton's method does not converge if we always choose \\\\(\\\\alpha^n=1\\\\) unless one starts with an initial guess \\\\(u^0\\\\) that is sufficiently close to the solution \\\\(u\\\\) of the nonlinear problem. In practice, we don't always have such an initial guess, and consequently taking full Newton steps (i.e., using \\\\(\\\\alpha=1\\\\)) does frequently not work.\\nA common strategy therefore is to use a smaller step length for the first few steps while the iterate \\\\(u^n\\\\) is still far away from the solution \\\\(u\\\\) and as we get closer use larger values for \\\\(\\\\alpha^n\\\\) until we can finally start to use full steps \\\\(\\\\alpha^n=1\\\\) as we are close enough to the solution. The question is of course how to choose \\\\(\\\\alpha^n\\\\). There are basically two widely used approaches: line search and trust region methods.\\nIn this program, we simply always choose the step length equal to 0.1. This makes sure that for the testcase at hand we do get convergence although it is clear that by not eventually reverting to full step lengths we forego the rapid, quadratic convergence that makes Newton's method so appealing. Obviously, this is a point one eventually has to address if the program was made into one that is meant to solve more realistic problems. We will comment on this issue some more in the results section, and use an even better approach in step-77.\\nSummary of the algorithm and testcase \\nOverall, the program we have here is not unlike step-6 in many regards. The layout of the main class is essentially the same. On the other hand, the driving algorithm in the run() function is different and works as follows: \\n\\nStart with the function \\\\(u^{0}\\\\equiv 0\\\\) and modify it in such a way that the values of \\\\(u^0\\\\) along the boundary equal the correct boundary values \\\\(g\\\\) (this happens in the call to AffineConstraints::distribute()). Set \\\\(n=0\\\\). \\n\\n\\n\\nCompute the Newton update by solving the system  \\\\(A^{n}\\\\;\\\\delta\\n  U^{n}=b^{n}\\\\) with boundary condition \\\\(\\\\delta u^{n}=0\\\\) on \\\\(\\\\partial \\\\Omega\\\\). \\n\\n\\n\\nCompute a step length \\\\(\\\\alpha^n\\\\). In this program, we always set \\\\(\\\\alpha^n=0.1\\\\). To make things easier to extend later on, this happens in a function of its own, namely in MinimalSurfaceProblem::determine_step_length. (The strategy of always choosing \\\\(\\\\alpha^n=0.1\\\\) is of course not optimal \\u2013 we should choose a step length that works for a given search direction \\u2013 but it requires a bit of work to do that. In the end, we leave these sorts of things to external packages: step-77 does that.) \\n\\n\\n\\nThe new approximation of the solution is given by \\\\(u^{n+1}=u^{n}+\\\\alpha^n \\\\delta u^{n}\\\\). \\n\\n\\n\\nIf \\\\(n\\\\) is a multiple of 5 then refine the mesh, transfer the solution \\\\(u^{n+1}\\\\) to the new mesh and set the values of \\\\(u^{n+1}\\\\) in such a way that along the boundary we have \\\\(u^{n+1}|_{\\\\partial\\\\Gamma}=g\\\\). Note that this isn't automatically guaranteed even though by construction we had that before mesh refinement \\\\(u^{n+1}|_{\\\\partial\\\\Gamma}=g\\\\) because mesh refinement adds new nodes to the mesh where we have to interpolate the old solution to the new nodes upon bringing the solution from the old to the new mesh. The values we choose by interpolation may be close to the exact boundary conditions but are, in general, nonetheless not the correct values. \\n\\n\\n\\nSet \\\\(n\\\\leftarrow n+1\\\\) and go to step 2.  \\n\\nThe testcase we solve is chosen as follows: We seek to find the solution of minimal surface over the unit disk  \\\\(\\\\Omega=\\\\{\\\\mathbf x: \\\\|\\\\mathbf\\nx\\\\|<1\\\\}\\\\subset {\\\\mathbb R}^2\\\\) where the surface attains the values \\\\(u(x,y)|{\\\\partial\\\\Omega} = g(x,y) \\\\dealcoloneq \\\\sin(2 \\\\pi (x+y))\\\\) along the boundary.\\n The commented program\\n Include files\\nThe first few files have already been covered in previous examples and will thus not be further commented on.\\n\\u00a0 #include <deal.II/base/quadrature_lib.h>\\n\\u00a0 #include <deal.II/base/function.h>\\n\\u00a0 #include <deal.II/base/utilities.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/vector.h>\\n\\u00a0 #include <deal.II/lac/full_matrix.h>\\n\\u00a0 #include <deal.II/lac/sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/dynamic_sparsity_pattern.h>\\n\\u00a0 #include <deal.II/lac/solver_cg.h>\\n\\u00a0 #include <deal.II/lac/precondition.h>\\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/grid/tria.h>\\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/grid_refinement.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/fe/fe_values.h>\\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/error_estimator.h>\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 #include <fstream>\\n\\u00a0 #include <iostream>\\n\\u00a0 \\nWe will use adaptive mesh refinement between Newton iterations. To do so, we need to be able to work with a solution on the new mesh, although it was computed on the old one. The SolutionTransfer class transfers the solution from the old to the new mesh:\\n\\u00a0 #include <deal.II/numerics/solution_transfer.h>\\n\\u00a0 \\nWe then open a namespace for this program and import everything from the dealii namespace into it, as in previous programs:\\n\\u00a0 namespace Step15\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n The MinimalSurfaceProblem class template\\nThe class template is basically the same as in step-6. Three additions are made:\\nThere are two solution vectors, one for the Newton update \\\\(\\\\delta u^n\\\\), and one for the current iterate \\\\(u^n\\\\).\\nThe single AffineConstraints<> object in step-6 that is used to store boundary conditions and hanging node constraints, is replaced by two different objects of the same type: zero_constraints and nonzero_constraints. The former contains homogeneous boundary conditions to be used for the residual and solution updates, while the latter contains the correct boundary conditions for the solution. Both objects also contain the hanging nodes constraints.\\nThe setup_system function takes an argument that denotes whether this is the first time it is called or not. The difference is that the first time around we need to distribute the degrees of freedom and set the solution vector for \\\\(u^n\\\\) to the correct size. The following times, the function is called after we have already done these steps as part of refining the mesh in refine_mesh.\\nWe then also need a few new functions: compute_residual() is a function that computes the norm of the nonlinear (discrete) residual. We use this function to monitor convergence of the Newton iteration. The function takes a step length \\\\(\\\\alpha^n\\\\) as argument to compute the residual of  \\\\(u^n + \\\\alpha^n\\n   \\\\; \\\\delta u^n\\\\). This is something one typically needs for step length control, although we will not use this feature here. Finally, determine_step_length() computes the step length \\\\(\\\\alpha^n\\\\) in each Newton iteration. As discussed in the introduction, we here use a fixed step length and leave implementing a better strategy as an exercise. (step-77 does this differently: It simply uses an external package for the whole solution process, and a good line search strategy is part of what that package provides.)\\n\\n\\u00a0   template <int dim>\\n\\u00a0   class MinimalSurfaceProblem\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     MinimalSurfaceProblem();\\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void   setup_system();\\n\\u00a0     void   assemble_system();\\n\\u00a0     void   solve();\\n\\u00a0     void   refine_mesh();\\n\\u00a0     double compute_residual(const double alpha) const;\\n\\u00a0     double determine_step_length() const;\\n\\u00a0     void   output_results(const unsigned int refinement_cycle) const;\\n\\u00a0 \\n\\u00a0     Triangulation<dim> triangulation;\\n\\u00a0 \\n\\u00a0     DoFHandler<dim> dof_handler;\\n\\u00a0     const FE_Q<dim> fe;\\n\\u00a0 \\n\\u00a0     AffineConstraints<double> zero_constraints;\\n\\u00a0     AffineConstraints<double> nonzero_constraints;\\n\\u00a0 \\n\\u00a0     SparsityPattern      sparsity_pattern;\\n\\u00a0     SparseMatrix<double> system_matrix;\\n\\u00a0 \\n\\u00a0     Vector<double> current_solution;\\n\\u00a0     Vector<double> newton_update;\\n\\u00a0     Vector<double> system_rhs;\\n\\u00a0   };\\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nDoFHandlerDefinition dof_handler.h:317\\nFE_QDefinition fe_q.h:554\\nSparseMatrixDefinition sparse_matrix.h:520\\nSparsityPatternDefinition sparsity_pattern.h:343\\nTriangulationDefinition tria.h:1323\\nVectorDefinition vector.h:120\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\n Boundary condition\\nThe boundary condition is implemented just like in step-4. It is chosen as \\\\(g(x,y)=\\\\sin(2 \\\\pi (x+y))\\\\):\\n\\u00a0   template <int dim>\\n\\u00a0   class BoundaryValues : public Function<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     virtual double value(const Point<dim>  &p,\\n\\u00a0                          const unsigned int component = 0) const override;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   double BoundaryValues<dim>::value(const Point<dim> &p,\\n\\u00a0                                     const unsigned int /*component*/) const\\n\\u00a0   {\\n\\u00a0     return std::sin(2 * numbers::PI * (p[0] + p[1]));\\n\\u00a0   }\\n\\u00a0 \\nFunctionDefinition function.h:152\\nFunction::valuevirtual RangeNumberType value(const Point< dim > &p, const unsigned int component=0) const\\nPointDefinition point.h:111\\nnumbers::PIstatic constexpr double PIDefinition numbers.h:259\\nstd::sin::VectorizedArray< Number, width > sin(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6589\\n The MinimalSurfaceProblem class implementation\\n MinimalSurfaceProblem::MinimalSurfaceProblem\\nThe constructor and destructor of the class are the same as in the first few tutorials.\\n\\u00a0   template <int dim>\\n\\u00a0   MinimalSurfaceProblem<dim>::MinimalSurfaceProblem()\\n\\u00a0     : dof_handler(triangulation)\\n\\u00a0     , fe(2)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\n MinimalSurfaceProblem::setup_system\\nAs always in the setup-system function, we set up the variables of the finite element method. There are some differences to step-6, because we need to construct two AffineConstraint<> objects.\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::setup_system()\\n\\u00a0   {\\n\\u00a0     dof_handler.distribute_dofs(fe);\\n\\u00a0     current_solution.reinit(dof_handler.n_dofs());\\n\\u00a0 \\n\\u00a0     zero_constraints.clear();\\n\\u00a0     VectorTools::interpolate_boundary_values(dof_handler,\\n\\u00a0                                              0,\\n\\u00a0                                              Functions::ZeroFunction<dim>(),\\n\\u00a0                                              zero_constraints);\\n\\u00a0     DoFTools::make_hanging_node_constraints(dof_handler, zero_constraints);\\n\\u00a0     zero_constraints.close();\\n\\u00a0 \\n\\u00a0     nonzero_constraints.clear();\\n\\u00a0     VectorTools::interpolate_boundary_values(dof_handler,\\n\\u00a0                                              0,\\n\\u00a0                                              BoundaryValues<dim>(),\\n\\u00a0                                              nonzero_constraints);\\n\\u00a0 \\n\\u00a0     DoFTools::make_hanging_node_constraints(dof_handler, nonzero_constraints);\\n\\u00a0     nonzero_constraints.close();\\n\\u00a0 \\n\\u00a0     newton_update.reinit(dof_handler.n_dofs());\\n\\u00a0     system_rhs.reinit(dof_handler.n_dofs());\\n\\u00a0 \\n\\u00a0     DynamicSparsityPattern dsp(dof_handler.n_dofs());\\n\\u00a0     DoFTools::make_sparsity_pattern(dof_handler, dsp, zero_constraints);\\n\\u00a0 \\n\\u00a0     sparsity_pattern.copy_from(dsp);\\n\\u00a0     system_matrix.reinit(sparsity_pattern);\\n\\u00a0   }\\n\\u00a0 \\nDynamicSparsityPatternDefinition dynamic_sparsity_pattern.h:322\\nFunctions::ZeroFunctionDefinition function.h:510\\nDoFTools::make_hanging_node_constraintsvoid make_hanging_node_constraints(const DoFHandler< dim, spacedim > &dof_handler, AffineConstraints< number > &constraints)Definition dof_tools_constraints.cc:3073\\nDoFTools::make_sparsity_patternvoid make_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, SparsityPatternBase &sparsity_pattern, const AffineConstraints< number > &constraints={}, const bool keep_constrained_dofs=true, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id)Definition dof_tools_sparsity.cc:56\\nVectorTools::interpolate_boundary_valuesvoid interpolate_boundary_values(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const std::map< types::boundary_id, const Function< spacedim, number > * > &function_map, std::map< types::global_dof_index, number > &boundary_values, const ComponentMask &component_mask={})\\n MinimalSurfaceProblem::assemble_system\\nThis function does the same as in the previous tutorials except that now, of course, the matrix and right hand side functions depend on the previous iteration's solution. As discussed in the introduction, we need to use zero boundary values for the Newton updates; this is done by using the zero_constraints object when assembling into the global matrix and vector.\\nThe top of the function contains the usual boilerplate code, setting up the objects that allow us to evaluate shape functions at quadrature points and temporary storage locations for the local matrices and vectors, as well as for the gradients of the previous solution at the quadrature points. We then start the loop over all cells:\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::assemble_system()\\n\\u00a0   {\\n\\u00a0     const QGauss<dim> quadrature_formula(fe.degree + 1);\\n\\u00a0 \\n\\u00a0     system_matrix = 0;\\n\\u00a0     system_rhs    = 0;\\n\\u00a0 \\n\\u00a0     FEValues<dim> fe_values(fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_gradients | update_quadrature_points |\\n\\u00a0                               update_JxW_values);\\n\\u00a0 \\n\\u00a0     const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n\\u00a0     Vector<double>     cell_rhs(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     std::vector<Tensor<1, dim>> old_solution_gradients(n_q_points);\\n\\u00a0 \\n\\u00a0     std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       {\\n\\u00a0         cell_matrix = 0;\\n\\u00a0         cell_rhs    = 0;\\n\\u00a0 \\n\\u00a0         fe_values.reinit(cell);\\n\\u00a0 \\nFEValuesDefinition fe_values.h:63\\nFullMatrixDefinition full_matrix.h:79\\nQGaussDefinition quadrature_lib.h:40\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\nFor the assembly of the linear system, we have to obtain the values of the previous solution's gradients at the quadrature points. There is a standard way of doing this: the FEValues::get_function_gradients function takes a vector that represents a finite element field defined on a DoFHandler, and evaluates the gradients of this field at the quadrature points of the cell with which the FEValues object has last been reinitialized. The values of the gradients at all quadrature points are then written into the second argument:\\n\\u00a0         fe_values.get_function_gradients(current_solution,\\n\\u00a0                                          old_solution_gradients);\\n\\u00a0 \\nWith this, we can then do the integration loop over all quadrature points and shape functions. Having just computed the gradients of the old solution in the quadrature points, we are able to compute the coefficients \\\\(a_{n}\\\\) in these points. The assembly of the system itself then looks similar to what we always do with the exception of the nonlinear terms, as does copying the results from the local objects into the global ones:\\n\\u00a0         for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0           {\\n\\u00a0             const double coeff =\\n\\u00a0               1.0 / std::sqrt(1 + old_solution_gradients[q] *\\n\\u00a0                                     old_solution_gradients[q]);\\n\\u00a0 \\n\\u00a0             for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0               {\\n\\u00a0                 for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0                   cell_matrix(i, j) +=\\n\\u00a0                     (((fe_values.shape_grad(i, q)      // ((\\\\nabla \\\\phi_i\\n\\u00a0                        * coeff                         //   * a_n\\n\\u00a0                        * fe_values.shape_grad(j, q))   //   * \\\\nabla \\\\phi_j)\\n\\u00a0                       -                                //  -\\n\\u00a0                       (fe_values.shape_grad(i, q)      //  (\\\\nabla \\\\phi_i\\n\\u00a0                        * coeff * coeff * coeff         //   * a_n^3\\n\\u00a0                        * (fe_values.shape_grad(j, q)   //   * (\\\\nabla \\\\phi_j\\n\\u00a0                           * old_solution_gradients[q]) //      * \\\\nabla u_n)\\n\\u00a0                        * old_solution_gradients[q]))   //   * \\\\nabla u_n)))\\n\\u00a0                      * fe_values.JxW(q));              // * dx\\n\\u00a0 \\n\\u00a0                 cell_rhs(i) -= (fe_values.shape_grad(i, q)  // \\\\nabla \\\\phi_i\\n\\u00a0                                 * coeff                     // * a_n\\n\\u00a0                                 * old_solution_gradients[q] // * \\\\nabla u_n\\n\\u00a0                                 * fe_values.JxW(q));        // * dx\\n\\u00a0               }\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         cell->get_dof_indices(local_dof_indices);\\n\\u00a0         zero_constraints.distribute_local_to_global(\\n\\u00a0           cell_matrix, cell_rhs, local_dof_indices, system_matrix, system_rhs);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nstd::sqrt::VectorizedArray< Number, width > sqrt(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6869\\n MinimalSurfaceProblem::solve\\nThe solve function is the same as always. At the end of the solution process we update the current solution by setting \\\\(u^{n+1}=u^n+\\\\alpha^n\\\\;\\\\delta u^n\\\\).\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::solve()\\n\\u00a0   {\\n\\u00a0     SolverControl            solver_control(system_rhs.size(),\\n\\u00a0                                  system_rhs.l2_norm() * 1e-6);\\n\\u00a0     SolverCG<Vector<double>> solver(solver_control);\\n\\u00a0 \\n\\u00a0     PreconditionSSOR<SparseMatrix<double>> preconditioner;\\n\\u00a0     preconditioner.initialize(system_matrix, 1.2);\\n\\u00a0 \\n\\u00a0     solver.solve(system_matrix, newton_update, system_rhs, preconditioner);\\n\\u00a0 \\n\\u00a0     zero_constraints.distribute(newton_update);\\n\\u00a0 \\n\\u00a0     const double alpha = determine_step_length();\\n\\u00a0     current_solution.add(alpha, newton_update);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nPreconditionSSORDefinition precondition.h:1778\\nPreconditionSSOR::initializevoid initialize(const MatrixType &A, const AdditionalData &parameters=AdditionalData())\\nSolverCGDefinition solver_cg.h:179\\nSolverControlDefinition solver_control.h:67\\n MinimalSurfaceProblem::refine_mesh\\nThe first part of this function is the same as in step-6... However, after refining the mesh we have to transfer the old solution to the new one which we do with the help of the SolutionTransfer class. The process is slightly convoluted, so let us describe it in detail:\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::refine_mesh()\\n\\u00a0   {\\n\\u00a0     Vector<float> estimated_error_per_cell(triangulation.n_active_cells());\\n\\u00a0 \\n\\u00a0     KellyErrorEstimator<dim>::estimate(\\n\\u00a0       dof_handler,\\n\\u00a0       QGauss<dim - 1>(fe.degree + 1),\\n\\u00a0       std::map<types::boundary_id, const Function<dim> *>(),\\n\\u00a0       current_solution,\\n\\u00a0       estimated_error_per_cell);\\n\\u00a0 \\n\\u00a0     GridRefinement::refine_and_coarsen_fixed_number(triangulation,\\n\\u00a0                                                     estimated_error_per_cell,\\n\\u00a0                                                     0.3,\\n\\u00a0                                                     0.03);\\n\\u00a0 \\nKellyErrorEstimator::estimatestatic void estimate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Quadrature< dim - 1 > &quadrature, const std::map< types::boundary_id, const Function< spacedim, Number > * > &neumann_bc, const ReadVector< Number > &solution, Vector< float > &error, const ComponentMask &component_mask={}, const Function< spacedim > *coefficients=nullptr, const unsigned int n_threads=numbers::invalid_unsigned_int, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id, const types::material_id material_id=numbers::invalid_material_id, const Strategy strategy=cell_diameter_over_24)\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nunsigned int\\nGridRefinement::refine_and_coarsen_fixed_numbervoid refine_and_coarsen_fixed_number(Triangulation< dim, spacedim > &triangulation, const Vector< Number > &criteria, const double top_fraction_of_cells, const double bottom_fraction_of_cells, const unsigned int max_n_cells=std::numeric_limits< unsigned int >::max())Definition grid_refinement.cc:318\\nThen we need an additional step: if, for example, you flag a cell that is once more refined than its neighbor, and that neighbor is not flagged for refinement, we would end up with a jump of two refinement levels across a cell interface. To avoid these situations, the library will silently also have to refine the neighbor cell once. It does so by calling the Triangulation::prepare_coarsening_and_refinement function before actually doing the refinement and coarsening. This function flags a set of additional cells for refinement or coarsening, to enforce rules like the one-hanging-node rule. The cells that are flagged for refinement and coarsening after calling this function are exactly the ones that will actually be refined or coarsened. Usually, you don't have to do this by hand (Triangulation::execute_coarsening_and_refinement does this for you). However, we need to initialize the SolutionTransfer class and it needs to know the final set of cells that will be coarsened or refined in order to store the data from the old mesh and transfer to the new one. Thus, we call the function by hand:\\n\\u00a0     triangulation.prepare_coarsening_and_refinement();\\n\\u00a0 \\nparallel::distributed::Triangulation::prepare_coarsening_and_refinementvirtual bool prepare_coarsening_and_refinement() overrideDefinition tria.cc:2805\\nWith this out of the way, we initialize a SolutionTransfer object with the present DoFHandler. We make a copy of the solution vector and attach it to the SolutionTransfer. Now we can actually execute the refinement and create the new matrices and vectors including the vector current_solution, that will hold the current solution on the new mesh after calling SolutionTransfer::interpolate():\\n\\u00a0     SolutionTransfer<dim> solution_transfer(dof_handler);\\n\\u00a0     const Vector<double>  coarse_solution = current_solution;\\n\\u00a0     solution_transfer.prepare_for_coarsening_and_refinement(coarse_solution);\\n\\u00a0 \\n\\u00a0     triangulation.execute_coarsening_and_refinement();\\n\\u00a0 \\n\\u00a0     setup_system();\\n\\u00a0 \\n\\u00a0     solution_transfer.interpolate(coarse_solution, current_solution);\\n\\u00a0 \\nSolutionTransferDefinition solution_transfer.h:337\\nparallel::distributed::Triangulation::execute_coarsening_and_refinementvirtual void execute_coarsening_and_refinement() overrideDefinition tria.cc:3320\\nOn the new mesh, there are different hanging nodes, computed in setup_system() above. To be on the safe side, we should make sure that the current solution's vector entries satisfy the hanging node constraints (see the discussion in the documentation of the SolutionTransfer class for why this is necessary) and boundary values. As explained at the end of the introduction, the interpolated solution does not automatically satisfy the boundary values even if the solution before refinement had the correct boundary values.\\n\\u00a0     nonzero_constraints.distribute(current_solution);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n MinimalSurfaceProblem::compute_residual\\nIn order to monitor convergence, we need a way to compute the norm of the (discrete) residual, i.e., the norm of the vector \\\\(\\\\left<F(u^n),\\\\varphi_i\\\\right>\\\\) with  \\\\(F(u)=-\\\\nabla \\\\cdot \\\\left(\\n   \\\\frac{1}{\\\\sqrt{1+|\\\\nabla u|^{2}}}\\\\nabla u \\\\right)\\\\) as discussed in the introduction. It turns out that (although we don't use this feature in the current version of the program) one needs to compute the residual \\\\(\\\\left<F(u^n+\\\\alpha^n\\\\;\\\\delta u^n),\\\\varphi_i\\\\right>\\\\) when determining optimal step lengths, and so this is what we implement here: the function takes the step length \\\\(\\\\alpha^n\\\\) as an argument. The original functionality is of course obtained by passing a zero as argument.\\nIn the function below, we first set up a vector for the residual, and then a vector for the evaluation point \\\\(u^n+\\\\alpha^n\\\\;\\\\delta u^n\\\\). This is followed by the same boilerplate code we use for all integration operations:\\n\\u00a0   template <int dim>\\n\\u00a0   double MinimalSurfaceProblem<dim>::compute_residual(const double alpha) const\\n\\u00a0   {\\n\\u00a0     Vector<double> residual(dof_handler.n_dofs());\\n\\u00a0 \\n\\u00a0     Vector<double> evaluation_point(dof_handler.n_dofs());\\n\\u00a0     evaluation_point = current_solution;\\n\\u00a0     evaluation_point.add(alpha, newton_update);\\n\\u00a0 \\n\\u00a0     const QGauss<dim> quadrature_formula(fe.degree + 1);\\n\\u00a0     FEValues<dim>     fe_values(fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_gradients | update_quadrature_points |\\n\\u00a0                               update_JxW_values);\\n\\u00a0 \\n\\u00a0     const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     Vector<double>              cell_residual(dofs_per_cell);\\n\\u00a0     std::vector<Tensor<1, dim>> gradients(n_q_points);\\n\\u00a0 \\n\\u00a0     std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       {\\n\\u00a0         cell_residual = 0;\\n\\u00a0         fe_values.reinit(cell);\\n\\u00a0 \\nThe actual computation is much as in assemble_system(). We first evaluate the gradients of \\\\(u^n+\\\\alpha^n\\\\,\\\\delta u^n\\\\) at the quadrature points, then compute the coefficient \\\\(a_n\\\\), and then plug it all into the formula for the residual:\\n\\u00a0         fe_values.get_function_gradients(evaluation_point, gradients);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0         for (unsigned int q = 0; q < n_q_points; ++q)\\n\\u00a0           {\\n\\u00a0             const double coeff =\\n\\u00a0               1. / std::sqrt(1 + gradients[q] * gradients[q]);\\n\\u00a0 \\n\\u00a0             for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0               cell_residual(i) -= (fe_values.shape_grad(i, q) // \\\\nabla \\\\phi_i\\n\\u00a0                                    * coeff                    // * a_n\\n\\u00a0                                    * gradients[q]             // * \\\\nabla u_n\\n\\u00a0                                    * fe_values.JxW(q));       // * dx\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         cell->get_dof_indices(local_dof_indices);\\n\\u00a0         zero_constraints.distribute_local_to_global(cell_residual,\\n\\u00a0                                                     local_dof_indices,\\n\\u00a0                                                     residual);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     return residual.l2_norm();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n MinimalSurfaceProblem::determine_step_length\\nAs discussed in the introduction, Newton's method frequently does not converge if we always take full steps, i.e., compute  \\\\(u^{n+1}=u^n+\\\\delta\\n   u^n\\\\). Rather, one needs a damping parameter (step length) \\\\(\\\\alpha^n\\\\) and set \\\\(u^{n+1}=u^n+\\\\alpha^n\\\\delta u^n\\\\). This function is the one called to compute \\\\(\\\\alpha^n\\\\).\\nHere, we simply always return 0.1. This is of course a sub-optimal choice: ideally, what one wants is that the step size goes to one as we get closer to the solution, so that we get to enjoy the rapid quadratic convergence of Newton's method. We will discuss better strategies below in the results section, and step-77 also covers this aspect.\\n\\u00a0   template <int dim>\\n\\u00a0   double MinimalSurfaceProblem<dim>::determine_step_length() const\\n\\u00a0   {\\n\\u00a0     return 0.1;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n MinimalSurfaceProblem::output_results\\nThis last function to be called from run() outputs the current solution (and the Newton update) in graphical form as a VTU file. It is entirely the same as what has been used in previous tutorials.\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::output_results(\\n\\u00a0     const unsigned int refinement_cycle) const\\n\\u00a0   {\\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0 \\n\\u00a0     data_out.attach_dof_handler(dof_handler);\\n\\u00a0     data_out.add_data_vector(current_solution, \\\"solution\\\");\\n\\u00a0     data_out.add_data_vector(newton_update, \\\"update\\\");\\n\\u00a0     data_out.build_patches();\\n\\u00a0 \\n\\u00a0     const std::string filename =\\n\\u00a0       \\\"solution-\\\" + Utilities::int_to_string(refinement_cycle, 2) + \\\".vtu\\\";\\n\\u00a0     std::ofstream output(filename);\\n\\u00a0     data_out.write_vtu(output);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOutDefinition data_out.h:147\\nUtilities::int_to_stringstd::string int_to_string(const unsigned int value, const unsigned int digits=numbers::invalid_unsigned_int)Definition utilities.cc:470\\n MinimalSurfaceProblem::run\\nIn the run function, we build the first grid and then have the top-level logic for the Newton iteration.\\nAs described in the introduction, the domain is the unit disk around the origin, created in the same way as shown in step-6. The mesh is globally refined twice followed later on by several adaptive cycles.\\nBefore starting the Newton loop, we also need to do ensure that the first Newton iterate already has the correct boundary values, as discussed in the introduction.\\n\\u00a0   template <int dim>\\n\\u00a0   void MinimalSurfaceProblem<dim>::run()\\n\\u00a0   {\\n\\u00a0     GridGenerator::hyper_ball(triangulation);\\n\\u00a0     triangulation.refine_global(2);\\n\\u00a0 \\n\\u00a0     setup_system();\\n\\u00a0     nonzero_constraints.distribute(current_solution);\\n\\u00a0 \\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nGridGenerator::hyper_ballvoid hyper_ball(Triangulation< dim > &tria, const Point< dim > &center=Point< dim >(), const double radius=1., const bool attach_spherical_manifold_on_boundary_cells=false)\\nThe Newton iteration starts next. We iterate until the (norm of the) residual computed at the end of the previous iteration is less than \\\\(10^{-3}\\\\), as checked at the end of the do { ... } while loop that starts here. Because we don't have a reasonable value to initialize the variable, we just use the largest value that can be represented as a double.\\n\\u00a0     double       last_residual_norm = std::numeric_limits<double>::max();\\n\\u00a0     unsigned int refinement_cycle   = 0;\\n\\u00a0     do\\n\\u00a0       {\\n\\u00a0         std::cout << \\\"Mesh refinement step \\\" << refinement_cycle << std::endl;\\n\\u00a0 \\n\\u00a0         if (refinement_cycle != 0)\\n\\u00a0           refine_mesh();\\n\\u00a0 \\nOn every mesh we do exactly five Newton steps. We print the initial residual here and then start the iterations on this mesh.\\nIn every Newton step the system matrix and the right hand side have to be computed first, after which we store the norm of the right hand side as the residual to check against when deciding whether to stop the iterations. We then solve the linear system (the function also updates \\\\(u^{n+1}=u^n+\\\\alpha^n\\\\;\\\\delta u^n\\\\)) and output the norm of the residual at the end of this Newton step.\\nAfter the end of this loop, we then also output the solution on the current mesh in graphical form and increment the counter for the mesh refinement cycle.\\n\\u00a0         std::cout << \\\"  Initial residual: \\\" << compute_residual(0) << std::endl;\\n\\u00a0 \\n\\u00a0         for (unsigned int inner_iteration = 0; inner_iteration < 5;\\n\\u00a0              ++inner_iteration)\\n\\u00a0           {\\n\\u00a0             assemble_system();\\n\\u00a0             last_residual_norm = system_rhs.l2_norm();\\n\\u00a0 \\n\\u00a0             solve();\\n\\u00a0 \\n\\u00a0             std::cout << \\\"  Residual: \\\" << compute_residual(0) << std::endl;\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         output_results(refinement_cycle);\\n\\u00a0 \\n\\u00a0         ++refinement_cycle;\\n\\u00a0         std::cout << std::endl;\\n\\u00a0       }\\n\\u00a0     while (last_residual_norm > 1e-2);\\n\\u00a0   }\\n\\u00a0 } // namespace Step15\\n\\u00a0 \\n The main function\\nFinally the main function. This follows the scheme of all other main functions:\\n\\u00a0 int main()\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       using namespace Step15;\\n\\u00a0 \\n\\u00a0       MinimalSurfaceProblem<2> problem;\\n\\u00a0       problem.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0 \\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   return 0;\\n\\u00a0 }\\n Results\\nThe output of the program looks as follows: Mesh refinement step 0\\n  Initial residual: 1.53143\\n  Residual: 1.08746\\n  Residual: 0.966748\\n  Residual: 0.859602\\n  Residual: 0.766462\\n  Residual: 0.685475\\n \\nMesh refinement step 1\\n  Initial residual: 0.868959\\n  Residual: 0.762125\\n  Residual: 0.677792\\n  Residual: 0.605762\\n  Residual: 0.542748\\n  Residual: 0.48704\\n \\nMesh refinement step 2\\n  Initial residual: 0.426445\\n  Residual: 0.382731\\n  Residual: 0.343865\\n  Residual: 0.30918\\n  Residual: 0.278147\\n  Residual: 0.250327\\n \\nMesh refinement step 3\\n  Initial residual: 0.282026\\n  Residual: 0.253146\\n  Residual: 0.227414\\n  Residual: 0.20441\\n  Residual: 0.183803\\n  Residual: 0.165319\\n \\nMesh refinement step 4\\n  Initial residual: 0.154404\\n  Residual: 0.138723\\n  Residual: 0.124694\\n  Residual: 0.112124\\n  Residual: 0.100847\\n  Residual: 0.0907222\\n \\n....\\nObviously, the scheme converges, if not very fast. We will come back to strategies for accelerating the method below.\\nOne can visualize the solution after each set of five Newton iterations, i.e., on each of the meshes on which we approximate the solution. This yields the following set of images:\\n             It is clearly visible, that the solution minimizes the surface after each refinement. The solution converges to a picture one would imagine a soap bubble to be that is located inside a wire loop that is bent like the boundary. Also it is visible, how the boundary is smoothed out after each refinement. On the coarse mesh, the boundary doesn't look like a sine, whereas it does the finer the mesh gets.\\nThe mesh is mostly refined near the boundary, where the solution increases or decreases strongly, whereas it is coarsened on the inside of the domain, where nothing interesting happens, because there isn't much change in the solution. The ninth solution and mesh are shown here:\\n     Possibilities for extensions\\nThe program shows the basic structure of a solver for a nonlinear, stationary problem. However, it does not converge particularly fast, for good reasons:\\n\\nThe program always takes a step size of 0.1. This precludes the rapid, quadratic convergence for which Newton's method is typically chosen.\\nIt does not connect the nonlinear iteration with the mesh refinement iteration.\\n\\nObviously, a better program would have to address these two points. We will discuss them in the following.\\nStep length control \\nNewton's method has two well known properties:\\nIt may not converge from arbitrarily chosen starting points. Rather, a starting point has to be close enough to the solution to guarantee convergence. However, we can enlarge the area from which Newton's method converges by damping the iteration using a step length 0<  \\\\(\\\\alpha^n\\\\le\\n  1\\\\).\\nIt exhibits rapid convergence of quadratic order if (i) the step length is chosen as \\\\(\\\\alpha^n=1\\\\), and (ii) it does in fact converge with this choice of step length.\\n\\nA consequence of these two observations is that a successful strategy is to choose \\\\(\\\\alpha^n<1\\\\) for the initial iterations until the iterate has come close enough to allow for convergence with full step length, at which point we want to switch to \\\\(\\\\alpha^n=1\\\\). The question is how to choose \\\\(\\\\alpha^n\\\\) in an automatic fashion that satisfies these criteria.\\nWe do not want to review the literature on this topic here, but only briefly mention that there are two fundamental approaches to the problem: backtracking line search and trust region methods. The former is more widely used for partial differential equations and essentially does the following:\\nCompute a search direction\\nSee if the resulting residual of \\\\(u^n + \\\\alpha^n\\\\;\\\\delta u^n\\\\) with \\\\(\\\\alpha^n=1\\\\) is \\\"substantially smaller\\\" than that of \\\\(u^n\\\\) alone.\\nIf so, then take \\\\(\\\\alpha^n=1\\\\).\\nIf not, try whether the residual is \\\"substantially smaller\\\" with \\\\(\\\\alpha^n=2/3\\\\).\\nIf so, then take \\\\(\\\\alpha^n=2/3\\\\).\\nIf not, try whether the residual is \\\"substantially smaller\\\" with \\\\(\\\\alpha^n=(2/3)^2\\\\).\\nEtc. One can of course choose other factors \\\\(r, r^2, \\\\ldots\\\\) than the  \\\\(2/3,\\n(2/3)^2, \\\\ldots\\\\) chosen above, for \\\\(0<r<1\\\\). It is obvious where the term \\\"backtracking\\\" comes from: we try a long step, but if that doesn't work we try a shorter step, and ever shorter step, etc. The function determine_step_length() is written the way it is to support exactly this kind of use case.\\n\\nWhether we accept a particular step length \\\\(\\\\alpha^n\\\\) depends on how we define \\\"substantially smaller\\\". There are a number of ways to do so, but without going into detail let us just mention that the most common ones are to use the Wolfe and Armijo-Goldstein conditions. For these, one can show the following:\\nThere is always a step length \\\\(\\\\alpha^n\\\\) for which the conditions are satisfied, i.e., the iteration never gets stuck as long as the problem is convex.\\nIf we are close enough to the solution, then the conditions allow for \\\\(\\\\alpha^n=1\\\\), thereby enabling quadratic convergence.\\n\\nWe will not dwell on this here any further but leave the implementation of such algorithms as an exercise. We note, however, that when implemented correctly then it is a common observation that most reasonably nonlinear problems can be solved in anywhere between 5 and 15 Newton iterations to engineering accuracy \\u2014 substantially fewer than we need with the current version of the program.\\nMore details on globalization methods including backtracking can be found, for example, in [103] and [164].\\nA separate point, very much worthwhile making, however, is that in practice the implementation of efficient nonlinear solvers is about as complicated as the implementation of efficient finite element methods. One should not attempt to reinvent the wheel by implementing all of the necessary steps oneself. Substantial pieces of the puzzle are already available in the LineMinimization::line_search() function and could be used to this end. But, instead, just like building finite element solvers on libraries such as deal.II, one should be building nonlinear solvers on libraries such as SUNDIALS. In fact, deal.II has interfaces to SUNDIALS and in particular to its nonlinear solver sub-package KINSOL through the SUNDIALS::KINSOL class. It would not be very difficult to base the current problem on that interface \\u2013 indeed, that is what step-77 does.\\nIntegrating mesh refinement and nonlinear and linear solvers \\nWe currently do exactly 5 iterations on each mesh. But is this optimal? One could ask the following questions:\\nMaybe it is worthwhile doing more iterations on the initial meshes since there, computations are cheap.\\nOn the other hand, we do not want to do too many iterations on every mesh: yes, we could drive the residual to zero on every mesh, but that would only mean that the nonlinear iteration error is far smaller than the discretization error.\\nShould we use solve the linear systems in each Newton step with higher or lower accuracy?\\n\\nUltimately, what this boils down to is that we somehow need to couple the discretization error on the current mesh with the nonlinear residual we want to achieve with the Newton iterations on a given mesh, and to the linear iteration we want to achieve with the CG method within each Newton iterations.\\nHow to do this is, again, not entirely trivial, and we again leave it as a future exercise.\\nUsing automatic differentiation to compute the Jacobian matrix \\nAs outlined in the introduction, when solving a nonlinear problem of the form     \\n\\\\[\\n    F(u) \\\\dealcoloneq\\n    -\\\\nabla \\\\cdot \\\\left( \\\\frac{1}{\\\\sqrt{1+|\\\\nabla u|^{2}}}\\\\nabla u \\\\right)\\n    = 0\\n  \\\\]\\n\\n we use a Newton iteration that requires us to repeatedly solve the linear partial differential equation   \\n\\\\begin{align*}\\n    F'(u^{n},\\\\delta u^{n}) &=- F(u^{n})\\n  \\\\end{align*}\\n\\n so that we can compute the update   \\n\\\\begin{align*}\\n    u^{n+1}&=u^{n}+\\\\alpha^n \\\\delta u^{n}\\n  \\\\end{align*}\\n\\n with the solution \\\\(\\\\delta u^{n}\\\\) of the Newton step. For the problem here, we could compute the derivative \\\\(F'(u,\\\\delta u)\\\\) by hand and obtained         \\n\\\\[\\n  F'(u,\\\\delta u)\\n  =\\n  - \\\\nabla \\\\cdot \\\\left( \\\\frac{1}{\\\\left(1+|\\\\nabla u|^{2}\\\\right)^{\\\\frac{1}{2}}}\\\\nabla\\n  \\\\delta u \\\\right) +\\n  \\\\nabla \\\\cdot \\\\left( \\\\frac{\\\\nabla u \\\\cdot\\n  \\\\nabla \\\\delta u}{\\\\left(1+|\\\\nabla u|^{2}\\\\right)^{\\\\frac{3}{2}}} \\\\nabla u\\n  \\\\right).\\n  \\\\]\\n\\n But this is already a sizable expression that is cumbersome both to derive and to implement. It is also, in some sense, duplicative: If we implement what \\\\(F(u)\\\\) is somewhere in the code, then \\\\(F'(u,\\\\delta u)\\\\) is not an independent piece of information but is something that, at least in principle, a computer should be able to infer itself. Wouldn't it be nice if that could actually happen? That is, if we really only had to implement \\\\(F(u)\\\\), and \\\\(F'(u,\\\\delta u)\\\\) was then somehow done implicitly? That is in fact possible, and runs under the name \\\"automatic differentiation\\\". step-71 discusses this very concept in general terms, and step-72 illustrates how this can be applied in practice for the very problem we are considering here.\\nStoring the Jacobian matrix in lower-precision floating point variables \\nOn modern computer systems, accessing data in main memory takes far longer than actually doing something with it: We can do many floating point operations for the time it takes to load one floating point number from memory onto the processor. Unfortunately, when we do things such as matrix-vector products, we only multiply each matrix entry once with another number (the corresponding entry of the vector) and then we add it to something else \\u2013 so two floating point operations for one load. (Strictly speaking, we also have to load the corresponding vector entry, but at least sometimes we get to re-use that vector entry in doing the products that correspond to the next row of the matrix.) This is a fairly low \\\"arithmetic intensity\\\", and consequently we spend most of our time during matrix-vector products waiting for data to arrive from memory rather than actually doing floating point operations.\\nThis is of course one of the rationales for the \\\"matrix-free\\\" approach to solving linear systems (see step-37, for example). But if you don't quite want to go all that way to change the structure of the program, then here is a different approach: Storing the system matrix (the \\\"Jacobian\\\") in single-precision instead of double precision floating point numbers (i.e., using float instead of double as the data type). This reduces the amount of memory necessary by a factor of 1.5 (each matrix entry in a SparseMatrix object requires storing the column index \\u2013 4 bytes \\u2013 and the actual value \\u2013 either 4 or 8 bytes), and consequently will speed up matrix-vector products by a factor of around 1.5 as well because, as pointed out above, most of the time is spent loading data from memory and loading 2/3 the amount of data should be roughly 3/2 times as fast. All of this could be done using SparseMatrix<float> as the data type for the system matrix. (In principle, we would then also like it if the SparseDirectUMFPACK solver we use in this program computes and stores its sparse decomposition in float arithmetic. This is not currently implemented, though could be done.)\\nOf course, there is a downside to this: Lower precision data storage also implies that we will not solve the linear system of the Newton step as accurately as we might with double precision. At least while we are far away from the solution of the nonlinear problem, this may not be terrible: If we can do a Newton iteration in half the time, we can afford to do a couple more Newton steps if the search directions aren't as good. But it turns out that even that isn't typically necessary: Both theory and computational experience shows that it is entirely sufficient to store the Jacobian matrix in single precision as long as one stores the right hand side in double precision*. A great overview of why this is so, along with numerical experiments that also consider \\\"half precision\\\" floating point numbers, can be found in [127] .\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2012 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Author: Sven Wetterauer, University of Heidelberg, 2012\\n */\\n \\n \\n \\n#include <deal.II/base/quadrature_lib.h>\\n#include <deal.II/base/function.h>\\n#include <deal.II/base/utilities.h>\\n \\n#include <deal.II/lac/vector.h>\\n#include <deal.II/lac/full_matrix.h>\\n#include <deal.II/lac/sparse_matrix.h>\\n#include <deal.II/lac/dynamic_sparsity_pattern.h>\\n#include <deal.II/lac/solver_cg.h>\\n#include <deal.II/lac/precondition.h>\\n#include <deal.II/lac/affine_constraints.h>\\n \\n#include <deal.II/grid/tria.h>\\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/grid_refinement.h>\\n \\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/dofs/dof_tools.h>\\n \\n#include <deal.II/fe/fe_values.h>\\n#include <deal.II/fe/fe_q.h>\\n \\n#include <deal.II/numerics/vector_tools.h>\\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/error_estimator.h>\\n \\n \\n#include <fstream>\\n#include <iostream>\\n \\n \\n#include <deal.II/numerics/solution_transfer.h>\\n \\nnamespace Step15\\n{\\n using namespace dealii;\\n \\n \\n \\n \\n template <int dim>\\n class MinimalSurfaceProblem\\n  {\\n public:\\n    MinimalSurfaceProblem();\\n void run();\\n \\n private:\\n void   setup_system();\\n void   assemble_system();\\n void   solve();\\n void   refine_mesh();\\n double compute_residual(const double alpha) const;\\n double determine_step_length() const;\\n void   output_results(const unsigned int refinement_cycle) const;\\n \\n Triangulation<dim> triangulation;\\n \\n DoFHandler<dim> dof_handler;\\n const FE_Q<dim> fe;\\n \\n AffineConstraints<double> zero_constraints;\\n AffineConstraints<double> nonzero_constraints;\\n \\n SparsityPattern      sparsity_pattern;\\n SparseMatrix<double> system_matrix;\\n \\n Vector<double> current_solution;\\n Vector<double> newton_update;\\n Vector<double> system_rhs;\\n  };\\n \\n \\n \\n template <int dim>\\n class BoundaryValues : public Function<dim>\\n  {\\n public:\\n virtual double value(const Point<dim>  &p,\\n const unsigned int component = 0) const override;\\n  };\\n \\n \\n template <int dim>\\n double BoundaryValues<dim>::value(const Point<dim> &p,\\n const unsigned int /*component*/) const\\n {\\n return std::sin(2 * numbers::PI * (p[0] + p[1]));\\n  }\\n \\n \\n \\n \\n template <int dim>\\n  MinimalSurfaceProblem<dim>::MinimalSurfaceProblem()\\n    : dof_handler(triangulation)\\n    , fe(2)\\n  {}\\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::setup_system()\\n  {\\n    dof_handler.distribute_dofs(fe);\\n    current_solution.reinit(dof_handler.n_dofs());\\n \\n    zero_constraints.clear();\\n VectorTools::interpolate_boundary_values(dof_handler,\\n                                             0,\\n Functions::ZeroFunction<dim>(),\\n                                             zero_constraints);\\n DoFTools::make_hanging_node_constraints(dof_handler, zero_constraints);\\n    zero_constraints.close();\\n \\n    nonzero_constraints.clear();\\n VectorTools::interpolate_boundary_values(dof_handler,\\n                                             0,\\n                                             BoundaryValues<dim>(),\\n                                             nonzero_constraints);\\n \\n DoFTools::make_hanging_node_constraints(dof_handler, nonzero_constraints);\\n    nonzero_constraints.close();\\n \\n    newton_update.reinit(dof_handler.n_dofs());\\n    system_rhs.reinit(dof_handler.n_dofs());\\n \\n DynamicSparsityPattern dsp(dof_handler.n_dofs());\\n DoFTools::make_sparsity_pattern(dof_handler, dsp, zero_constraints);\\n \\n    sparsity_pattern.copy_from(dsp);\\n    system_matrix.reinit(sparsity_pattern);\\n  }\\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::assemble_system()\\n  {\\n const QGauss<dim> quadrature_formula(fe.degree + 1);\\n \\n    system_matrix = 0;\\n    system_rhs    = 0;\\n \\n FEValues<dim> fe_values(fe,\\n                            quadrature_formula,\\n update_gradients | update_quadrature_points |\\n update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n Vector<double>     cell_rhs(dofs_per_cell);\\n \\n    std::vector<Tensor<1, dim>> old_solution_gradients(n_q_points);\\n \\n    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n \\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      {\\n cell_matrix = 0;\\n        cell_rhs    = 0;\\n \\n        fe_values.reinit(cell);\\n \\n        fe_values.get_function_gradients(current_solution,\\n                                         old_solution_gradients);\\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n          {\\n const double coeff =\\n              1.0 / std::sqrt(1 + old_solution_gradients[q] *\\n                                    old_solution_gradients[q]);\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n              {\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n cell_matrix(i, j) +=\\n                    (((fe_values.shape_grad(i, q)      // ((\\\\nabla \\\\phi_i\\n                       * coeff                         //   * a_n\\n                       * fe_values.shape_grad(j, q))   //   * \\\\nabla \\\\phi_j)\\n                      -                                //  -\\n                      (fe_values.shape_grad(i, q)      //  (\\\\nabla \\\\phi_i\\n                       * coeff * coeff * coeff         //   * a_n^3\\n                       * (fe_values.shape_grad(j, q)   //   * (\\\\nabla \\\\phi_j\\n                          * old_solution_gradients[q]) //      * \\\\nabla u_n)\\n                       * old_solution_gradients[q]))   //   * \\\\nabla u_n)))\\n                     * fe_values.JxW(q));              // * dx\\n \\n                cell_rhs(i) -= (fe_values.shape_grad(i, q)  // \\\\nabla \\\\phi_i\\n                                * coeff                     // * a_n\\n                                * old_solution_gradients[q] // * \\\\nabla u_n\\n                                * fe_values.JxW(q));        // * dx\\n              }\\n          }\\n \\n        cell->get_dof_indices(local_dof_indices);\\n        zero_constraints.distribute_local_to_global(\\n          cell_matrix, cell_rhs, local_dof_indices, system_matrix, system_rhs);\\n      }\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::solve()\\n  {\\n SolverControl            solver_control(system_rhs.size(),\\n                                 system_rhs.l2_norm() * 1e-6);\\n SolverCG<Vector<double>> solver(solver_control);\\n \\n PreconditionSSOR<SparseMatrix<double>> preconditioner;\\n    preconditioner.initialize(system_matrix, 1.2);\\n \\n    solver.solve(system_matrix, newton_update, system_rhs, preconditioner);\\n \\n    zero_constraints.distribute(newton_update);\\n \\n const double alpha = determine_step_length();\\n    current_solution.add(alpha, newton_update);\\n  }\\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::refine_mesh()\\n  {\\n Vector<float> estimated_error_per_cell(triangulation.n_active_cells());\\n \\n KellyErrorEstimator<dim>::estimate(\\n      dof_handler,\\n QGauss<dim - 1>(fe.degree + 1),\\n      std::map<types::boundary_id, const Function<dim> *>(),\\n      current_solution,\\n      estimated_error_per_cell);\\n \\n GridRefinement::refine_and_coarsen_fixed_number(triangulation,\\n                                                    estimated_error_per_cell,\\n                                                    0.3,\\n                                                    0.03);\\n \\n triangulation.prepare_coarsening_and_refinement();\\n \\n SolutionTransfer<dim> solution_transfer(dof_handler);\\n const Vector<double>  coarse_solution = current_solution;\\n    solution_transfer.prepare_for_coarsening_and_refinement(coarse_solution);\\n \\n triangulation.execute_coarsening_and_refinement();\\n \\n    setup_system();\\n \\n    solution_transfer.interpolate(coarse_solution, current_solution);\\n \\n    nonzero_constraints.distribute(current_solution);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n double MinimalSurfaceProblem<dim>::compute_residual(const double alpha) const\\n {\\n Vector<double> residual(dof_handler.n_dofs());\\n \\n Vector<double> evaluation_point(dof_handler.n_dofs());\\n    evaluation_point = current_solution;\\n    evaluation_point.add(alpha, newton_update);\\n \\n const QGauss<dim> quadrature_formula(fe.degree + 1);\\n FEValues<dim>     fe_values(fe,\\n                            quadrature_formula,\\n update_gradients | update_quadrature_points |\\n update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n Vector<double> cell_residual(dofs_per_cell);\\n    std::vector<Tensor<1, dim>> gradients(n_q_points);\\n \\n    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n \\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      {\\n cell_residual = 0;\\n        fe_values.reinit(cell);\\n \\n        fe_values.get_function_gradients(evaluation_point, gradients);\\n \\n \\n for (unsigned int q = 0; q < n_q_points; ++q)\\n          {\\n const double coeff =\\n              1. / std::sqrt(1 + gradients[q] * gradients[q]);\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n cell_residual(i) -= (fe_values.shape_grad(i, q) // \\\\nabla \\\\phi_i\\n                                   * coeff                    // * a_n\\n                                   * gradients[q]             // * \\\\nabla u_n\\n                                   * fe_values.JxW(q));       // * dx\\n          }\\n \\n        cell->get_dof_indices(local_dof_indices);\\n        zero_constraints.distribute_local_to_global(cell_residual,\\n                                                    local_dof_indices,\\n                                                    residual);\\n      }\\n \\n return residual.l2_norm();\\n  }\\n \\n \\n \\n \\n template <int dim>\\n double MinimalSurfaceProblem<dim>::determine_step_length() const\\n {\\n return 0.1;\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::output_results(\\n const unsigned int refinement_cycle) const\\n {\\n DataOut<dim> data_out;\\n \\n    data_out.attach_dof_handler(dof_handler);\\n    data_out.add_data_vector(current_solution, \\\"solution\\\");\\n    data_out.add_data_vector(newton_update, \\\"update\\\");\\n    data_out.build_patches();\\n \\n const std::string filename =\\n \\\"solution-\\\" + Utilities::int_to_string(refinement_cycle, 2) + \\\".vtu\\\";\\n    std::ofstream output(filename);\\n    data_out.write_vtu(output);\\n  }\\n \\n \\n \\n template <int dim>\\n void MinimalSurfaceProblem<dim>::run()\\n  {\\n GridGenerator::hyper_ball(triangulation);\\n triangulation.refine_global(2);\\n \\n    setup_system();\\n    nonzero_constraints.distribute(current_solution);\\n \\n double       last_residual_norm = std::numeric_limits<double>::max();\\n unsigned int refinement_cycle   = 0;\\n do\\n      {\\n        std::cout << \\\"Mesh refinement step \\\" << refinement_cycle << std::endl;\\n \\n if (refinement_cycle != 0)\\n          refine_mesh();\\n \\n        std::cout << \\\"  Initial residual: \\\" << compute_residual(0) << std::endl;\\n \\n for (unsigned int inner_iteration = 0; inner_iteration < 5;\\n             ++inner_iteration)\\n          {\\n            assemble_system();\\n            last_residual_norm = system_rhs.l2_norm();\\n \\n            solve();\\n \\n            std::cout << \\\"  Residual: \\\" << compute_residual(0) << std::endl;\\n          }\\n \\n        output_results(refinement_cycle);\\n \\n        ++refinement_cycle;\\n        std::cout << std::endl;\\n      }\\n while (last_residual_norm > 1e-2);\\n  }\\n} // namespace Step15\\n \\n \\nint main()\\n{\\n try\\n    {\\n using namespace Step15;\\n \\n      MinimalSurfaceProblem<2> problem;\\n      problem.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n \\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n return 0;\\n}\\naffine_constraints.h\\nDataOutInterface::write_vtuvoid write_vtu(std::ostream &out) constDefinition data_out_base.cc:7692\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\ndof_handler.h\\ndof_tools.h\\ndynamic_sparsity_pattern.h\\nerror_estimator.h\\nfe_values.h\\nfe_q.h\\nfull_matrix.h\\nfunction.h\\ngrid_refinement.h\\ntria.h\\ngrid_generator.h\\nutilities.h\\nEvaluationFlags::gradients@ gradientsDefinition evaluation_flags.h:54\\nLocalIntegrators::Advection::cell_matrixvoid cell_matrix(FullMatrix< double > &M, const FEValuesBase< dim > &fe, const FEValuesBase< dim > &fetest, const ArrayView< const std::vector< double > > &velocity, const double factor=1.)Definition advection.h:74\\nLocalIntegrators::Advection::cell_residualvoid cell_residual(Vector< double > &result, const FEValuesBase< dim > &fe, const std::vector< Tensor< 1, dim > > &input, const ArrayView< const std::vector< double > > &velocity, double factor=1.)Definition advection.h:130\\ndata_out.h\\nsolution_transfer.h\\nprecondition.h\\nquadrature_lib.h\\nsolver_cg.h\\nsparse_matrix.h\\nvector.h\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"