"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_59.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-59 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-59 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-59 tutorial program\\n\\n\\nThis tutorial depends on step-37.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\nThe symmetric interior penalty formulation for the Laplacian\\nFace integration support in MatrixFree and FEFaceEvaluation\\nThe FE_DGQHermite element\\nAn approximate block-Jacobi smoother using the fast diagonalization method\\n\\n The commented program\\n\\nEquation data\\nMatrix-free implementation\\n\\n\\n Results\\n\\nProgram output\\nComparison of efficiency at different polynomial degrees\\nEvaluation of efficiency of ingredients\\nPossibilities for extension\\n\\n The plain program\\n   \\n\\n\\n This program was contributed by Katharina Kormann and Martin Kronbichler.\\nThis work was partly supported by the German Research Foundation (DFG) through the project \\\"High-order discontinuous Galerkin for the exa-scale\\\" (ExaDG) within the priority program \\\"Software for Exascale Computing\\\" (SPPEXA). \\n Introduction\\nMatrix-free operator evaluation enables very efficient implementations of discretization with high-order polynomial bases due to a method called sum factorization. This concept has been introduced in the step-37 and step-48 tutorial programs. In this tutorial program, we extend those concepts to discontinuous Galerkin (DG) schemes that include face integrals, a class of methods where high orders are particularly widespread.\\nThe underlying idea of the matrix-free evaluation is the same as for continuous elements: The matrix-vector product that appears in an iterative solver or multigrid smoother is not implemented by a classical sparse matrix kernel, but instead applied implicitly by the evaluation of the underlying integrals on the fly. For tensor product shape functions that are integrated with a tensor product quadrature rule, this evaluation is particularly efficient by using the sum-factorization technique, which decomposes the initially \\\\((k+1)^{2d}\\\\) operations for interpolation involving \\\\((k+1)^d\\\\) vector entries with associated shape functions at degree \\\\(k\\\\) in \\\\(d\\\\) dimensions to \\\\((k+1)^d\\\\) quadrature points into \\\\(d\\\\) one-dimensional operations of cost \\\\((k+1)^{d+1}\\\\) each. In 3D, this reduces the order of complexity by two powers in \\\\(k\\\\). When measured as the complexity per degree of freedom, the complexity is \\\\(\\\\mathcal O(k)\\\\) in the polynomial degree. Due to the presence of face integrals in DG, and due to the fact that operations on quadrature points involve more memory transfer, which both scale as \\\\(\\\\mathcal O(1)\\\\), the observed complexity is often constant for moderate \\\\(k\\\\leq 10\\\\). This means that a high order method can be evaluated with the same throughput in terms of degrees of freedom per second as a low-order method.\\nMore information on the algorithms are available in the preprint \\nFast matrix-free evaluation of discontinuous Galerkin finite element operators by Martin Kronbichler and Katharina Kormann, arXiv:1711.03590.\\nThe symmetric interior penalty formulation for the Laplacian\\nFor this tutorial program, we exemplify the matrix-free DG framework for the interior penalty discretization of the Laplacian, i.e., the same scheme as the one used for the step-39 tutorial program. The discretization of the Laplacian is given by the following weak form     \\n\\\\begin{align*}\\n&\\\\sum_{K\\\\in\\\\text{cells}} \\\\left(\\\\nabla v_h, \\\\nabla u_h\\\\right)_{K}+\\\\\\\\\\n&\\\\sum_{F\\\\in\\\\text{faces}}\\\\Big(-\\\\left<\\\\jump{v_h}, \\\\average{\\\\nabla u_h}\\\\right>_{F} - \\\\left<\\\\average{\\\\nabla v_h}, \\\\jump{u_h}\\\\right>_{F} + \\\\left<\\\\jump{v_h}, \\\\sigma \\\\jump{u_h}\\\\right>_{F}\\\\Big) \\\\\\\\\\n&= \\\\sum_{K\\\\in\\\\text{cells}}\\\\left(v_h, f\\\\right)_{K},\\n\\\\end{align*}\\n\\n where  \\\\(\\\\jump{v} = v^- \\\\mathbf{n}^- + v^+ \\\\mathbf{n}^+ = \\\\mathbf n^{-}\\n\\\\left(v^- - v^+\\\\right)\\\\) denotes the directed jump of the quantity \\\\(v\\\\) from the two associated cells \\\\(K^-\\\\) and \\\\(K^+\\\\), and \\\\(\\\\average{v}=\\\\frac{v^- + v^+}{2}\\\\) is the average from both sides.\\nThe terms in the equation represent the cell integral after integration by parts, the primal consistency term that arises at the element interfaces due to integration by parts and insertion of an average flux, the adjoint consistency term that is added for restoring symmetry of the underlying matrix, and a penalty term with factor \\\\(\\\\sigma\\\\), whose magnitude is equal the length of the cells in direction normal to face multiplied by \\\\(k(k+1)\\\\), see step-39. The penalty term is chosen such that an inverse estimate holds and the final weak form is coercive, i.e., positive definite in the discrete setting. The adjoint consistency term and the penalty term involve the jump \\\\(\\\\jump{u_h}\\\\) at the element interfaces, which disappears for the analytic solution \\\\(u\\\\). Thus, these terms are consistent with the original PDE, ensuring that the method can retain optimal orders of convergence.\\nIn the implementation below, we implement the weak form above by moving the normal vector \\\\(\\\\mathbf{n}^-\\\\) from the jump terms to the derivatives to form a normal derivative of the form \\\\(\\\\mathbf{n}^-\\\\cdot \\\\nabla u_h\\\\). This makes the implementation on quadrature points slightly more efficient because we only need to work with scalar terms rather than tensors, and is mathematically equivalent.\\nFor boundary conditions, we use the so-called mirror principle that defines artificial exterior values \\\\(u^+\\\\) by extrapolation from the interior solution \\\\(u^-\\\\) combined with the given boundary data, setting  \\\\(u^+ = -u^- + 2\\ng_\\\\text{D}\\\\) and \\\\(\\\\mathbf{n}^-\\\\cdot \\\\nabla u^+ = \\\\mathbf{n}^-\\\\cdot \\\\nabla u^-\\\\) on Dirichlet boundaries and \\\\(u^+=u^-\\\\) and  \\\\(\\\\mathbf{n}^-\\\\cdot \\\\nabla u^+ =\\n-\\\\mathbf{n}^-\\\\cdot \\\\nabla u^- + 2 g_\\\\text{N}\\\\) on Neumann boundaries, for given Dirichlet values \\\\(g_\\\\text{D}\\\\) and Neumann values \\\\(g_\\\\text{N}\\\\). These expressions are then inserted in the above weak form. Contributions involving the known quantities \\\\(g_\\\\text{D}\\\\) and \\\\(g_\\\\text{N}\\\\) are eventually moved to the right hand side, whereas the unknown value \\\\(u^-\\\\) is retained on the left hand side and contributes to the matrix terms similarly as interior faces. Upon these manipulations, the same weak form as in step-39 is obtained.\\nFace integration support in MatrixFree and FEFaceEvaluation\\nThe matrix-free framework of deal.II provides the necessary infrastructure to implement the action of the discretized equation above. As opposed to the MatrixFree::cell_loop() that we used in step-37 and step-48, we now build a code in terms of MatrixFree::loop() that takes three function pointers, one for the cell integrals, one for the inner face integrals, and one for the boundary face integrals (in analogy to the design of MeshWorker used in the step-39 tutorial program). In each of these three functions, we then implement the respective terms on the quadrature points. For interpolation between the vector entries and the values and gradients on quadrature points, we use the class FEEvaluation for cell contributions and FEFaceEvaluation for face contributions. The basic usage of these functions has been discussed extensively in the step-37 tutorial program.\\nIn MatrixFree::loop(), all interior faces are visited exactly once, so one must make sure to compute the contributions from both the test functions \\\\(v_h^-\\\\) and \\\\(v_h^+\\\\). Given the fact that the test functions on both sides are indeed independent, the weak form above effectively means that we submit the same contribution to both an FEFaceEvaluation object called phi_inner and phi_outer for testing with the normal derivative of the test function, and values with opposite sign for testing with the values of the test function, because the latter involves opposite signs due to the jump term. For faces between cells of different refinement level, the integration is done from the refined side, and FEFaceEvaluation automatically performs interpolation to a subface on the coarse side. Thus, a hanging node never appears explicitly in a user implementation of a weak form.\\nThe fact that each face is visited exactly once also applies to those faces at subdomain boundaries between different processors when parallelized with MPI, where one cell belongs to one processor and one to the other. The setup in MatrixFree::reinit() splits the faces between the two sides, and eventually only reports the faces actually handled locally in MatrixFree::n_inner_face_batches() and MatrixFree::n_boundary_face_batches(), respectively. Note that, in analogy to the cell integrals discussed in step-37, deal.II applies vectorization over several faces to use SIMD, working on something we call a batch of faces with a single instruction. The face batches are independent from the cell batches, even though the time at which face integrals are processed is kept close to the time when the cell integrals of the respective cells are processed, in order to increase the data locality.\\nAnother thing that is new in this program is the fact that we no longer split the vector access like FEEvaluation::read_dof_values() or FEEvaluation::distribute_local_to_global() from the evaluation and integration steps, but call combined functions FEEvaluation::gather_evaluate() and FEEvaluation::integrate_scatter(), respectively. This is useful for face integrals because, depending on what gets evaluated on the faces, not all vector entries of a cell must be touched in the first place. Think for example of the case of the nodal element FE_DGQ with node points on the element surface: If we are interested in the shape function values on a face, only \\\\((k+ 1)^{d-1}\\\\) degrees of freedom contribute to them in a non-trivial way (in a more technical way of speaking, only \\\\((k+1)^{d-1}\\\\) shape functions have a nonzero support on the face and return true for FiniteElement::has_support_on_face()). When compared to the \\\\((k+1)^d\\\\) degrees of freedom of a cell, this is one power less.\\nNow of course we are not interested in only the function values, but also the derivatives on the cell. Fortunately, there is an element in deal.II that extends this property of reduced access also for derivatives on faces, the FE_DGQHermite element.\\nThe FE_DGQHermite element\\nThe element FE_DGQHermite belongs to the family of FE_DGQ elements, i.e., its shape functions are a tensor product of 1D polynomials and the element is fully discontinuous. As opposed to the nodal character in the usual FE_DGQ element, the FE_DGQHermite element is a mixture of nodal contributions and derivative contributions based on a Hermite-like concept. The underlying polynomial class is Polynomials::HermiteLikeInterpolation and can be summarized as follows: For cubic polynomials, we use two polynomials to represent the function value and first derivative at the left end of the unit interval, \\\\(x=0\\\\), and two polynomials to represent the function value and first derivative and the right end of the unit interval, \\\\(x=1\\\\). At the opposite ends, both the value and first derivative of the shape functions are zero, ensuring that only two out of the four basis functions contribute to values and derivative on the respective end. However, we deviate from the classical Hermite interpolation in not strictly assigning one degree of freedom for the value and one for the first derivative, but rather allow the first derivative to be a linear combination of the first and the second shape function. This is done to improve the conditioning of the interpolation. Also, when going to degrees beyond three, we add node points in the element interior in a Lagrange-like fashion, combined with double zeros in the points \\\\(x=0\\\\) and \\\\(x=1\\\\). The position of these extra nodes is determined by the zeros of some Jacobi polynomials as explained in the description of the class Polynomials::HermiteLikeInterpolation.\\nUsing this element, we only need to access \\\\(2(k+1)^{d-1}\\\\) degrees of freedom for computing both values and derivatives on a face. The check whether the Hermite property is fulfilled is done transparently inside FEFaceEvaluation::gather_evaluate() and FEFaceEvaluation::integrate_scatter() that check the type of the basis and reduce the access to data if possible. Obviously, this would not be possible if we had separated FEFaceEvaluation::read_dof_values() from FEFaceEvaluation::evaluate(), because the amount of entries we need to read depends on the type of the derivative (only values, first derivative, etc.) and thus must be given to read_dof_values().\\nThis optimization is not only useful for computing the face integrals, but also for the MPI ghost layer exchange: In a naive exchange, we would need to send all degrees of freedom of a cell to another processor if the other processor is responsible for computing the face's contribution. Since we know that only some of the degrees of freedom in the evaluation with FEFaceEvaluation are touched, it is natural to only exchange the relevant ones. The MatrixFree::loop() function has support for a selected data exchange when combined with LinearAlgebra::distributed::Vector. To make this happen, we need to tell the loop what kind of evaluation on faces we are going to do, using an argument of type MatrixFree::DataAccessOnFaces, as can be seen in the implementation of LaplaceOperator::vmult() below. The way data is exchanged in that case is as follows: The ghost layer data in the vector still pretends to represent all degrees of freedom, such that FEFaceEvaluation can continue to read the values as if the cell were a locally owned one. The data exchange routines take care of the task for packing and unpacking the data into this format. While this sounds pretty complicated, we will show in the results section below that this really pays off by comparing the performance to a baseline code that does not specify the data access on faces.\\nAn approximate block-Jacobi smoother using the fast diagonalization method\\nIn the tradition of the step-37 program, we again solve a Poisson problem with a geometric multigrid preconditioner inside a conjugate gradient solver. Instead of computing the diagonal and use the basic PreconditionChebyshev as a smoother, we choose a different strategy in this tutorial program. We implement a block-Jacobi preconditioner, where a block refers to all degrees of freedom on a cell. Rather than building the full cell matrix and applying its LU factorization (or inverse) in the preconditioner \\u2014 an operation that would be heavily memory bandwidth bound and thus pretty slow \\u2014 we approximate the inverse of the block by a special technique called fast diagonalization method.\\nThe idea of the method is to take use of the structure of the cell matrix. In case of the Laplacian with constant coefficients discretized on a Cartesian mesh, the cell matrix \\\\(L\\\\) can be written as   \\n\\\\begin{align*}\\nL &= A_1 \\\\otimes M_0 + M_1 \\\\otimes A_0\\n\\\\end{align*}\\n\\n in 2D and   \\n\\\\begin{align*}\\nL &= A_2 \\\\otimes M_1 \\\\otimes M_0 + M_2 \\\\otimes A_1 \\\\otimes M_0 + M_2 \\\\otimes M_1 \\\\otimes A_0\\n\\\\end{align*}\\n\\n in 3D. The matrices \\\\(A_0\\\\) and \\\\(A_1\\\\) denote the 1D Laplace matrix (including the cell and face term associated to the current cell values \\\\(u^-_h\\\\) and \\\\(v^-_h\\\\)) and \\\\(M_0\\\\) and \\\\(M_1\\\\) are the mass matrices. Note that this simple tensor product structure is lost once there are non-constant coefficients on the cell or the geometry is not constant any more. We mention that a similar setup could also be used to replace the computed integrals with this final tensor product form of the matrices, which would cut the operations for the operator evaluation into less than half. However, given the fact that this only holds for Cartesian cells and constant coefficients, which is a pretty narrow case, we refrain from pursuing this idea.\\nInterestingly, the exact inverse of the matrix \\\\(L\\\\) can be found through tensor products due to a method introduced by Lynch et al. [149] in 1964,    \\n\\\\begin{align*}\\nL^{-1} &= S_1 \\\\otimes S_0 (\\\\Lambda_1 \\\\otimes I + I \\\\otimes \\\\Lambda_0)^{-1}\\nS_1^\\\\mathrm T \\\\otimes S_0^\\\\mathrm T,\\n\\\\end{align*}\\n\\n where \\\\(S_d\\\\) is the matrix of eigenvectors to the generalized eigenvalue problem in the given tensor direction \\\\(d\\\\):   \\n\\\\begin{align*}\\nA_d s  &= \\\\lambda M_d s, \\\\quad d = 0, \\\\ldots,\\\\mathrm{dim-1},\\n\\\\end{align*}\\n\\n and \\\\(\\\\Lambda_d\\\\) is the diagonal matrix representing the generalized eigenvalues \\\\(\\\\lambda\\\\). Note that the vectors \\\\(s\\\\) are such that they simultaneously diagonalize \\\\(A_d\\\\) and \\\\(M_d\\\\), i.e.  \\\\(S_d^{\\\\mathrm T} A_d S_d =\\n\\\\Lambda_d\\\\) and \\\\(S_d^{\\\\mathrm T} M_d S_d = I\\\\).\\nThe deal.II library implements a class using this concept, called TensorProductMatrixSymmetricSum.\\nFor the sake of this program, we stick with constant coefficients and Cartesian meshes, even though an approximate version based on tensor products would still be possible for a more general mesh, and the operator evaluation itself is of course generic. Also, we do not bother with adaptive meshes where the multigrid algorithm would need to get access to flux matrices over the edges of different refinement, as explained in step-39. One thing we do, however, is to still wrap our block-Jacobi preconditioner inside PreconditionChebyshev. That class relieves us from finding an appropriate relaxation parameter (which would be around 0.7 in 2D and 0.5 in 3D for the block-Jacobi smoother), and often increases smoothing efficiency somewhat over plain Jacobi smoothing, especially when using several iterations.\\nNote that the block-Jacobi smoother has an additional benefit: The fast diagonalization method can also be interpreted as a change from the Hermite-like polynomials underlying FE_DGQHermite to a basis where the cell Laplacian is diagonal. Thus, it cancels the effect of the basis, and we get the same iteration counts irrespective of whether we use FE_DGQHermite or FE_DGQ. This is in contrast to using the PreconditionChebyshev class with only the diagonal (a point-Jacobi scheme), where FE_DGQ and FE_DGQHermite do indeed behave differently and FE_DGQ needs fewer iterations than FE_DGQHermite, despite the modification made to the Hermite-like shape functions to ensure a good conditioning.\\n The commented program\\nThe include files are essentially the same as in step-37, with the exception of the finite element class FE_DGQHermite instead of FE_Q. All functionality for matrix-free computations on face integrals is already contained in fe_evaluation.h.\\n\\u00a0 #include <deal.II/base/quadrature_lib.h>\\n\\u00a0 #include <deal.II/base/function.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/lac/full_matrix.h>\\n\\u00a0 #include <deal.II/lac/solver_cg.h>\\n\\u00a0 #include <deal.II/lac/la_parallel_vector.h>\\n\\u00a0 #include <deal.II/lac/precondition.h>\\n\\u00a0 #include <deal.II/lac/tensor_product_matrix.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/fe/fe_dgq.h>\\n\\u00a0 #include <deal.II/fe/fe_tools.h>\\n\\u00a0 #include <deal.II/fe/mapping_q1.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/grid/tria.h>\\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/grid_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/multigrid/multigrid.h>\\n\\u00a0 #include <deal.II/multigrid/mg_transfer_matrix_free.h>\\n\\u00a0 #include <deal.II/multigrid/mg_tools.h>\\n\\u00a0 #include <deal.II/multigrid/mg_coarse.h>\\n\\u00a0 #include <deal.II/multigrid/mg_smoother.h>\\n\\u00a0 #include <deal.II/multigrid/mg_matrix.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 \\n\\u00a0 #include <deal.II/matrix_free/matrix_free.h>\\n\\u00a0 #include <deal.II/matrix_free/fe_evaluation.h>\\n\\u00a0 \\n\\u00a0 #include <iostream>\\n\\u00a0 #include <fstream>\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 namespace Step59\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\nAs in step-37, we collect the dimension and polynomial degree as constants here at the top of the program for simplicity. As opposed to step-37, we choose a really high order method this time with degree 8 where any implementation not using sum factorization would become prohibitively slow compared to the implementation with MatrixFree which provides an efficiency that is essentially the same as at degrees two or three. Furthermore, all classes in this tutorial program are templated, so it would be easy to select the degree at run time from an input file or a command-line argument by adding instantiations of the appropriate degrees in the main() function.\\n\\u00a0   const unsigned int degree_finite_element = 8;\\n\\u00a0   const unsigned int dimension             = 3;\\n\\u00a0 \\n Equation data\\nIn analogy to step-7, we define an analytic solution that we try to reproduce with our discretization. Since the aim of this tutorial is to show matrix-free methods, we choose one of the simplest possibilities, namely a cosine function whose derivatives are simple enough for us to compute analytically. Further down, the wave number 2.4 we select here will be matched with the domain extent in \\\\(x\\\\)-direction that is 2.5, such that we obtain a periodic solution at \\\\(x = 2.5\\\\) including \\\\(6pi\\\\) or three full wave revolutions in the cosine. The first function defines the solution and its gradient for expressing the analytic solution for the Dirichlet and Neumann boundary conditions, respectively. Furthermore, a class representing the negative Laplacian of the solution is used to represent the right hand side (forcing) function that we use to match the given analytic solution in the discretized version (manufactured solution).\\n\\u00a0   template <int dim>\\n\\u00a0   class Solution : public Function<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     virtual double value(const Point<dim> &p,\\n\\u00a0                          const unsigned int = 0) const override final\\n\\u00a0     {\\n\\u00a0       double val = 1.;\\n\\u00a0       for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0         val *= std::cos(numbers::PI * 2.4 * p[d]);\\n\\u00a0       return val;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     virtual Tensor<1, dim> gradient(const Point<dim> &p,\\n\\u00a0                                     const unsigned int = 0) const override final\\n\\u00a0     {\\n\\u00a0       const double   arg = numbers::PI * 2.4;\\n\\u00a0       Tensor<1, dim> grad;\\n\\u00a0       for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0         {\\n\\u00a0           grad[d] = 1.;\\n\\u00a0           for (unsigned int e = 0; e < dim; ++e)\\n\\u00a0             if (d == e)\\n\\u00a0               grad[d] *= -arg * std::sin(arg * p[e]);\\n\\u00a0             else\\n\\u00a0               grad[d] *= std::cos(arg * p[e]);\\n\\u00a0         }\\n\\u00a0       return grad;\\n\\u00a0     }\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim>\\n\\u00a0   class RightHandSide : public Function<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     virtual double value(const Point<dim> &p,\\n\\u00a0                          const unsigned int = 0) const override final\\n\\u00a0     {\\n\\u00a0       const double arg = numbers::PI * 2.4;\\n\\u00a0       double       val = 1.;\\n\\u00a0       for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0         val *= std::cos(arg * p[d]);\\n\\u00a0       return dim * arg * arg * val;\\n\\u00a0     }\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFunctionDefinition function.h:152\\nFunction::gradientvirtual Tensor< 1, dim, RangeNumberType > gradient(const Point< dim > &p, const unsigned int component=0) const\\nFunction::valuevirtual RangeNumberType value(const Point< dim > &p, const unsigned int component=0) const\\nPointDefinition point.h:111\\nTensorDefinition tensor.h:471\\nPhysics::Elasticity::Kinematics::dSymmetricTensor< 2, dim, Number > d(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\nnumbers::PIstatic constexpr double PIDefinition numbers.h:259\\nstd::cos::VectorizedArray< Number, width > cos(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6609\\nstd::sin::VectorizedArray< Number, width > sin(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6589\\n Matrix-free implementation\\nThe LaplaceOperator class is similar to the respective class in step-37. A significant difference is that we do not derive the class from MatrixFreeOperators::Base because we want to present some additional features of MatrixFree::loop() that are not available in the general-purpose class MatrixFreeOperators::Base. We derive the class from the Subscriptor class to be able to use the operator within the Chebyshev preconditioner because that preconditioner stores the underlying matrix via a SmartPointer.\\nGiven that we implement a complete matrix interface by hand, we need to add an initialize() function, an m() function, a vmult() function, and a Tvmult() function that were previously provided by MatrixFreeOperators::Base. Our LaplaceOperator also contains a member function get_penalty_factor() that centralizes the selection of the penalty parameter in the symmetric interior penalty method according to step-39.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   class LaplaceOperator : public Subscriptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     using value_type = number;\\n\\u00a0 \\n\\u00a0     LaplaceOperator() = default;\\n\\u00a0 \\n\\u00a0     void initialize(std::shared_ptr<const MatrixFree<dim, number>> data);\\n\\u00a0 \\n\\u00a0     void clear();\\n\\u00a0 \\n\\u00a0     types::global_dof_index m() const;\\n\\u00a0 \\n\\u00a0     void initialize_dof_vector(\\n\\u00a0       LinearAlgebra::distributed::Vector<number> &vec) const;\\n\\u00a0 \\n\\u00a0     std::shared_ptr<const MatrixFree<dim, number>> get_matrix_free() const;\\n\\u00a0 \\n\\u00a0     void vmult(LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0                const LinearAlgebra::distributed::Vector<number> &src) const;\\n\\u00a0 \\n\\u00a0     void Tvmult(LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0                 const LinearAlgebra::distributed::Vector<number> &src) const;\\n\\u00a0 \\n\\u00a0     number get_penalty_factor() const\\n\\u00a0     {\\n\\u00a0       return 1.0 * fe_degree * (fe_degree + 1);\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void\\n\\u00a0     apply_cell(const MatrixFree<dim, number>                    &data,\\n\\u00a0                LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0                const LinearAlgebra::distributed::Vector<number> &src,\\n\\u00a0                const std::pair<unsigned int, unsigned int> &cell_range) const;\\n\\u00a0 \\n\\u00a0     void\\n\\u00a0     apply_face(const MatrixFree<dim, number>                    &data,\\n\\u00a0                LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0                const LinearAlgebra::distributed::Vector<number> &src,\\n\\u00a0                const std::pair<unsigned int, unsigned int> &face_range) const;\\n\\u00a0 \\n\\u00a0     void apply_boundary(\\n\\u00a0       const MatrixFree<dim, number>                    &data,\\n\\u00a0       LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0       const LinearAlgebra::distributed::Vector<number> &src,\\n\\u00a0       const std::pair<unsigned int, unsigned int>      &face_range) const;\\n\\u00a0 \\n\\u00a0     std::shared_ptr<const MatrixFree<dim, number>> data;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nLinearAlgebra::distributed::VectorDefinition la_parallel_vector.h:250\\nMatrixFreeDefinition matrix_free.h:113\\nSubscriptorDefinition subscriptor.h:60\\nunsigned int\\nThe PreconditionBlockJacobi class defines our custom preconditioner for this problem. As opposed to step-37 which was based on the matrix diagonal, we here compute an approximate inversion of the diagonal blocks in the discontinuous Galerkin method by using the so-called fast diagonalization method discussed in the introduction.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   class PreconditionBlockJacobi\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     using value_type = number;\\n\\u00a0 \\n\\u00a0     void clear()\\n\\u00a0     {\\n\\u00a0       cell_matrices.clear();\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     void initialize(const LaplaceOperator<dim, fe_degree, number> &op);\\n\\u00a0 \\n\\u00a0     void vmult(LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0                const LinearAlgebra::distributed::Vector<number> &src) const;\\n\\u00a0 \\n\\u00a0     void Tvmult(LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0                 const LinearAlgebra::distributed::Vector<number> &src) const\\n\\u00a0     {\\n\\u00a0       vmult(dst, src);\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     std::shared_ptr<const MatrixFree<dim, number>> data;\\n\\u00a0     std::vector<TensorProductMatrixSymmetricSum<dim,\\n\\u00a0                                                 VectorizedArray<number>,\\n\\u00a0                                                 fe_degree + 1>>\\n\\u00a0       cell_matrices;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nPreconditionBlockJacobiDefinition precondition_block.h:381\\nPreconditionBlockJacobi::vmultvoid vmult(Vector< number2 > &, const Vector< number2 > &) const\\nPreconditionBlockJacobi::Tvmultvoid Tvmult(Vector< number2 > &, const Vector< number2 > &) const\\nPreconditionBlockJacobi::numbertypename MatrixType::value_type numberDefinition precondition_block.h:386\\nPreconditionBlock< MatrixType, typename MatrixType::value_type >::clearvoid clear()\\nPreconditionBlock< MatrixType, typename MatrixType::value_type >::initializevoid initialize(const MatrixType &A, const AdditionalData parameters)\\nTensorProductMatrixSymmetricSumDefinition tensor_product_matrix.h:113\\nVectorizedArrayDefinition vectorization.h:445\\nThis free-standing function is used in both the LaplaceOperator and PreconditionBlockJacobi classes to adjust the ghost range. This function is necessary because some of the vectors that the vmult() functions are supplied with are not initialized properly with LaplaceOperator::initialize_dof_vector that includes the correct layout of ghost entries, but instead comes from the MGTransferMatrixFree class that has no notion on the ghost selection of the matrix-free classes. To avoid index confusion, we must adjust the ghost range before actually doing something with these vectors. Since the vectors are kept around in the multigrid smoother and transfer classes, a vector whose ghost range has once been adjusted will remain in this state throughout the lifetime of the object, so we can use a shortcut at the start of the function to see whether the partitioner object of the distributed vector, which is stored as a shared pointer, is the same as the layout expected by MatrixFree, which is stored in a data structure accessed by MatrixFree::get_dof_info(0), where the 0 indicates the DoFHandler number from which this was extracted; we only use a single DoFHandler in MatrixFree, so the only valid number is 0 here.\\n\\u00a0   template <int dim, typename number>\\n\\u00a0   void adjust_ghost_range_if_necessary(\\n\\u00a0     const MatrixFree<dim, number>                    &data,\\n\\u00a0     const LinearAlgebra::distributed::Vector<number> &vec)\\n\\u00a0   {\\n\\u00a0     if (vec.get_partitioner().get() ==\\n\\u00a0         data.get_dof_info(0).vector_partitioner.get())\\n\\u00a0       return;\\n\\u00a0 \\n\\u00a0     LinearAlgebra::distributed::Vector<number> copy_vec(vec);\\n\\u00a0     const_cast<LinearAlgebra::distributed::Vector<number> &>(vec).reinit(\\n\\u00a0       data.get_dof_info(0).vector_partitioner);\\n\\u00a0     const_cast<LinearAlgebra::distributed::Vector<number> &>(vec)\\n\\u00a0       .copy_locally_owned_data_from(copy_vec);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nThe next five functions to clear and initialize the LaplaceOperator class, to return the shared pointer holding the MatrixFree data container, as well as the correct initialization of the vector and operator sizes are the same as in step-37 or rather MatrixFreeOperators::Base.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void LaplaceOperator<dim, fe_degree, number>::clear()\\n\\u00a0   {\\n\\u00a0     data.reset();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void LaplaceOperator<dim, fe_degree, number>::initialize(\\n\\u00a0     std::shared_ptr<const MatrixFree<dim, number>> data)\\n\\u00a0   {\\n\\u00a0     this->data = data;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   std::shared_ptr<const MatrixFree<dim, number>>\\n\\u00a0   LaplaceOperator<dim, fe_degree, number>::get_matrix_free() const\\n\\u00a0   {\\n\\u00a0     return data;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void LaplaceOperator<dim, fe_degree, number>::initialize_dof_vector(\\n\\u00a0     LinearAlgebra::distributed::Vector<number> &vec) const\\n\\u00a0   {\\n\\u00a0     data->initialize_dof_vector(vec);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   types::global_dof_index LaplaceOperator<dim, fe_degree, number>::m() const\\n\\u00a0   {\\n\\u00a0     Assert(data.get() != nullptr, ExcNotInitialized());\\n\\u00a0     return data->get_dof_handler().n_dofs();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nAssert#define Assert(cond, exc)Definition exceptions.h:1638\\nThis function implements the action of the LaplaceOperator on a vector src and stores the result in the vector dst. When compared to step-37, there are four new features present in this call.\\nThe first new feature is the adjust_ghost_range_if_necessary function mentioned above that is needed to fit the vectors to the layout expected by FEEvaluation and FEFaceEvaluation in the cell and face functions.\\nThe second new feature is the fact that we do not implement a vmult_add() function as we did in step-37 (through the virtual function MatrixFreeOperators::Base::vmult_add()), but directly implement a vmult() functionality. Since both cell and face integrals will sum into the destination vector, we must of course zero the vector somewhere. For DG elements, we are given two options \\u2013 one is to use FEEvaluation::set_dof_values() instead of FEEvaluation::distribute_local_to_global() in the apply_cell function below. This works because the loop layout in MatrixFree is such that cell integrals always touch a given vector entry before the face integrals. However, this really only works for fully discontinuous bases where every cell has its own degrees of freedom, without any sharing with neighboring results. An alternative setup, the one chosen here, is to let the MatrixFree::loop() take care of zeroing the vector. This can be thought of as simply calling dst = 0; somewhere in the code. The implementation is more involved for supported vectors such as LinearAlgebra::distributed::Vector, because we aim to not zero the whole vector at once. Doing the zero operation on a small enough pieces of a few thousands of vector entries has the advantage that the vector entries that get zeroed remain in caches before they are accessed again in FEEvaluation::distribute_local_to_global() and FEFaceEvaluation::distribute_local_to_global(). Since matrix-free operator evaluation is really fast, just zeroing a large vector can amount to up to a 25% of the operator evaluation time, and we obviously want to avoid this cost. This option of zeroing the vector is also available for MatrixFree::cell_loop and for continuous bases, even though it was not used in the step-37 or step-48 tutorial programs.\\nThe third new feature is the way we provide the functions to compute on cells, inner faces, and boundary faces: The class MatrixFree has a function called loop that takes three function pointers to the three cases, allowing to separate the implementations of different things. As explained in step-37, these function pointers can be std::function objects or member functions of a class. In this case, we use pointers to member functions.\\nThe final new feature are the last two arguments of type MatrixFree::DataAccessOnFaces that can be given to MatrixFree::loop(). This class passes the type of data access for face integrals to the MPI data exchange routines LinearAlgebra::distributed::Vector::update_ghost_values() and LinearAlgebra::distributed::Vector::compress() of the parallel vectors. The purpose is to not send all degrees of freedom of a neighboring element, but to reduce the amount of data to what is really needed for the computations at hand. The data exchange is a real bottleneck in particular for high-degree DG methods, therefore a more restrictive way of exchange is often beneficial. The enum field MatrixFree::DataAccessOnFaces can take the value none, which means that no face integrals at all are done, which would be analogous to MatrixFree::cell_loop(), the value values meaning that only shape function values (but no derivatives) are used on faces, and the value gradients when also first derivatives on faces are accessed besides the values. A value unspecified means that all degrees of freedom will be exchanged for the faces that are located at the processor boundaries and designated to be worked on at the local processor.\\nTo see how the data can be reduced, think of the case of the nodal element FE_DGQ with node points on the element surface, where only \\\\((k+1)^{d-1}\\\\) degrees of freedom contribute to the values on a face for polynomial degree \\\\(k\\\\) in \\\\(d\\\\) space dimensions, out of the \\\\((k+1)^d\\\\) degrees of freedom of a cell. A similar reduction is also possible for the interior penalty method that evaluates values and first derivatives on the faces. When using a Hermite-like basis in 1d, only up to two basis functions contribute to the value and derivative. The class FE_DGQHermite implements a tensor product of this concept, as discussed in the introduction. Thus, only \\\\(2(k+1)^{d-1}\\\\) degrees of freedom must be exchanged for each face, which is a clear win once \\\\(k\\\\) gets larger than four or five. Note that this reduced exchange of FE_DGQHermite is valid also on meshes with curved boundaries, as the derivatives are taken on the reference element, whereas the geometry only mixes them on the inside. Thus, this is different from the attempt to obtain \\\\(C^1\\\\) continuity with continuous Hermite-type shape functions where the non-Cartesian case changes the picture significantly. Obviously, on non-Cartesian meshes the derivatives also include tangential derivatives of shape functions beyond the normal derivative, but those only need the function values on the element surface, too. Should the element not provide any compression, the loop automatically exchanges all entries for the affected cells.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void LaplaceOperator<dim, fe_degree, number>::vmult(\\n\\u00a0     LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0     const LinearAlgebra::distributed::Vector<number> &src) const\\n\\u00a0   {\\n\\u00a0     adjust_ghost_range_if_necessary(*data, dst);\\n\\u00a0     adjust_ghost_range_if_necessary(*data, src);\\n\\u00a0     data->loop(&LaplaceOperator::apply_cell,\\n\\u00a0                &LaplaceOperator::apply_face,\\n\\u00a0                &LaplaceOperator::apply_boundary,\\n\\u00a0                this,\\n\\u00a0                dst,\\n\\u00a0                src,\\n\\u00a0                /*zero_dst =*/true,\\n\\u00a0                MatrixFree<dim, number>::DataAccessOnFaces::gradients,\\n\\u00a0                MatrixFree<dim, number>::DataAccessOnFaces::gradients);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nSince the Laplacian is symmetric, the Tvmult() (needed by the multigrid smoother interfaces) operation is simply forwarded to the vmult() case.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void LaplaceOperator<dim, fe_degree, number>::Tvmult(\\n\\u00a0     LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0     const LinearAlgebra::distributed::Vector<number> &src) const\\n\\u00a0   {\\n\\u00a0     vmult(dst, src);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nThe cell operation is very similar to step-37. We do not use a coefficient here, though. The second difference is that we replaced the two steps of FEEvaluation::read_dof_values() followed by FEEvaluation::evaluate() by a single function call FEEvaluation::gather_evaluate() which internally calls the sequence of the two individual methods. Likewise, FEEvaluation::integrate_scatter() implements the sequence of FEEvaluation::integrate() followed by FEEvaluation::distribute_local_to_global(). In this case, these new functions merely save two lines of code. However, we use them for the analogy with FEFaceEvaluation where they are more important as explained below.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void LaplaceOperator<dim, fe_degree, number>::apply_cell(\\n\\u00a0     const MatrixFree<dim, number>                    &data,\\n\\u00a0     LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0     const LinearAlgebra::distributed::Vector<number> &src,\\n\\u00a0     const std::pair<unsigned int, unsigned int>      &cell_range) const\\n\\u00a0   {\\n\\u00a0     FEEvaluation<dim, fe_degree, fe_degree + 1, 1, number> phi(data);\\n\\u00a0     for (unsigned int cell = cell_range.first; cell < cell_range.second; ++cell)\\n\\u00a0       {\\n\\u00a0         phi.reinit(cell);\\n\\u00a0         phi.gather_evaluate(src, EvaluationFlags::gradients);\\n\\u00a0         for (const unsigned int q : phi.quadrature_point_indices())\\n\\u00a0           phi.submit_gradient(phi.get_gradient(q), q);\\n\\u00a0         phi.integrate_scatter(EvaluationFlags::gradients, dst);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFEEvaluationDefinition fe_evaluation.h:1355\\nEvaluationFlags::gradients@ gradientsDefinition evaluation_flags.h:54\\nThe face operation implements the terms of the interior penalty method in analogy to step-39, as explained in the introduction. We need two evaluator objects for this task, one for handling the solution that comes from the cell on one of the two sides of an interior face, and one for handling the solution from the other side. The evaluators for face integrals are called FEFaceEvaluation and take a boolean argument in the second slot of the constructor to indicate which of the two sides the evaluator should belong two. In FEFaceEvaluation and MatrixFree, we call one of the two sides the interior one and the other the exterior one. The name exterior refers to the fact that the evaluator from both sides will return the same normal vector. For the interior side, the normal vector points outwards, whereas it points inwards on the other side, and is opposed to the outer normal vector of that cell. Apart from the new class name, we again get a range of items to work with in analogy to what was discussed in step-37, but for the interior faces in this case. Note that the data structure of MatrixFree forms batches of faces that are analogous to the batches of cells for the cell integrals. All faces within a batch involve different cell numbers but have the face number within the reference cell, have the same refinement configuration (no refinement or the same subface), and the same orientation, to keep SIMD operations simple and efficient.\\nNote that there is no implied meaning in interior versus exterior except the logic decision of the orientation of the normal, which is pretty random internally. One can in no way rely on a certain pattern of assigning interior versus exterior flags, as the decision is made for the sake of access regularity and uniformity in the MatrixFree setup routines. Since most sane DG methods are conservative, i.e., fluxes look the same from both sides of an interface, the mathematics are unaltered if the interior/exterior flags are switched and normal vectors get the opposite sign.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void LaplaceOperator<dim, fe_degree, number>::apply_face(\\n\\u00a0     const MatrixFree<dim, number>                    &data,\\n\\u00a0     LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0     const LinearAlgebra::distributed::Vector<number> &src,\\n\\u00a0     const std::pair<unsigned int, unsigned int>      &face_range) const\\n\\u00a0   {\\n\\u00a0     FEFaceEvaluation<dim, fe_degree, fe_degree + 1, 1, number> phi_inner(data,\\n\\u00a0                                                                          true);\\n\\u00a0     FEFaceEvaluation<dim, fe_degree, fe_degree + 1, 1, number> phi_outer(data,\\n\\u00a0                                                                          false);\\n\\u00a0     for (unsigned int face = face_range.first; face < face_range.second; ++face)\\n\\u00a0       {\\nFEFaceEvaluationDefinition fe_evaluation.h:1818\\nOn a given batch of faces, we first update the pointers to the current face and then access the vector. As mentioned above, we combine the vector access with the evaluation. In the case of face integrals, the data access into the vector can be reduced for the special case of an FE_DGQHermite basis as explained for the data exchange above: Since only \\\\(2(k+1)^{d-1}\\\\) out of the \\\\((k+1)^d\\\\) cell degrees of freedom get multiplied by a non-zero value or derivative of a shape function, this structure can be utilized for the evaluation, significantly reducing the data access. The reduction of the data access is not only beneficial because it reduces the data in flight and thus helps caching, but also because the data access to faces is often more irregular than for cell integrals when gathering values from cells that are farther apart in the index list of cells.\\n\\u00a0         phi_inner.reinit(face);\\n\\u00a0         phi_inner.gather_evaluate(src,\\n\\u00a0                                   EvaluationFlags::values |\\n\\u00a0                                     EvaluationFlags::gradients);\\n\\u00a0         phi_outer.reinit(face);\\n\\u00a0         phi_outer.gather_evaluate(src,\\n\\u00a0                                   EvaluationFlags::values |\\n\\u00a0                                     EvaluationFlags::gradients);\\n\\u00a0 \\nEvaluationFlags::values@ valuesDefinition evaluation_flags.h:50\\nThe next two statements compute the penalty parameter for the interior penalty method. As explained in the introduction, we would like to have a scaling like \\\\(\\\\frac{1}{h_\\\\text{i}}\\\\) of the length \\\\(h_\\\\text{i}\\\\) normal to the face. For a general non-Cartesian mesh, this length must be computed by the product of the inverse Jacobian times the normal vector in real coordinates. From this vector of dim components, we must finally pick the component that is oriented normal to the reference cell. In the geometry data stored in MatrixFree, a permutation of the components in the Jacobian is applied such that this latter direction is always the last component dim-1 (this is beneficial because reference-cell derivative sorting can be made agnostic of the direction of the face). This means that we can simply access the last entry dim-1 and must not look up the local face number in data.get_face_info(face).interior_face_no and data.get_face_info(face).exterior_face_no. Finally, we must also take the absolute value of these factors as the normal could point into either positive or negative direction.\\n\\u00a0         const VectorizedArray<number> inverse_length_normal_to_face =\\n\\u00a0           0.5 * (std::abs((phi_inner.normal_vector(0) *\\n\\u00a0                            phi_inner.inverse_jacobian(0))[dim - 1]) +\\n\\u00a0                  std::abs((phi_outer.normal_vector(0) *\\n\\u00a0                            phi_outer.inverse_jacobian(0))[dim - 1]));\\n\\u00a0         const VectorizedArray<number> sigma =\\n\\u00a0           inverse_length_normal_to_face * get_penalty_factor();\\n\\u00a0 \\nstd::abs::VectorizedArray< Number, width > abs(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6927\\nIn the loop over the quadrature points, we eventually compute all contributions to the interior penalty scheme. According to the formulas in the introduction, the value of the test function gets multiplied by the difference of the jump in the solution times the penalty parameter and the average of the normal derivative in real space. Since the two evaluators for interior and exterior sides get different signs due to the jump, we pass the result with a different sign here. The normal derivative of the test function gets multiplied by the negative jump in the solution between the interior and exterior side. This term, coined adjoint consistency term, must also include the factor of \\\\(\\\\frac{1}{2}\\\\) in the code in accordance with its relation to the primal consistency term that gets the factor of one half due to the average in the test function slot.\\n\\u00a0         for (const unsigned int q : phi_inner.quadrature_point_indices())\\n\\u00a0           {\\n\\u00a0             const VectorizedArray<number> solution_jump =\\n\\u00a0               (phi_inner.get_value(q) - phi_outer.get_value(q));\\n\\u00a0             const VectorizedArray<number> average_normal_derivative =\\n\\u00a0               (phi_inner.get_normal_derivative(q) +\\n\\u00a0                phi_outer.get_normal_derivative(q)) *\\n\\u00a0               number(0.5);\\n\\u00a0             const VectorizedArray<number> test_by_value =\\n\\u00a0               solution_jump * sigma - average_normal_derivative;\\n\\u00a0 \\n\\u00a0             phi_inner.submit_value(test_by_value, q);\\n\\u00a0             phi_outer.submit_value(-test_by_value, q);\\n\\u00a0 \\n\\u00a0             phi_inner.submit_normal_derivative(-solution_jump * number(0.5), q);\\n\\u00a0             phi_outer.submit_normal_derivative(-solution_jump * number(0.5), q);\\n\\u00a0           }\\n\\u00a0 \\nOnce we are done with the loop over quadrature points, we can do the sum factorization operations for the integration loops on faces and sum the results into the result vector, using the integrate_scatter function. The name scatter reflects the distribution of the vector data into scattered positions in the vector using the same pattern as in gather_evaluate. Like before, the combined integrate + write operation allows us to reduce the data access.\\n\\u00a0         phi_inner.integrate_scatter(EvaluationFlags::values |\\n\\u00a0                                       EvaluationFlags::gradients,\\n\\u00a0                                     dst);\\n\\u00a0         phi_outer.integrate_scatter(EvaluationFlags::values |\\n\\u00a0                                       EvaluationFlags::gradients,\\n\\u00a0                                     dst);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nThe boundary face function follows by and large the interior face function. The only difference is the fact that we do not have a separate FEFaceEvaluation object that provides us with exterior values \\\\(u^+\\\\), but we must define them from the boundary conditions and interior values \\\\(u^-\\\\). As explained in the introduction, we use  \\\\(u^+ = -u^- + 2\\n   g_\\\\text{D}\\\\) and  \\\\(\\\\mathbf{n}^-\\\\cdot \\\\nabla u^+ = \\\\mathbf{n}^-\\\\cdot \\\\nabla\\n   u^-\\\\) on Dirichlet boundaries and \\\\(u^+=u^-\\\\) and  \\\\(\\\\mathbf{n}^-\\\\cdot \\\\nabla\\n   u^+ = -\\\\mathbf{n}^-\\\\cdot \\\\nabla u^- + 2 g_\\\\text{N}\\\\) on Neumann boundaries. Since this operation implements the homogeneous part, i.e., the matrix-vector product, we must neglect the boundary functions \\\\(g_\\\\text{D}\\\\) and \\\\(g_\\\\text{N}\\\\) here, and added them to the right hand side in LaplaceProblem::compute_rhs(). Note that due to extension of the solution \\\\(u^-\\\\) to the exterior via \\\\(u^+\\\\), we can keep all factors \\\\(0.5\\\\) the same as in the inner face function, see also the discussion in step-39.\\nThere is one catch at this point: The implementation below uses a boolean variable is_dirichlet to switch between the Dirichlet and the Neumann cases. However, we solve a problem where we also want to impose periodic boundary conditions on some boundaries, namely along those in the \\\\(x\\\\) direction. One might wonder how those conditions should be handled here. The answer is that MatrixFree automatically treats periodic boundaries as what they are technically, namely an inner face where the solution values of two adjacent cells meet and must be treated by proper numerical fluxes. Thus, all the faces on the periodic boundaries will appear in the apply_face() function and not in this one.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void LaplaceOperator<dim, fe_degree, number>::apply_boundary(\\n\\u00a0     const MatrixFree<dim, number>                    &data,\\n\\u00a0     LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0     const LinearAlgebra::distributed::Vector<number> &src,\\n\\u00a0     const std::pair<unsigned int, unsigned int>      &face_range) const\\n\\u00a0   {\\n\\u00a0     FEFaceEvaluation<dim, fe_degree, fe_degree + 1, 1, number> phi_inner(data,\\n\\u00a0                                                                          true);\\n\\u00a0     for (unsigned int face = face_range.first; face < face_range.second; ++face)\\n\\u00a0       {\\n\\u00a0         phi_inner.reinit(face);\\n\\u00a0         phi_inner.gather_evaluate(src,\\n\\u00a0                                   EvaluationFlags::values |\\n\\u00a0                                     EvaluationFlags::gradients);\\n\\u00a0 \\n\\u00a0         const VectorizedArray<number> inverse_length_normal_to_face = std::abs((\\n\\u00a0           phi_inner.normal_vector(0) * phi_inner.inverse_jacobian(0))[dim - 1]);\\n\\u00a0         const VectorizedArray<number> sigma =\\n\\u00a0           inverse_length_normal_to_face * get_penalty_factor();\\n\\u00a0 \\n\\u00a0         const bool is_dirichlet = (data.get_boundary_id(face) == 0);\\n\\u00a0 \\n\\u00a0         for (const unsigned int q : phi_inner.quadrature_point_indices())\\n\\u00a0           {\\n\\u00a0             const VectorizedArray<number> u_inner = phi_inner.get_value(q);\\n\\u00a0             const VectorizedArray<number> u_outer =\\n\\u00a0               is_dirichlet ? -u_inner : u_inner;\\n\\u00a0             const VectorizedArray<number> normal_derivative_inner =\\n\\u00a0               phi_inner.get_normal_derivative(q);\\n\\u00a0             const VectorizedArray<number> normal_derivative_outer =\\n\\u00a0               is_dirichlet ? normal_derivative_inner : -normal_derivative_inner;\\n\\u00a0             const VectorizedArray<number> solution_jump = (u_inner - u_outer);\\n\\u00a0             const VectorizedArray<number> average_normal_derivative =\\n\\u00a0               (normal_derivative_inner + normal_derivative_outer) * number(0.5);\\n\\u00a0             const VectorizedArray<number> test_by_value =\\n\\u00a0               solution_jump * sigma - average_normal_derivative;\\n\\u00a0             phi_inner.submit_normal_derivative(-solution_jump * number(0.5), q);\\n\\u00a0             phi_inner.submit_value(test_by_value, q);\\n\\u00a0           }\\n\\u00a0         phi_inner.integrate_scatter(EvaluationFlags::values |\\n\\u00a0                                       EvaluationFlags::gradients,\\n\\u00a0                                     dst);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nNext we turn to the preconditioner initialization. As explained in the introduction, we want to construct an (approximate) inverse of the cell matrices from a product of 1d mass and Laplace matrices. Our first task is to compute the 1d matrices, which we do by first creating a 1d finite element. Instead of anticipating FE_DGQHermite<1> here, we get the finite element's name from DoFHandler, replace the dim argument (2 or 3) by 1 to create a 1d name, and construct the 1d element by using FETools.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void PreconditionBlockJacobi<dim, fe_degree, number>::initialize(\\n\\u00a0     const LaplaceOperator<dim, fe_degree, number> &op)\\n\\u00a0   {\\n\\u00a0     data = op.get_matrix_free();\\n\\u00a0 \\n\\u00a0     std::string name = data->get_dof_handler().get_fe().get_name();\\n\\u00a0     name.replace(name.find('<') + 1, 1, \\\"1\\\");\\n\\u00a0     std::unique_ptr<FiniteElement<1>> fe_1d = FETools::get_fe_by_name<1>(name);\\n\\u00a0 \\nFETools::get_fe_by_namestd::unique_ptr< FiniteElement< dim, spacedim > > get_fe_by_name(const std::string &name)\\nAs for computing the 1d matrices on the unit element, we simply write down what a typical assembly procedure over rows and columns of the matrix as well as the quadrature points would do. We select the same Laplace matrices once and for all using the coefficients 0.5 for interior faces (but possibly scaled differently in different directions as a result of the mesh). Thus, we make a slight mistake at the Dirichlet boundary (where the correct factor would be 1 for the derivative terms and 2 for the penalty term, see step-39) or at the Neumann boundary where the factor should be zero. Since we only use this class as a smoother inside a multigrid scheme, this error is not going to have any significant effect and merely affects smoothing quality.\\n\\u00a0     const unsigned int                                 N = fe_degree + 1;\\n\\u00a0     FullMatrix<double>                                 laplace_unscaled(N, N);\\n\\u00a0     std::array<Table<2, VectorizedArray<number>>, dim> mass_matrices;\\n\\u00a0     std::array<Table<2, VectorizedArray<number>>, dim> laplace_matrices;\\n\\u00a0     for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0       {\\n\\u00a0         mass_matrices[d].reinit(N, N);\\n\\u00a0         laplace_matrices[d].reinit(N, N);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     const QGauss<1> quadrature(N);\\n\\u00a0     for (unsigned int i = 0; i < N; ++i)\\n\\u00a0       for (unsigned int j = 0; j < N; ++j)\\n\\u00a0         {\\n\\u00a0           double sum_mass = 0, sum_laplace = 0;\\n\\u00a0           for (unsigned int q = 0; q < quadrature.size(); ++q)\\n\\u00a0             {\\n\\u00a0               sum_mass += (fe_1d->shape_value(i, quadrature.point(q)) *\\n\\u00a0                            fe_1d->shape_value(j, quadrature.point(q))) *\\n\\u00a0                           quadrature.weight(q);\\n\\u00a0               sum_laplace += (fe_1d->shape_grad(i, quadrature.point(q))[0] *\\n\\u00a0                               fe_1d->shape_grad(j, quadrature.point(q))[0]) *\\n\\u00a0                              quadrature.weight(q);\\n\\u00a0             }\\n\\u00a0           for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0             mass_matrices[d](i, j) = sum_mass;\\n\\u00a0 \\nFullMatrixDefinition full_matrix.h:79\\nQGaussDefinition quadrature_lib.h:40\\nThe left and right boundary terms assembled by the next two statements appear to have somewhat arbitrary signs, but those are correct as can be verified by looking at step-39 and inserting the value -1 and 1 for the normal vector in the 1d case.\\n\\u00a0           sum_laplace +=\\n\\u00a0             (1. * fe_1d->shape_value(i, Point<1>()) *\\n\\u00a0                fe_1d->shape_value(j, Point<1>()) * op.get_penalty_factor() +\\n\\u00a0              0.5 * fe_1d->shape_grad(i, Point<1>())[0] *\\n\\u00a0                fe_1d->shape_value(j, Point<1>()) +\\n\\u00a0              0.5 * fe_1d->shape_grad(j, Point<1>())[0] *\\n\\u00a0                fe_1d->shape_value(i, Point<1>()));\\n\\u00a0 \\n\\u00a0           sum_laplace +=\\n\\u00a0             (1. * fe_1d->shape_value(i, Point<1>(1.0)) *\\n\\u00a0                fe_1d->shape_value(j, Point<1>(1.0)) * op.get_penalty_factor() -\\n\\u00a0              0.5 * fe_1d->shape_grad(i, Point<1>(1.0))[0] *\\n\\u00a0                fe_1d->shape_value(j, Point<1>(1.0)) -\\n\\u00a0              0.5 * fe_1d->shape_grad(j, Point<1>(1.0))[0] *\\n\\u00a0                fe_1d->shape_value(i, Point<1>(1.0)));\\n\\u00a0 \\n\\u00a0           laplace_unscaled(i, j) = sum_laplace;\\n\\u00a0         }\\n\\u00a0 \\nNext, we go through the cells and pass the scaled matrices to TensorProductMatrixSymmetricSum to actually compute the generalized eigenvalue problem for representing the inverse: Since the matrix approximation is constructed as \\\\(A\\\\otimes M + M\\\\otimes A\\\\) and the weights are constant for each element, we can apply all weights on the Laplace matrix and simply keep the mass matrices unscaled. In the loop over cells, we want to make use of the geometry compression provided by the MatrixFree class and check if the current geometry is the same as on the last cell batch, in which case there is nothing to do. This compression can be accessed by FEEvaluation::get_mapping_data_index_offset() once reinit() has been called.\\nOnce we have accessed the inverse Jacobian through the FEEvaluation access function (we take the one for the zeroth quadrature point as they should be the same on all quadrature points for a Cartesian cell), we check that it is diagonal and then extract the determinant of the original Jacobian, i.e., the inverse of the determinant of the inverse Jacobian, and set the weight as \\\\(\\\\text{det}(J) / h_d^2\\\\) according to the 1d Laplacian times \\\\(d-1\\\\) copies of the mass matrix.\\n\\u00a0     cell_matrices.clear();\\n\\u00a0     FEEvaluation<dim, fe_degree, fe_degree + 1, 1, number> phi(*data);\\n\\u00a0     unsigned int old_mapping_data_index = numbers::invalid_unsigned_int;\\n\\u00a0     for (unsigned int cell = 0; cell < data->n_cell_batches(); ++cell)\\n\\u00a0       {\\n\\u00a0         phi.reinit(cell);\\n\\u00a0 \\n\\u00a0         if (phi.get_mapping_data_index_offset() == old_mapping_data_index)\\n\\u00a0           continue;\\n\\u00a0 \\n\\u00a0         Tensor<2, dim, VectorizedArray<number>> inverse_jacobian =\\n\\u00a0           phi.inverse_jacobian(0);\\n\\u00a0 \\n\\u00a0         for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0           for (unsigned int e = 0; e < dim; ++e)\\n\\u00a0             if (d != e)\\n\\u00a0               for (unsigned int v = 0; v < VectorizedArray<number>::size(); ++v)\\n\\u00a0                 AssertThrow(inverse_jacobian[d][e][v] == 0.,\\n\\u00a0                             ExcNotImplemented());\\n\\u00a0 \\n\\u00a0         VectorizedArray<number> jacobian_determinant = inverse_jacobian[0][0];\\n\\u00a0         for (unsigned int e = 1; e < dim; ++e)\\n\\u00a0           jacobian_determinant *= inverse_jacobian[e][e];\\n\\u00a0         jacobian_determinant = 1. / jacobian_determinant;\\n\\u00a0 \\n\\u00a0         for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0           {\\n\\u00a0             const VectorizedArray<number> scaling_factor =\\n\\u00a0               inverse_jacobian[d][d] * inverse_jacobian[d][d] *\\n\\u00a0               jacobian_determinant;\\n\\u00a0 \\nAssertThrow#define AssertThrow(cond, exc)Definition exceptions.h:1739\\nnumbers::invalid_unsigned_intstatic const unsigned int invalid_unsigned_intDefinition types.h:220\\nOnce we know the factor by which we should scale the Laplace matrix, we apply this weight to the unscaled DG Laplace matrix and send the array to the class TensorProductMatrixSymmetricSum for computing the generalized eigenvalue problem mentioned in the introduction.\\n\\u00a0             for (unsigned int i = 0; i < N; ++i)\\n\\u00a0               for (unsigned int j = 0; j < N; ++j)\\n\\u00a0                 laplace_matrices[d](i, j) =\\n\\u00a0                   scaling_factor * laplace_unscaled(i, j);\\n\\u00a0           }\\n\\u00a0         if (cell_matrices.size() <= phi.get_mapping_data_index_offset())\\n\\u00a0           cell_matrices.resize(phi.get_mapping_data_index_offset() + 1);\\n\\u00a0         cell_matrices[phi.get_mapping_data_index_offset()].reinit(\\n\\u00a0           mass_matrices, laplace_matrices);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nThe vmult function for the approximate block-Jacobi preconditioner is very simple in the DG context: We simply need to read the values of the current cell batch, apply the inverse for the given entry in the array of tensor product matrix, and write the result back. In this loop, we overwrite the content in dst rather than first setting the entries to zero. This is legitimate for a DG method because every cell has independent degrees of freedom. Furthermore, we manually write out the loop over all cell batches, rather than going through MatrixFree::cell_loop(). We do this because we know that we are not going to need data exchange over the MPI network here as all computations are done on the cells held locally on each processor.\\n\\u00a0   template <int dim, int fe_degree, typename number>\\n\\u00a0   void PreconditionBlockJacobi<dim, fe_degree, number>::vmult(\\n\\u00a0     LinearAlgebra::distributed::Vector<number>       &dst,\\n\\u00a0     const LinearAlgebra::distributed::Vector<number> &src) const\\n\\u00a0   {\\n\\u00a0     adjust_ghost_range_if_necessary(*data, dst);\\n\\u00a0     adjust_ghost_range_if_necessary(*data, src);\\n\\u00a0 \\n\\u00a0     FEEvaluation<dim, fe_degree, fe_degree + 1, 1, number> phi(*data);\\n\\u00a0     for (unsigned int cell = 0; cell < data->n_cell_batches(); ++cell)\\n\\u00a0       {\\n\\u00a0         phi.reinit(cell);\\n\\u00a0         phi.read_dof_values(src);\\n\\u00a0         cell_matrices[phi.get_mapping_data_index_offset()].apply_inverse(\\n\\u00a0           ArrayView<VectorizedArray<number>>(phi.begin_dof_values(),\\n\\u00a0                                              phi.dofs_per_cell),\\n\\u00a0           ArrayView<const VectorizedArray<number>>(phi.begin_dof_values(),\\n\\u00a0                                                    phi.dofs_per_cell));\\n\\u00a0         phi.set_dof_values(dst);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nArrayViewDefinition array_view.h:88\\nThe definition of the LaplaceProblem class is very similar to step-37. One difference is the fact that we add the element degree as a template argument to the class, which would allow us to more easily include more than one degree in the same program by creating different instances in the main() function. The second difference is the selection of the element, FE_DGQHermite, which is specialized for this kind of equations.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   class LaplaceProblem\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     LaplaceProblem();\\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     void setup_system();\\n\\u00a0     void compute_rhs();\\n\\u00a0     void solve();\\n\\u00a0     void analyze_results() const;\\n\\u00a0 \\n\\u00a0 #ifdef DEAL_II_WITH_P4EST\\n\\u00a0     parallel::distributed::Triangulation<dim> triangulation;\\n\\u00a0 #else\\n\\u00a0     Triangulation<dim> triangulation;\\n\\u00a0 #endif\\n\\u00a0 \\n\\u00a0     const FE_DGQHermite<dim> fe;\\n\\u00a0     DoFHandler<dim>          dof_handler;\\n\\u00a0 \\n\\u00a0     MappingQ1<dim> mapping;\\n\\u00a0 \\n\\u00a0     using SystemMatrixType = LaplaceOperator<dim, fe_degree, double>;\\n\\u00a0     SystemMatrixType system_matrix;\\n\\u00a0 \\n\\u00a0     using LevelMatrixType = LaplaceOperator<dim, fe_degree, float>;\\n\\u00a0     MGLevelObject<LevelMatrixType> mg_matrices;\\n\\u00a0 \\n\\u00a0     LinearAlgebra::distributed::Vector<double> solution;\\n\\u00a0     LinearAlgebra::distributed::Vector<double> system_rhs;\\n\\u00a0 \\n\\u00a0     double             setup_time;\\n\\u00a0     ConditionalOStream pcout;\\n\\u00a0     ConditionalOStream time_details;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   LaplaceProblem<dim, fe_degree>::LaplaceProblem()\\n\\u00a0 #ifdef DEAL_II_WITH_P4EST\\n\\u00a0     : triangulation(MPI_COMM_WORLD,\\n\\u00a0                     Triangulation<dim>::limit_level_difference_at_vertices,\\n\\u00a0                     parallel::distributed::Triangulation<\\n\\u00a0                       dim>::construct_multigrid_hierarchy)\\n\\u00a0 #else\\n\\u00a0     : triangulation(Triangulation<dim>::limit_level_difference_at_vertices)\\n\\u00a0 #endif\\n\\u00a0     , fe(fe_degree)\\n\\u00a0     , dof_handler(triangulation)\\n\\u00a0     , setup_time(0.)\\n\\u00a0     , pcout(std::cout, Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)\\n\\u00a0     , time_details(std::cout,\\n\\u00a0                    false &&\\n\\u00a0                      Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nConditionalOStreamDefinition conditional_ostream.h:80\\nDoFHandlerDefinition dof_handler.h:317\\nFE_DGQHermiteDefinition fe_dgq.h:504\\nMGLevelObjectDefinition mg_level_object.h:49\\nMappingQ1Definition mapping_q1.h:55\\nTriangulationDefinition tria.h:1323\\nparallel::distributed::TriangulationDefinition tria.h:268\\nInitializeLibrary::MPI@ MPI\\nUtilitiesDefinition communication_pattern_base.h:30\\nstdSTL namespace.\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\nThe setup function differs in two aspects from step-37. The first is that we do not need to interpolate any constraints for the discontinuous ansatz space, and simply pass a dummy AffineConstraints object into Matrixfree::reinit(). The second change arises because we need to tell MatrixFree to also initialize the data structures for faces. We do this by setting update flags for the inner and boundary faces, respectively. On the boundary faces, we need both the function values, their gradients, JxW values (for integration), the normal vectors, and quadrature points (for the evaluation of the boundary conditions), whereas we only need shape function values, gradients, JxW values, and normal vectors for interior faces. The face data structures in MatrixFree are always built as soon as one of mapping_update_flags_inner_faces or mapping_update_flags_boundary_faces are different from the default value update_default of UpdateFlags.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void LaplaceProblem<dim, fe_degree>::setup_system()\\n\\u00a0   {\\n\\u00a0     Timer time;\\n\\u00a0     setup_time = 0;\\n\\u00a0 \\n\\u00a0     system_matrix.clear();\\n\\u00a0     mg_matrices.clear_elements();\\n\\u00a0 \\n\\u00a0     dof_handler.distribute_dofs(fe);\\n\\u00a0     dof_handler.distribute_mg_dofs();\\n\\u00a0 \\n\\u00a0     pcout << \\\"Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n\\u00a0           << std::endl;\\n\\u00a0 \\n\\u00a0     setup_time += time.wall_time();\\n\\u00a0     time_details << \\\"Distribute DoFs               \\\" << time.wall_time() << \\\" s\\\"\\n\\u00a0                  << std::endl;\\n\\u00a0     time.restart();\\n\\u00a0 \\n\\u00a0     AffineConstraints<double> dummy;\\n\\u00a0     dummy.close();\\n\\u00a0 \\n\\u00a0     {\\n\\u00a0       typename MatrixFree<dim, double>::AdditionalData additional_data;\\n\\u00a0       additional_data.tasks_parallel_scheme =\\n\\u00a0         MatrixFree<dim, double>::AdditionalData::none;\\n\\u00a0       additional_data.mapping_update_flags =\\n\\u00a0         (update_gradients | update_JxW_values | update_quadrature_points);\\n\\u00a0       additional_data.mapping_update_flags_inner_faces =\\n\\u00a0         (update_gradients | update_JxW_values | update_normal_vectors);\\n\\u00a0       additional_data.mapping_update_flags_boundary_faces =\\n\\u00a0         (update_gradients | update_JxW_values | update_normal_vectors |\\n\\u00a0          update_quadrature_points);\\n\\u00a0       const auto system_mf_storage =\\n\\u00a0         std::make_shared<MatrixFree<dim, double>>();\\n\\u00a0       system_mf_storage->reinit(\\n\\u00a0         mapping, dof_handler, dummy, QGauss<1>(fe.degree + 1), additional_data);\\n\\u00a0       system_matrix.initialize(system_mf_storage);\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     system_matrix.initialize_dof_vector(solution);\\n\\u00a0     system_matrix.initialize_dof_vector(system_rhs);\\n\\u00a0 \\n\\u00a0     setup_time += time.wall_time();\\n\\u00a0     time_details << \\\"Setup matrix-free system      \\\" << time.wall_time() << \\\" s\\\"\\n\\u00a0                  << std::endl;\\n\\u00a0     time.restart();\\n\\u00a0 \\n\\u00a0     const unsigned int nlevels = triangulation.n_global_levels();\\n\\u00a0     mg_matrices.resize(0, nlevels - 1);\\n\\u00a0 \\n\\u00a0     for (unsigned int level = 0; level < nlevels; ++level)\\n\\u00a0       {\\n\\u00a0         typename MatrixFree<dim, float>::AdditionalData additional_data;\\n\\u00a0         additional_data.tasks_parallel_scheme =\\n\\u00a0           MatrixFree<dim, float>::AdditionalData::none;\\n\\u00a0         additional_data.mapping_update_flags =\\n\\u00a0           (update_gradients | update_JxW_values);\\n\\u00a0         additional_data.mapping_update_flags_inner_faces =\\n\\u00a0           (update_gradients | update_JxW_values);\\n\\u00a0         additional_data.mapping_update_flags_boundary_faces =\\n\\u00a0           (update_gradients | update_JxW_values);\\n\\u00a0         additional_data.mg_level = level;\\n\\u00a0         const auto mg_mf_storage_level =\\n\\u00a0           std::make_shared<MatrixFree<dim, float>>();\\n\\u00a0         mg_mf_storage_level->reinit(mapping,\\n\\u00a0                                     dof_handler,\\n\\u00a0                                     dummy,\\n\\u00a0                                     QGauss<1>(fe.degree + 1),\\n\\u00a0                                     additional_data);\\n\\u00a0 \\n\\u00a0         mg_matrices[level].initialize(mg_mf_storage_level);\\n\\u00a0       }\\n\\u00a0     setup_time += time.wall_time();\\n\\u00a0     time_details << \\\"Setup matrix-free levels      \\\" << time.wall_time() << \\\" s\\\"\\n\\u00a0                  << std::endl;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nAffineConstraints::closevoid close()\\nTimerDefinition timer.h:117\\nTimer::wall_timedouble wall_time() constDefinition timer.cc:262\\nparallel::TriangulationBase::n_global_levelsvirtual unsigned int n_global_levels() const overrideDefinition tria_base.cc:141\\nlevelunsigned int levelDefinition grid_out.cc:4626\\nupdate_normal_vectors@ update_normal_vectorsNormal vectors.Definition fe_update_flags.h:141\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\nMatrixFree::AdditionalDataDefinition matrix_free.h:184\\nMatrixFree::AdditionalData::tasks_parallel_schemeTasksParallelScheme tasks_parallel_schemeDefinition matrix_free.h:347\\nThe computation of the right hand side is a bit more complicated than in step-37. The cell term now consists of the negative Laplacian of the analytical solution, RightHandSide, for which we need to first split up the Point of VectorizedArray fields, i.e., a batch of points, into a single point by evaluating all lanes in the VectorizedArray separately. Remember that the number of lanes depends on the hardware; it could be 1 for systems that do not offer vectorization (or where deal.II does not have intrinsics), but it could also be 8 or 16 on AVX-512 of recent Intel architectures.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void LaplaceProblem<dim, fe_degree>::compute_rhs()\\n\\u00a0   {\\n\\u00a0     Timer time;\\n\\u00a0     system_rhs                          = 0;\\n\\u00a0     const MatrixFree<dim, double> &data = *system_matrix.get_matrix_free();\\n\\u00a0     FEEvaluation<dim, fe_degree>   phi(data);\\n\\u00a0     RightHandSide<dim>             rhs_func;\\n\\u00a0     Solution<dim>                  exact_solution;\\n\\u00a0     for (unsigned int cell = 0; cell < data.n_cell_batches(); ++cell)\\n\\u00a0       {\\n\\u00a0         phi.reinit(cell);\\n\\u00a0         for (const unsigned int q : phi.quadrature_point_indices())\\n\\u00a0           {\\n\\u00a0             VectorizedArray<double> rhs_val = VectorizedArray<double>();\\n\\u00a0             Point<dim, VectorizedArray<double>> point_batch =\\n\\u00a0               phi.quadrature_point(q);\\n\\u00a0             for (unsigned int v = 0; v < VectorizedArray<double>::size(); ++v)\\n\\u00a0               {\\n\\u00a0                 Point<dim> single_point;\\n\\u00a0                 for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0                   single_point[d] = point_batch[d][v];\\n\\u00a0                 rhs_val[v] = rhs_func.value(single_point);\\n\\u00a0               }\\n\\u00a0             phi.submit_value(rhs_val, q);\\n\\u00a0           }\\n\\u00a0         phi.integrate_scatter(EvaluationFlags::values, system_rhs);\\n\\u00a0       }\\n\\u00a0 \\nSecondly, we also need to apply the Dirichlet and Neumann boundary conditions. This function is the missing part of to the function LaplaceOperator::apply_boundary() function once the exterior solution values \\\\(u^+ = -u^- + 2 g_\\\\text{D}\\\\) and  \\\\(\\\\mathbf{n}^-\\\\cdot \\\\nabla u^+ =\\n   \\\\mathbf{n}^-\\\\cdot \\\\nabla u^-\\\\) on Dirichlet boundaries and \\\\(u^+=u^-\\\\) and  \\\\(\\\\mathbf{n}^-\\\\cdot \\\\nabla u^+ = -\\\\mathbf{n}^-\\\\cdot \\\\nabla u^- + 2\\n   g_\\\\text{N}\\\\) on Neumann boundaries are inserted and expanded in terms of the boundary functions \\\\(g_\\\\text{D}\\\\) and \\\\(g_\\\\text{N}\\\\). One thing to remember is that we move the boundary conditions to the right hand side, so the sign is the opposite from what we imposed on the solution part.\\nWe could have issued both the cell and the boundary part through a MatrixFree::loop part, but we choose to manually write the full loop over all faces to learn how the index layout of face indices is set up in MatrixFree: Both the inner faces and the boundary faces share the index range, and all batches of inner faces have lower numbers than the batches of boundary cells. A single index for both variants allows us to easily use the same data structure FEFaceEvaluation for both cases that attaches to the same data field, just at different positions. The number of inner face batches (where a batch is due to the combination of several faces into one for vectorization) is given by MatrixFree::n_inner_face_batches(), whereas the number of boundary face batches is given by MatrixFree::n_boundary_face_batches().\\n\\u00a0     FEFaceEvaluation<dim, fe_degree> phi_face(data, true);\\n\\u00a0     for (unsigned int face = data.n_inner_face_batches();\\n\\u00a0          face < data.n_inner_face_batches() + data.n_boundary_face_batches();\\n\\u00a0          ++face)\\n\\u00a0       {\\n\\u00a0         phi_face.reinit(face);\\n\\u00a0 \\n\\u00a0         const VectorizedArray<double> inverse_length_normal_to_face = std::abs(\\n\\u00a0           (phi_face.normal_vector(0) * phi_face.inverse_jacobian(0))[dim - 1]);\\n\\u00a0         const VectorizedArray<double> sigma =\\n\\u00a0           inverse_length_normal_to_face * system_matrix.get_penalty_factor();\\n\\u00a0 \\n\\u00a0         for (const unsigned int q : phi_face.quadrature_point_indices())\\n\\u00a0           {\\n\\u00a0             VectorizedArray<double> test_value = VectorizedArray<double>(),\\n\\u00a0                                     test_normal_derivative =\\n\\u00a0                                       VectorizedArray<double>();\\n\\u00a0             Point<dim, VectorizedArray<double>> point_batch =\\n\\u00a0               phi_face.quadrature_point(q);\\n\\u00a0 \\n\\u00a0             for (unsigned int v = 0; v < VectorizedArray<double>::size(); ++v)\\n\\u00a0               {\\n\\u00a0                 Point<dim> single_point;\\n\\u00a0                 for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0                   single_point[d] = point_batch[d][v];\\n\\u00a0 \\nThe MatrixFree class lets us query the boundary_id of the current face batch. Remember that MatrixFree sets up the batches for vectorization such that all faces within a batch have the same properties, which includes their boundary_id. Thus, we can query that id here for the current face index face and either impose the Dirichlet case (where we add something to the function value) or the Neumann case (where we add something to the normal derivative).\\n\\u00a0                 if (data.get_boundary_id(face) == 0)\\n\\u00a0                   test_value[v] = 2.0 * exact_solution.value(single_point);\\n\\u00a0                 else\\n\\u00a0                   {\\n\\u00a0                     Tensor<1, dim> normal;\\n\\u00a0                     for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0                       normal[d] = phi_face.normal_vector(q)[d][v];\\n\\u00a0                     test_normal_derivative[v] =\\n\\u00a0                       -normal * exact_solution.gradient(single_point);\\n\\u00a0                   }\\n\\u00a0               }\\n\\u00a0             phi_face.submit_value(test_value * sigma - test_normal_derivative,\\n\\u00a0                                   q);\\n\\u00a0             phi_face.submit_normal_derivative(-0.5 * test_value, q);\\n\\u00a0           }\\n\\u00a0         phi_face.integrate_scatter(EvaluationFlags::values |\\n\\u00a0                                      EvaluationFlags::gradients,\\n\\u00a0                                    system_rhs);\\n\\u00a0       }\\n\\u00a0 \\nSince we have manually run the loop over cells rather than using MatrixFree::loop(), we must not forget to perform the data exchange with MPI - or actually, we would not need that for DG elements here because each cell carries its own degrees of freedom and cell and boundary integrals only evaluate quantities on the locally owned cells. The coupling to neighboring subdomain only comes in by the inner face integrals, which we have not done here. That said, it does not hurt to call this function here, so we do it as a reminder of what happens inside MatrixFree::loop().\\n\\u00a0     system_rhs.compress(VectorOperation::add);\\n\\u00a0     setup_time += time.wall_time();\\n\\u00a0     time_details << \\\"Compute right hand side       \\\" << time.wall_time()\\n\\u00a0                  << \\\" s\\\\n\\\";\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nVectorOperation::add@ addDefinition vector_operation.h:53\\nThe solve() function is copied almost verbatim from step-37. We set up the same multigrid ingredients, namely the level transfer, a smoother, and a coarse grid solver. The only difference is the fact that we do not use the diagonal of the Laplacian for the preconditioner of the Chebyshev iteration used for smoothing, but instead our newly resolved class PreconditionBlockJacobi. The mechanisms are the same, though.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void LaplaceProblem<dim, fe_degree>::solve()\\n\\u00a0   {\\n\\u00a0     Timer                            time;\\n\\u00a0     MGTransferMatrixFree<dim, float> mg_transfer;\\n\\u00a0     mg_transfer.build(dof_handler);\\n\\u00a0     setup_time += time.wall_time();\\n\\u00a0     time_details << \\\"MG build transfer time        \\\" << time.wall_time()\\n\\u00a0                  << \\\" s\\\\n\\\";\\n\\u00a0     time.restart();\\n\\u00a0 \\n\\u00a0     using SmootherType =\\n\\u00a0       PreconditionChebyshev<LevelMatrixType,\\n\\u00a0                             LinearAlgebra::distributed::Vector<float>,\\n\\u00a0                             PreconditionBlockJacobi<dim, fe_degree, float>>;\\n\\u00a0     mg::SmootherRelaxation<SmootherType,\\n\\u00a0                            LinearAlgebra::distributed::Vector<float>>\\n\\u00a0                                                          mg_smoother;\\n\\u00a0     MGLevelObject<typename SmootherType::AdditionalData> smoother_data;\\n\\u00a0     smoother_data.resize(0, triangulation.n_global_levels() - 1);\\n\\u00a0     for (unsigned int level = 0; level < triangulation.n_global_levels();\\n\\u00a0          ++level)\\n\\u00a0       {\\n\\u00a0         if (level > 0)\\n\\u00a0           {\\n\\u00a0             smoother_data[level].smoothing_range     = 15.;\\n\\u00a0             smoother_data[level].degree              = 3;\\n\\u00a0             smoother_data[level].eig_cg_n_iterations = 10;\\n\\u00a0           }\\n\\u00a0         else\\n\\u00a0           {\\n\\u00a0             smoother_data[0].smoothing_range = 2e-2;\\n\\u00a0             smoother_data[0].degree          = numbers::invalid_unsigned_int;\\n\\u00a0             smoother_data[0].eig_cg_n_iterations = mg_matrices[0].m();\\n\\u00a0           }\\n\\u00a0         smoother_data[level].preconditioner =\\n\\u00a0           std::make_shared<PreconditionBlockJacobi<dim, fe_degree, float>>();\\n\\u00a0         smoother_data[level].preconditioner->initialize(mg_matrices[level]);\\n\\u00a0       }\\n\\u00a0     mg_smoother.initialize(mg_matrices, smoother_data);\\n\\u00a0 \\n\\u00a0     MGCoarseGridApplySmoother<LinearAlgebra::distributed::Vector<float>>\\n\\u00a0       mg_coarse;\\n\\u00a0     mg_coarse.initialize(mg_smoother);\\n\\u00a0 \\n\\u00a0     mg::Matrix<LinearAlgebra::distributed::Vector<float>> mg_matrix(\\n\\u00a0       mg_matrices);\\n\\u00a0 \\n\\u00a0     Multigrid<LinearAlgebra::distributed::Vector<float>> mg(\\n\\u00a0       mg_matrix, mg_coarse, mg_transfer, mg_smoother, mg_smoother);\\n\\u00a0 \\n\\u00a0     PreconditionMG<dim,\\n\\u00a0                    LinearAlgebra::distributed::Vector<float>,\\n\\u00a0                    MGTransferMatrixFree<dim, float>>\\n\\u00a0       preconditioner(dof_handler, mg, mg_transfer);\\n\\u00a0 \\n\\u00a0     SolverControl solver_control(10000, 1e-12 * system_rhs.l2_norm());\\n\\u00a0     SolverCG<LinearAlgebra::distributed::Vector<double>> cg(solver_control);\\n\\u00a0     setup_time += time.wall_time();\\n\\u00a0     time_details << \\\"MG build smoother time        \\\" << time.wall_time()\\n\\u00a0                  << \\\"s\\\\n\\\";\\n\\u00a0     pcout << \\\"Total setup time              \\\" << setup_time << \\\" s\\\\n\\\";\\n\\u00a0 \\n\\u00a0     time.reset();\\n\\u00a0     time.start();\\n\\u00a0     cg.solve(system_matrix, solution, system_rhs, preconditioner);\\n\\u00a0 \\n\\u00a0     pcout << \\\"Time solve (\\\" << solver_control.last_step() << \\\" iterations)    \\\"\\n\\u00a0           << time.wall_time() << \\\" s\\\" << std::endl;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nMGCoarseGridApplySmootherDefinition mg_coarse.h:40\\nMGCoarseGridApplySmoother::initializevoid initialize(const MGSmootherBase< VectorType > &coarse_smooth)\\nMGLevelObject::resizevoid resize(const unsigned int new_minlevel, const unsigned int new_maxlevel, Args &&...args)Definition mg_level_object.h:256\\nMGTransferMatrixFreeDefinition mg_transfer_matrix_free.h:57\\nMGTransferMatrixFree::buildvoid build(const DoFHandler< dim > &dof_handler, const std::vector< std::shared_ptr< const Utilities::MPI::Partitioner > > &external_partitioners=std::vector< std::shared_ptr< const Utilities::MPI::Partitioner > >())Definition mg_transfer_matrix_free.cc:98\\nMultigridDefinition multigrid.h:163\\nPreconditionChebyshevDefinition precondition.h:2105\\nPreconditionMGDefinition multigrid.h:501\\nSolverCGDefinition solver_cg.h:179\\nSolverControlDefinition solver_control.h:67\\nmg::MatrixDefinition mg_matrix.h:46\\nmg::SmootherRelaxationDefinition mg_smoother.h:186\\nmgDefinition mg.h:81\\nSince we have solved a problem with analytic solution, we want to verify the correctness of our implementation by computing the L2 error of the numerical result against the analytic solution.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void LaplaceProblem<dim, fe_degree>::analyze_results() const\\n\\u00a0   {\\n\\u00a0     Vector<float> error_per_cell(triangulation.n_active_cells());\\n\\u00a0     VectorTools::integrate_difference(mapping,\\n\\u00a0                                       dof_handler,\\n\\u00a0                                       solution,\\n\\u00a0                                       Solution<dim>(),\\n\\u00a0                                       error_per_cell,\\n\\u00a0                                       QGauss<dim>(fe.degree + 2),\\n\\u00a0                                       VectorTools::L2_norm);\\n\\u00a0     pcout << \\\"Verification via L2 error:    \\\"\\n\\u00a0           << std::sqrt(\\n\\u00a0                Utilities::MPI::sum(error_per_cell.norm_sqr(), MPI_COMM_WORLD))\\n\\u00a0           << std::endl;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nVectorDefinition vector.h:120\\nUtilities::MPI::sumT sum(const T &t, const MPI_Comm mpi_communicator)\\nVectorTools::L2_norm@ L2_normDefinition vector_tools_common.h:112\\nVectorTools::integrate_differencevoid integrate_difference(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const ReadVector< Number > &fe_function, const Function< spacedim, Number > &exact_solution, OutVector &difference, const Quadrature< dim > &q, const NormType &norm, const Function< spacedim, double > *weight=nullptr, const double exponent=2.)\\nstd::sqrt::VectorizedArray< Number, width > sqrt(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6869\\nThe run() function sets up the initial grid and then runs the multigrid program in the usual way. As a domain, we choose a rectangle with periodic boundary conditions in the \\\\(x\\\\)-direction, a Dirichlet condition on the front face in \\\\(y\\\\) direction (i.e., the face with index number 2, with boundary id equal to 0), and Neumann conditions on the back face as well as the two faces in \\\\(z\\\\) direction for the 3d case (with boundary id equal to 1). The extent of the domain is a bit different in the \\\\(x\\\\) direction (where we want to achieve a periodic solution given the definition of Solution) as compared to the \\\\(y\\\\) and \\\\(z\\\\) directions.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void LaplaceProblem<dim, fe_degree>::run()\\n\\u00a0   {\\n\\u00a0     const unsigned int n_ranks =\\n\\u00a0       Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD);\\n\\u00a0     pcout << \\\"Running with \\\" << n_ranks << \\\" MPI process\\\"\\n\\u00a0           << (n_ranks > 1 ? \\\"es\\\" : \\\"\\\") << \\\", element \\\" << fe.get_name()\\n\\u00a0           << std::endl\\n\\u00a0           << std::endl;\\n\\u00a0     for (unsigned int cycle = 0; cycle < 9 - dim; ++cycle)\\n\\u00a0       {\\n\\u00a0         pcout << \\\"Cycle \\\" << cycle << std::endl;\\n\\u00a0 \\n\\u00a0         if (cycle == 0)\\n\\u00a0           {\\n\\u00a0             Point<dim> upper_right;\\n\\u00a0             upper_right[0] = 2.5;\\n\\u00a0             for (unsigned int d = 1; d < dim; ++d)\\n\\u00a0               upper_right[d] = 2.8;\\n\\u00a0             GridGenerator::hyper_rectangle(triangulation,\\n\\u00a0                                            Point<dim>(),\\n\\u00a0                                            upper_right);\\n\\u00a0             triangulation.begin_active()->face(0)->set_boundary_id(10);\\n\\u00a0             triangulation.begin_active()->face(1)->set_boundary_id(11);\\n\\u00a0             triangulation.begin_active()->face(2)->set_boundary_id(0);\\n\\u00a0             for (unsigned int f = 3;\\n\\u00a0                  f < triangulation.begin_active()->n_faces();\\n\\u00a0                  ++f)\\n\\u00a0               triangulation.begin_active()->face(f)->set_boundary_id(1);\\n\\u00a0 \\n\\u00a0             std::vector<GridTools::PeriodicFacePair<\\n\\u00a0               typename Triangulation<dim>::cell_iterator>>\\n\\u00a0               periodic_faces;\\n\\u00a0             GridTools::collect_periodic_faces(\\n\\u00a0               triangulation, 10, 11, 0, periodic_faces);\\n\\u00a0             triangulation.add_periodicity(periodic_faces);\\n\\u00a0 \\n\\u00a0             triangulation.refine_global(6 - 2 * dim);\\n\\u00a0           }\\n\\u00a0         triangulation.refine_global(1);\\n\\u00a0         setup_system();\\n\\u00a0         compute_rhs();\\n\\u00a0         solve();\\n\\u00a0         analyze_results();\\n\\u00a0         pcout << std::endl;\\n\\u00a0       };\\n\\u00a0   }\\n\\u00a0 } // namespace Step59\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTriaIteratorDefinition tria_iterator.h:574\\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nTriangulation::begin_activeactive_cell_iterator begin_active(const unsigned int level=0) const\\nparallel::distributed::Triangulation::add_periodicityvirtual void add_periodicity(const std::vector<::GridTools::PeriodicFacePair< cell_iterator > > &) overrideDefinition tria.cc:3806\\nGridGenerator::hyper_rectanglevoid hyper_rectangle(Triangulation< dim, spacedim > &tria, const Point< dim > &p1, const Point< dim > &p2, const bool colorize=false)\\nGridTools::collect_periodic_facesvoid collect_periodic_faces(const MeshType &mesh, const types::boundary_id b_id1, const types::boundary_id b_id2, const unsigned int direction, std::vector< PeriodicFacePair< typename MeshType::cell_iterator > > &matched_pairs, const Tensor< 1, MeshType::space_dimension > &offset=::Tensor< 1, MeshType::space_dimension >(), const FullMatrix< double > &matrix=FullMatrix< double >())Definition grid_tools_dof_handlers.cc:2299\\nUtilities::MPI::n_mpi_processesunsigned int n_mpi_processes(const MPI_Comm mpi_communicator)Definition mpi.cc:92\\nGridTools::PeriodicFacePairDefinition grid_tools.h:2333\\nThere is nothing unexpected in the main() function. We call MPI_Init() through the MPI_InitFinalize class, pass on the two parameters on the dimension and the degree set at the top of the file, and run the Laplace problem.\\n\\u00a0 int main(int argc, char *argv[])\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       using namespace Step59;\\n\\u00a0 \\n\\u00a0       Utilities::MPI::MPI_InitFinalize mpi_init(argc, argv, 1);\\n\\u00a0 \\n\\u00a0       LaplaceProblem<dimension, degree_finite_element> laplace_problem;\\n\\u00a0       laplace_problem.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return 0;\\n\\u00a0 }\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\n Results\\nProgram output\\nLike in step-37, we evaluate the multigrid solver in terms of run time. In two space dimensions with elements of degree 8, a possible output could look as follows: Running with 12 MPI processes, element FE_DGQHermite<2>(8)\\n \\nCycle 0\\nNumber of degrees of freedom: 5184\\nTotal setup time              0.0282445 s\\nTime solve (14 iterations)    0.0110712 s\\nVerification via L2 error:    1.66232e-07\\n \\nCycle 1\\nNumber of degrees of freedom: 20736\\nTotal setup time              0.0126282 s\\nTime solve (14 iterations)    0.0157021 s\\nVerification via L2 error:    2.91505e-10\\n \\nCycle 2\\nNumber of degrees of freedom: 82944\\nTotal setup time              0.0227573 s\\nTime solve (14 iterations)    0.026568 s\\nVerification via L2 error:    6.64514e-13\\n \\nCycle 3\\nNumber of degrees of freedom: 331776\\nTotal setup time              0.0604685 s\\nTime solve (14 iterations)    0.0628356 s\\nVerification via L2 error:    5.57513e-13\\n \\nCycle 4\\nNumber of degrees of freedom: 1327104\\nTotal setup time              0.154359 s\\nTime solve (13 iterations)    0.219555 s\\nVerification via L2 error:    3.08139e-12\\n \\nCycle 5\\nNumber of degrees of freedom: 5308416\\nTotal setup time              0.467764 s\\nTime solve (13 iterations)    1.1821 s\\nVerification via L2 error:    3.90334e-12\\n \\nCycle 6\\nNumber of degrees of freedom: 21233664\\nTotal setup time              1.73263 s\\nTime solve (13 iterations)    5.21054 s\\nVerification via L2 error:    4.94543e-12\\nLike in step-37, the number of CG iterations remains constant with increasing problem size. The iteration counts are a bit higher, which is because we use a lower degree of the Chebyshev polynomial (2 vs 5 in step-37) and because the interior penalty discretization has a somewhat larger spread in eigenvalues. Nonetheless, 13 iterations to reduce the residual by 12 orders of magnitude, or almost a factor of 9 per iteration, indicates an overall very efficient method. In particular, we can solve a system with 21 million degrees of freedom in 5 seconds when using 12 cores, which is a very good efficiency. Of course, in 2D we are well inside the regime of roundoff for a polynomial degree of 8 \\u2013 as a matter of fact, around 83k DoFs or 0.025s would have been enough to fully converge this (simple) analytic solution here.\\nNot much changes if we run the program in three spatial dimensions, except for the fact that we now use do something more useful with the higher polynomial degree and increasing mesh sizes, as the roundoff errors are only obtained at the finest mesh. Still, it is remarkable that we can solve a 3D Laplace problem with a wave of three periods to roundoff accuracy on a twelve-core machine pretty easily - using about 3.5 GB of memory in total for the second to largest case with 24m DoFs, taking not more than eight seconds. The largest case uses 30GB of memory with 191m DoFs.\\nRunning with 12 MPI processes, element FE_DGQHermite<3>(8)\\n \\nCycle 0\\nNumber of degrees of freedom: 5832\\nTotal setup time              0.0210681 s\\nTime solve (15 iterations)    0.0956945 s\\nVerification via L2 error:    0.0297194\\n \\nCycle 1\\nNumber of degrees of freedom: 46656\\nTotal setup time              0.0452428 s\\nTime solve (15 iterations)    0.113827 s\\nVerification via L2 error:    9.55733e-05\\n \\nCycle 2\\nNumber of degrees of freedom: 373248\\nTotal setup time              0.190423 s\\nTime solve (15 iterations)    0.218309 s\\nVerification via L2 error:    2.6868e-07\\n \\nCycle 3\\nNumber of degrees of freedom: 2985984\\nTotal setup time              0.627914 s\\nTime solve (15 iterations)    1.0595 s\\nVerification via L2 error:    4.6918e-10\\n \\nCycle 4\\nNumber of degrees of freedom: 23887872\\nTotal setup time              2.85215 s\\nTime solve (15 iterations)    8.30576 s\\nVerification via L2 error:    9.38583e-13\\n \\nCycle 5\\nNumber of degrees of freedom: 191102976\\nTotal setup time              16.1324 s\\nTime solve (15 iterations)    65.57 s\\nVerification via L2 error:    3.17875e-13\\nComparison of efficiency at different polynomial degrees\\nIn the introduction and in-code comments, it was mentioned several times that high orders are treated very efficiently with the FEEvaluation and FEFaceEvaluation evaluators. Now, we want to substantiate these claims by looking at the throughput of the 3D multigrid solver for various polynomial degrees. We collect the times as follows: We first run a solver at problem size close to ten million, indicated in the first four table rows, and record the timings. Then, we normalize the throughput by recording the number of million degrees of freedom solved per second (MDoFs/s) to be able to compare the efficiency of the different degrees, which is computed by dividing the number of degrees of freedom by the solver time.\\n\\n\\ndegree 1 2 3 4 5 6 7 8 9 10 11 12  \\n\\nNumber of DoFs 2097152 7077888 16777216 32768000 7077888 11239424 16777216 23887872 32768000 43614208 7077888 8998912  \\n\\nNumber of iterations 13 12 12 12 13 13 15 15 17 19 18 18  \\n\\nSolver time [s] 0.713 2.150 4.638 8.803 2.041 3.295 5.723 8.306 12.75 19.25 3.530 4.814  \\n\\nMDoFs/s 2.94 3.29 3.62 3.72 3.47 3.41 2.93 2.88 2.57 2.27 2.01 1.87  \\n\\nWe clearly see how the efficiency per DoF initially improves until it reaches a maximum for the polynomial degree \\\\(k=4\\\\). This effect is surprising, not only because higher polynomial degrees often yield a vastly better solution, but especially also when having matrix-based schemes in mind where the denser coupling at higher degree leads to a monotonously decreasing throughput (and a drastic one in 3D, with \\\\(k=4\\\\) being more than ten times slower than \\\\(k=1\\\\)!). For higher degrees, the throughput decreases a bit, which is both due to an increase in the number of iterations (going from 12 at \\\\(k=2,3,4\\\\) to 19 at \\\\(k=10\\\\)) and due to the \\\\(\\\\mathcal O(k)\\\\) complexity of operator evaluation. Nonetheless, efficiency as the time to solution would be still better for higher polynomial degrees because they have better convergence rates (at least for problems as simple as this one): For \\\\(k=12\\\\), we reach roundoff accuracy already with 1 million DoFs (solver time less than a second), whereas for \\\\(k=8\\\\) we need 24 million DoFs and 8 seconds. For \\\\(k=5\\\\), the error is around \\\\(10^{-9}\\\\) with 57m DoFs and thus still far away from roundoff, despite taking 16 seconds.\\nNote that the above numbers are a bit pessimistic because they include the time it takes the Chebyshev smoother to compute an eigenvalue estimate, which is around 10 percent of the solver time. If the system is solved several times (as e.g. common in fluid dynamics), this eigenvalue cost is only paid once and faster times become available.\\nEvaluation of efficiency of ingredients\\nFinally, we take a look at some of the special ingredients presented in this tutorial program, namely the FE_DGQHermite basis in particular and the specification of MatrixFree::DataAccessOnFaces. In the following table, the third row shows the optimized solver above, the fourth row shows the timings with only the MatrixFree::DataAccessOnFaces set to unspecified rather than the optimal gradients, and the last one with replacing FE_DGQHermite by the basic FE_DGQ elements where both the MPI exchange are more expensive and the operations done by FEFaceEvaluation::gather_evaluate() and FEFaceEvaluation::integrate_scatter().\\n\\n\\ndegree 1 2 3 4 5 6 7 8 9 10 11 12  \\n\\nNumber of DoFs 2097152 7077888 16777216 32768000 7077888 11239424 16777216 23887872 32768000 43614208 7077888 8998912  \\n\\nSolver time optimized as in tutorial [s] 0.713 2.150 4.638 8.803 2.041 3.295 5.723 8.306 12.75 19.25 3.530 4.814  \\n\\nSolver time MatrixFree::DataAccessOnFaces::unspecified [s] 0.711 2.151 4.675 8.968 2.243 3.655 6.277 9.082 13.50 20.05 3.817 5.178  \\n\\nSolver time FE_DGQ [s] 0.712 2.041 5.066 9.335 2.379 3.802 6.564 9.714 14.54 22.76 4.148 5.857  \\n\\nThe data in the table shows that not using MatrixFree::DataAccessOnFaces increases costs by around 10% for higher polynomial degrees. For lower degrees, the difference is obviously less pronounced because the volume-to-surface ratio is more beneficial and less data needs to be exchanged. The difference is larger when looking at the matrix-vector product only, rather than the full multigrid solver shown here, with around 20% worse timings just because of the MPI communication.\\nFor \\\\(k=1\\\\) and \\\\(k=2\\\\), the Hermite-like basis functions do obviously not really pay off (indeed, for \\\\(k=1\\\\) the polynomials are exactly the same as for FE_DGQ) and the results are similar as with the FE_DGQ basis. However, for degrees starting at three, we see an increasing advantage for FE_DGQHermite, showing the effectiveness of these basis functions.\\nPossibilities for extension\\nAs mentioned in the introduction, the fast diagonalization method as realized here is tied to a Cartesian mesh with constant coefficients. When dealing with meshes that contain deformed cells or with variable coefficients, it is common to determine a nearby Cartesian mesh cell as an approximation. This can be done with the class TensorProductMatrixSymmetricSumCollection. Here, one can insert cell matrices similarly to the PreconditionBlockJacobi::initialize() function of this tutorial program. The benefit of the collection class is that cells on which the coefficient of the PDE has the same value can re-use the same Laplacian matrix, which reduces the memory consumption for the inverse matrices. As compared to the algorithm implemented in this tutorial program, one would define the length scales as the distances between opposing faces. For continuous elements, the code project <a href=https://github.com/peterrum/dealii-dd-and-schwarz\\\">Cache-optimized and\\nlow-overhead implementations of multigrid smoothers for high-order FEM\\ncomputations</a> presents the computation for continuous elements. There is\\ncurrently no infrastructure in deal.II to automatically generate the 1D\\nmatrices for discontinuous elements with SIP-DG discretization, as opposed to\\ncontinuous elements, where we provide\\nTensorProductMatrixCreator::create_laplace_tensor_product_matrix().\\n\\nAnother way of extending the program would be to include support for adaptive\\nmeshes. While the classical approach of defining interface operations at edges\\nof different refinement level, as discussed in @ref step_39 \\\"step-39\\\", is one possibility,\\nfor Poisson-type problems another option is typically more beneficial. Using\\nthe class MGTransferGlobalCoarsening, which is explained in the @ref step_75 \\\"step-75\\\"\\ntutorial program, one can deal with meshes of hanging nodes on all levels. An\\nalgorithmic improvement can be obtained by combining the discontinuous\\nfunction space with the auxiliary continuous finite element space of the same\\npolynomial degree. This idea, introduced by Antonietti et al.\\n@cite antonietti2016uniform in 2016, allows making the multigrid convergence\\nindependent of the penalty parameter. As demonstrated by Fehn et al.\\n@cite fehn2020hybrid, this also gives considerably lower iteration counts than\\na multigrid solver directly working on levels with discontinuous function\\nspaces. The latter work also proposes p-multigrid techniques and combination\\nwith algebraic multigrid coarse spaces as a means to efficiently solve Poisson\\nproblems with high-order discontinuous Galerkin discretizations on complicated\\ngeometries, representing the current state-of-the-art for simple Poisson-type\\nproblems. The class MGTransferGlobalCoarsening provides features for each of\\nthese three coarsening variants, the discontinuous-continuous auxiliary\\nfunction concept, p-multigrid, and traditional h-multigrid. The main\\ningredient is to define an appropriate MGTwoLevelTransfer object and call\\nMGTwoLevelTransfer::reinit_geometric_transfer() or\\nMGTwoLevelTransfer::reinit_polynomial_transfer(), respectively.\\n\\n\\n@anchor step_59-PlainProg <a></a>\\n<h1> The plain program</h1>\\n@include \\\"step-59.cc\\\" \\n\\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"