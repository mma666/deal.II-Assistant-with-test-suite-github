"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_48.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-48 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-48 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-48 tutorial program\\n\\n\\nThis tutorial depends on step-25, step-37.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\n Problem statement and discretization \\nImplementation of constraints\\n Parallelization \\n The test case \\n\\n The commented program\\n\\nSineGordonOperation\\n\\nSineGordonOperation::SineGordonOperation\\nSineGordonOperation::local_apply\\nSineGordonOperation::apply\\n\\nEquation data\\nSineGordonProblem class\\n\\nSineGordonProblem::SineGordonProblem\\nSineGordonProblem::make_grid_and_dofs\\nSineGordonProblem::output_results\\nSineGordonProblem::run\\n\\nThe main function\\n\\n\\n Results\\n\\nComparison with a sparse matrix\\nParallel run in 2D and 3D\\nPossibilities for extensions\\n\\n The plain program\\n   \\n\\n This program was contributed by Katharina Kormann and Martin Kronbichler.\\nThe algorithm for the matrix-vector product is based on the article A generic interface for parallel cell-based finite element operator application by Martin Kronbichler and Katharina Kormann, Computers and Fluids 63:135\\u2013147, 2012, and the paper \\\"Parallel finite element operator application: Graph partitioning and coloring\\\" by Katharina Kormann and Martin Kronbichler in: Proceedings of the 7th IEEE International Conference on e-Science, 2011. \\n Introduction\\nThis program demonstrates how to use the cell-based implementation of finite element operators with the MatrixFree class, first introduced in step-37, to solve nonlinear partial differential equations. Moreover, we have another look at the handling of constraints within the matrix-free framework. Finally, we will use an explicit time-stepping method to solve the problem and introduce Gauss-Lobatto finite elements that are very convenient in this case since their mass matrix can be accurately approximated by a diagonal, and thus trivially invertible, matrix. The two ingredients to this property are firstly a distribution of the nodal points of Lagrange polynomials according to the point distribution of the Gauss-Lobatto quadrature rule. Secondly, the quadrature is done with the same Gauss-Lobatto quadrature rule. In this formula, the integrals  \\\\(\\\\int_K \\\\varphi_i \\\\varphi_j\\ndx\\\\approx \\\\sum_q \\\\varphi_i \\\\varphi_j \\\\mathrm{det}(J) \\\\big |_{x_q}\\\\) become zero whenever \\\\(i\\\\neq j\\\\), because exactly one function \\\\(\\\\varphi_j\\\\) is one and all others zero in the points defining the Lagrange polynomials. Moreover, the Gauss-Lobatto distribution of nodes of Lagrange polynomials clusters the nodes towards the element boundaries. This results in a well-conditioned polynomial basis for high-order discretization methods. Indeed, the condition number of an FE_Q elements with equidistant nodes grows exponentially with the degree, which destroys any benefit for orders of about five and higher. For this reason, Gauss-Lobatto points are the default distribution for the FE_Q element (but at degrees one and two, those are equivalent to the equidistant points).\\nProblem statement and discretization \\nAs an example, we choose to solve the sine-Gordon soliton equation       \\n\\\\begin{eqnarray*}\\nu_{tt} &=& \\\\Delta u -\\\\sin(u) \\\\quad\\\\mbox{for}\\\\quad (x,t) \\\\in\\n\\\\Omega \\\\times (t_0,t_f],\\\\\\\\\\n{\\\\mathbf n} \\\\cdot \\\\nabla u &=& 0\\n\\\\quad\\\\mbox{for}\\\\quad (x,t) \\\\in \\\\partial\\\\Omega \\\\times (t_0,t_f],\\\\\\\\\\nu(x,t_0) &=& u_0(x).\\n\\\\end{eqnarray*}\\n\\nthat was already introduced in step-25. As a simple explicit time integration method, we choose leap frog scheme using the second-order formulation of the equation. With this time stepping, the scheme reads in weak form\\n\\n\\\\begin{eqnarray*}\\n(v,u^{n+1}) = (v,2 u^n-u^{n-1} -\\n(\\\\Delta t)^2 \\\\sin(u^n)) - (\\\\nabla v, (\\\\Delta t)^2 \\\\nabla u^n),\\n\\\\end{eqnarray*}\\n\\n where  v denotes a test function and the index n stands for the time step number.\\nFor the spatial discretization, we choose FE_Q elements with basis functions defined to interpolate the support points of the Gauss-Lobatto quadrature rule. Moreover, when we compute the integrals over the basis functions to form the mass matrix and the operator on the right hand side of the equation above, we use the Gauss-Lobatto quadrature rule with the same support points as the node points of the finite element to evaluate the integrals. Since the finite element is Lagrangian, this will yield a diagonal mass matrix on the left hand side of the equation, making the solution of the linear system in each time step trivial.\\nUsing this quadrature rule, for a pth order finite element, we use a (2p-1)th order accurate formula to evaluate the integrals. Since the product of two pth order basis functions when computing a mass matrix gives a function with polynomial degree 2p in each direction, the integrals are not computed exactly. However, the overall convergence properties are not disturbed by the quadrature error on meshes with affine element shapes with L2 errors proportional to hp+1. Note however that order reduction with sub-optimal convergence rates of the L2 error of O(hp) or even O(hp-1) for some 3D setups has been reported in literature on deformed (non-affine) element shapes for wave equations when the integrand is not a polynomial any more.\\nApart from the fact that we avoid solving linear systems with this type of elements when using explicit time-stepping, they come with two other advantages. When we are using the sum-factorization approach to evaluate the finite element operator (cf. step-37), we have to evaluate the function at the quadrature points. In the case of Gauss-Lobatto elements, where quadrature points and node points of the finite element coincide, this operation is trivial since the value of the function at the quadrature points is given by its one-dimensional coefficients. In this way, the arithmetic work for the finite element operator evaluation is reduced by approximately a factor of two compared to the generic Gaussian quadrature.\\nTo sum up the discussion, by using the right finite element and quadrature rule combination, we end up with a scheme where we only need to compute the right hand side vector corresponding to the formulation above and then multiply it by the inverse of the diagonal mass matrix in each time step. In practice, of course, we extract the diagonal elements and invert them only once at the beginning of the program.\\nImplementation of constraints\\nThe usual way to handle constraints in deal.II is to use the AffineConstraints class that builds a sparse matrix storing information about which degrees of freedom (DoF) are constrained and how they are constrained. This format uses an unnecessarily large amount of memory since there are not so many different types of constraints: for example, in the case of hanging nodes when using linear finite element on every cell, most constraints have the form \\\\(x_k = \\\\frac 12 x_i + \\\\frac 12 x_j\\\\) where the coefficients \\\\(\\\\frac 12\\\\) are always the same and only \\\\(i,j,k\\\\) are different. While storing this redundant information is not a problem in general because it is only needed once during matrix and right hand side assembly, it becomes a bottleneck in the matrix-free approach since there this information has to be accessed every time we apply the operator, and the remaining components of the operator evaluation are so fast. Thus, instead of an AffineConstraints object, MatrixFree uses a variable that we call constraint_pool that collects the weights of the different constraints. Then, only an identifier of each constraint in the mesh instead of all the weights have to be stored. Moreover, the constraints are not applied in a pre- and postprocessing step but rather as we evaluate the finite element operator. Therefore, the constraint information is embedded into the variable indices_local_to_global that is used to extract the cell information from the global vector. If a DoF is constrained, the indices_local_to_global variable contains the global indices of the DoFs that it is constrained to. Then, we have another variable constraint_indicator at hand that holds, for each cell, the local indices of DoFs that are constrained as well as the identifier of the type of constraint. Fortunately, you will not see these data structures in the example program since the class FEEvaluation takes care of the constraints without user interaction.\\nIn the presence of hanging nodes, the diagonal mass matrix obtained on the element level via the Gauss-Lobatto quadrature/node point procedure does not directly translate to a diagonal global mass matrix, as following the constraints on rows and columns would also add off-diagonal entries. As explained in Kormann (2016), interpolating constraints on a vector, which maintains the diagonal shape of the mass matrix, is consistent with the equations up to an error of the same magnitude as the quadrature error. In the program below, we will simply assemble the diagonal of the mass matrix as if it were a vector to enable this approximation.\\nParallelization \\nThe MatrixFree class comes with the option to be parallelized on three levels: MPI parallelization on clusters of distributed nodes, thread parallelization scheduled by the Threading Building Blocks library, and finally with a vectorization by working on a batch of two (or more) cells via SIMD data type (sometimes called cross-element or external vectorization). As we have already discussed in step-37, you will get best performance by using an instruction set specific to your system, e.g. with the cmake variable -DCMAKE_CXX_FLAGS=\\\"-march=native\\\". The MPI parallelization was already exploited in step-37. Here, we additionally consider thread parallelization with TBB. This is fairly simple, as all we need to do is to tell the initialization of the MatrixFree object about the fact that we want to use a thread parallel scheme through the variable MatrixFree::AdditionalData::thread_parallel_scheme. During setup, a dependency graph is set up similar to the one described in the workstream_paper , which allows to schedule the work of the local_apply function on chunks of cells without several threads accessing the same vector indices. As opposed to the WorkStream loops, some additional clever tricks to avoid global synchronizations as described in Kormann and Kronbichler (2011) are also applied.\\nNote that this program is designed to be run with a distributed triangulation (parallel::distributed::Triangulation), which requires deal.II to be configured with p4est as described in the deal.II ReadMe file. However, a non-distributed triangulation is also supported, in which case the computation will be run in serial.\\nThe test case \\nIn our example, we choose the initial value to be    \\n\\\\begin{eqnarray*} u(x,t) =\\n\\\\prod_{i=1}^{d} -4 \\\\arctan \\\\left(\\n\\\\frac{m}{\\\\sqrt{1-m^2}}\\\\frac{\\\\sin\\\\left(\\\\sqrt{1-m^2} t +c_2\\\\right)}{\\\\cosh(mx_i+c_1)}\\\\right)\\n\\\\end{eqnarray*}\\n\\n and solve the equation over the time interval [-10,10]. The constants are chosen to be \\\\(c_1=c_1=0\\\\) and  m=0.5. As mentioned in step-25, in one dimension u as a function of t is the exact solution of the sine-Gordon equation. For higher dimension, this is however not the case.\\n The commented program\\nThe necessary files from the deal.II library.\\n\\u00a0 #include <deal.II/base/multithread_info.h>\\n\\u00a0 #include <deal.II/base/utilities.h>\\n\\u00a0 #include <deal.II/base/function.h>\\n\\u00a0 #include <deal.II/base/conditional_ostream.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 #include <deal.II/lac/vector.h>\\n\\u00a0 #include <deal.II/grid/tria.h>\\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 #include <deal.II/fe/fe_values.h>\\n\\u00a0 #include <deal.II/fe/mapping_q1.h>\\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/distributed/tria.h>\\n\\u00a0 \\nThis includes the data structures for the efficient implementation of matrix-free methods.\\n\\u00a0 #include <deal.II/lac/la_parallel_vector.h>\\n\\u00a0 #include <deal.II/matrix_free/matrix_free.h>\\n\\u00a0 #include <deal.II/matrix_free/fe_evaluation.h>\\n\\u00a0 \\n\\u00a0 #include <fstream>\\n\\u00a0 #include <iostream>\\n\\u00a0 #include <iomanip>\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 namespace Step48\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\nWe start by defining two global variables to collect all parameters subject to changes at one place: One for the dimension and one for the finite element degree. The dimension is used in the main function as a template argument for the actual classes (like in all other deal.II programs), whereas the degree of the finite element is more crucial, as it is passed as a template argument to the implementation of the Sine-Gordon operator. Therefore, it needs to be a compile-time constant.\\n\\u00a0   const unsigned int dimension = 2;\\n\\u00a0   const unsigned int fe_degree = 4;\\n\\u00a0 \\n\\u00a0 \\n SineGordonOperation\\nThe SineGordonOperation class implements the cell-based operation that is needed in each time step. This nonlinear operation can be implemented straight-forwardly based on the MatrixFree class, in the same way as a linear operation would be treated by this implementation of the finite element operator application. We apply two template arguments to the class, one for the dimension and one for the degree of the finite element. This is a difference to other functions in deal.II where only the dimension is a template argument. This is necessary to provide the inner loops in FEEvaluation with information about loop lengths etc., which is essential for efficiency. On the other hand, it makes it more challenging to implement the degree as a run-time parameter.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   class SineGordonOperation\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     SineGordonOperation(const MatrixFree<dim, double> &data_in,\\n\\u00a0                         const double                   time_step);\\n\\u00a0 \\n\\u00a0     void apply(LinearAlgebra::distributed::Vector<double> &dst,\\n\\u00a0                const std::vector<LinearAlgebra::distributed::Vector<double> *>\\n\\u00a0                  &src) const;\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     const MatrixFree<dim, double>             &data;\\n\\u00a0     const VectorizedArray<double>              delta_t_sqr;\\n\\u00a0     LinearAlgebra::distributed::Vector<double> inv_mass_matrix;\\n\\u00a0 \\n\\u00a0     void local_apply(\\n\\u00a0       const MatrixFree<dim, double>                                   &data,\\n\\u00a0       LinearAlgebra::distributed::Vector<double>                      &dst,\\n\\u00a0       const std::vector<LinearAlgebra::distributed::Vector<double> *> &src,\\n\\u00a0       const std::pair<unsigned int, unsigned int> &cell_range) const;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nLinearAlgebra::distributed::VectorDefinition la_parallel_vector.h:250\\nMatrixFreeDefinition matrix_free.h:113\\nVectorizedArrayDefinition vectorization.h:445\\n SineGordonOperation::SineGordonOperation\\nThis is the constructor of the SineGordonOperation class. It receives a reference to the MatrixFree holding the problem information and the time step size as input parameters. The initialization routine sets up the mass matrix. Since we use Gauss-Lobatto elements, the mass matrix is a diagonal matrix and can be stored as a vector. The computation of the mass matrix diagonal is simple to achieve with the data structures provided by FEEvaluation: Just loop over all cell batches, i.e., collections of cells due to SIMD vectorization, and integrate over the function that is constant one on all quadrature points by using the integrate function with true argument at the slot for values. Finally, we invert the diagonal entries to have the inverse mass matrix directly available in each time step.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   SineGordonOperation<dim, fe_degree>::SineGordonOperation(\\n\\u00a0     const MatrixFree<dim, double> &data_in,\\n\\u00a0     const double                   time_step)\\n\\u00a0     : data(data_in)\\n\\u00a0     , delta_t_sqr(make_vectorized_array(time_step * time_step))\\n\\u00a0   {\\n\\u00a0     data.initialize_dof_vector(inv_mass_matrix);\\n\\u00a0 \\n\\u00a0     FEEvaluation<dim, fe_degree> fe_eval(data);\\n\\u00a0 \\n\\u00a0     for (unsigned int cell = 0; cell < data.n_cell_batches(); ++cell)\\n\\u00a0       {\\n\\u00a0         fe_eval.reinit(cell);\\n\\u00a0         for (const unsigned int q : fe_eval.quadrature_point_indices())\\n\\u00a0           fe_eval.submit_value(make_vectorized_array(1.), q);\\n\\u00a0         fe_eval.integrate(EvaluationFlags::values);\\n\\u00a0         fe_eval.distribute_local_to_global(inv_mass_matrix);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     inv_mass_matrix.compress(VectorOperation::add);\\n\\u00a0     for (unsigned int k = 0; k < inv_mass_matrix.locally_owned_size(); ++k)\\n\\u00a0       if (inv_mass_matrix.local_element(k) > 1e-15)\\n\\u00a0         inv_mass_matrix.local_element(k) =\\n\\u00a0           1. / inv_mass_matrix.local_element(k);\\n\\u00a0       else\\n\\u00a0         inv_mass_matrix.local_element(k) = 1;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFEEvaluationDefinition fe_evaluation.h:1355\\nEvaluationFlags::values@ valuesDefinition evaluation_flags.h:50\\nVectorOperation::add@ addDefinition vector_operation.h:53\\nmake_vectorized_arrayVectorizedArray< Number, width > make_vectorized_array(const Number &u)Definition vectorization.h:801\\n SineGordonOperation::local_apply\\nThis operator implements the core operation of the program, the integration over a range of cells for the nonlinear operator of the Sine-Gordon problem. The implementation is based on the FEEvaluation class as in step-37. Due to the special structure in Gauss-Lobatto elements, certain operations become simpler, in particular the evaluation of shape function values on quadrature points which is simply the injection of the values of cell degrees of freedom. The MatrixFree class detects possible structure of the finite element at quadrature points when initializing, which is then automatically used by FEEvaluation for selecting the most appropriate numerical kernel.\\nThe nonlinear function that we have to evaluate for the time stepping routine includes the value of the function at the present time current as well as the value at the previous time step old. Both values are passed to the operator in the collection of source vectors src, which is simply a std::vector of pointers to the actual solution vectors. This construct of collecting several source vectors into one is necessary as the cell loop in MatrixFree takes exactly one source and one destination vector, even if we happen to use many vectors like the two in this case. Note that the cell loop accepts any valid class for input and output, which does not only include vectors but general data types. However, only in case it encounters a LinearAlgebra::distributed::Vector<Number> or a std::vector collecting these vectors, it calls functions that exchange ghost data due to MPI at the beginning and the end of the loop. In the loop over the cells, we first have to read in the values in the vectors related to the local values. Then, we evaluate the value and the gradient of the current solution vector and the values of the old vector at the quadrature points. Next, we combine the terms in the scheme in the loop over the quadrature points. Finally, we integrate the result against the test function and accumulate the result to the global solution vector dst.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void SineGordonOperation<dim, fe_degree>::local_apply(\\n\\u00a0     const MatrixFree<dim>                                           &data,\\n\\u00a0     LinearAlgebra::distributed::Vector<double>                      &dst,\\n\\u00a0     const std::vector<LinearAlgebra::distributed::Vector<double> *> &src,\\n\\u00a0     const std::pair<unsigned int, unsigned int> &cell_range) const\\n\\u00a0   {\\n\\u00a0     AssertDimension(src.size(), 2);\\n\\u00a0     FEEvaluation<dim, fe_degree> current(data), old(data);\\n\\u00a0     for (unsigned int cell = cell_range.first; cell < cell_range.second; ++cell)\\n\\u00a0       {\\n\\u00a0         current.reinit(cell);\\n\\u00a0         old.reinit(cell);\\n\\u00a0 \\n\\u00a0         current.read_dof_values(*src[0]);\\n\\u00a0         old.read_dof_values(*src[1]);\\n\\u00a0 \\n\\u00a0         current.evaluate(EvaluationFlags::values | EvaluationFlags::gradients);\\n\\u00a0         old.evaluate(EvaluationFlags::values);\\n\\u00a0 \\n\\u00a0         for (const unsigned int q : current.quadrature_point_indices())\\n\\u00a0           {\\n\\u00a0             const VectorizedArray<double> current_value = current.get_value(q);\\n\\u00a0             const VectorizedArray<double> old_value     = old.get_value(q);\\n\\u00a0 \\n\\u00a0             current.submit_value(2. * current_value - old_value -\\n\\u00a0                                    delta_t_sqr * std::sin(current_value),\\n\\u00a0                                  q);\\n\\u00a0             current.submit_gradient(-delta_t_sqr * current.get_gradient(q), q);\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0         current.integrate(EvaluationFlags::values | EvaluationFlags::gradients);\\n\\u00a0         current.distribute_local_to_global(dst);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nAssertDimension#define AssertDimension(dim1, dim2)Definition exceptions.h:1985\\nEvaluationFlags::gradients@ gradientsDefinition evaluation_flags.h:54\\nstd::sin::VectorizedArray< Number, width > sin(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6589\\n SineGordonOperation::apply\\nThis function performs the time stepping routine based on the cell-local strategy. Note that we need to set the destination vector to zero before we add the integral contributions of the current time step (via the FEEvaluation::distribute_local_to_global() call). In this tutorial, we let the cell-loop do the zero operation via the fifth true argument passed to MatrixFree::cell_loop. The loop can schedule the zero operation closer to the operations on vector entries for supported vector entries, thereby possibly increasing data locality (the vector entries that first get zeroed are later re-used in the distribute_local_to_global() call). The structure of the cell loop is implemented in the cell finite element operator class. On each cell it applies the routine defined as the local_apply() method of the class SineGordonOperation, i.e., this. One could also provide a function with the same signature that is not part of a class. Finally, the result of the integration is multiplied by the inverse mass matrix.\\n\\u00a0   template <int dim, int fe_degree>\\n\\u00a0   void SineGordonOperation<dim, fe_degree>::apply(\\n\\u00a0     LinearAlgebra::distributed::Vector<double>                      &dst,\\n\\u00a0     const std::vector<LinearAlgebra::distributed::Vector<double> *> &src) const\\n\\u00a0   {\\n\\u00a0     data.cell_loop(\\n\\u00a0       &SineGordonOperation<dim, fe_degree>::local_apply, this, dst, src, true);\\n\\u00a0     dst.scale(inv_mass_matrix);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n Equation data\\nWe define a time-dependent function that is used as initial value. Different solutions can be obtained by varying the starting time. This function, taken from step-25, would represent an analytic solution in 1d for all times, but is merely used for setting some starting solution of interest here. More elaborate choices that could test the convergence of this program are given in step-25.\\n\\u00a0   template <int dim>\\n\\u00a0   class InitialCondition : public Function<dim>\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     InitialCondition(const unsigned int n_components = 1,\\n\\u00a0                      const double       time         = 0.)\\n\\u00a0       : Function<dim>(n_components, time)\\n\\u00a0     {}\\n\\u00a0     virtual double value(const Point<dim> &p,\\n\\u00a0                          const unsigned int /*component*/) const override\\n\\u00a0     {\\n\\u00a0       double t = this->get_time();\\n\\u00a0 \\n\\u00a0       const double m  = 0.5;\\n\\u00a0       const double c1 = 0.;\\n\\u00a0       const double c2 = 0.;\\n\\u00a0       const double factor =\\n\\u00a0         (m / std::sqrt(1. - m * m) * std::sin(std::sqrt(1. - m * m) * t + c2));\\n\\u00a0       double result = 1.;\\n\\u00a0       for (unsigned int d = 0; d < dim; ++d)\\n\\u00a0         result *= -4. * std::atan(factor / std::cosh(m * p[d] + c1));\\n\\u00a0       return result;\\n\\u00a0     }\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFunctionTime< numbers::NumberTraits< double >::real_type >::get_timenumbers::NumberTraits< double >::real_type get_time() const\\nFunctionDefinition function.h:152\\nFunction::valuevirtual RangeNumberType value(const Point< dim > &p, const unsigned int component=0) const\\nPointDefinition point.h:111\\nstd::coshinline ::VectorizedArray< Number, width > cosh(const ::VectorizedArray< Number, width > &x)Definition vectorization.h:6709\\nstd::sqrt::VectorizedArray< Number, width > sqrt(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6869\\nstd::ataninline ::VectorizedArray< Number, width > atan(const ::VectorizedArray< Number, width > &x)Definition vectorization.h:6689\\n SineGordonProblem class\\nThis is the main class that builds on the class in step-25. However, we replaced the SparseMatrix<double> class by the MatrixFree class to store the geometry data. Also, we use a distributed triangulation in this example.\\n\\u00a0   template <int dim>\\n\\u00a0   class SineGordonProblem\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     SineGordonProblem();\\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     ConditionalOStream pcout;\\n\\u00a0 \\n\\u00a0     void make_grid_and_dofs();\\n\\u00a0     void output_results(const unsigned int timestep_number);\\n\\u00a0 \\n\\u00a0 #ifdef DEAL_II_WITH_P4EST\\n\\u00a0     parallel::distributed::Triangulation<dim> triangulation;\\n\\u00a0 #else\\n\\u00a0     Triangulation<dim> triangulation;\\n\\u00a0 #endif\\n\\u00a0     const FE_Q<dim> fe;\\n\\u00a0     DoFHandler<dim> dof_handler;\\n\\u00a0 \\n\\u00a0     MappingQ1<dim> mapping;\\n\\u00a0 \\n\\u00a0     AffineConstraints<double> constraints;\\n\\u00a0     IndexSet                  locally_relevant_dofs;\\n\\u00a0 \\n\\u00a0     MatrixFree<dim, double> matrix_free_data;\\n\\u00a0 \\n\\u00a0     LinearAlgebra::distributed::Vector<double> solution, old_solution,\\n\\u00a0       old_old_solution;\\n\\u00a0 \\n\\u00a0     const unsigned int n_global_refinements;\\n\\u00a0     double             time, time_step;\\n\\u00a0     const double       final_time;\\n\\u00a0     const double       cfl_number;\\n\\u00a0     const unsigned int output_timestep_skip;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nConditionalOStreamDefinition conditional_ostream.h:80\\nDoFHandlerDefinition dof_handler.h:317\\nFE_QDefinition fe_q.h:554\\nIndexSetDefinition index_set.h:70\\nMappingQ1Definition mapping_q1.h:55\\nTriangulationDefinition tria.h:1323\\nparallel::distributed::TriangulationDefinition tria.h:268\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\n SineGordonProblem::SineGordonProblem\\nThis is the constructor of the SineGordonProblem class. The time interval and time step size are defined here. Moreover, we use the degree of the finite element that we defined at the top of the program to initialize a FE_Q finite element based on Gauss-Lobatto support points. These points are convenient because in conjunction with a QGaussLobatto quadrature rule of the same order they give a diagonal mass matrix without compromising accuracy too much (note that the integration is inexact, though), see also the discussion in the introduction. Note that FE_Q selects the Gauss-Lobatto nodal points by default due to their improved conditioning versus equidistant points. To make things more explicit, we state the selection of the nodal points nonetheless.\\n\\u00a0   template <int dim>\\n\\u00a0   SineGordonProblem<dim>::SineGordonProblem()\\n\\u00a0     : pcout(std::cout, Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)\\n\\u00a0 #ifdef DEAL_II_WITH_P4EST\\n\\u00a0     , triangulation(MPI_COMM_WORLD)\\n\\u00a0 #endif\\n\\u00a0     , fe(QGaussLobatto<1>(fe_degree + 1))\\n\\u00a0     , dof_handler(triangulation)\\n\\u00a0     , n_global_refinements(10 - 2 * dim)\\n\\u00a0     , time(-10)\\n\\u00a0     , time_step(10.)\\n\\u00a0     , final_time(10.)\\n\\u00a0     , cfl_number(.1 / fe_degree)\\n\\u00a0     , output_timestep_skip(200)\\n\\u00a0   {}\\n\\u00a0 \\nQGaussLobattoDefinition quadrature_lib.h:140\\nInitializeLibrary::MPI@ MPI\\nUtilitiesDefinition communication_pattern_base.h:30\\nstdSTL namespace.\\n SineGordonProblem::make_grid_and_dofs\\nAs in step-25 this functions sets up a cube grid in dim dimensions of extent \\\\([-15,15]\\\\). We refine the mesh more in the center of the domain since the solution is concentrated there. We first refine all cells whose center is within a radius of 11, and then refine once more for a radius 6. This simple ad hoc refinement could be done better by adapting the mesh to the solution using error estimators during the time stepping as done in other example programs, and using parallel::distributed::SolutionTransfer to transfer the solution to the new mesh.\\n\\u00a0   template <int dim>\\n\\u00a0   void SineGordonProblem<dim>::make_grid_and_dofs()\\n\\u00a0   {\\n\\u00a0     GridGenerator::hyper_cube(triangulation, -15, 15);\\n\\u00a0     triangulation.refine_global(n_global_refinements);\\n\\u00a0     {\\n\\u00a0       for (const auto &cell : triangulation.active_cell_iterators())\\n\\u00a0         if (cell->is_locally_owned())\\n\\u00a0           if (cell->center().norm() < 11)\\n\\u00a0             cell->set_refine_flag();\\n\\u00a0       triangulation.execute_coarsening_and_refinement();\\n\\u00a0 \\n\\u00a0       for (const auto &cell : triangulation.active_cell_iterators())\\n\\u00a0         if (cell->is_locally_owned())\\n\\u00a0           if (cell->center().norm() < 6)\\n\\u00a0             cell->set_refine_flag();\\n\\u00a0       triangulation.execute_coarsening_and_refinement();\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0     pcout << \\\"   Number of global active cells: \\\"\\n\\u00a0           << triangulation.n_global_active_cells() << std::endl;\\n\\u00a0 \\n\\u00a0     dof_handler.distribute_dofs(fe);\\n\\u00a0 \\n\\u00a0     pcout << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n\\u00a0           << std::endl;\\n\\u00a0 \\n\\u00a0 \\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nparallel::TriangulationBase::n_global_active_cellsvirtual types::global_cell_index n_global_active_cells() const overrideDefinition tria_base.cc:151\\nparallel::distributed::Triangulation::execute_coarsening_and_refinementvirtual void execute_coarsening_and_refinement() overrideDefinition tria.cc:3320\\ncenterPoint< 3 > centerDefinition data_out_base.cc:267\\nGridGenerator::hyper_cubevoid hyper_cube(Triangulation< dim, spacedim > &tria, const double left=0., const double right=1., const bool colorize=false)\\nWe generate hanging node constraints for ensuring continuity of the solution. As in step-40, we need to equip the constraint matrix with the IndexSet of locally active and locally relevant degrees of freedom to avoid it consuming too much memory for big problems. Next, the MatrixFree object for the problem is set up. Note that we specify a particular scheme for shared-memory parallelization (hence one would use multithreading for intra-node parallelism and not MPI; we here choose the standard option \\u2014 if we wanted to disable shared memory parallelization even in case where there is more than one TBB thread available in the program, we would choose MatrixFree::AdditionalData::TasksParallelScheme::none). Also note that, instead of using the default QGauss quadrature argument, we supply a QGaussLobatto quadrature formula to enable the desired behavior. Finally, three solution vectors are initialized. MatrixFree expects a particular layout of ghost indices (as it handles index access in MPI-local numbers that need to match between the vector and MatrixFree), so we just ask it to initialize the vectors to be sure the ghost exchange is properly handled.\\n\\u00a0     locally_relevant_dofs =\\n\\u00a0       DoFTools::extract_locally_relevant_dofs(dof_handler);\\n\\u00a0     constraints.clear();\\n\\u00a0     constraints.reinit(dof_handler.locally_owned_dofs(), locally_relevant_dofs);\\n\\u00a0     DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n\\u00a0     constraints.close();\\n\\u00a0 \\n\\u00a0     typename MatrixFree<dim>::AdditionalData additional_data;\\n\\u00a0     additional_data.tasks_parallel_scheme =\\n\\u00a0       MatrixFree<dim>::AdditionalData::TasksParallelScheme::partition_partition;\\n\\u00a0 \\n\\u00a0     matrix_free_data.reinit(mapping,\\n\\u00a0                             dof_handler,\\n\\u00a0                             constraints,\\n\\u00a0                             QGaussLobatto<1>(fe_degree + 1),\\n\\u00a0                             additional_data);\\n\\u00a0 \\n\\u00a0     matrix_free_data.initialize_dof_vector(solution);\\n\\u00a0     old_solution.reinit(solution);\\n\\u00a0     old_old_solution.reinit(solution);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nMatrixFree::reinitvoid reinit(const MappingType &mapping, const DoFHandler< dim > &dof_handler, const AffineConstraints< number2 > &constraint, const QuadratureType &quad, const AdditionalData &additional_data=AdditionalData())\\nDoFTools::make_hanging_node_constraintsvoid make_hanging_node_constraints(const DoFHandler< dim, spacedim > &dof_handler, AffineConstraints< number > &constraints)Definition dof_tools_constraints.cc:3073\\nDoFTools::extract_locally_relevant_dofsIndexSet extract_locally_relevant_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1164\\nMatrixFree::AdditionalDataDefinition matrix_free.h:184\\nMatrixFree::AdditionalData::tasks_parallel_schemeTasksParallelScheme tasks_parallel_schemeDefinition matrix_free.h:347\\n SineGordonProblem::output_results\\nThis function prints the norm of the solution and writes the solution vector to a file. The norm is standard (except for the fact that we need to accumulate the norms over all processors for the parallel grid which we do via the VectorTools::compute_global_error() function), and the second is similar to what we did in step-40 or step-37. Note that we can use the same vector for output as the one used during computations: The vectors in the matrix-free framework always provide full information on all locally owned cells (this is what is needed in the local evaluations, too), including ghost vector entries on these cells. This is the only data that is needed in the VectorTools::integrate_difference() function as well as in DataOut. The only action to take at this point is to make sure that the vector updates its ghost values before we read from them, and to reset ghost values once done. This is a feature present only in the LinearAlgebra::distributed::Vector class. Distributed vectors with PETSc and Trilinos, on the other hand, need to be copied to special vectors including ghost values (see the relevant section in step-40). If we also wanted to access all degrees of freedom on ghost cells (e.g. when computing error estimators that use the jump of solution over cell boundaries), we would need more information and create a vector initialized with locally relevant dofs just as in step-40. Observe also that we need to distribute constraints for output - they are not filled during computations (rather, they are interpolated on the fly in the matrix-free method FEEvaluation::read_dof_values()).\\n\\u00a0   template <int dim>\\n\\u00a0   void\\n\\u00a0   SineGordonProblem<dim>::output_results(const unsigned int timestep_number)\\n\\u00a0   {\\n\\u00a0     constraints.distribute(solution);\\n\\u00a0 \\n\\u00a0     Vector<float> norm_per_cell(triangulation.n_active_cells());\\n\\u00a0     solution.update_ghost_values();\\n\\u00a0     VectorTools::integrate_difference(mapping,\\n\\u00a0                                       dof_handler,\\n\\u00a0                                       solution,\\n\\u00a0                                       Functions::ZeroFunction<dim>(),\\n\\u00a0                                       norm_per_cell,\\n\\u00a0                                       QGauss<dim>(fe_degree + 1),\\n\\u00a0                                       VectorTools::L2_norm);\\n\\u00a0     const double solution_norm =\\n\\u00a0       VectorTools::compute_global_error(triangulation,\\n\\u00a0                                         norm_per_cell,\\n\\u00a0                                         VectorTools::L2_norm);\\n\\u00a0 \\n\\u00a0     pcout << \\\"   Time:\\\" << std::setw(8) << std::setprecision(3) << time\\n\\u00a0           << \\\", solution norm: \\\" << std::setprecision(5) << std::setw(7)\\n\\u00a0           << solution_norm << std::endl;\\n\\u00a0 \\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0 \\n\\u00a0     data_out.attach_dof_handler(dof_handler);\\n\\u00a0     data_out.add_data_vector(solution, \\\"solution\\\");\\n\\u00a0     data_out.build_patches(mapping);\\n\\u00a0 \\n\\u00a0     data_out.write_vtu_with_pvtu_record(\\n\\u00a0       \\\"./\\\", \\\"solution\\\", timestep_number, MPI_COMM_WORLD, 3);\\n\\u00a0 \\n\\u00a0     solution.zero_out_ghost_values();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nDataOutDefinition data_out.h:147\\nFunctions::ZeroFunctionDefinition function.h:510\\nQGaussDefinition quadrature_lib.h:40\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nVectorDefinition vector.h:120\\nVectorTools::compute_global_errordouble compute_global_error(const Triangulation< dim, spacedim > &tria, const InVector &cellwise_error, const NormType &norm, const double exponent=2.)\\nVectorTools::L2_norm@ L2_normDefinition vector_tools_common.h:112\\nVectorTools::integrate_differencevoid integrate_difference(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const ReadVector< Number > &fe_function, const Function< spacedim, Number > &exact_solution, OutVector &difference, const Quadrature< dim > &q, const NormType &norm, const Function< spacedim, double > *weight=nullptr, const double exponent=2.)\\n SineGordonProblem::run\\nThis function is called by the main function and steps into the subroutines of the class.\\nAfter printing some information about the parallel setup, the first action is to set up the grid and the cell operator. Then, the time step is computed from the CFL number given in the constructor and the finest mesh size. The finest mesh size is computed as the diameter of the last cell in the triangulation, which is the last cell on the finest level of the mesh. This is only possible for meshes where all elements on a level have the same size, otherwise, one needs to loop over all cells. Note that we need to query all the processors for their finest cell since not all processors might hold a region where the mesh is at the finest level. Then, we readjust the time step a little to hit the final time exactly.\\n\\u00a0   template <int dim>\\n\\u00a0   void SineGordonProblem<dim>::run()\\n\\u00a0   {\\n\\u00a0     {\\n\\u00a0       pcout << \\\"Number of MPI ranks:            \\\"\\n\\u00a0             << Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD) << std::endl;\\n\\u00a0       pcout << \\\"Number of threads on each rank: \\\"\\n\\u00a0             << MultithreadInfo::n_threads() << std::endl;\\n\\u00a0       const unsigned int n_vect_doubles = VectorizedArray<double>::size();\\n\\u00a0       const unsigned int n_vect_bits    = 8 * sizeof(double) * n_vect_doubles;\\n\\u00a0       pcout << \\\"Vectorization over \\\" << n_vect_doubles\\n\\u00a0             << \\\" doubles = \\\" << n_vect_bits << \\\" bits (\\\"\\n\\u00a0             << Utilities::System::get_current_vectorization_level() << ')'\\n\\u00a0             << std::endl\\n\\u00a0             << std::endl;\\n\\u00a0     }\\n\\u00a0     make_grid_and_dofs();\\n\\u00a0 \\n\\u00a0     const double local_min_cell_diameter =\\n\\u00a0       triangulation.last()->diameter() / std::sqrt(dim);\\n\\u00a0     const double global_min_cell_diameter =\\n\\u00a0       -Utilities::MPI::max(-local_min_cell_diameter, MPI_COMM_WORLD);\\n\\u00a0     time_step = cfl_number * global_min_cell_diameter;\\n\\u00a0     time_step = (final_time - time) / (int((final_time - time) / time_step));\\n\\u00a0     pcout << \\\"   Time step size: \\\" << time_step\\n\\u00a0           << \\\", finest cell: \\\" << global_min_cell_diameter << std::endl\\n\\u00a0           << std::endl;\\n\\u00a0 \\nMultithreadInfo::n_threadsstatic unsigned int n_threads()Definition multithread_info.cc:127\\nTriangulation::lastcell_iterator last() const\\nVectorizedArrayBase< VectorizedArray< Number, width >, 1 >::sizestatic constexpr std::size_t size()Definition vectorization.h:285\\nUtilities::MPI::n_mpi_processesunsigned int n_mpi_processes(const MPI_Comm mpi_communicator)Definition mpi.cc:92\\nUtilities::MPI::maxT max(const T &t, const MPI_Comm mpi_communicator)\\nUtilities::System::get_current_vectorization_levelstd::string get_current_vectorization_level()Definition utilities.cc:938\\nNext the initial value is set. Since we have a two-step time stepping method, we also need a value of the solution at time-time_step. For accurate results, one would need to compute this from the time derivative of the solution at initial time, but here we ignore this difficulty and just set it to the initial value function at that artificial time.\\nWe then go on by writing the initial state to file and collecting the two starting solutions in a std::vector of pointers that get later consumed by the SineGordonOperation::apply() function. Next, an instance of the  SineGordonOperation class  based on the finite element degree specified at the top of this file is set up.\\n\\u00a0     VectorTools::interpolate(mapping,\\n\\u00a0                              dof_handler,\\n\\u00a0                              InitialCondition<dim>(1, time),\\n\\u00a0                              solution);\\n\\u00a0     VectorTools::interpolate(mapping,\\n\\u00a0                              dof_handler,\\n\\u00a0                              InitialCondition<dim>(1, time - time_step),\\n\\u00a0                              old_solution);\\n\\u00a0     output_results(0);\\n\\u00a0 \\n\\u00a0     std::vector<LinearAlgebra::distributed::Vector<double> *>\\n\\u00a0       previous_solutions({&old_solution, &old_old_solution});\\n\\u00a0 \\n\\u00a0     SineGordonOperation<dim, fe_degree> sine_gordon_op(matrix_free_data,\\n\\u00a0                                                        time_step);\\n\\u00a0 \\nVectorTools::interpolatevoid interpolate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Function< spacedim, typename VectorType::value_type > &function, VectorType &vec, const ComponentMask &component_mask={})\\nNow loop over the time steps. In each iteration, we shift the solution vectors by one and call the apply function of the SineGordonOperator class. Then, we write the solution to a file. We clock the wall times for the computational time needed as wall as the time needed to create the output and report the numbers when the time stepping is finished.\\nNote how this shift is implemented: We simply call the swap method on the two vectors which swaps only some pointers without the need to copy data around, a relatively expensive operation within an explicit time stepping method. Let us see what happens in more detail: First, we exchange old_solution with old_old_solution, which means that old_old_solution gets old_solution, which is what we expect. Similarly, old_solution gets the content from solution in the next step. After this, solution holds old_old_solution, but that will be overwritten during this step.\\n\\u00a0     unsigned int timestep_number = 1;\\n\\u00a0 \\n\\u00a0     Timer  timer;\\n\\u00a0     double wtime       = 0;\\n\\u00a0     double output_time = 0;\\n\\u00a0     for (time += time_step; time <= final_time;\\n\\u00a0          time += time_step, ++timestep_number)\\n\\u00a0       {\\n\\u00a0         timer.restart();\\n\\u00a0         old_old_solution.swap(old_solution);\\n\\u00a0         old_solution.swap(solution);\\n\\u00a0         sine_gordon_op.apply(solution, previous_solutions);\\n\\u00a0         wtime += timer.wall_time();\\n\\u00a0 \\n\\u00a0         timer.restart();\\n\\u00a0         if (timestep_number % output_timestep_skip == 0)\\n\\u00a0           output_results(timestep_number / output_timestep_skip);\\n\\u00a0 \\n\\u00a0         output_time += timer.wall_time();\\n\\u00a0       }\\n\\u00a0     timer.restart();\\n\\u00a0     output_results(timestep_number / output_timestep_skip + 1);\\n\\u00a0     output_time += timer.wall_time();\\n\\u00a0 \\n\\u00a0     pcout << std::endl\\n\\u00a0           << \\\"   Performed \\\" << timestep_number << \\\" time steps.\\\" << std::endl;\\n\\u00a0 \\n\\u00a0     pcout << \\\"   Average wallclock time per time step: \\\"\\n\\u00a0           << wtime / timestep_number << 's' << std::endl;\\n\\u00a0 \\n\\u00a0     pcout << \\\"   Spent \\\" << output_time << \\\"s on output and \\\" << wtime\\n\\u00a0           << \\\"s on computations.\\\" << std::endl;\\n\\u00a0   }\\n\\u00a0 } // namespace Step48\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTimerDefinition timer.h:117\\nTimer::restartvoid restart()Definition timer.h:896\\n The main function\\nAs in step-40, we initialize MPI at the start of the program. Since we will in general mix MPI parallelization with threads, we also set the third argument in MPI_InitFinalize that controls the number of threads to an invalid number, which means that the TBB library chooses the number of threads automatically, typically to the number of available cores in the system. As an alternative, you can also set this number manually if you want to set a specific number of threads (e.g. when MPI-only is required).\\n\\u00a0 int main(int argc, char **argv)\\n\\u00a0 {\\n\\u00a0   using namespace Step48;\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\n\\u00a0   Utilities::MPI::MPI_InitFinalize mpi_initialization(\\n\\u00a0     argc, argv, numbers::invalid_unsigned_int);\\n\\u00a0 \\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       SineGordonProblem<dimension> sg_problem;\\n\\u00a0       sg_problem.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0 \\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return 0;\\n\\u00a0 }\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\nnumbers::invalid_unsigned_intstatic const unsigned int invalid_unsigned_intDefinition types.h:220\\n Results\\nComparison with a sparse matrix\\nIn order to demonstrate the gain in using the MatrixFree class instead of the standard deal.II assembly routines for evaluating the information from old time steps, we study a simple serial run of the code on a nonadaptive mesh. Since much time is spent on evaluating the sine function, we do not only show the numbers of the full sine-Gordon equation but also for the wave equation (the sine-term skipped from the sine-Gordon equation). We use both second and fourth order elements. The results are summarized in the following table.\\n\\n\\n\\u00a0 wave equation sine-Gordon  \\n\\n\\u00a0 MF SpMV dealii MF dealii  \\n\\n2D, \\\\(\\\\mathcal{Q}_2\\\\) 0.0106 0.00971 0.109 0.0243 0.124  \\n\\n2D, \\\\(\\\\mathcal{Q}_4\\\\) 0.0328 0.0706 0.528 0.0714 0.502  \\n\\n3D, \\\\(\\\\mathcal{Q}_2\\\\) 0.0151 0.0320 0.331 0.0376 0.364  \\n\\n3D, \\\\(\\\\mathcal{Q}_4\\\\) 0.0918 0.844 6.83 0.194 6.95  \\n\\nIt is apparent that the matrix-free code outperforms the standard assembly routines in deal.II by far. In 3D and for fourth order elements, one operator evaluation is also almost ten times as fast as a sparse matrix-vector product.\\nParallel run in 2D and 3D\\nWe start with the program output obtained on a workstation with 12 cores / 24 threads (one Intel Xeon E5-2687W v4 CPU running at 3.2 GHz, hyperthreading enabled), running the program in release mode: \\\\$ make run\\nNumber of MPI ranks:            1\\nNumber of threads on each rank: 24\\nVectorization over 4 doubles = 256 bits (AVX)\\n \\n   Number of global active cells: 15412\\n   Number of degrees of freedom: 249065\\n   Time step size: 0.00292997, finest cell: 0.117188\\n \\n   Time:     -10, solution norm:  9.5599\\n   Time:   -9.41, solution norm:  17.678\\n   Time:   -8.83, solution norm:  23.504\\n   Time:   -8.24, solution norm:    27.5\\n   Time:   -7.66, solution norm:  29.513\\n   Time:   -7.07, solution norm:  29.364\\n   Time:   -6.48, solution norm:   27.23\\n   Time:    -5.9, solution norm:  23.527\\n   Time:   -5.31, solution norm:  18.439\\n   Time:   -4.73, solution norm:  11.935\\n   Time:   -4.14, solution norm:  5.5284\\n   Time:   -3.55, solution norm:  8.0354\\n   Time:   -2.97, solution norm:  14.707\\n   Time:   -2.38, solution norm:      20\\n   Time:    -1.8, solution norm:  22.834\\n   Time:   -1.21, solution norm:  22.771\\n   Time:  -0.624, solution norm:  20.488\\n   Time: -0.0381, solution norm:  16.697\\n   Time:   0.548, solution norm:  11.221\\n   Time:    1.13, solution norm:  5.3912\\n   Time:    1.72, solution norm:  8.4528\\n   Time:    2.31, solution norm:  14.335\\n   Time:    2.89, solution norm:  18.555\\n   Time:    3.48, solution norm:  20.894\\n   Time:    4.06, solution norm:  21.305\\n   Time:    4.65, solution norm:  19.903\\n   Time:    5.24, solution norm:  16.864\\n   Time:    5.82, solution norm:  12.223\\n   Time:    6.41, solution norm:   6.758\\n   Time:    6.99, solution norm:  7.2423\\n   Time:    7.58, solution norm:  12.888\\n   Time:    8.17, solution norm:  17.273\\n   Time:    8.75, solution norm:  19.654\\n   Time:    9.34, solution norm:  19.838\\n   Time:    9.92, solution norm:  17.964\\n   Time:      10, solution norm:  17.595\\n \\n   Performed 6826 time steps.\\n   Average wallclock time per time step: 0.0013453s\\n   Spent 14.976s on output and 9.1831s on computations.\\nIn 3D, the respective output looks like \\\\$ make run\\nNumber of MPI ranks:            1\\nNumber of threads on each rank: 24\\nVectorization over 4 doubles = 256 bits (AVX)\\n \\n   Number of global active cells: 17592\\n   Number of degrees of freedom: 1193881\\n   Time step size: 0.0117233, finest cell: 0.46875\\n \\n   Time:     -10, solution norm:  29.558\\n   Time:   -7.66, solution norm:  129.13\\n   Time:   -5.31, solution norm:  67.753\\n   Time:   -2.97, solution norm:  79.245\\n   Time:  -0.621, solution norm:  123.52\\n   Time:    1.72, solution norm:  43.525\\n   Time:    4.07, solution norm:  93.285\\n   Time:    6.41, solution norm:  97.722\\n   Time:    8.76, solution norm:  36.734\\n   Time:      10, solution norm:  94.115\\n \\n   Performed 1706 time steps.\\n   Average wallclock time per time step: 0.0084542s\\n   Spent 16.766s on output and 14.423s on computations.\\nIt takes 0.008 seconds for one time step with more than a million degrees of freedom (note that we would need many processors to reach such numbers when solving linear systems).\\nIf we replace the thread-parallelization by a pure MPI parallelization, the timings change into: \\\\$ mpirun -n 24 ./step-48\\nNumber of MPI ranks:            24\\nNumber of threads on each rank: 1\\nVectorization over 4 doubles = 256 bits (AVX)\\n...\\n   Performed 1706 time steps.\\n   Average wallclock time per time step: 0.0051747s\\n   Spent 2.0535s on output and 8.828s on computations.\\nWe observe a dramatic speedup for the output (which makes sense, given that most code of the output is not parallelized via threads, whereas it is for MPI), but less than the theoretical factor of 12 we would expect from the parallelism. More interestingly, the computations also get faster when switching from the threads-only variant to the MPI-only variant. This is a general observation for the MatrixFree framework (as of updating this data in 2019). The main reason is that the decisions regarding work on conflicting cell batches made to enable execution in parallel are overly pessimistic: While they ensure that no work on neighboring cells is done on different threads at the same time, this conservative setting implies that data from neighboring cells is also evicted from caches by the time neighbors get touched. Furthermore, the current scheme is not able to provide a constant load for all 24 threads for the given mesh with 17,592 cells.\\nThe current program allows to also mix MPI parallelization with thread parallelization. This is most beneficial when running programs on clusters with multiple nodes, using MPI for the inter-node parallelization and threads for the intra-node parallelization. On the workstation used above, we can run threads in the hyperthreading region (i.e., using 2 threads for each of the 12 MPI ranks). An important setting for mixing MPI with threads is to ensure proper binning of tasks to CPUs. On many clusters the placing is either automatically via the mpirun/mpiexec environment, or there can be manual settings. Here, we simply report the run times the plain version of the program (noting that things could be improved towards the timings of the MPI-only program when proper pinning is done): \\\\$ mpirun -n 12 ./step-48\\nNumber of MPI ranks:            12\\nNumber of threads on each rank: 2\\nVectorization over 4 doubles = 256 bits (AVX)\\n...\\n   Performed 1706 time steps.\\n   Average wallclock time per time step: 0.0056651s\\n   Spent 2.5175s on output and 9.6646s on computations.\\nPossibilities for extensions\\nThere are several things in this program that could be improved to make it even more efficient (besides improved boundary conditions and physical stuff as discussed in step-25):\\n\\n\\nFaster evaluation of sine terms: As becomes obvious from the comparison of the plain wave equation and the sine-Gordon equation above, the evaluation of the sine terms dominates the total time for the finite element operator application. There are a few reasons for this: Firstly, the deal.II sine computation of a VectorizedArray field is not vectorized (as opposed to the rest of the operator application). This could be cured by handing the sine computation to a library with vectorized sine computations like Intel's math kernel library (MKL). By using the function vdSin in MKL, the program uses half the computing time in 2D and 40 percent less time in 3D. On the other hand, the sine computation is structurally much more complicated than the simple arithmetic operations like additions and multiplications in the rest of the local operation.\\n\\n\\n\\nHigher order time stepping: While the implementation allows for arbitrary order in the spatial part (by adjusting the degree of the finite element), the time stepping scheme is a standard second-order leap-frog scheme. Since solutions in wave propagation problems are usually very smooth, the error is likely dominated by the time stepping part. Of course, this could be cured by using smaller time steps (at a fixed spatial resolution), but it would be more efficient to use higher order time stepping as well. While it would be straight-forward to do so for a first-order system (use some Runge\\u2013Kutta scheme of higher order, probably combined with adaptive time step selection like the Dormand\\u2013Prince method), it is more challenging for the second-order formulation. At least in the finite difference community, people usually use the PDE to find spatial correction terms that improve the temporal error.\\n\\n\\n\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2012 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Authors: Katharina Kormann, Martin Kronbichler, Uppsala University, 2011-2012\\n */\\n \\n \\n#include <deal.II/base/multithread_info.h>\\n#include <deal.II/base/utilities.h>\\n#include <deal.II/base/function.h>\\n#include <deal.II/base/conditional_ostream.h>\\n#include <deal.II/base/timer.h>\\n#include <deal.II/lac/vector.h>\\n#include <deal.II/grid/tria.h>\\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/dofs/dof_tools.h>\\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/lac/affine_constraints.h>\\n#include <deal.II/fe/fe_q.h>\\n#include <deal.II/fe/fe_values.h>\\n#include <deal.II/fe/mapping_q1.h>\\n#include <deal.II/numerics/vector_tools.h>\\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/distributed/tria.h>\\n \\n#include <deal.II/lac/la_parallel_vector.h>\\n#include <deal.II/matrix_free/matrix_free.h>\\n#include <deal.II/matrix_free/fe_evaluation.h>\\n \\n#include <fstream>\\n#include <iostream>\\n#include <iomanip>\\n \\n \\nnamespace Step48\\n{\\n using namespace dealii;\\n \\n const unsigned int dimension = 2;\\n const unsigned int fe_degree = 4;\\n \\n \\n \\n template <int dim, int fe_degree>\\n class SineGordonOperation\\n  {\\n public:\\n    SineGordonOperation(const MatrixFree<dim, double> &data_in,\\n const double                   time_step);\\n \\n void apply(LinearAlgebra::distributed::Vector<double> &dst,\\n const std::vector<LinearAlgebra::distributed::Vector<double> *>\\n                 &src) const;\\n \\n private:\\n const MatrixFree<dim, double>             &data;\\n const VectorizedArray<double>              delta_t_sqr;\\n LinearAlgebra::distributed::Vector<double> inv_mass_matrix;\\n \\n void local_apply(\\n const MatrixFree<dim, double>                                   &data,\\n LinearAlgebra::distributed::Vector<double>                      &dst,\\n const std::vector<LinearAlgebra::distributed::Vector<double> *> &src,\\n const std::pair<unsigned int, unsigned int> &cell_range) const;\\n  };\\n \\n \\n \\n \\n template <int dim, int fe_degree>\\n  SineGordonOperation<dim, fe_degree>::SineGordonOperation(\\n const MatrixFree<dim, double> &data_in,\\n const double                   time_step)\\n    : data(data_in)\\n    , delta_t_sqr(make_vectorized_array(time_step * time_step))\\n  {\\n    data.initialize_dof_vector(inv_mass_matrix);\\n \\n FEEvaluation<dim, fe_degree> fe_eval(data);\\n \\n for (unsigned int cell = 0; cell < data.n_cell_batches(); ++cell)\\n      {\\n        fe_eval.reinit(cell);\\n for (const unsigned int q : fe_eval.quadrature_point_indices())\\n          fe_eval.submit_value(make_vectorized_array(1.), q);\\n        fe_eval.integrate(EvaluationFlags::values);\\n        fe_eval.distribute_local_to_global(inv_mass_matrix);\\n      }\\n \\n    inv_mass_matrix.compress(VectorOperation::add);\\n for (unsigned int k = 0; k < inv_mass_matrix.locally_owned_size(); ++k)\\n if (inv_mass_matrix.local_element(k) > 1e-15)\\n        inv_mass_matrix.local_element(k) =\\n          1. / inv_mass_matrix.local_element(k);\\n else\\n        inv_mass_matrix.local_element(k) = 1;\\n  }\\n \\n \\n \\n \\n \\n template <int dim, int fe_degree>\\n void SineGordonOperation<dim, fe_degree>::local_apply(\\n const MatrixFree<dim>                                           &data,\\n LinearAlgebra::distributed::Vector<double>                      &dst,\\n const std::vector<LinearAlgebra::distributed::Vector<double> *> &src,\\n const std::pair<unsigned int, unsigned int> &cell_range) const\\n {\\n AssertDimension(src.size(), 2);\\n FEEvaluation<dim, fe_degree> current(data), old(data);\\n for (unsigned int cell = cell_range.first; cell < cell_range.second; ++cell)\\n      {\\n        current.reinit(cell);\\n        old.reinit(cell);\\n \\n        current.read_dof_values(*src[0]);\\n        old.read_dof_values(*src[1]);\\n \\n        current.evaluate(EvaluationFlags::values | EvaluationFlags::gradients);\\n        old.evaluate(EvaluationFlags::values);\\n \\n for (const unsigned int q : current.quadrature_point_indices())\\n          {\\n const VectorizedArray<double> current_value = current.get_value(q);\\n const VectorizedArray<double> old_value     = old.get_value(q);\\n \\n            current.submit_value(2. * current_value - old_value -\\n                                   delta_t_sqr * std::sin(current_value),\\n                                 q);\\n            current.submit_gradient(-delta_t_sqr * current.get_gradient(q), q);\\n          }\\n \\n        current.integrate(EvaluationFlags::values | EvaluationFlags::gradients);\\n        current.distribute_local_to_global(dst);\\n      }\\n  }\\n \\n \\n \\n \\n template <int dim, int fe_degree>\\n void SineGordonOperation<dim, fe_degree>::apply(\\n LinearAlgebra::distributed::Vector<double>                      &dst,\\n const std::vector<LinearAlgebra::distributed::Vector<double> *> &src) const\\n {\\n    data.cell_loop(\\n      &SineGordonOperation<dim, fe_degree>::local_apply, this, dst, src, true);\\n    dst.scale(inv_mass_matrix);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n class InitialCondition : public Function<dim>\\n  {\\n public:\\n    InitialCondition(const unsigned int n_components = 1,\\n const double       time         = 0.)\\n      : Function<dim>(n_components, time)\\n    {}\\n virtual double value(const Point<dim> &p,\\n const unsigned int /*component*/) const override\\n {\\n double t = this->get_time();\\n \\n const double m  = 0.5;\\n const double c1 = 0.;\\n const double c2 = 0.;\\n const double factor =\\n        (m / std::sqrt(1. - m * m) * std::sin(std::sqrt(1. - m * m) * t + c2));\\n double result = 1.;\\n for (unsigned int d = 0; d < dim; ++d)\\n        result *= -4. * std::atan(factor / std::cosh(m * p[d] + c1));\\n return result;\\n    }\\n  };\\n \\n \\n \\n \\n template <int dim>\\n class SineGordonProblem\\n  {\\n public:\\n    SineGordonProblem();\\n void run();\\n \\n private:\\n ConditionalOStream pcout;\\n \\n void make_grid_and_dofs();\\n void output_results(const unsigned int timestep_number);\\n \\n#ifdef DEAL_II_WITH_P4EST\\n parallel::distributed::Triangulation<dim> triangulation;\\n#else\\n Triangulation<dim> triangulation;\\n#endif\\n const FE_Q<dim> fe;\\n DoFHandler<dim> dof_handler;\\n \\n MappingQ1<dim> mapping;\\n \\n AffineConstraints<double> constraints;\\n IndexSet                  locally_relevant_dofs;\\n \\n MatrixFree<dim, double> matrix_free_data;\\n \\n LinearAlgebra::distributed::Vector<double> solution, old_solution,\\n      old_old_solution;\\n \\n const unsigned int n_global_refinements;\\n double             time, time_step;\\n const double       final_time;\\n const double       cfl_number;\\n const unsigned int output_timestep_skip;\\n  };\\n \\n \\n \\n template <int dim>\\n  SineGordonProblem<dim>::SineGordonProblem()\\n    : pcout(std::cout, Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)\\n#ifdef DEAL_II_WITH_P4EST\\n    , triangulation(MPI_COMM_WORLD)\\n#endif\\n    , fe(QGaussLobatto<1>(fe_degree + 1))\\n    , dof_handler(triangulation)\\n    , n_global_refinements(10 - 2 * dim)\\n    , time(-10)\\n    , time_step(10.)\\n    , final_time(10.)\\n    , cfl_number(.1 / fe_degree)\\n    , output_timestep_skip(200)\\n  {}\\n \\n \\n template <int dim>\\n void SineGordonProblem<dim>::make_grid_and_dofs()\\n  {\\n GridGenerator::hyper_cube(triangulation, -15, 15);\\n triangulation.refine_global(n_global_refinements);\\n    {\\n for (const auto &cell : triangulation.active_cell_iterators())\\n        if (cell->is_locally_owned())\\n          if (cell->center().norm() < 11)\\n            cell->set_refine_flag();\\n triangulation.execute_coarsening_and_refinement();\\n \\n for (const auto &cell : triangulation.active_cell_iterators())\\n        if (cell->is_locally_owned())\\n          if (cell->center().norm() < 6)\\n            cell->set_refine_flag();\\n triangulation.execute_coarsening_and_refinement();\\n    }\\n \\n    pcout << \\\"   Number of global active cells: \\\"\\n          << triangulation.n_global_active_cells() << std::endl;\\n \\n    dof_handler.distribute_dofs(fe);\\n \\n    pcout << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n          << std::endl;\\n \\n \\n    locally_relevant_dofs =\\n DoFTools::extract_locally_relevant_dofs(dof_handler);\\n    constraints.clear();\\n    constraints.reinit(dof_handler.locally_owned_dofs(), locally_relevant_dofs);\\n DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n    constraints.close();\\n \\n typename MatrixFree<dim>::AdditionalData additional_data;\\n    additional_data.tasks_parallel_scheme =\\n MatrixFree<dim>::AdditionalData::TasksParallelScheme::partition_partition;\\n \\n    matrix_free_data.reinit(mapping,\\n                            dof_handler,\\n                            constraints,\\n QGaussLobatto<1>(fe_degree + 1),\\n                            additional_data);\\n \\n    matrix_free_data.initialize_dof_vector(solution);\\n    old_solution.reinit(solution);\\n    old_old_solution.reinit(solution);\\n  }\\n \\n \\n \\n \\n template <int dim>\\n void\\n  SineGordonProblem<dim>::output_results(const unsigned int timestep_number)\\n  {\\n    constraints.distribute(solution);\\n \\n Vector<float> norm_per_cell(triangulation.n_active_cells());\\n    solution.update_ghost_values();\\n VectorTools::integrate_difference(mapping,\\n                                      dof_handler,\\n                                      solution,\\n Functions::ZeroFunction<dim>(),\\n                                      norm_per_cell,\\n QGauss<dim>(fe_degree + 1),\\n VectorTools::L2_norm);\\n const double solution_norm =\\n VectorTools::compute_global_error(triangulation,\\n                                        norm_per_cell,\\n VectorTools::L2_norm);\\n \\n    pcout << \\\"   Time:\\\" << std::setw(8) << std::setprecision(3) << time\\n          << \\\", solution norm: \\\" << std::setprecision(5) << std::setw(7)\\n          << solution_norm << std::endl;\\n \\n DataOut<dim> data_out;\\n \\n    data_out.attach_dof_handler(dof_handler);\\n    data_out.add_data_vector(solution, \\\"solution\\\");\\n    data_out.build_patches(mapping);\\n \\n    data_out.write_vtu_with_pvtu_record(\\n \\\"./\\\", \\\"solution\\\", timestep_number, MPI_COMM_WORLD, 3);\\n \\n    solution.zero_out_ghost_values();\\n  }\\n \\n \\n \\n template <int dim>\\n void SineGordonProblem<dim>::run()\\n  {\\n    {\\n      pcout << \\\"Number of MPI ranks:            \\\"\\n            << Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD) << std::endl;\\n      pcout << \\\"Number of threads on each rank: \\\"\\n            << MultithreadInfo::n_threads() << std::endl;\\n const unsigned int n_vect_doubles = VectorizedArray<double>::size();\\n const unsigned int n_vect_bits    = 8 * sizeof(double) * n_vect_doubles;\\n      pcout << \\\"Vectorization over \\\" << n_vect_doubles\\n            << \\\" doubles = \\\" << n_vect_bits << \\\" bits (\\\"\\n            << Utilities::System::get_current_vectorization_level() << ')'\\n            << std::endl\\n            << std::endl;\\n    }\\n    make_grid_and_dofs();\\n \\n const double local_min_cell_diameter =\\n triangulation.last()->diameter() / std::sqrt(dim);\\n const double global_min_cell_diameter =\\n      -Utilities::MPI::max(-local_min_cell_diameter, MPI_COMM_WORLD);\\n    time_step = cfl_number * global_min_cell_diameter;\\n    time_step = (final_time - time) / (int((final_time - time) / time_step));\\n    pcout << \\\"   Time step size: \\\" << time_step\\n          << \\\", finest cell: \\\" << global_min_cell_diameter << std::endl\\n          << std::endl;\\n \\n \\n VectorTools::interpolate(mapping,\\n                             dof_handler,\\n                             InitialCondition<dim>(1, time),\\n                             solution);\\n VectorTools::interpolate(mapping,\\n                             dof_handler,\\n                             InitialCondition<dim>(1, time - time_step),\\n                             old_solution);\\n    output_results(0);\\n \\n    std::vector<LinearAlgebra::distributed::Vector<double> *>\\n      previous_solutions({&old_solution, &old_old_solution});\\n \\n    SineGordonOperation<dim, fe_degree> sine_gordon_op(matrix_free_data,\\n                                                       time_step);\\n \\n unsigned int timestep_number = 1;\\n \\n Timer  timer;\\n double wtime       = 0;\\n double output_time = 0;\\n for (time += time_step; time <= final_time;\\n         time += time_step, ++timestep_number)\\n      {\\n        timer.restart();\\n        old_old_solution.swap(old_solution);\\n        old_solution.swap(solution);\\n        sine_gordon_op.apply(solution, previous_solutions);\\n        wtime += timer.wall_time();\\n \\n        timer.restart();\\n if (timestep_number % output_timestep_skip == 0)\\n          output_results(timestep_number / output_timestep_skip);\\n \\n        output_time += timer.wall_time();\\n      }\\n    timer.restart();\\n    output_results(timestep_number / output_timestep_skip + 1);\\n    output_time += timer.wall_time();\\n \\n    pcout << std::endl\\n          << \\\"   Performed \\\" << timestep_number << \\\" time steps.\\\" << std::endl;\\n \\n    pcout << \\\"   Average wallclock time per time step: \\\"\\n          << wtime / timestep_number << 's' << std::endl;\\n \\n    pcout << \\\"   Spent \\\" << output_time << \\\"s on output and \\\" << wtime\\n          << \\\"s on computations.\\\" << std::endl;\\n  }\\n} // namespace Step48\\n \\n \\n \\n \\nint main(int argc, char **argv)\\n{\\n using namespace Step48;\\n using namespace dealii;\\n \\n Utilities::MPI::MPI_InitFinalize mpi_initialization(\\n    argc, argv, numbers::invalid_unsigned_int);\\n \\n try\\n    {\\n      SineGordonProblem<dimension> sg_problem;\\n      sg_problem.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n \\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n \\n return 0;\\n}\\naffine_constraints.h\\nDataOutInterface::write_vtu_with_pvtu_recordstd::string write_vtu_with_pvtu_record(const std::string &directory, const std::string &filename_without_extension, const unsigned int counter, const MPI_Comm mpi_communicator, const unsigned int n_digits_for_counter=numbers::invalid_unsigned_int, const unsigned int n_groups=0) constDefinition data_out_base.cc:7854\\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\nLinearAlgebra::distributed::Vector::scalevoid scale(const Vector< Number, MemorySpace > &scaling_factors)\\nMatrixFree::cell_loopvoid cell_loop(const std::function< void(const MatrixFree< dim, Number, VectorizedArrayType > &, OutVector &, const InVector &, const std::pair< unsigned int, unsigned int > &)> &cell_operation, OutVector &dst, const InVector &src, const bool zero_dst_vector=false) const\\nTimer::wall_timedouble wall_time() constDefinition timer.cc:262\\nconditional_ostream.h\\ntria.h\\ndof_handler.h\\ndof_tools.h\\nfe_values.h\\nfe_evaluation.h\\nfe_q.h\\nfunction.h\\ntria.h\\ngrid_generator.h\\nutilities.h\\nla_parallel_vector.h\\nmapping_q1.h\\nmatrix_free.h\\nmultithread_info.h\\nLocalIntegrators::Divergence::normdouble norm(const FEValuesBase< dim > &fe, const ArrayView< const std::vector< Tensor< 1, dim > > > &Du)Definition divergence.h:471\\nPhysics::Elasticity::Kinematics::dSymmetricTensor< 2, dim, Number > d(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\nUtilities::MPI::this_mpi_processunsigned int this_mpi_process(const MPI_Comm mpi_communicator)Definition mpi.cc:107\\nWorkStream::internal::tbb_no_coloring::runvoid run(const Iterator &begin, const std_cxx20::type_identity_t< Iterator > &end, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length, const unsigned int chunk_size)Definition work_stream.h:471\\ndata_out.h\\ntimer.h\\nvector.h\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"