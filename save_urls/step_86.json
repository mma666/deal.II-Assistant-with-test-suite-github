"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_86.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-86 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-86 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-86 tutorial program\\n\\n\\nThis tutorial depends on step-26, step-40.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\n Mapping the heat equation onto an ordinary differential equation formulation \\n Mapping the differential equation formulation to the time stepper\\n Complication 1: Dirichlet boundary values \\n Complication 2: Mesh refinement \\n Structure of the code \\n The test case \\n\\n The commented program\\n\\nThe HeatEquation class\\n\\nThe HeatEquation constructor\\nThe HeatEquation::setup_system() function\\nThe HeatEquation::output_results() function\\nThe HeatEquation::implicit_function() function\\nThe HeatEquation::assemble_implicit_jacobian() function\\nThe HeatEquation::solve_with_jacobian() function\\nThe HeatEquation::prepare_for_coarsening_and_refinement() function\\nThe HeatEquation::transfer_solution_vectors_to_new_mesh() function\\nThe HeatEquation::update_current_constraints() function\\nThe HeatEquation::run() function\\n\\nThe main() function\\n\\n\\n Results\\n\\nPossibilities for extensions\\n\\n The plain program\\n   \\n\\n\\n This program was contributed by Wolfgang Bangerth (Colorado State University), Stefano Zampini (King Abdullah University of Science and Technology), and Luca Heltai (University of Pisa).\\nThis material is based upon work partially supported by National Science Foundation grants OAC-1835673, DMS-1821210, and EAR-1925595.  \\n\\n Introduction\\nstep-26 solved the simple heat equation, one of the prototypical examples of time dependent problems:               \\n\\\\begin{align*}\\n  \\\\frac{\\\\partial u(\\\\mathbf x, t)}{\\\\partial t}\\n  -\\n  \\\\Delta u(\\\\mathbf x, t)\\n  &=\\n  f(\\\\mathbf x, t),\\n  \\\\qquad\\\\qquad &&\\n  \\\\forall \\\\mathbf x \\\\in \\\\Omega, t\\\\in (0,T),\\n  \\\\\\\\\\n  u(\\\\mathbf x, 0) &= u_0(\\\\mathbf x) &&\\n  \\\\forall \\\\mathbf x \\\\in \\\\Omega,\\n  \\\\\\\\\\n  u(\\\\mathbf x, t) &= g(\\\\mathbf x,t) &&\\n  \\\\forall \\\\mathbf x \\\\in \\\\partial\\\\Omega, t \\\\in (0,T).\\n\\\\end{align*}\\n\\n While that program showed a number of advanced techniques such as using adaptive mesh refinement for time-dependent problems, it did not address one big issue: It hand-rolls its own time stepping scheme, which in that program is the simple Crank-Nicolson method with a fixed time step. This is neither accurate nor efficient: We should be using a higher-order time stepping algorithm, and we should use one of the many ways to efficiently and automatically choose the length of the time step in response to the accuracy obtained.\\nThis would of course require quite a lot of development effort \\u2013 unless, of course, you do what we always advise: You build on what others have already done and have likely done in a way far superior to what one can do by oneself. In the current case, deal.II has interfaces to two such libraries: SUNDIALS, the SUite of Nonlinear and DIfferential/ALgebraic equation Solvers (and here specifically the Runge-Kutta-type solvers wrapped in the SUNDIALS::ARKode class), and PETSc's TS sub-package (wrapped in the PETScWrappers::TimeStepper class). In this program, we will use the PETSc TS interfaces.\\nWhile we're at it with updating step-26, we will also make the program run in parallel \\u2013 a minor change if you've read step-40, for example.\\nMapping the heat equation onto an ordinary differential equation formulation \\nBoth the PETSc TS and the SUNDIALS interfaces require that we first write the partial differential equation in the form of a system of ordinary differential equations. To this end, let us turn around the approach we used in step-26. There, we first discretized in time, obtaining a PDE to be solved at each time step that we could then discretize using the finite element method. This approach is called the \\\"Rothe method\\\". Instead, here, we use what's called the \\\"Method of Lines\\\" where we first discretize in space, obtaining a system of ordinary differential equations to which we can apply traditional time steppers. (There are some trade-offs between these two strategies, principally around using dynamically changing meshes; we will get back to this issue later on.)\\nTo get this started, we take the equation above and multiply it by a test function \\\\(\\\\varphi(\\\\mathbf x)\\\\) and integrate by parts to get a weak form: We seek a function \\\\(u(\\\\mathbf x, t)\\\\) that for all test functions \\\\(\\\\varphi \\\\in H^1_0(\\\\Omega)\\\\) satisfies                \\n\\\\begin{align*}\\n\\\\left(\\\\varphi(\\\\mathbf x),\\n  \\\\frac{\\\\partial u(\\\\mathbf x, t)}{\\\\partial t} \\\\right)_\\\\Omega\\n  +\\n\\\\left(\\\\nabla \\\\varphi(\\\\mathbf x),\\n  \\\\nabla u(\\\\mathbf x, t) \\\\right)_\\\\Omega\\n  &=\\n\\\\left(\\\\varphi(\\\\mathbf x),\\n  f(\\\\mathbf x, t) \\\\right)_\\\\Omega,\\n  \\\\\\\\\\n\\\\left(\\\\varphi(\\\\mathbf x),  u(\\\\mathbf x, 0)\\\\right)_\\\\Omega &=\\n\\\\left(\\\\varphi(\\\\mathbf x), u_0(\\\\mathbf x)\\\\right)_\\\\Omega, &&\\n  \\\\\\\\\\n  u(\\\\mathbf x, t) &= g(\\\\mathbf x,t) &&\\n  \\\\forall \\\\mathbf x \\\\in \\\\partial\\\\Omega, t \\\\in (0,T).\\n\\\\end{align*}\\n\\n (Integration by parts would ordinarily result in boundary terms unless one has Dirichlet boundary conditions \\u2013 possibly nonzero \\u2013 all around the boundary. We will assume that this is indeed so herein.)\\nWe then discretize by restricting ourself to finite element functions of the form   \\n\\\\begin{align*}\\nu_h(\\\\mathbf x,t) = \\\\sum_j U_j(t) \\\\varphi_j(\\\\mathbf x),\\n\\\\end{align*}\\n\\n which leads to the problem of finding a function \\\\(u_h(\\\\mathbf x, t)\\\\) that for all discrete test functions \\\\(\\\\varphi \\\\in V_h(\\\\Omega)\\\\) satisfies                \\n\\\\begin{align*}\\n\\\\left(\\\\varphi_i(\\\\mathbf x),\\n  \\\\frac{\\\\partial u_h(\\\\mathbf x, t)}{\\\\partial t} \\\\right)_\\\\Omega\\n  +\\n\\\\left(\\\\nabla \\\\varphi_j(\\\\mathbf x),\\n  \\\\nabla u_h(\\\\mathbf x, t) \\\\right)_\\\\Omega\\n  &=\\n\\\\left(\\\\varphi_i(\\\\mathbf x),\\n  f(\\\\mathbf x, t) \\\\right)_\\\\Omega,\\n  \\\\\\\\\\n\\\\left(\\\\varphi_i(\\\\mathbf x),  u_h(\\\\mathbf x, 0)\\\\right)_\\\\Omega &=\\n\\\\left(\\\\varphi_i(\\\\mathbf x), u_0(\\\\mathbf x)\\\\right)_\\\\Omega, &&\\n  \\\\\\\\\\n  u_h(\\\\mathbf x, t) &= g_h(\\\\mathbf x,t) &&\\n  \\\\forall \\\\mathbf x \\\\in \\\\partial\\\\Omega, t \\\\in (0,T),\\n\\\\end{align*}\\n\\n where \\\\(g_h\\\\) is an interpolant of the function \\\\(g\\\\) on the boundary.\\nThis equation can be rewritten in matrix form in the usual way, by expanding \\\\(u_h\\\\) into its coefficients times shape function form, pulling the sum over \\\\(j\\\\) out of the integrals, and then considering that choosing test function \\\\(\\\\varphi_i\\\\) leads to the \\\\(i\\\\)th row of the linear system. This then gives us          \\n\\\\begin{align*}\\n  M\\n  \\\\frac{\\\\partial U(t)}{\\\\partial t}\\n  +\\n  AU(t)\\n  &=\\n  F(t),\\n  \\\\\\\\\\n  U(0) = U_0,\\n\\\\end{align*}\\n\\n plus appropriate boundary conditions.\\nThere are now two perspectives on how one should proceed. If we were to use the SUNDIALS::ARKode wrappers to solve this linear system, we would bring the \\\\(AU\\\\) term to the right hand side and consider the ODE         \\n\\\\begin{align*}\\n  M\\n  \\\\frac{\\\\partial U(t)}{\\\\partial t}\\n  &=\\n  -\\n  AU(t)\\n  +\\n  F(t),\\n\\\\end{align*}\\n\\n which matches the form stated in the documentation of SUNDIALS::ARKode. In particular, ARKode is able to deal with the fact that the time derivative is multiplied by the mass matrix \\\\(M\\\\), which is always there when using finite elements.\\nOn the other hand, when using the PETScWrappers::TimeStepper class, we can solve ODEs that are stated in a general \\\"implicit\\\" form, and in that case we simply bring everything to the left hand side and obtain            \\n\\\\begin{align*}\\n  \\\\underbrace{\\n    M\\n    \\\\frac{\\\\partial U(t)}{\\\\partial t}\\n    +\\n    AU(t)\\n    -\\n    F(t)\\n  }_{=:R(t,U,\\\\dot U)}\\n  =\\n  0.\\n\\\\end{align*}\\n\\n This matches the form \\\\(R(t,U,\\\\dot U) = 0\\\\) you can find in the documentation of PETScWrappers::TimeStepper if you identify the time dependent function \\\\(y=y(t)\\\\) used there with our solution vector \\\\(U(t)\\\\), and our notation \\\\(R(t,U,\\\\dot U)\\\\) instead of the \\\\(F(t,y,\\\\dot y)\\\\) used there and which we rename because we want to use \\\\(F\\\\) as the right hand side vector of the ODE indicating forcing terms.\\nThis program uses the PETScWrappers::TimeStepper class, and so we will take the latter viewpoint. (It is worth noting that SUNDIALS also has a package that can solve ODEs in implicit form, wrapped by the SUNDIALS::IDA class.) In what follows, we will continue to use \\\\(U(t)\\\\) as the function we seek, even though the documentation of the class uses \\\\(y(t)\\\\).\\nMapping the differential equation formulation to the time stepper\\nHaving identified how we want to see the problem (namely, as an \\\"implicit\\\" ODE), the question is how we describe the problem to the time stepper. Conceptually, all of the wrappers for time stepping packages we support in deal.II only requires us to provide them with a very limited set of operations. Specifically, for the implicit formulation used by PETScWrappers::TimeStepper, all we need to implement are functions that provide the following:\\nA way, for a given \\\\(t,U,\\\\dot U\\\\), to evaluate the residual vector \\\\(R(t,U,\\\\dot U)\\\\).\\nA way, for a given \\\\(t,U,\\\\dot U, \\\\alpha\\\\), to set up a matrix  \\\\(J := \\\\dfrac{\\\\partial R}{\\\\partial y} +\\n  \\\\alpha \\\\dfrac{\\\\partial R}{\\\\partial \\\\dot y}\\\\). This is often called the \\\"Jacobian\\\" of the implicit function \\\\(R\\\\), perhaps with a small abuse of terminology. In the current case, this matrix is \\\\(J=A + \\\\alpha M\\\\). If you have read through step-26, it is probably not lost on you that this matrix appears prominently there as well \\u2013 with \\\\(\\\\alpha\\\\) being a multiple of the inverse of the time step (which there we had denoted by \\\\(k_n\\\\)). Importantly, for the linear problem we consider here, \\\\(J\\\\) is a linear combination of matrices that do not depend on \\\\(U\\\\).\\nA way to solve a linear system with this matrix \\\\(J\\\\).\\n\\nThat's really it. If we can provide these three functions, PETSc will do the rest (as would, for example, SUNDIALS::ARKode or, if you prefer the implicit form, SUNDIALS::IDA). It will not be very difficult to set these things up. In practice, the way this will work is that inside the run() function, we will set up lambda functions that can access the information of the surrounding scopes and that return the requested information.\\nIn practice, we often want to provide a fourth function:\\nA callback that is called at the end of each time step and that is provided with the current solution and other information that can be used to \\\"monitor\\\" the progress of computations. One of the ways in which this can be used is to output visualization data every few time steps.\\n\\nComplication 1: Dirichlet boundary values \\nWhile we like to say that all nodes in a finite element mesh are \\\"degrees of freedom\\\", this is not actually true if we have Dirichlet boundary conditions: Degrees of \\\"freedom\\\" located along Dirichlet boundaries have specific values that are prescribed by the boundary conditions and are, consequently, not \\\"free\\\". Moreover, while the form         \\n\\\\begin{align*}\\n  M\\n  \\\\frac{\\\\partial U(t)}{\\\\partial t}\\n  &=\\n  -\\n  AU(t)\\n  +\\n  F(t)\\n\\\\end{align*}\\n\\n suggests that all elements of the vector \\\\(U(t)\\\\) satisfy a differential equation, this is not actually true for those components of \\\\(U\\\\) that correspond to boundary nodes; rather, their values are prescribed and will in general not satisfy the equation. On second thought, you will also find that the same sort of issue happens as well with handing nodes: These, too, are not really \\\"free\\\" but constrained to values that are derived from the values of neighboring nodes.\\nFor the same reason as we will also discuss in the next section, all of this is easier using the Rothe method we have used in all previous tutorials. There, we end up with a PDE at every time step for which we can independently prescribe boundary conditions and hanging node constraints, and we then deal with those by modifying the matrix and right hand side appropriately. Here, with the method of lines, things are slightly more complicated.\\nNot too complicated, however: Like with the mesh refinement issues of the next section, Dirichlet boundary conditions (and constrained degrees of freedom in general) are something every PDE solver has to deal with, and because the people who write ODE solvers also have PDEs in mind, they needed to address these cases too and so the interfaces we use are prepared for it. Specifically, what we need to do is mark which entries of the solution vector (i.e., which degrees of freedom) are \\\"algebraic\\\" \\u2013 that is, satisfy an algebraic, rather than a differential, equation. The way we will do this is that the ODE integrator interface requires us to provide a \\\"callback\\\" function that it can call and that needs to return an IndexSet object in which all the algebraic degrees of freedom are listed. At the end of each solution stage, a second callback then needs to provide the ability to take a solution vector and set its constrained (algebraic) entries to their correct values; for us, that will mean setting boundary values and hanging node constraints correctly.\\n(We note that when dealing with Differential-Algebraic Equations (DAEs), the algebraic components identified by the first callback mentioned above contain hanging nodes, Dirichlet boundary nodes, and degrees of freedom for which the equation does not provide time derivatives. This is not of concern to us here, and so we will not dwell on the issue \\u2013 or the fact that the second callback in the case of DAEs should set only a subset of the algebraic components \\u2013 and instead refer to the examples shown in the SUNDIALS::IDA class documentation.)\\nComplication 2: Mesh refinement \\nWhen stating an ODE in the form         \\n\\\\begin{align*}\\n  M\\n  \\\\frac{\\\\partial U(t)}{\\\\partial t}\\n  &=\\n  -\\n  AU(t)\\n  +\\n  F(t),\\n\\\\end{align*}\\n\\n or one of the reformulations discussed above, there is an implicit assumption that the number of entries in the vector \\\\(U\\\\) stays constant and that each entry continues to correspond to the same quantity. But if you use mesh refinement, this is not the case: The number of unknowns will go up or down whenever you refine or coarsen the mesh, and the 42nd (or any other) degree of freedom may be located at an entirely different physical location after mesh refinement than where it was located below. In other words, the size of vectors and what individual vector entries mean changes when we do mesh refinement. The ODE form we derived above after spatial discretization simply ceases to be meaningful at these times.\\nThis was precisely why, in all previous time-dependent tutorial programs, we have adopted the Rothe approach. There, one first discretizes in time, and obtains a PDE at each time step. This PDE can then be discretized anew \\u2013 if one really wanted to, with an entirely different mesh in each time step, though we typically don't go that far. On the other hand, being able to use external ODE integrators is* undoubtedly very useful, and so let us see if we can shoehorn the mesh refinement complication into what external ODE integrators do. This is, in practice, not as difficult as it may at first sound because, perhaps not surprisingly, ODE integrators are written by people who want to solve problems like ours, and so they have had to deal with the same kinds of complications we are discussing here.\\nThe way we approach the situation from a conceptual perspective is that we break things into \\\"time slabs\\\". Let's say we want to solve on the time interval \\\\([0,T]\\\\), then we break things into slabs \\\\([0=\\\\tau_0,\\\\tau_1], [\\\\tau_1,\\\\tau_2], \\\\ldots [\\\\tau_{n-1},\\\\tau_n=T]\\\\) where the break points satisfy \\\\(\\\\tau_{k-1}<\\\\tau_k\\\\). On each time slab, we keep the mesh the same, and so we can call into our time integrator. At the end of a time slab, we then save the solution, refine the mesh, set up other data structures, and restore the solution on the new mesh; then we start the time integrator again at the start of the new time slab. This approach guarantees that for the purposes of ODE solvers, we really only ever give them something that can rightfully be considered an ODE system. A disadvantage is that we typically want to refine or coarsen the mesh relatively frequently (in large-scale codes one often chooses to refine and coarsen the mesh every 10-20 time steps), and that limits the efficiency of time integrators: They gain much of their advantage from being able to choose the time step length automatically, but there is often a cost associated with starting up; if the slabs are too short, then neither the start-up cost nor the benefit of potentially long time steps are realized.\\nIn practice, good integrators such as those in PETSc TS can deal with this transparently. We just have to give them a way to call back into our code at the end of each time step to ask whether we want to refine the mesh and do some prep work; and a second function that the integrator can then call to do the actual refinement and interpolate solution vectors from old to new mesh. You will see in the run() function that none of this is difficult to explain to the ODE integrator.\\nStructure of the code \\nCompared to the code structure of step-26, the program the current tutorial is principally based on, there are essentially two sets of changes:\\n\\nThe program runs in parallel via MPI. This may, at first, seem like a major change, but the reality is that it does not show up in a very large number of places. If you compare step-6 (a sequential Laplace solver) with step-40 (its parallel version), you will see that it takes maybe 20 or 30 extra lines of code to make a simple code run in parallel. These are principally related to keeping track which cells and degrees of freedom are locally owned, or are on ghost cells. We will also have to use matrix and vector data types that are MPI-aware, and we will get those from the PETScWrappers namespace given that we are already using PETSc for the time stepping.\\nIn step-26 (and most other tutorials), the logic that drives the program's execution lives in the run() function: You can see the loop over all time steps, and which functions are called where within the loop. Here, however, this is no longer the case. In essence, in run(), we create an object of type PETScWrappers::TimeStepper, and after some set-up, we turn over control to that object's PETScWrappers::TimeStepper::solve() function that contains the loop over time steps and the logic that decides how large the time step needs to be, what needs to happen when, etc. In other words, the details of the program's logic are no longer visible. Instead, what you have to provide to the PETScWrappers::TimeStepper object is a series of \\\"callbacks\\\": Functions that the time stepper can call whenever appropriate. These callbacks are typically small lambda functions that, if the functionality required only takes a few lines of code do exactly that or, otherwise, call larger member functions of the main class.\\n\\nThe test case \\nThe program solves the heat equation, which with all right hand sides, initial, and boundary values reads as               \\n\\\\begin{align*}\\n  \\\\frac{\\\\partial u(\\\\mathbf x, t)}{\\\\partial t}\\n  -\\n  \\\\Delta u(\\\\mathbf x, t)\\n  &=\\n  f(\\\\mathbf x, t),\\n  \\\\qquad\\\\qquad &&\\n  \\\\forall \\\\mathbf x \\\\in \\\\Omega, t\\\\in (0,T),\\n  \\\\\\\\\\n  u(\\\\mathbf x, 0) &= u_0(\\\\mathbf x) &&\\n  \\\\forall \\\\mathbf x \\\\in \\\\Omega,\\n  \\\\\\\\\\n  u(\\\\mathbf x, t) &= g(\\\\mathbf x,t) &&\\n  \\\\forall \\\\mathbf x \\\\in \\\\partial\\\\Omega, t \\\\in (0,T).\\n\\\\end{align*}\\n\\n The right hand side \\\\(f\\\\), initial conditions \\\\(u_0\\\\), and Dirichlet boundary values \\\\(g\\\\) are all specified in an input file heat_equation.prm in which these functions are provided as expressions that are parsed and evaluated at run time using the Functions::ParsedFunction<dim> class. The version of this file that is distributed with the library uses        \\n\\\\begin{align*}\\n  f(\\\\mathbf x,t) &= 0, \\\\\\\\\\n  u_0(\\\\mathbf x) &= 0, \\\\\\\\\\n  g(\\\\mathbf x,t) &= \\\\begin{cases}\\n    \\\\cos(4\\\\pi t) & \\\\text{if @f$x=-1@f$}, \\\\\\\\\\n    -\\\\cos(4\\\\pi t) & \\\\text{if @f$x=1@f$}, \\\\\\\\\\n    0 & \\\\text{otherwise}\\n\\\\end{align*}\\n\\n but this is easily changed.\\nThe program's input file also contains two sections that control the time stepper: subsection Time stepper\\n  subsection Running parameters\\n    set final time              = 5\\n    set initial step size       = 0.025\\n    set initial time            = 0\\n    set match final time        = false\\n    set maximum number of steps = -1\\n    set options prefix          =\\n    set solver type             = beuler\\n  end\\n \\n  subsection Error control\\n    set absolute error tolerance = -1\\n    set relative error tolerance = -1\\n    set adaptor type             = none\\n    set ignore algebraic lte     = true\\n    set maximum step size        = -1\\n    set minimum step size        = -1\\n  end\\nend\\n The first of these two sections describes things such as the end time up to which we want to run the program, the initial time step size, and the type of the time stepper (where beuler indicates \\\"backward Euler\\\"; other choices are listed here. We will play with some of these parameters in the results section. As usual when using PETSc solvers, these runtime configuration options can always be complemented (or overridden) via command line options.\\n The commented program\\nThe program starts with the usual prerequisites, all of which you will know by now from either step-26 (the heat equation solver) or step-40 (the parallel Laplace solver):\\n\\u00a0 #include <deal.II/base/utilities.h>\\n\\u00a0 #include <deal.II/base/quadrature_lib.h>\\n\\u00a0 #include <deal.II/base/function.h>\\n\\u00a0 #include <deal.II/base/logstream.h>\\n\\u00a0 #include <deal.II/base/index_set.h>\\n\\u00a0 #include <deal.II/base/parsed_function.h>\\n\\u00a0 #include <deal.II/base/parameter_acceptor.h>\\n\\u00a0 #include <deal.II/base/conditional_ostream.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 #include <deal.II/lac/vector.h>\\n\\u00a0 #include <deal.II/lac/full_matrix.h>\\n\\u00a0 #include <deal.II/lac/dynamic_sparsity_pattern.h>\\n\\u00a0 #include <deal.II/lac/petsc_vector.h>\\n\\u00a0 #include <deal.II/lac/petsc_sparse_matrix.h>\\n\\u00a0 #include <deal.II/lac/petsc_solver.h>\\n\\u00a0 #include <deal.II/lac/petsc_precondition.h>\\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/lac/sparsity_tools.h>\\n\\u00a0 #include <deal.II/distributed/tria.h>\\n\\u00a0 #include <deal.II/distributed/grid_refinement.h>\\n\\u00a0 #include <deal.II/distributed/solution_transfer.h>\\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/grid_out.h>\\n\\u00a0 #include <deal.II/dofs/dof_handler.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 #include <deal.II/fe/fe_values.h>\\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 #include <deal.II/numerics/error_estimator.h>\\n\\u00a0 #include <deal.II/numerics/solution_transfer.h>\\n\\u00a0 #include <deal.II/numerics/matrix_tools.h>\\n\\u00a0 \\nThe only new include file relevant here is the one that provides us with the PETScWrappers::TimerStepper class:\\n\\u00a0 #include <deal.II/lac/petsc_ts.h>\\n\\u00a0 \\n\\u00a0 #include <fstream>\\n\\u00a0 #include <iostream>\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 namespace Step86\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n The HeatEquation class\\nAt its core, this program's principal structure can be understood quite easily if you know step-26 (for the heat equation) and step-40 (for how a parallel solver looks like). It has many of the usual member functions and member variables that for convenience of documentation we will list towards the top of the class so that we can document the remainder separately below.\\nWe derive the main class from ParameterAcceptor to make dealing with run time parameters and reading them a parameter file easier. step-60 and step-70 have already explained how this works.\\n\\u00a0   template <int dim>\\n\\u00a0   class HeatEquation : public ParameterAcceptor\\n\\u00a0   {\\n\\u00a0   public:\\n\\u00a0     HeatEquation(const MPI_Comm mpi_communicator);\\n\\u00a0     void run();\\n\\u00a0 \\n\\u00a0   private:\\n\\u00a0     const MPI_Comm mpi_communicator;\\n\\u00a0 \\n\\u00a0     ConditionalOStream pcout;\\n\\u00a0     TimerOutput        computing_timer;\\n\\u00a0 \\n\\u00a0     parallel::distributed::Triangulation<dim> triangulation;\\n\\u00a0     FE_Q<dim>                                 fe;\\n\\u00a0     DoFHandler<dim>                           dof_handler;\\n\\u00a0 \\n\\u00a0     IndexSet locally_owned_dofs;\\n\\u00a0     IndexSet locally_relevant_dofs;\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     void setup_system(const double time);\\n\\u00a0 \\n\\u00a0     void output_results(const double                      time,\\n\\u00a0                         const unsigned int                timestep_number,\\n\\u00a0                         const PETScWrappers::MPI::Vector &solution);\\n\\u00a0 \\nConditionalOStreamDefinition conditional_ostream.h:80\\nDoFHandlerDefinition dof_handler.h:317\\nFE_QDefinition fe_q.h:554\\nIndexSetDefinition index_set.h:70\\nMPI_Comm\\nPETScWrappers::MPI::VectorDefinition petsc_vector.h:158\\nParameterAcceptorDefinition parameter_acceptor.h:359\\nTimerOutputDefinition timer.h:549\\nparallel::distributed::TriangulationDefinition tria.h:268\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\nAt this point, we start to deviate from the \\\"manual\\\" way of solving time dependent problems. High level packages for the solution of ODEs usually expect the user to provide a function that computes the residual of the equation and the time derivative of the solution.\\nThis allows those packages to abstract away the details of how the time derivative is defined (e.g., backward Euler, Crank-Nicolson, etc.) and allow the user to provide a uniform interface, irrespective of the time stepper used. PETSc TS and SUNDIALS are two examples of such packages, and they require the user to provide C style call-backs to compute residuals, Jacobians, etc. In deal.II, we wrap these C style libraries with our own wrappers that use a style closer to c++, and that allows us to simply define the callbacks via lambda functions. Several of these lambda functions will simply call member functions to do the actual work.\\nTo make it clear what we are doing here, we start by defining functions with the same name of the interface that we will use later on (and documented both in the introduction of this program as well in the documentation of the PETScWrappers::TimeStepper class). These are the functions that compute the right hand side, the \\\"Jacobian matrix\\\" (see the introduction for how that is defined), and that can solve a linear system with the Jacobian matrix. At the bottom of the following block, we also declare the matrix object that will store this Jacobian.\\nNote that all of these functions receive the current solution (and, where necessary, its time derivative) as inputs. This is because the solution vector \\u2013 i.e., the variable that stores the current state of the solution \\u2013 is kept inside the time integrator object. This is useful: If we kept a copy of the solution vector as a member variable of the current class, one would continuously have to wonder whether it is still in sync with the version of the solution vector the time integrator object internally believes is the currently correct version of this vector. As a consequence, we do not store such a copy here: Whenever a function requires access to the current value of the solution, it receives it from the time integrator as a const argument. The same observation can be made about the variable that stores the current time, or the current length of the time step, or the number of time steps performed so far: They are all kept inside the time stepping object.\\n\\u00a0     void implicit_function(const double                      time,\\n\\u00a0                            const PETScWrappers::MPI::Vector &solution,\\n\\u00a0                            const PETScWrappers::MPI::Vector &solution_dot,\\n\\u00a0                            PETScWrappers::MPI::Vector       &residual);\\n\\u00a0 \\n\\u00a0     void\\n\\u00a0     assemble_implicit_jacobian(const double                      time,\\n\\u00a0                                const PETScWrappers::MPI::Vector &solution,\\n\\u00a0                                const PETScWrappers::MPI::Vector &solution_dot,\\n\\u00a0                                const double                      shift);\\n\\u00a0 \\n\\u00a0     void solve_with_jacobian(const PETScWrappers::MPI::Vector &src,\\n\\u00a0                              PETScWrappers::MPI::Vector       &residual);\\n\\u00a0 \\n\\u00a0     PETScWrappers::MPI::SparseMatrix jacobian_matrix;\\n\\u00a0 \\n\\u00a0 \\nPETScWrappers::MPI::SparseMatrixDefinition petsc_sparse_matrix.h:367\\nIn this tutorial program, similar to what we did in step-26, we want to adapt the mesh at regular time intervals. However, if we are using an external time stepper, we need to make sure that the time stepper is aware of the mesh changes \\u2013 see the discussion on this topic in the introduction. In particular, the time stepper must support the fact that the number of degrees of freedom may change, and it must support the fact that all internal vectors (e.g., the solution vector and all intermediate stages used to compute the current time derivative) will need to be transferred to the new mesh. For reasons that will be discussed in the implementation below, we split the mesh refinement operation into two functions: The first will mark which cells to refine, based on a given solution vector from which we can compute error indicators; the second one will do the actual mesh refinement and transfer a set of vectors from the old to the new mesh.\\n\\u00a0     void prepare_for_coarsening_and_refinement(\\n\\u00a0       const PETScWrappers::MPI::Vector &solution);\\n\\u00a0 \\n\\u00a0     void transfer_solution_vectors_to_new_mesh(\\n\\u00a0       const double                                   time,\\n\\u00a0       const std::vector<PETScWrappers::MPI::Vector> &all_in,\\n\\u00a0       std::vector<PETScWrappers::MPI::Vector>       &all_out);\\n\\u00a0 \\nAs also discussed in the introduction, we also have to deal with \\\"algebraic\\\" solution components, i.e., degrees of freedom that are not really free but instead have values determined either by boundary conditions or by hanging node constraints. While the values of the boundary conditions can change from time step to time step (because the function \\\\(g(\\\\mathbf x,t)\\\\) may indeed depend on time), hanging node constraints remain the same as long as the mesh remains the same. As a consequence, we will keep an AffineConstraints object that stores the hanging node constraints and that is only updated when the mesh changes, and then an AffineConstraints object current_constraints that we will initialize with the hanging node constraints and then add the constraints due to Dirichlet boundary values at a specified time. This evaluation of boundary values and combining of constraints happens in the update_current_constraints() function.\\nAt one place in the program, we will also need an object that constrains the same degrees of freedom, but with zero values even if the boundary values for the solution are non-zero; we will keep this modified set of constraints in homogeneous_constraints.\\n\\u00a0     AffineConstraints<double> hanging_nodes_constraints;\\n\\u00a0     AffineConstraints<double> current_constraints;\\n\\u00a0     AffineConstraints<double> homogeneous_constraints;\\n\\u00a0 \\n\\u00a0     void update_current_constraints(const double time);\\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nThe remainder of the class is simply consumed with objects that describe either the functioning of the time stepper, when and how to do mesh refinement, and the objects that describe right hand side, initial conditions, and boundary conditions for the PDE. Since we already derive our class from ParameterAcceptor, we exploit its facilities to parse also the parameters of the functions that define the initial value, the right hand side, and the boundary values. We therefore create three ParameterAcceptorProxy objects, which wrap the actual ParsedFunction class into objects that ParameterAcceptor can handle.\\n\\u00a0     PETScWrappers::TimeStepperData time_stepper_data;\\n\\u00a0 \\n\\u00a0     unsigned int initial_global_refinement;\\n\\u00a0     unsigned int max_delta_refinement_level;\\n\\u00a0     unsigned int mesh_adaptation_frequency;\\n\\u00a0 \\n\\u00a0     ParameterAcceptorProxy<Functions::ParsedFunction<dim>>\\n\\u00a0       right_hand_side_function;\\n\\u00a0     ParameterAcceptorProxy<Functions::ParsedFunction<dim>>\\n\\u00a0       initial_value_function;\\n\\u00a0     ParameterAcceptorProxy<Functions::ParsedFunction<dim>>\\n\\u00a0       boundary_values_function;\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0 \\nPETScWrappers::TimeStepperDataDefinition petsc_ts.h:45\\nParameterAcceptorProxyDefinition parameter_acceptor.h:697\\n The HeatEquation constructor\\nThe constructor is responsible for initializing all member variables of the class. This is relatively straightforward, and includes setting up parameters to be set upon reading from an input file via the ParameterAcceptor mechanism previously detailed in step-60 and step-70.\\n\\u00a0   template <int dim>\\n\\u00a0   HeatEquation<dim>::HeatEquation(const MPI_Comm mpi_communicator)\\n\\u00a0     : ParameterAcceptor(\\\"/Heat Equation/\\\")\\n\\u00a0     , mpi_communicator(mpi_communicator)\\n\\u00a0     , pcout(std::cout,\\n\\u00a0             (Utilities::MPI::this_mpi_process(mpi_communicator) == 0))\\n\\u00a0     , computing_timer(mpi_communicator,\\n\\u00a0                       pcout,\\n\\u00a0                       TimerOutput::summary,\\n\\u00a0                       TimerOutput::wall_times)\\n\\u00a0     , triangulation(mpi_communicator,\\n\\u00a0                     typename Triangulation<dim>::MeshSmoothing(\\n\\u00a0                       Triangulation<dim>::smoothing_on_refinement |\\n\\u00a0                       Triangulation<dim>::smoothing_on_coarsening))\\n\\u00a0     , fe(1)\\n\\u00a0     , dof_handler(triangulation)\\n\\u00a0     , time_stepper_data(\\\"\\\",\\n\\u00a0                         \\\"beuler\\\",\\n\\u00a0                         /* start time */ 0.0,\\n\\u00a0                         /* end time */ 1.0,\\n\\u00a0                         /* initial time step */ 0.025)\\n\\u00a0     , initial_global_refinement(5)\\n\\u00a0     , max_delta_refinement_level(2)\\n\\u00a0     , mesh_adaptation_frequency(0)\\n\\u00a0     , right_hand_side_function(\\\"/Heat Equation/Right hand side\\\")\\n\\u00a0     , initial_value_function(\\\"/Heat Equation/Initial value\\\")\\n\\u00a0     , boundary_values_function(\\\"/Heat Equation/Boundary values\\\")\\n\\u00a0   {\\n\\u00a0     enter_subsection(\\\"Time stepper\\\");\\n\\u00a0     {\\n\\u00a0       enter_my_subsection(this->prm);\\n\\u00a0       {\\n\\u00a0         time_stepper_data.add_parameters(this->prm);\\n\\u00a0       }\\n\\u00a0       leave_my_subsection(this->prm);\\n\\u00a0     }\\n\\u00a0     leave_subsection();\\n\\u00a0 \\n\\u00a0     add_parameter(\\\"Initial global refinement\\\",\\n\\u00a0                   initial_global_refinement,\\n\\u00a0                   \\\"Number of times the mesh is refined globally before \\\"\\n\\u00a0                   \\\"starting the time stepping.\\\");\\n\\u00a0     add_parameter(\\\"Maximum delta refinement level\\\",\\n\\u00a0                   max_delta_refinement_level,\\n\\u00a0                   \\\"Maximum number of local refinement levels.\\\");\\n\\u00a0     add_parameter(\\\"Mesh adaptation frequency\\\",\\n\\u00a0                   mesh_adaptation_frequency,\\n\\u00a0                   \\\"When to adapt the mesh.\\\");\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTriangulationDefinition tria.h:1323\\nInitializeLibrary::MPI@ MPI\\nUtilitiesDefinition communication_pattern_base.h:30\\nstdSTL namespace.\\n The HeatEquation::setup_system() function\\nThis function is not very different from what we do in many other programs (including step-26). We enumerate degrees of freedom, output some information about then, build constraint objects (recalling that we put hanging node constraints into their separate object), and then also build an AffineConstraint object that contains both the hanging node constraints as well as constraints corresponding to zero Dirichlet boundary conditions. This last object is needed since we impose the constraints through algebraic equations. While technically it would be possible to use the time derivative of the boundary function as a boundary conditions for the time derivative of the solution, this is not done here. Instead, we impose the boundary conditions through algebraic equations, and therefore the time derivative of the boundary conditions is not part of the algebraic system, and we need zero boundary conditions on the time derivative of the solution when computing the residual. We use the homogeneous_constraints object for this purpose.\\nFinally, we create the actual non-homogeneous current_constraints by calling `update_current_constraints). These are also used during the assembly and during the residual evaluation.\\n\\u00a0   template <int dim>\\n\\u00a0   void HeatEquation<dim>::setup_system(const double time)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"setup system\\\");\\n\\u00a0 \\n\\u00a0     dof_handler.distribute_dofs(fe);\\n\\u00a0     pcout << std::endl\\n\\u00a0           << \\\"Number of active cells: \\\" << triangulation.n_active_cells()\\n\\u00a0           << std::endl\\n\\u00a0           << \\\"Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n\\u00a0           << std::endl\\n\\u00a0           << std::endl;\\n\\u00a0 \\n\\u00a0     locally_owned_dofs = dof_handler.locally_owned_dofs();\\n\\u00a0     locally_relevant_dofs =\\n\\u00a0       DoFTools::extract_locally_relevant_dofs(dof_handler);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     hanging_nodes_constraints.clear();\\n\\u00a0     hanging_nodes_constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n\\u00a0     DoFTools::make_hanging_node_constraints(dof_handler,\\n\\u00a0                                             hanging_nodes_constraints);\\n\\u00a0     hanging_nodes_constraints.make_consistent_in_parallel(locally_owned_dofs,\\n\\u00a0                                                           locally_relevant_dofs,\\n\\u00a0                                                           mpi_communicator);\\n\\u00a0     hanging_nodes_constraints.close();\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     homogeneous_constraints.clear();\\n\\u00a0     homogeneous_constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n\\u00a0     homogeneous_constraints.merge(hanging_nodes_constraints);\\n\\u00a0     VectorTools::interpolate_boundary_values(dof_handler,\\n\\u00a0                                              0,\\n\\u00a0                                              Functions::ZeroFunction<dim>(),\\n\\u00a0                                              homogeneous_constraints);\\n\\u00a0     homogeneous_constraints.make_consistent_in_parallel(locally_owned_dofs,\\n\\u00a0                                                         locally_relevant_dofs,\\n\\u00a0                                                         mpi_communicator);\\n\\u00a0     homogeneous_constraints.close();\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     update_current_constraints(time);\\n\\u00a0 \\n\\u00a0 \\nFunctions::ZeroFunctionDefinition function.h:510\\nTimerOutput::ScopeDefinition timer.h:557\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\nDoFTools::make_hanging_node_constraintsvoid make_hanging_node_constraints(const DoFHandler< dim, spacedim > &dof_handler, AffineConstraints< number > &constraints)Definition dof_tools_constraints.cc:3073\\nDoFTools::extract_locally_relevant_dofsIndexSet extract_locally_relevant_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1164\\nVectorTools::interpolate_boundary_valuesvoid interpolate_boundary_values(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const std::map< types::boundary_id, const Function< spacedim, number > * > &function_map, std::map< types::global_dof_index, number > &boundary_values, const ComponentMask &component_mask={})\\nThe final block of code resets and initializes the matrix object with the appropriate sparsity pattern. Recall that we do not store solution vectors in this class (the time integrator object does that internally) and so do not have to resize and initialize them either.\\n\\u00a0     DynamicSparsityPattern dsp(locally_relevant_dofs);\\n\\u00a0     DoFTools::make_sparsity_pattern(dof_handler,\\n\\u00a0                                     dsp,\\n\\u00a0                                     homogeneous_constraints,\\n\\u00a0                                     false);\\n\\u00a0     SparsityTools::distribute_sparsity_pattern(dsp,\\n\\u00a0                                                locally_owned_dofs,\\n\\u00a0                                                mpi_communicator,\\n\\u00a0                                                locally_relevant_dofs);\\n\\u00a0 \\n\\u00a0     jacobian_matrix.reinit(locally_owned_dofs,\\n\\u00a0                            locally_owned_dofs,\\n\\u00a0                            dsp,\\n\\u00a0                            mpi_communicator);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nDynamicSparsityPatternDefinition dynamic_sparsity_pattern.h:322\\nDoFTools::make_sparsity_patternvoid make_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, SparsityPatternBase &sparsity_pattern, const AffineConstraints< number > &constraints={}, const bool keep_constrained_dofs=true, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id)Definition dof_tools_sparsity.cc:56\\nSparsityTools::distribute_sparsity_patternvoid distribute_sparsity_pattern(DynamicSparsityPattern &dsp, const IndexSet &locally_owned_rows, const MPI_Comm mpi_comm, const IndexSet &locally_relevant_rows)Definition sparsity_tools.cc:1020\\n The HeatEquation::output_results() function\\nThis function is called from \\\"monitor\\\" function that is called in turns by the time stepper in each time step. We use it to write the solution to a file, and provide graphical output through paraview or visit. We also write a pvd file, which groups all metadata about the .vtu files into a single file that can be used to load the full time dependent solution in paraview.\\n\\u00a0   template <int dim>\\n\\u00a0   void HeatEquation<dim>::output_results(const double       time,\\n\\u00a0                                          const unsigned int timestep_number,\\n\\u00a0                                          const PETScWrappers::MPI::Vector &y)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"output results\\\");\\n\\u00a0 \\n\\u00a0     DataOut<dim> data_out;\\n\\u00a0     data_out.attach_dof_handler(dof_handler);\\n\\u00a0     data_out.add_data_vector(y, \\\"U\\\");\\n\\u00a0     data_out.build_patches();\\n\\u00a0 \\n\\u00a0     data_out.set_flags(DataOutBase::VtkFlags(time, timestep_number));\\n\\u00a0 \\n\\u00a0     const std::string filename =\\n\\u00a0       \\\"solution-\\\" + Utilities::int_to_string(timestep_number, 3) + \\\".vtu\\\";\\n\\u00a0     data_out.write_vtu_in_parallel(filename, mpi_communicator);\\n\\u00a0 \\n\\u00a0     if (Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n\\u00a0       {\\n\\u00a0         static std::vector<std::pair<double, std::string>> times_and_names;\\n\\u00a0         times_and_names.emplace_back(time, filename);\\n\\u00a0 \\n\\u00a0         std::ofstream pvd_output(\\\"solution.pvd\\\");\\n\\u00a0         DataOutBase::write_pvd_record(pvd_output, times_and_names);\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOutDefinition data_out.h:147\\nDataOutBase::write_pvd_recordvoid write_pvd_record(std::ostream &out, const std::vector< std::pair< double, std::string > > &times_and_names)Definition data_out_base.cc:6256\\nUtilities::MPI::this_mpi_processunsigned int this_mpi_process(const MPI_Comm mpi_communicator)Definition mpi.cc:107\\nUtilities::int_to_stringstd::string int_to_string(const unsigned int value, const unsigned int digits=numbers::invalid_unsigned_int)Definition utilities.cc:470\\nDataOutBase::VtkFlagsDefinition data_out_base.h:1127\\n The HeatEquation::implicit_function() function\\nAs discussed in the introduction, we describe the ODE system to the time stepper via its residual,   \\n\\\\[\\n   R(t,U,\\\\dot U) = M \\\\frac{\\\\partial U(t)}{\\\\partial t} + AU(t) - F(t).\\n   \\\\]\\n\\n The following function computes it, given vectors for \\\\(U,\\\\dot U\\\\) that we will denote by y and y_dot because that's how they are called in the documentation of the PETScWrappers::TimeStepper class.\\nAt the top of the function, we do the usual set up when computing integrals. We face two minor difficulties here: the first is that the y and y_dot vectors we get as input are read only, but we need to make sure they satisfy the correct boundary conditions and so have to set elements in these vectors. The second is that we need to compute the residual, and therefore in general we need to evaluate solution values and gradients inside locally owned cells, and for this need access to degrees of freedom which may be owned by neighboring processors. To address these issues, we create (non-ghosted) writable copies of the input vectors, apply boundary conditions and hanging node current_constraints; and then copy these vectors to ghosted vectors before we can do anything sensible with them.\\n\\u00a0   template <int dim>\\n\\u00a0   void\\n\\u00a0   HeatEquation<dim>::implicit_function(const double                      time,\\n\\u00a0                                        const PETScWrappers::MPI::Vector &y,\\n\\u00a0                                        const PETScWrappers::MPI::Vector &y_dot,\\n\\u00a0                                        PETScWrappers::MPI::Vector &residual)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"implicit function\\\");\\n\\u00a0 \\n\\u00a0     PETScWrappers::MPI::Vector tmp_solution(locally_owned_dofs,\\n\\u00a0                                             mpi_communicator);\\n\\u00a0     PETScWrappers::MPI::Vector tmp_solution_dot(locally_owned_dofs,\\n\\u00a0                                                 mpi_communicator);\\n\\u00a0     tmp_solution     = y;\\n\\u00a0     tmp_solution_dot = y_dot;\\n\\u00a0 \\n\\u00a0     update_current_constraints(time);\\n\\u00a0     current_constraints.distribute(tmp_solution);\\n\\u00a0     homogeneous_constraints.distribute(tmp_solution_dot);\\n\\u00a0 \\n\\u00a0     PETScWrappers::MPI::Vector locally_relevant_solution(locally_owned_dofs,\\n\\u00a0                                                          locally_relevant_dofs,\\n\\u00a0                                                          mpi_communicator);\\n\\u00a0     PETScWrappers::MPI::Vector locally_relevant_solution_dot(\\n\\u00a0       locally_owned_dofs, locally_relevant_dofs, mpi_communicator);\\n\\u00a0     locally_relevant_solution     = tmp_solution;\\n\\u00a0     locally_relevant_solution_dot = tmp_solution_dot;\\n\\u00a0 \\n\\u00a0 \\n\\u00a0     const QGauss<dim> quadrature_formula(fe.degree + 1);\\n\\u00a0     FEValues<dim>     fe_values(fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_values | update_gradients |\\n\\u00a0                               update_quadrature_points | update_JxW_values);\\n\\u00a0 \\n\\u00a0     const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0     const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0     std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     std::vector<Tensor<1, dim>> solution_gradients(n_q_points);\\n\\u00a0     std::vector<double>         solution_dot_values(n_q_points);\\n\\u00a0 \\n\\u00a0     Vector<double> cell_residual(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     right_hand_side_function.set_time(time);\\n\\u00a0 \\nFEValuesDefinition fe_values.h:63\\nQGaussDefinition quadrature_lib.h:40\\nVectorDefinition vector.h:120\\nupdate_values@ update_valuesShape function values.Definition fe_update_flags.h:75\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\nNow for computing the actual residual. Recall that we wan to compute the vector   \\n\\\\[\\n   R(t,U,\\\\dot U) = M \\\\frac{\\\\partial U(t)}{\\\\partial t} + AU(t) - F(t).\\n   \\\\]\\n\\n We could do that by actually forming the matrices \\\\(M\\\\) and \\\\(A\\\\), but this is not efficient. Instead, recall (by writing out how the elements of \\\\(M\\\\) and \\\\(A\\\\) are defined, and exchanging integrals and sums) that the \\\\(i\\\\)th element of the residual vector is given by             \\n\\\\begin{align*}\\n   R(t,U,\\\\dot U)_i\\n   &= \\\\sum_j \\\\int_\\\\Omega \\\\varphi_i(\\\\mathbf x, t) \\\\varphi_j(\\\\mathbf x, t)\\n   {\\\\partial U_j(t)}{\\\\partial t}\\n   + \\\\sum_j \\\\int_\\\\Omega \\\\nabla \\\\varphi_i(\\\\mathbf x, t) \\\\cdot \\\\nabla\\n   \\\\varphi_j(\\\\mathbf x, t) U_j(t)\\n   - \\\\int_\\\\Omega \\\\varphi_i f(\\\\mathbf x, t)\\n   \\\\\\\\ &=\\n   \\\\int_\\\\Omega \\\\varphi_i(\\\\mathbf x, t) u_h(\\\\mathbf x, t)\\n   + \\\\int_\\\\Omega \\\\nabla \\\\varphi_i(\\\\mathbf x, t) \\\\cdot \\\\nabla\\n   u_h(\\\\mathbf x, t)\\n   - \\\\int_\\\\Omega \\\\varphi_i f(\\\\mathbf x, t).\\n   \\\\end{align*}\\n\\n We can compute these integrals efficiently by breaking them up into a sum over all cells and then applying quadrature. For the integrand, we need to evaluate the solution and its gradient at the quadrature points within each locally owned cell, and for this, we need also access to degrees of freedom that may be owned by neighboring processors. We therefore use the locally_relevant_solution and and locally_relevant_solution_dot vectors.\\n\\u00a0     residual = 0;\\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       if (cell->is_locally_owned())\\n\\u00a0         {\\n\\u00a0           fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0           fe_values.get_function_gradients(locally_relevant_solution,\\n\\u00a0                                            solution_gradients);\\n\\u00a0           fe_values.get_function_values(locally_relevant_solution_dot,\\n\\u00a0                                         solution_dot_values);\\n\\u00a0 \\n\\u00a0           cell->get_dof_indices(local_dof_indices);\\n\\u00a0 \\n\\u00a0           cell_residual = 0;\\n\\u00a0           for (const unsigned int q : fe_values.quadrature_point_indices())\\n\\u00a0             for (const unsigned int i : fe_values.dof_indices())\\n\\u00a0               {\\n\\u00a0                 cell_residual(i) +=\\n\\u00a0                   (fe_values.shape_value(i, q) *       // [phi_i(x_q) *\\n\\u00a0                      solution_dot_values[q]            //  u(x_q)\\n\\u00a0                    +                                   //  +\\n\\u00a0                    fe_values.shape_grad(i, q) *        //  grad phi_i(x_q) *\\n\\u00a0                      solution_gradients[q]             //  grad u(x_q)\\n\\u00a0                    -                                   //  -\\n\\u00a0                    fe_values.shape_value(i, q) *       //  phi_i(x_q) *\\n\\u00a0                      right_hand_side_function.value(   \\n\\u00a0                        fe_values.quadrature_point(q))) //  f(x_q)]\\n\\u00a0                   * fe_values.JxW(q);                  // * dx\\n\\u00a0               }\\n\\u00a0           current_constraints.distribute_local_to_global(cell_residual,\\n\\u00a0                                                          local_dof_indices,\\n\\u00a0                                                          residual);\\n\\u00a0         }\\n\\u00a0     residual.compress(VectorOperation::add);\\n\\u00a0 \\nint\\nVectorOperation::add@ addDefinition vector_operation.h:53\\nThe end result of the operations above is a vector that contains the residual vector, having taken into account the constraints due to hanging nodes and Dirichlet boundary conditions (by virtue of having used current_constraints.distribute_local_to_global() to add the local contributions to the global vector. At the end of the day, the residual vector \\\\(r\\\\) will be used in the solution of linear systems of the form \\\\(J z = r\\\\) with the \\\"Jacobian\\\" matrix that we define below. We want to achieve that for algebraic components, the algebraic components of \\\\(z\\\\) have predictable values that achieve the purposes discussed in the following. We do this by ensuring that the entries corresponding to algebraic components in the residual \\\\(r\\\\) have specific values, and then we will do the same in the next function for the matrix; for this, you will have to know that the rows and columns of the matrix corresponding to constrained entries are zero with the exception of the diagonal entries. We will manually set that diagonal entry to one, and so \\\\(z_i=r_i\\\\) for algebraic components.\\nFrom the point of view of the residual vector, if the input y vector does not contain the correct values on constrained degrees of freedom (hanging nodes or boundary conditions), we need to communicate this to the time stepper, and we do so by setting the residual to the actual difference between the input y vector and the our local copy of it, in which we have applied the constraints (see the top of the function where we called current_constraints.distribute(tmp_solution) and a similar operation on the time derivative). Since we have made a copy of the input vector for this purpose, we use it to compute the residual value. However, there is a difference between hanging nodes constraints and boundary conditions: we do not want to make hanging node constraints actually depend on their dependent degrees of freedom, since this would imply that we are actually solving for the dependent degrees of freedom. This is not what we are actually doing, however, since hanging nodes are not actually solved for. They are eliminated from the system by the call to AffineConstraints::distribute_local_to_global() above. From the point of view of the Jacobian matrix, we are effectively setting hanging nodes to an artificial value (usually zero), and we simply want to make sure that we solve for those degrees of freedom a posteriori, by calling the function AffineConstraints::distribute().\\nHere we therefore check that the residual is equal to the input value on the constrained dofs corresponding to hanging nodes (i.e., those for which the lines of the current_constraints contain at least one other entry), and to the difference between the input vector and the actual solution on those constraints that correspond to boundary conditions.\\n\\u00a0     for (const auto &c : current_constraints.get_lines())\\n\\u00a0       if (locally_owned_dofs.is_element(c.index))\\n\\u00a0         {\\n\\u00a0           if (c.entries.empty()) /* no dependencies -> a Dirichlet node */\\n\\u00a0             residual[c.index] = y[c.index] - tmp_solution[c.index];\\n\\u00a0           else /* has dependencies -> a hanging node */\\n\\u00a0             residual[c.index] = y[c.index];\\n\\u00a0         }\\n\\u00a0     residual.compress(VectorOperation::insert);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nVectorOperation::insert@ insertDefinition vector_operation.h:49\\n The HeatEquation::assemble_implicit_jacobian() function\\nThe next operation is to compute the \\\"Jacobian\\\", which PETSc TS defines as the matrix    \\n\\\\[\\n   J_\\\\alpha = \\\\dfrac{\\\\partial R}{\\\\partial y} + \\\\alpha \\\\dfrac{\\\\partial\\n   R}{\\\\partial \\\\dot y}\\n   \\\\]\\n\\n which, for the current linear problem, is simply   \\n\\\\[\\n   J_\\\\alpha = A + \\\\alpha M\\n   \\\\]\\n\\n and which is in particular independent of time and the current solution vectors \\\\(y\\\\) and \\\\(\\\\dot y\\\\).\\nHaving seen the assembly of matrices before, there is little that should surprise you in the actual assembly here:\\n\\u00a0   template <int dim>\\n\\u00a0   void HeatEquation<dim>::assemble_implicit_jacobian(\\n\\u00a0     const double /* time */,\\n\\u00a0     const PETScWrappers::MPI::Vector & /* y */,\\n\\u00a0     const PETScWrappers::MPI::Vector & /* y_dot */,\\n\\u00a0     const double alpha)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"assemble implicit Jacobian\\\");\\n\\u00a0 \\n\\u00a0     const QGauss<dim> quadrature_formula(fe.degree + 1);\\n\\u00a0     FEValues<dim>     fe_values(fe,\\n\\u00a0                             quadrature_formula,\\n\\u00a0                             update_values | update_gradients |\\n\\u00a0                               update_quadrature_points | update_JxW_values);\\n\\u00a0 \\n\\u00a0     const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0 \\n\\u00a0     std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0 \\n\\u00a0     FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n\\u00a0 \\n\\u00a0     jacobian_matrix = 0;\\n\\u00a0     for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0       if (cell->is_locally_owned())\\n\\u00a0         {\\n\\u00a0           fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0           cell->get_dof_indices(local_dof_indices);\\n\\u00a0 \\n\\u00a0           cell_matrix = 0;\\n\\u00a0           for (const unsigned int q : fe_values.quadrature_point_indices())\\n\\u00a0             for (const unsigned int i : fe_values.dof_indices())\\n\\u00a0               for (const unsigned int j : fe_values.dof_indices())\\n\\u00a0                 {\\n\\u00a0                   cell_matrix(i, j) +=\\n\\u00a0                     (fe_values.shape_grad(i, q) *      // grad phi_i(x_q) *\\n\\u00a0                        fe_values.shape_grad(j, q)      // grad phi_j(x_q)\\n\\u00a0                      + alpha *                         \\n\\u00a0                          fe_values.shape_value(i, q) * // phi_i(x_q) *\\n\\u00a0                          fe_values.shape_value(j, q)   // phi_j(x_q)\\n\\u00a0                      ) *\\n\\u00a0                     fe_values.JxW(q); // * dx\\n\\u00a0                 }\\n\\u00a0           current_constraints.distribute_local_to_global(cell_matrix,\\n\\u00a0                                                          local_dof_indices,\\n\\u00a0                                                          jacobian_matrix);\\n\\u00a0         }\\n\\u00a0     jacobian_matrix.compress(VectorOperation::add);\\n\\u00a0 \\nFullMatrixDefinition full_matrix.h:79\\nThe only interesting part is the following. Recall that we modified the residual vector's entries corresponding to the algebraic components of the solution in the previous function. The outcome of calling current_constraints.distribute_local_to_global() a few lines above is that the global matrix has zero rows and columns for the algebraic (constrained) components of the solution; the function puts a value on the diagonal that is nonzero and has about the same size as the remaining diagonal entries of the matrix. What this diagonal value is is unknown to us \\u2013 in other cases where we call current_constraints.distribute_local_to_global() on both the left side matrix and the right side vector, as in most other tutorial programs, the matrix diagonal entries and right hand side values are chosen in such a way that the result of solving a linear system is what we want it to be, but the scaling is done automatically.\\nThis is not good enough for us here, because we are building the right hand side independently from the matrix in different functions. Thus, for any constrained degree of freedom, we set the diagonal of the Jacobian to be one. This leaves the Jacobian matrix invertible, consistent with what the time stepper expects, and it also makes sure that if we did not make a mistake in the residual and/or in the Jacbian matrix, then asking the time stepper to check the Jacobian with a finite difference method will produce the correct result. This can be activated at run time via passing the -snes_test_jacobian option on the command line.\\n\\u00a0     for (const auto &c : current_constraints.get_lines())\\n\\u00a0       jacobian_matrix.set(c.index, c.index, 1.0);\\n\\u00a0     jacobian_matrix.compress(VectorOperation::insert);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n The HeatEquation::solve_with_jacobian() function\\nThis is the function that actually solves the linear system with the Jacobian matrix we have previously built (in a call to the previous function during the current time step or another earlier one \\u2013 time steppers are quite sophisticated in determining internally whether it is necessary to update the Jacobian matrix, or whether one can reuse it for another time step without rebuilding it; this is similar to how one can re-use the Newton matrix for several Newton steps, see for example the discussion in step-77). We could in principle not provide this function to the time stepper, and instead select a specific solver on the command line by using the -ksp_* options of PETSc. However, by providing this function, we can use a specific solver and preconditioner for the linear system, and still have the possibility to change them on the command line.\\nProviding a specific solver is more in line with the way we usually do things in other deal.II examples, while letting PETSc choose a generic solver, and changing it on the command line via -ksp_type is more in line with the way PETSc is usually used, and it can be a convenient approach when we are experimenting to find an optimal solver for our problem. Both options are available here, since we can still change both the solver and the preconditioner on the command line via -user_ksp_type and -user_pc_type options.\\nIn any case, recall that the Jacobian we built in the previous function is always of the form   \\n\\\\[\\n   J_\\\\alpha = \\\\alpha M + A\\n   \\\\]\\n\\n where \\\\(M\\\\) is a mass matrix and \\\\(A\\\\) a Laplace matrix. \\\\(M\\\\) is symmetric and positive definite; \\\\(A\\\\) is symmetric and at least positive semidefinite; \\\\(\\\\alpha> 0\\\\). As a consequence, the Jacobian matrix is a symmetric and positive definite matrix, which we can efficiently solve with the Conjugate Gradient method, along with either SSOR or (if available) the algebraic multigrid implementation provided by PETSc (via the Hypre package) as preconditioner. In practice, if you wanted to solve \\\"real\\\" problems, one would spend some time finding which preconditioner is optimal, perhaps using PETSc's ability to read solver and preconditioner choices from the command line. But this is not the focus of this tutorial program, and so we just go with the following:\\n\\u00a0   template <int dim>\\n\\u00a0   void\\n\\u00a0   HeatEquation<dim>::solve_with_jacobian(const PETScWrappers::MPI::Vector &src,\\n\\u00a0                                          PETScWrappers::MPI::Vector       &dst)\\n\\u00a0   {\\n\\u00a0     TimerOutput::Scope t(computing_timer, \\\"solve with Jacobian\\\");\\n\\u00a0 \\n\\u00a0 #if defined(PETSC_HAVE_HYPRE)\\n\\u00a0     PETScWrappers::PreconditionBoomerAMG preconditioner;\\n\\u00a0     preconditioner.initialize(jacobian_matrix);\\n\\u00a0 #else\\n\\u00a0     PETScWrappers::PreconditionSSOR preconditioner;\\n\\u00a0     preconditioner.initialize(\\n\\u00a0       jacobian_matrix, PETScWrappers::PreconditionSSOR::AdditionalData(1.0));\\n\\u00a0 #endif\\n\\u00a0 \\n\\u00a0     SolverControl           solver_control(1000, 1e-8 * src.l2_norm());\\n\\u00a0     PETScWrappers::SolverCG cg(solver_control);\\n\\u00a0     cg.set_prefix(\\\"user_\\\");\\n\\u00a0 \\n\\u00a0     cg.solve(jacobian_matrix, dst, src, preconditioner);\\n\\u00a0 \\n\\u00a0     pcout << \\\"     \\\" << solver_control.last_step() << \\\" linear iterations.\\\"\\n\\u00a0           << std::endl;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nPETScWrappers::PreconditionBoomerAMGDefinition petsc_precondition.h:621\\nPETScWrappers::PreconditionBoomerAMG::initializevoid initialize(const MatrixBase &matrix, const AdditionalData &additional_data=AdditionalData())Definition petsc_precondition.cc:684\\nPETScWrappers::PreconditionSSORDefinition petsc_precondition.h:359\\nPETScWrappers::PreconditionSSOR::initializevoid initialize(const MatrixBase &matrix, const AdditionalData &additional_data=AdditionalData())Definition petsc_precondition.cc:312\\nPETScWrappers::SolverCGDefinition petsc_solver.h:351\\nPETScWrappers::VectorBase::l2_normreal_type l2_norm() constDefinition petsc_vector_base.cc:677\\nSolverControlDefinition solver_control.h:67\\nPETScWrappers::PreconditionSSOR::AdditionalDataDefinition petsc_precondition.h:366\\n The HeatEquation::prepare_for_coarsening_and_refinement() function\\nThe next block of functions deals with mesh refinement. We split this process up into a \\\"decide whether and what you want to refine\\\" and a \\\"please transfer these vectors from old to new mesh\\\" phase, where the first also deals with marking cells for refinement. (The decision whether or not to refine is done in the lambda function that calls the current function.)\\nBreaking things into a \\\"mark cells\\\" function and into a \\\"execute mesh\\n   adaptation and transfer solution vectors\\\" function is awkward, though conceptually not difficult to understand. These two pieces of code should really be part of the same function, as they are in step-26. The issue is with what PETScWrappers::TimeStepper provides us with in these callbacks. Specifically, the \\\"decide whether and what you want to refine\\\" callback has access to the current solution, and so can evaluate (spatial) error estimators to decide which cells to refine. The second callback that transfers vectors from old to new mesh gets a bunch of vectors, but without the semantic information on which of these is the current solution vector. As a consequence, it cannot do the marking of cells for refinement or coarsening, and we have to do that from the first callback.\\nIn practice, however, the problem is minor. The first of these two functions is essentially identical to the first half of the corresponding function in step-26, with the only difference that it uses the parallel::distributed::GridRefinement namespace function instead of the serial one. Once again, we make sure that we never fall below the minimum refinement level, and above the maximum one, that we can select from the parameter file.\\n\\u00a0   template <int dim>\\n\\u00a0   void HeatEquation<dim>::prepare_for_coarsening_and_refinement(\\n\\u00a0     const PETScWrappers::MPI::Vector &y)\\n\\u00a0   {\\n\\u00a0     PETScWrappers::MPI::Vector locally_relevant_solution(locally_owned_dofs,\\n\\u00a0                                                          locally_relevant_dofs,\\n\\u00a0                                                          mpi_communicator);\\n\\u00a0     locally_relevant_solution = y;\\n\\u00a0 \\n\\u00a0     Vector<float> estimated_error_per_cell(triangulation.n_active_cells());\\n\\u00a0     KellyErrorEstimator<dim>::estimate(dof_handler,\\n\\u00a0                                        QGauss<dim - 1>(fe.degree + 1),\\n\\u00a0                                        {},\\n\\u00a0                                        locally_relevant_solution,\\n\\u00a0                                        estimated_error_per_cell);\\n\\u00a0 \\n\\u00a0     parallel::distributed::GridRefinement::refine_and_coarsen_fixed_fraction(\\n\\u00a0       triangulation, estimated_error_per_cell, 0.6, 0.4);\\n\\u00a0 \\n\\u00a0     const unsigned int max_grid_level =\\n\\u00a0       initial_global_refinement + max_delta_refinement_level;\\n\\u00a0     const unsigned int min_grid_level = initial_global_refinement;\\n\\u00a0 \\n\\u00a0     if (triangulation.n_levels() > max_grid_level)\\n\\u00a0       for (const auto &cell :\\n\\u00a0            triangulation.active_cell_iterators_on_level(max_grid_level))\\n\\u00a0         cell->clear_refine_flag();\\n\\u00a0     for (const auto &cell :\\n\\u00a0          triangulation.active_cell_iterators_on_level(min_grid_level))\\n\\u00a0       cell->clear_coarsen_flag();\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nKellyErrorEstimator::estimatestatic void estimate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Quadrature< dim - 1 > &quadrature, const std::map< types::boundary_id, const Function< spacedim, Number > * > &neumann_bc, const ReadVector< Number > &solution, Vector< float > &error, const ComponentMask &component_mask={}, const Function< spacedim > *coefficients=nullptr, const unsigned int n_threads=numbers::invalid_unsigned_int, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id, const types::material_id material_id=numbers::invalid_material_id, const Strategy strategy=cell_diameter_over_24)\\nTriangulation::n_levelsunsigned int n_levels() const\\nparallel::distributed::GridRefinement::refine_and_coarsen_fixed_fractionvoid refine_and_coarsen_fixed_fraction(::Triangulation< dim, spacedim > &tria, const ::Vector< Number > &criteria, const double top_fraction_of_error, const double bottom_fraction_of_error, const VectorTools::NormType norm_type=VectorTools::L1_norm)Definition grid_refinement.cc:576\\n The HeatEquation::transfer_solution_vectors_to_new_mesh() function\\nThe following function then is the second half of the correspond function in step-26. It is called by the time stepper whenever it requires to transfer the solution and any intermediate stage vectors to a new mesh. We must make sure that all input vectors are transformed into ghosted vectors before the actual transfer is executed, and that we distribute the hanging node constraints on the output vectors as soon as we have interpolated the vectors to the new mesh \\u2013 i.e., that all constraints are satisfied on the vectors we transfer.\\nWe have no way to enforce boundary conditions at this stage, however. This is because the various vectors may correspond to solutions at previous time steps if the method used here is a multistep time integrator, and so may correspond to different time points that we are not privy to.\\nWhile this could be a problem if we used the values of the solution and of the intermediate stages on the constrained degrees of freedom to compute the errors, we do not do so. Instead, we compute the errors on the differential equation, and ignore algebraic constraints; therefore we do no need to guarantee that the boundary conditions are satisfied also in the intermediate stages.\\nWe have at our disposal the hanging node current_constraints alone, though, and therefore we enforce them on the output vectors, even if this is not really needed.\\n\\u00a0   template <int dim>\\n\\u00a0   void HeatEquation<dim>::transfer_solution_vectors_to_new_mesh(\\n\\u00a0     const double                                   time,\\n\\u00a0     const std::vector<PETScWrappers::MPI::Vector> &all_in,\\n\\u00a0     std::vector<PETScWrappers::MPI::Vector>       &all_out)\\n\\u00a0   {\\n\\u00a0     parallel::distributed::SolutionTransfer<dim, PETScWrappers::MPI::Vector>\\n\\u00a0       solution_trans(dof_handler);\\n\\u00a0 \\n\\u00a0     std::vector<PETScWrappers::MPI::Vector> all_in_ghosted(all_in.size());\\n\\u00a0     std::vector<const PETScWrappers::MPI::Vector *> all_in_ghosted_ptr(\\n\\u00a0       all_in.size());\\n\\u00a0     std::vector<PETScWrappers::MPI::Vector *> all_out_ptr(all_in.size());\\n\\u00a0     for (unsigned int i = 0; i < all_in.size(); ++i)\\n\\u00a0       {\\n\\u00a0         all_in_ghosted[i].reinit(locally_owned_dofs,\\n\\u00a0                                  locally_relevant_dofs,\\n\\u00a0                                  mpi_communicator);\\n\\u00a0         all_in_ghosted[i]     = all_in[i];\\n\\u00a0         all_in_ghosted_ptr[i] = &all_in_ghosted[i];\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     triangulation.prepare_coarsening_and_refinement();\\n\\u00a0     solution_trans.prepare_for_coarsening_and_refinement(all_in_ghosted_ptr);\\n\\u00a0     triangulation.execute_coarsening_and_refinement();\\n\\u00a0 \\n\\u00a0     setup_system(time);\\n\\u00a0 \\n\\u00a0     all_out.resize(all_in.size());\\n\\u00a0     for (unsigned int i = 0; i < all_in.size(); ++i)\\n\\u00a0       {\\n\\u00a0         all_out[i].reinit(locally_owned_dofs, mpi_communicator);\\n\\u00a0         all_out_ptr[i] = &all_out[i];\\n\\u00a0       }\\n\\u00a0     solution_trans.interpolate(all_out_ptr);\\n\\u00a0 \\n\\u00a0     for (PETScWrappers::MPI::Vector &v : all_out)\\n\\u00a0       hanging_nodes_constraints.distribute(v);\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\nparallel::distributed::SolutionTransferDefinition solution_transfer.h:224\\nparallel::distributed::Triangulation::execute_coarsening_and_refinementvirtual void execute_coarsening_and_refinement() overrideDefinition tria.cc:3320\\nparallel::distributed::Triangulation::prepare_coarsening_and_refinementvirtual bool prepare_coarsening_and_refinement() overrideDefinition tria.cc:2805\\n The HeatEquation::update_current_constraints() function\\nSince regenerating the constraints at each time step may be expensive, we make sure that we only do so when the time changes. We track time change by checking if the time of the boundary_values_function has changed, with respect to the time of the last call to this function. This will work most of the times, but not the very first time we call this function, since the time then may be zero and the time of the boundary_values_function is zero at construction time. We therefore also check if the number constraints in current_constraints, and if these are empty, we regenerate the constraints regardless of the time variable.\\n\\u00a0   template <int dim>\\n\\u00a0   void HeatEquation<dim>::update_current_constraints(const double time)\\n\\u00a0   {\\n\\u00a0     if (current_constraints.n_constraints() == 0 ||\\n\\u00a0         time != boundary_values_function.get_time())\\n\\u00a0       {\\n\\u00a0         TimerOutput::Scope t(computing_timer, \\\"update current constraints\\\");\\n\\u00a0 \\n\\u00a0         boundary_values_function.set_time(time);\\n\\u00a0         current_constraints.clear();\\n\\u00a0         current_constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n\\u00a0         current_constraints.merge(hanging_nodes_constraints);\\n\\u00a0         VectorTools::interpolate_boundary_values(dof_handler,\\n\\u00a0                                                  0,\\n\\u00a0                                                  boundary_values_function,\\n\\u00a0                                                  current_constraints);\\n\\u00a0         current_constraints.make_consistent_in_parallel(locally_owned_dofs,\\n\\u00a0                                                         locally_relevant_dofs,\\n\\u00a0                                                         mpi_communicator);\\n\\u00a0         current_constraints.close();\\n\\u00a0       }\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n The HeatEquation::run() function\\nWe have finally arrived at the main function of the class. At the top, it creates the mesh and sets up the variables that make up the linear system:\\n\\u00a0   template <int dim>\\n\\u00a0   void HeatEquation<dim>::run()\\n\\u00a0   {\\n\\u00a0     GridGenerator::hyper_L(triangulation);\\n\\u00a0     triangulation.refine_global(initial_global_refinement);\\n\\u00a0 \\n\\u00a0     setup_system(/* time */ 0);\\n\\u00a0 \\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nGridGenerator::hyper_Lvoid hyper_L(Triangulation< dim > &tria, const double left=-1., const double right=1., const bool colorize=false)\\nWe then set up the time stepping object and associate the matrix we will build whenever requested for both the Jacobian matrix (see the definition above of what the \\\"Jacobian\\\" actually refers to) and for the matrix that will be used as a preconditioner for the Jacobian.\\n\\u00a0     PETScWrappers::TimeStepper<PETScWrappers::MPI::Vector,\\n\\u00a0                                PETScWrappers::MPI::SparseMatrix>\\n\\u00a0       petsc_ts(time_stepper_data);\\n\\u00a0 \\n\\u00a0     petsc_ts.set_matrices(jacobian_matrix, jacobian_matrix);\\n\\u00a0 \\n\\u00a0 \\nPETScWrappers::TimeStepperDefinition petsc_ts.h:339\\nThe real work setting up the time stepping object starts here. As discussed in the introduction, the way the PETScWrappers::TimeStepper class is used is by inverting control: At the end of this function, we will call PETScWrappers::TimeStepper::solve() which internally will run the loop over time steps, and at the appropriate places call back into user code for whatever functionality is required. What we need to do is to hook up these callback functions by assigning appropriate lambda functions to member variables of the petsc_ts object.\\nWe start by creating lambda functions that provide information about the \\\"implicit function\\\" (i.e., that part of the right hand side of the ODE system that we want to treat implicitly \\u2013 which in our case is the entire right hand side), a function that assembles the Jacobian matrix, and a function that solves a linear system with the Jacobian.\\n\\u00a0     petsc_ts.implicit_function = [&](const double                      time,\\n\\u00a0                                      const PETScWrappers::MPI::Vector &y,\\n\\u00a0                                      const PETScWrappers::MPI::Vector &y_dot,\\n\\u00a0                                      PETScWrappers::MPI::Vector       &res) {\\n\\u00a0       this->implicit_function(time, y, y_dot, res);\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0     petsc_ts.setup_jacobian = [&](const double                      time,\\n\\u00a0                                   const PETScWrappers::MPI::Vector &y,\\n\\u00a0                                   const PETScWrappers::MPI::Vector &y_dot,\\n\\u00a0                                   const double                      alpha) {\\n\\u00a0       this->assemble_implicit_jacobian(time, y, y_dot, alpha);\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0     petsc_ts.solve_with_jacobian = [&](const PETScWrappers::MPI::Vector &src,\\n\\u00a0                                        PETScWrappers::MPI::Vector       &dst) {\\n\\u00a0       this->solve_with_jacobian(src, dst);\\n\\u00a0     };\\n\\u00a0 \\nThe next two callbacks deal with identifying and setting variables that are considered \\\"algebraic\\\" (rather than \\\"differential\\\"), i.e., for which we know what values they are supposed to have rather than letting their values be determined by the differential equation. We need to instruct the time stepper to ignore these components when computing the error in the residuals, and we do so by first providing a function that returns an IndexSet with the indices of these algebraic components of the solution vector (or rather, that subset of the locally-owned part of the vector that is algebraic, in case we are running in parallel). This first of the following two functions does that. Specifically, both nodes at which Dirichlet boundary conditions are applied, and hanging nodes are algebraically constrained. This function then returns a set of indices that is initially empty (but knows about the size of the index space) and which we then construct as the union of boundary and hanging node indices.\\nFollowing this, we then also need a function that, given a solution vector y and the current time, sets the algebraic components of that vector to their correct value. This comes down to ensuring that we have up to date constraints in the constraints variable, and then applying these constraints to the solution vector via AffineConstraints::distribute(). (It is perhaps worth noting that we could have achieved the same in solve_with_jacobian(). Whenever the time stepper solves a linear system, it follows up the call to the solver by calling the callback to set algebraic components correct. We could also have put the calls to update_current_constraints() and distribute() into the solve_with_jacobian function, but by not doing so, we can also replace the solve_with_jacobian function with a call to a PETSc solver, and we would still have the current_constraints correctly applied to the solution vector.)\\n\\u00a0     petsc_ts.algebraic_components = [&]() {\\n\\u00a0       IndexSet algebraic_set(dof_handler.n_dofs());\\n\\u00a0       algebraic_set.add_indices(DoFTools::extract_boundary_dofs(dof_handler));\\n\\u00a0       algebraic_set.add_indices(\\n\\u00a0         DoFTools::extract_hanging_node_dofs(dof_handler));\\n\\u00a0       return algebraic_set;\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0     petsc_ts.update_constrained_components =\\n\\u00a0       [&](const double time, PETScWrappers::MPI::Vector &y) {\\n\\u00a0         TimerOutput::Scope t(computing_timer, \\\"set algebraic components\\\");\\n\\u00a0         update_current_constraints(time);\\n\\u00a0         current_constraints.distribute(y);\\n\\u00a0       };\\n\\u00a0 \\n\\u00a0 \\nDoFTools::extract_boundary_dofsIndexSet extract_boundary_dofs(const DoFHandler< dim, spacedim > &dof_handler, const ComponentMask &component_mask={}, const std::set< types::boundary_id > &boundary_ids={})Definition dof_tools.cc:614\\nDoFTools::extract_hanging_node_dofsIndexSet extract_hanging_node_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1032\\nThe next two callbacks relate to mesh refinement. As discussed in the introduction, PETScWrappers::TimeStepper knows how to deal with the situation where we want to change the mesh. All we have to provide is a callback that returns true if we are at a point where we want to refine the mesh (and false otherwise) and that if we want to do mesh refinement does some prep work for that in the form of calling the prepare_for_coarsening_and_refinement function.\\nIf the first callback below returns true, then PETSc TS will do some clean-up operations, and call the second of the callback functions (petsc_ts.transfer_solution_vectors_to_new_mesh) with a collection of vectors that need to be interpolated from the old to the new mesh. This may include the current solution, perhaps the current time derivative of the solution, and in the case of multistep time integrators also the solutions of some previous time steps. We hand all of these over to the interpolate() member function of this class.\\n\\u00a0     petsc_ts.decide_and_prepare_for_remeshing =\\n\\u00a0       [&](const double /* time */,\\n\\u00a0           const unsigned int                step_number,\\n\\u00a0           const PETScWrappers::MPI::Vector &y) -> bool {\\n\\u00a0       if (step_number > 0 && this->mesh_adaptation_frequency > 0 &&\\n\\u00a0           step_number % this->mesh_adaptation_frequency == 0)\\n\\u00a0         {\\n\\u00a0           pcout << std::endl << \\\"Adapting the mesh...\\\" << std::endl;\\n\\u00a0           this->prepare_for_coarsening_and_refinement(y);\\n\\u00a0           return true;\\n\\u00a0         }\\n\\u00a0       else\\n\\u00a0         return false;\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0     petsc_ts.transfer_solution_vectors_to_new_mesh =\\n\\u00a0       [&](const double                                   time,\\n\\u00a0           const std::vector<PETScWrappers::MPI::Vector> &all_in,\\n\\u00a0           std::vector<PETScWrappers::MPI::Vector>       &all_out) {\\n\\u00a0         this->transfer_solution_vectors_to_new_mesh(time, all_in, all_out);\\n\\u00a0       };\\n\\u00a0 \\nThe final callback is a \\\"monitor\\\" that is called in each time step. Here we use it to create a graphical output. Perhaps a better scheme would output the solution at fixed time intervals, rather than in every time step, but this is not the main point of this program and so we go with the easy approach:\\n\\u00a0     petsc_ts.monitor = [&](const double                      time,\\n\\u00a0                            const PETScWrappers::MPI::Vector &y,\\n\\u00a0                            const unsigned int                step_number) {\\n\\u00a0       pcout << \\\"Time step \\\" << step_number << \\\" at t=\\\" << time << std::endl;\\n\\u00a0       this->output_results(time, step_number, y);\\n\\u00a0     };\\n\\u00a0 \\n\\u00a0 \\nWith all of this out of the way, the rest of the function is anticlimactic: We just have to initialize the solution vector with the initial conditions and call the function that does the time stepping, and everything else will happen automatically:\\n\\u00a0     PETScWrappers::MPI::Vector solution(locally_owned_dofs, mpi_communicator);\\n\\u00a0     VectorTools::interpolate(dof_handler, initial_value_function, solution);\\n\\u00a0 \\n\\u00a0     petsc_ts.solve(solution);\\n\\u00a0   }\\n\\u00a0 } // namespace Step86\\n\\u00a0 \\n\\u00a0 \\nVectorTools::interpolatevoid interpolate(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const Function< spacedim, typename VectorType::value_type > &function, VectorType &vec, const ComponentMask &component_mask={})\\n The main() function\\nThe rest of the program is as it always looks. We read run-time parameters from an input file via the ParameterAcceptor class in the same way as we showed in step-60 and step-70.\\n\\u00a0 int main(int argc, char **argv)\\n\\u00a0 {\\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       using namespace Step86;\\n\\u00a0 \\n\\u00a0       Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n\\u00a0       HeatEquation<2>                  heat_equation_solver(MPI_COMM_WORLD);\\n\\u00a0 \\n\\u00a0       const std::string input_filename =\\n\\u00a0         (argc > 1 ? argv[1] : \\\"heat_equation.prm\\\");\\n\\u00a0       ParameterAcceptor::initialize(input_filename, \\\"heat_equation_used.prm\\\");\\n\\u00a0       heat_equation_solver.run();\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0 \\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return 0;\\n\\u00a0 }\\nParameterAcceptor::initializestatic void initialize(const std::string &filename=\\\"\\\", const std::string &output_filename=\\\"\\\", const ParameterHandler::OutputStyle output_style_for_output_filename=ParameterHandler::Short, ParameterHandler &prm=ParameterAcceptor::prm, const ParameterHandler::OutputStyle output_style_for_filename=ParameterHandler::DefaultStyle)Definition parameter_acceptor.cc:80\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\n Results\\nWhen you run this program with the input file as is, you get output as follows: Number of active cells: 768\\nNumber of degrees of freedom: 833\\n \\nTime step 0 at t=0\\n     5 linear iterations.\\n     8 linear iterations.\\nTime step 1 at t=0.025\\n     6 linear iterations.\\nTime step 2 at t=0.05\\n     5 linear iterations.\\n     8 linear iterations.\\nTime step 3 at t=0.075\\n     6 linear iterations.\\nTime step 4 at t=0.1\\n     6 linear iterations.\\nTime step 5 at t=0.125\\n     6 linear iterations.\\nTime step 6 at t=0.15\\n     6 linear iterations.\\nTime step 7 at t=0.175\\n     6 linear iterations.\\nTime step 8 at t=0.2\\n     6 linear iterations.\\nTime step 9 at t=0.225\\n     6 linear iterations.\\n \\nAdapting the mesh...\\n \\nNumber of active cells: 1050\\nNumber of degrees of freedom: 1155\\n \\nTime step 10 at t=0.25\\n     5 linear iterations.\\n     8 linear iterations.\\nTime step 11 at t=0.275\\n     5 linear iterations.\\n     7 linear iterations.\\n \\n[...]\\n \\nTime step 195 at t=4.875\\n     6 linear iterations.\\nTime step 196 at t=4.9\\n     6 linear iterations.\\nTime step 197 at t=4.925\\n     6 linear iterations.\\nTime step 198 at t=4.95\\n     6 linear iterations.\\nTime step 199 at t=4.975\\n     5 linear iterations.\\n \\nAdapting the mesh...\\n \\nNumber of active cells: 1380\\nNumber of degrees of freedom: 1547\\n \\nTime step 200 at t=5\\n \\n \\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |      43.2s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| assemble implicit Jacobian      |       226 |      9.93s |        23% |\\n| implicit function               |       426 |      16.2s |        37% |\\n| output results                  |       201 |      9.74s |        23% |\\n| set algebraic components        |       200 |    0.0496s |      0.11% |\\n| setup system                    |        21 |     0.799s |       1.8% |\\n| solve with Jacobian             |       226 |      0.56s |       1.3% |\\n| update current constraints      |       201 |      1.53s |       3.5% |\\n+---------------------------------+-----------+------------+------------+\\nWe can generate output for this in the form of a video:  \\n\\n\\n\\n The solution here is driven by boundary values (the initial conditions are zero, and so is the right hand side of the equation). It takes a little bit of time for the boundary values to diffuse into the domain, and so the temperature (the solution of the heat equation) in the interior of the domain has a slight lag compared to the temperature at the boundary.\\nThe more interesting component of this program is how easy it is to play with the details of the time stepping algorithm. Recall that the solution above is controlled by the following parameters: subsection Time stepper\\n  subsection Running parameters\\n    set final time              = 5\\n    set initial step size       = 0.025\\n    set initial time            = 0\\n    set match final time        = false\\n    set maximum number of steps = -1\\n    set options prefix          =\\n    set solver type             = beuler\\n  end\\n \\n  subsection Error control\\n    set absolute error tolerance = -1\\n    set adaptor type             = none\\n    set ignore algebraic lte     = true\\n    set maximum step size        = -1\\n    set minimum step size        = -1\\n    set relative error tolerance = -1\\n  end\\nend\\n Of particular interest for us here is to set the time stepping algorithm and the adaptive time step control method. The latter is set to \\\"none\\\" above, but there are several alternative choices for this parameter. For example, we can set parameters as follows, subsection Time stepper\\n  subsection Running parameters\\n    set final time              = 5\\n    set initial step size       = 0.025\\n    set initial time            = 0\\n    set match final time        = false\\n    set maximum number of steps = -1\\n    set options prefix          =\\n    set solver type             = bdf\\n  end\\n \\n  subsection Error control\\n    set absolute error tolerance = 1e-2\\n    set relative error tolerance = 1e-2\\n    set adaptor type             = basic\\n    set ignore algebraic lte     = true\\n    set maximum step size        = 1\\n    set minimum step size        = 0.01\\n  end\\nend\\n What we do here is set the initial time step size to 0.025, and choose relatively large absolute and relative error tolerances of 0.01 for the time step size adaptation algorithm (for which we choose \\\"basic\\\"). We ask PETSc TS to use a Backward Differentiation Formula (BDF) method, and we get the following as output: ===========================================\\nNumber of active cells: 768\\nNumber of degrees of freedom: 833\\n \\nTime step 0 at t=0\\n     5 linear iterations.\\n     5 linear iterations.\\n     5 linear iterations.\\n     4 linear iterations.\\n     6 linear iterations.\\nTime step 1 at t=0.01\\n     5 linear iterations.\\n     5 linear iterations.\\nTime step 2 at t=0.02\\n     5 linear iterations.\\n     5 linear iterations.\\nTime step 3 at t=0.03\\n     5 linear iterations.\\n     5 linear iterations.\\nTime step 4 at t=0.042574\\n     5 linear iterations.\\n     5 linear iterations.\\nTime step 5 at t=0.0588392\\n     5 linear iterations.\\n     5 linear iterations.\\nTime step 6 at t=0.0783573\\n     5 linear iterations.\\n     7 linear iterations.\\n     5 linear iterations.\\n     7 linear iterations.\\nTime step 7 at t=0.100456\\n     5 linear iterations.\\n     7 linear iterations.\\n     5 linear iterations.\\n     7 linear iterations.\\nTime step 8 at t=0.124982\\n     5 linear iterations.\\n     8 linear iterations.\\n     5 linear iterations.\\n     7 linear iterations.\\nTime step 9 at t=0.153156\\n     6 linear iterations.\\n     5 linear iterations.\\n \\n[...]\\n \\nTime step 206 at t=4.96911\\n     5 linear iterations.\\n     5 linear iterations.\\nTime step 207 at t=4.99398\\n     5 linear iterations.\\n     5 linear iterations.\\nTime step 208 at t=5.01723\\n \\n \\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |       117s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| assemble implicit Jacobian      |       593 |      35.6s |        31% |\\n| implicit function               |      1101 |      56.3s |        48% |\\n| output results                  |       209 |      12.5s |        11% |\\n| set algebraic components        |       508 |     0.156s |      0.13% |\\n| setup system                    |        21 |      1.11s |      0.95% |\\n| solve with Jacobian             |       593 |      1.97s |       1.7% |\\n| update current constraints      |       509 |      4.53s |       3.9% |\\n+---------------------------------+-----------+------------+------------+\\n What is happening here is that apparently PETSc TS is not happy with our choice of initial time step size, and after several linear solves has reduced it to the minimum we allow it to, 0.01. The following time steps then run at a time step size of 0.01 until it decides to make it slightly larger again and (apparently) switches to a higher order method that requires more linear solves per time step but allows for a larger time step closer to our initial choice 0.025 again. It does not quite hit the final time of \\\\(T=5\\\\) with its time step choices, but we've got only ourselves to blame for that by setting set match final time        = false\\n in the input file.\\nNot all combinations of methods, time step adaptation algorithms, and other parameters are valid, but the main messages from the experiment above that you should take away are:\\nIt would undoubtedly be quite time consuming to implement many of the methods that PETSc TS offers for time stepping \\u2013 but with a program such as the one here, we don't need to: We can just select from the many methods PETSc TS already has.\\nAdaptive time step control is difficult; adaptive choice of which method or order to choose is perhaps even more difficult. None of the time dependent programs that came before the current one (say, step-23, step-26, step-31, step-58, and a good number of others) have either. Moreover, while deal.II is good at spatial adaptation of meshes, it is not a library written by experts in time step adaptation, and so will likely not gain this ability either. But, again, it doesn't have to: We can rely on a library written by experts in that area.\\n\\n Possibilities for extensions\\nThe program actually runs in parallel, even though we have not used that above. Specifically, if you have configured deal.II to use MPI, then you can do mpirun -np 8 ./step-86 heat_equation.prm to run the program with 8 processes.\\nFor the program as currently written (and in debug mode), this makes little difference: It will run about twice as fast, but take about 8 times as much CPU time. That is because the problem is just so small: Generally between 1000 and 2000 degrees of freedom. A good rule of thumb is that programs really only benefit from parallel computing if you have somewhere in the range of 50,000 to 100,000 unknowns per MPI process. But it is not difficult to adapt the program at hand here to run with a much finer mesh, or perhaps in 3d, so that one is beyond that limit and sees the benefits of parallel computing.\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2000 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Authors:\\n *   Wolfgang Bangerth, Colorado State University, 2024\\n *   Stefano Zampini, King Abdullah University of Science and Technology, 2024\\n */\\n \\n \\n#include <deal.II/base/utilities.h>\\n#include <deal.II/base/quadrature_lib.h>\\n#include <deal.II/base/function.h>\\n#include <deal.II/base/logstream.h>\\n#include <deal.II/base/index_set.h>\\n#include <deal.II/base/parsed_function.h>\\n#include <deal.II/base/parameter_acceptor.h>\\n#include <deal.II/base/conditional_ostream.h>\\n#include <deal.II/base/timer.h>\\n#include <deal.II/lac/vector.h>\\n#include <deal.II/lac/full_matrix.h>\\n#include <deal.II/lac/dynamic_sparsity_pattern.h>\\n#include <deal.II/lac/petsc_vector.h>\\n#include <deal.II/lac/petsc_sparse_matrix.h>\\n#include <deal.II/lac/petsc_solver.h>\\n#include <deal.II/lac/petsc_precondition.h>\\n#include <deal.II/lac/affine_constraints.h>\\n#include <deal.II/lac/sparsity_tools.h>\\n#include <deal.II/distributed/tria.h>\\n#include <deal.II/distributed/grid_refinement.h>\\n#include <deal.II/distributed/solution_transfer.h>\\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/grid_out.h>\\n#include <deal.II/dofs/dof_handler.h>\\n#include <deal.II/dofs/dof_tools.h>\\n#include <deal.II/fe/fe_q.h>\\n#include <deal.II/fe/fe_values.h>\\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/vector_tools.h>\\n#include <deal.II/numerics/error_estimator.h>\\n#include <deal.II/numerics/solution_transfer.h>\\n#include <deal.II/numerics/matrix_tools.h>\\n \\n#include <deal.II/lac/petsc_ts.h>\\n \\n#include <fstream>\\n#include <iostream>\\n \\n \\nnamespace Step86\\n{\\n using namespace dealii;\\n \\n template <int dim>\\n class HeatEquation : public ParameterAcceptor\\n  {\\n public:\\n    HeatEquation(const MPI_Comm mpi_communicator);\\n void run();\\n \\n private:\\n const MPI_Comm mpi_communicator;\\n \\n ConditionalOStream pcout;\\n TimerOutput        computing_timer;\\n \\n parallel::distributed::Triangulation<dim> triangulation;\\n FE_Q<dim>                                 fe;\\n DoFHandler<dim>                           dof_handler;\\n \\n IndexSet locally_owned_dofs;\\n IndexSet locally_relevant_dofs;\\n \\n \\n void setup_system(const double time);\\n \\n void output_results(const double                      time,\\n const unsigned int                timestep_number,\\n const PETScWrappers::MPI::Vector &solution);\\n \\n void implicit_function(const double                      time,\\n const PETScWrappers::MPI::Vector &solution,\\n const PETScWrappers::MPI::Vector &solution_dot,\\n PETScWrappers::MPI::Vector       &residual);\\n \\n void\\n    assemble_implicit_jacobian(const double                      time,\\n const PETScWrappers::MPI::Vector &solution,\\n const PETScWrappers::MPI::Vector &solution_dot,\\n const double                      shift);\\n \\n void solve_with_jacobian(const PETScWrappers::MPI::Vector &src,\\n PETScWrappers::MPI::Vector       &residual);\\n \\n PETScWrappers::MPI::SparseMatrix jacobian_matrix;\\n \\n \\n void prepare_for_coarsening_and_refinement(\\n const PETScWrappers::MPI::Vector &solution);\\n \\n void transfer_solution_vectors_to_new_mesh(\\n const double                                   time,\\n const std::vector<PETScWrappers::MPI::Vector> &all_in,\\n      std::vector<PETScWrappers::MPI::Vector>       &all_out);\\n \\n AffineConstraints<double> hanging_nodes_constraints;\\n AffineConstraints<double> current_constraints;\\n AffineConstraints<double> homogeneous_constraints;\\n \\n void update_current_constraints(const double time);\\n \\n PETScWrappers::TimeStepperData time_stepper_data;\\n \\n unsigned int initial_global_refinement;\\n unsigned int max_delta_refinement_level;\\n unsigned int mesh_adaptation_frequency;\\n \\n ParameterAcceptorProxy<Functions::ParsedFunction<dim>>\\n      right_hand_side_function;\\n ParameterAcceptorProxy<Functions::ParsedFunction<dim>>\\n      initial_value_function;\\n ParameterAcceptorProxy<Functions::ParsedFunction<dim>>\\n      boundary_values_function;\\n  };\\n \\n \\n template <int dim>\\n  HeatEquation<dim>::HeatEquation(const MPI_Comm mpi_communicator)\\n    : ParameterAcceptor(\\\"/Heat Equation/\\\")\\n    , mpi_communicator(mpi_communicator)\\n    , pcout(std::cout,\\n            (Utilities::MPI::this_mpi_process(mpi_communicator) == 0))\\n    , computing_timer(mpi_communicator,\\n                      pcout,\\n TimerOutput::summary,\\n TimerOutput::wall_times)\\n    , triangulation(mpi_communicator,\\n                    typename Triangulation<dim>::MeshSmoothing(\\n Triangulation<dim>::smoothing_on_refinement |\\n Triangulation<dim>::smoothing_on_coarsening))\\n    , fe(1)\\n    , dof_handler(triangulation)\\n    , time_stepper_data(\\\"\\\",\\n \\\"beuler\\\",\\n /* start time */ 0.0,\\n /* end time */ 1.0,\\n /* initial time step */ 0.025)\\n    , initial_global_refinement(5)\\n    , max_delta_refinement_level(2)\\n    , mesh_adaptation_frequency(0)\\n    , right_hand_side_function(\\\"/Heat Equation/Right hand side\\\")\\n    , initial_value_function(\\\"/Heat Equation/Initial value\\\")\\n    , boundary_values_function(\\\"/Heat Equation/Boundary values\\\")\\n  {\\n    enter_subsection(\\\"Time stepper\\\");\\n    {\\n      enter_my_subsection(this->prm);\\n      {\\n        time_stepper_data.add_parameters(this->prm);\\n      }\\n      leave_my_subsection(this->prm);\\n    }\\n    leave_subsection();\\n \\n    add_parameter(\\\"Initial global refinement\\\",\\n                  initial_global_refinement,\\n \\\"Number of times the mesh is refined globally before \\\"\\n \\\"starting the time stepping.\\\");\\n    add_parameter(\\\"Maximum delta refinement level\\\",\\n                  max_delta_refinement_level,\\n \\\"Maximum number of local refinement levels.\\\");\\n    add_parameter(\\\"Mesh adaptation frequency\\\",\\n                  mesh_adaptation_frequency,\\n \\\"When to adapt the mesh.\\\");\\n  }\\n \\n \\n \\n template <int dim>\\n void HeatEquation<dim>::setup_system(const double time)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"setup system\\\");\\n \\n    dof_handler.distribute_dofs(fe);\\n    pcout << std::endl\\n          << \\\"Number of active cells: \\\" << triangulation.n_active_cells()\\n          << std::endl\\n          << \\\"Number of degrees of freedom: \\\" << dof_handler.n_dofs()\\n          << std::endl\\n          << std::endl;\\n \\n    locally_owned_dofs = dof_handler.locally_owned_dofs();\\n    locally_relevant_dofs =\\n DoFTools::extract_locally_relevant_dofs(dof_handler);\\n \\n \\n    hanging_nodes_constraints.clear();\\n    hanging_nodes_constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n DoFTools::make_hanging_node_constraints(dof_handler,\\n                                            hanging_nodes_constraints);\\n    hanging_nodes_constraints.make_consistent_in_parallel(locally_owned_dofs,\\n                                                          locally_relevant_dofs,\\n                                                          mpi_communicator);\\n    hanging_nodes_constraints.close();\\n \\n \\n    homogeneous_constraints.clear();\\n    homogeneous_constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n    homogeneous_constraints.merge(hanging_nodes_constraints);\\n VectorTools::interpolate_boundary_values(dof_handler,\\n                                             0,\\n Functions::ZeroFunction<dim>(),\\n                                             homogeneous_constraints);\\n    homogeneous_constraints.make_consistent_in_parallel(locally_owned_dofs,\\n                                                        locally_relevant_dofs,\\n                                                        mpi_communicator);\\n    homogeneous_constraints.close();\\n \\n \\n    update_current_constraints(time);\\n \\n \\n DynamicSparsityPattern dsp(locally_relevant_dofs);\\n DoFTools::make_sparsity_pattern(dof_handler,\\n                                    dsp,\\n                                    homogeneous_constraints,\\n false);\\n SparsityTools::distribute_sparsity_pattern(dsp,\\n                                               locally_owned_dofs,\\n                                               mpi_communicator,\\n                                               locally_relevant_dofs);\\n \\n    jacobian_matrix.reinit(locally_owned_dofs,\\n                           locally_owned_dofs,\\n                           dsp,\\n                           mpi_communicator);\\n  }\\n \\n \\n template <int dim>\\n void HeatEquation<dim>::output_results(const double       time,\\n const unsigned int timestep_number,\\n const PETScWrappers::MPI::Vector &y)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"output results\\\");\\n \\n DataOut<dim> data_out;\\n    data_out.attach_dof_handler(dof_handler);\\n    data_out.add_data_vector(y, \\\"U\\\");\\n    data_out.build_patches();\\n \\n    data_out.set_flags(DataOutBase::VtkFlags(time, timestep_number));\\n \\n const std::string filename =\\n \\\"solution-\\\" + Utilities::int_to_string(timestep_number, 3) + \\\".vtu\\\";\\n    data_out.write_vtu_in_parallel(filename, mpi_communicator);\\n \\n if (Utilities::MPI::this_mpi_process(mpi_communicator) == 0)\\n      {\\n static std::vector<std::pair<double, std::string>> times_and_names;\\n        times_and_names.emplace_back(time, filename);\\n \\n        std::ofstream pvd_output(\\\"solution.pvd\\\");\\n DataOutBase::write_pvd_record(pvd_output, times_and_names);\\n      }\\n  }\\n \\n \\n \\n template <int dim>\\n void\\n  HeatEquation<dim>::implicit_function(const double                      time,\\n const PETScWrappers::MPI::Vector &y,\\n const PETScWrappers::MPI::Vector &y_dot,\\n PETScWrappers::MPI::Vector &residual)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"implicit function\\\");\\n \\n PETScWrappers::MPI::Vector tmp_solution(locally_owned_dofs,\\n                                            mpi_communicator);\\n PETScWrappers::MPI::Vector tmp_solution_dot(locally_owned_dofs,\\n                                                mpi_communicator);\\n    tmp_solution     = y;\\n    tmp_solution_dot = y_dot;\\n \\n    update_current_constraints(time);\\n    current_constraints.distribute(tmp_solution);\\n    homogeneous_constraints.distribute(tmp_solution_dot);\\n \\n PETScWrappers::MPI::Vector locally_relevant_solution(locally_owned_dofs,\\n                                                         locally_relevant_dofs,\\n                                                         mpi_communicator);\\n PETScWrappers::MPI::Vector locally_relevant_solution_dot(\\n      locally_owned_dofs, locally_relevant_dofs, mpi_communicator);\\n    locally_relevant_solution     = tmp_solution;\\n    locally_relevant_solution_dot = tmp_solution_dot;\\n \\n \\n const QGauss<dim> quadrature_formula(fe.degree + 1);\\n FEValues<dim>     fe_values(fe,\\n                            quadrature_formula,\\n update_values | update_gradients |\\n update_quadrature_points | update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n \\n    std::vector<Tensor<1, dim>> solution_gradients(n_q_points);\\n    std::vector<double>         solution_dot_values(n_q_points);\\n \\n Vector<double> cell_residual(dofs_per_cell);\\n \\n    right_hand_side_function.set_time(time);\\n \\n    residual = 0;\\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      if (cell->is_locally_owned())\\n        {\\n          fe_values.reinit(cell);\\n \\n          fe_values.get_function_gradients(locally_relevant_solution,\\n                                           solution_gradients);\\n          fe_values.get_function_values(locally_relevant_solution_dot,\\n                                        solution_dot_values);\\n \\n          cell->get_dof_indices(local_dof_indices);\\n \\n cell_residual = 0;\\n for (const unsigned int q : fe_values.quadrature_point_indices())\\n            for (const unsigned int i : fe_values.dof_indices())\\n              {\\n cell_residual(i) +=\\n                  (fe_values.shape_value(i, q) *       // [phi_i(x_q) *\\n                     solution_dot_values[q]            //  u(x_q)\\n                   +                                   //  +\\n                   fe_values.shape_grad(i, q) *        //  grad phi_i(x_q) *\\n                     solution_gradients[q]             //  grad u(x_q)\\n                   -                                   //  -\\n                   fe_values.shape_value(i, q) *       //  phi_i(x_q) *\\n                     right_hand_side_function.value(   \\n                       fe_values.quadrature_point(q))) //  f(x_q)]\\n                  * fe_values.JxW(q);                  // * dx\\n              }\\n          current_constraints.distribute_local_to_global(cell_residual,\\n                                                         local_dof_indices,\\n                                                         residual);\\n        }\\n    residual.compress(VectorOperation::add);\\n \\n for (const auto &c : current_constraints.get_lines())\\n      if (locally_owned_dofs.is_element(c.index))\\n        {\\n if (c.entries.empty()) /* no dependencies -> a Dirichlet node */\\n            residual[c.index] = y[c.index] - tmp_solution[c.index];\\n else /* has dependencies -> a hanging node */\\n            residual[c.index] = y[c.index];\\n        }\\n    residual.compress(VectorOperation::insert);\\n  }\\n \\n \\n template <int dim>\\n void HeatEquation<dim>::assemble_implicit_jacobian(\\n const double /* time */,\\n const PETScWrappers::MPI::Vector & /* y */,\\n const PETScWrappers::MPI::Vector & /* y_dot */,\\n const double alpha)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"assemble implicit Jacobian\\\");\\n \\n const QGauss<dim> quadrature_formula(fe.degree + 1);\\n FEValues<dim>     fe_values(fe,\\n                            quadrature_formula,\\n update_values | update_gradients |\\n update_quadrature_points | update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n \\n    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n \\n FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n \\n    jacobian_matrix = 0;\\n for (const auto &cell : dof_handler.active_cell_iterators())\\n      if (cell->is_locally_owned())\\n        {\\n          fe_values.reinit(cell);\\n \\n          cell->get_dof_indices(local_dof_indices);\\n \\n cell_matrix = 0;\\n for (const unsigned int q : fe_values.quadrature_point_indices())\\n            for (const unsigned int i : fe_values.dof_indices())\\n              for (const unsigned int j : fe_values.dof_indices())\\n                {\\n cell_matrix(i, j) +=\\n                    (fe_values.shape_grad(i, q) *      // grad phi_i(x_q) *\\n                       fe_values.shape_grad(j, q)      // grad phi_j(x_q)\\n                     + alpha *                         \\n                         fe_values.shape_value(i, q) * // phi_i(x_q) *\\n                         fe_values.shape_value(j, q)   // phi_j(x_q)\\n                     ) *\\n                    fe_values.JxW(q); // * dx\\n                }\\n          current_constraints.distribute_local_to_global(cell_matrix,\\n                                                         local_dof_indices,\\n                                                         jacobian_matrix);\\n        }\\n    jacobian_matrix.compress(VectorOperation::add);\\n \\n for (const auto &c : current_constraints.get_lines())\\n      jacobian_matrix.set(c.index, c.index, 1.0);\\n    jacobian_matrix.compress(VectorOperation::insert);\\n  }\\n \\n \\n template <int dim>\\n void\\n  HeatEquation<dim>::solve_with_jacobian(const PETScWrappers::MPI::Vector &src,\\n PETScWrappers::MPI::Vector       &dst)\\n  {\\n TimerOutput::Scope t(computing_timer, \\\"solve with Jacobian\\\");\\n \\n#if defined(PETSC_HAVE_HYPRE)\\n PETScWrappers::PreconditionBoomerAMG preconditioner;\\n    preconditioner.initialize(jacobian_matrix);\\n#else\\n PETScWrappers::PreconditionSSOR preconditioner;\\n    preconditioner.initialize(\\n      jacobian_matrix, PETScWrappers::PreconditionSSOR::AdditionalData(1.0));\\n#endif\\n \\n SolverControl           solver_control(1000, 1e-8 * src.l2_norm());\\n PETScWrappers::SolverCG cg(solver_control);\\n    cg.set_prefix(\\\"user_\\\");\\n \\n    cg.solve(jacobian_matrix, dst, src, preconditioner);\\n \\n    pcout << \\\"     \\\" << solver_control.last_step() << \\\" linear iterations.\\\"\\n          << std::endl;\\n  }\\n \\n \\n template <int dim>\\n void HeatEquation<dim>::prepare_for_coarsening_and_refinement(\\n const PETScWrappers::MPI::Vector &y)\\n  {\\n PETScWrappers::MPI::Vector locally_relevant_solution(locally_owned_dofs,\\n                                                         locally_relevant_dofs,\\n                                                         mpi_communicator);\\n    locally_relevant_solution = y;\\n \\n Vector<float> estimated_error_per_cell(triangulation.n_active_cells());\\n KellyErrorEstimator<dim>::estimate(dof_handler,\\n QGauss<dim - 1>(fe.degree + 1),\\n                                       {},\\n                                       locally_relevant_solution,\\n                                       estimated_error_per_cell);\\n \\n parallel::distributed::GridRefinement::refine_and_coarsen_fixed_fraction(\\n triangulation, estimated_error_per_cell, 0.6, 0.4);\\n \\n const unsigned int max_grid_level =\\n      initial_global_refinement + max_delta_refinement_level;\\n const unsigned int min_grid_level = initial_global_refinement;\\n \\n if (triangulation.n_levels() > max_grid_level)\\n for (const auto &cell :\\n triangulation.active_cell_iterators_on_level(max_grid_level))\\n        cell->clear_refine_flag();\\n for (const auto &cell :\\n triangulation.active_cell_iterators_on_level(min_grid_level))\\n      cell->clear_coarsen_flag();\\n  }\\n \\n \\n template <int dim>\\n void HeatEquation<dim>::transfer_solution_vectors_to_new_mesh(\\n const double                                   time,\\n const std::vector<PETScWrappers::MPI::Vector> &all_in,\\n    std::vector<PETScWrappers::MPI::Vector>       &all_out)\\n  {\\n parallel::distributed::SolutionTransfer<dim, PETScWrappers::MPI::Vector>\\n      solution_trans(dof_handler);\\n \\n    std::vector<PETScWrappers::MPI::Vector> all_in_ghosted(all_in.size());\\n    std::vector<const PETScWrappers::MPI::Vector *> all_in_ghosted_ptr(\\n      all_in.size());\\n    std::vector<PETScWrappers::MPI::Vector *> all_out_ptr(all_in.size());\\n for (unsigned int i = 0; i < all_in.size(); ++i)\\n      {\\n        all_in_ghosted[i].reinit(locally_owned_dofs,\\n                                 locally_relevant_dofs,\\n                                 mpi_communicator);\\n        all_in_ghosted[i]     = all_in[i];\\n        all_in_ghosted_ptr[i] = &all_in_ghosted[i];\\n      }\\n \\n triangulation.prepare_coarsening_and_refinement();\\n    solution_trans.prepare_for_coarsening_and_refinement(all_in_ghosted_ptr);\\n triangulation.execute_coarsening_and_refinement();\\n \\n    setup_system(time);\\n \\n    all_out.resize(all_in.size());\\n for (unsigned int i = 0; i < all_in.size(); ++i)\\n      {\\n        all_out[i].reinit(locally_owned_dofs, mpi_communicator);\\n        all_out_ptr[i] = &all_out[i];\\n      }\\n    solution_trans.interpolate(all_out_ptr);\\n \\n for (PETScWrappers::MPI::Vector &v : all_out)\\n      hanging_nodes_constraints.distribute(v);\\n  }\\n \\n \\n template <int dim>\\n void HeatEquation<dim>::update_current_constraints(const double time)\\n  {\\n if (current_constraints.n_constraints() == 0 ||\\n        time != boundary_values_function.get_time())\\n      {\\n TimerOutput::Scope t(computing_timer, \\\"update current constraints\\\");\\n \\n        boundary_values_function.set_time(time);\\n        current_constraints.clear();\\n        current_constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n        current_constraints.merge(hanging_nodes_constraints);\\n VectorTools::interpolate_boundary_values(dof_handler,\\n                                                 0,\\n                                                 boundary_values_function,\\n                                                 current_constraints);\\n        current_constraints.make_consistent_in_parallel(locally_owned_dofs,\\n                                                        locally_relevant_dofs,\\n                                                        mpi_communicator);\\n        current_constraints.close();\\n      }\\n  }\\n \\n \\n template <int dim>\\n void HeatEquation<dim>::run()\\n  {\\n GridGenerator::hyper_L(triangulation);\\n triangulation.refine_global(initial_global_refinement);\\n \\n    setup_system(/* time */ 0);\\n \\n PETScWrappers::TimeStepper<PETScWrappers::MPI::Vector,\\n PETScWrappers::MPI::SparseMatrix>\\n      petsc_ts(time_stepper_data);\\n \\n    petsc_ts.set_matrices(jacobian_matrix, jacobian_matrix);\\n \\n \\n    petsc_ts.implicit_function = [&](const double                      time,\\n const PETScWrappers::MPI::Vector &y,\\n const PETScWrappers::MPI::Vector &y_dot,\\n PETScWrappers::MPI::Vector       &res) {\\n      this->implicit_function(time, y, y_dot, res);\\n    };\\n \\n    petsc_ts.setup_jacobian = [&](const double                      time,\\n const PETScWrappers::MPI::Vector &y,\\n const PETScWrappers::MPI::Vector &y_dot,\\n const double                      alpha) {\\n      this->assemble_implicit_jacobian(time, y, y_dot, alpha);\\n    };\\n \\n    petsc_ts.solve_with_jacobian = [&](const PETScWrappers::MPI::Vector &src,\\n PETScWrappers::MPI::Vector       &dst) {\\n      this->solve_with_jacobian(src, dst);\\n    };\\n \\n    petsc_ts.algebraic_components = [&]() {\\n IndexSet algebraic_set(dof_handler.n_dofs());\\n      algebraic_set.add_indices(DoFTools::extract_boundary_dofs(dof_handler));\\n      algebraic_set.add_indices(\\n DoFTools::extract_hanging_node_dofs(dof_handler));\\n return algebraic_set;\\n    };\\n \\n    petsc_ts.update_constrained_components =\\n      [&](const double time, PETScWrappers::MPI::Vector &y) {\\n TimerOutput::Scope t(computing_timer, \\\"set algebraic components\\\");\\n        update_current_constraints(time);\\n        current_constraints.distribute(y);\\n      };\\n \\n \\n    petsc_ts.decide_and_prepare_for_remeshing =\\n      [&](const double /* time */,\\n const unsigned int                step_number,\\n const PETScWrappers::MPI::Vector &y) -> bool {\\n if (step_number > 0 && this->mesh_adaptation_frequency > 0 &&\\n          step_number % this->mesh_adaptation_frequency == 0)\\n        {\\n          pcout << std::endl << \\\"Adapting the mesh...\\\" << std::endl;\\n          this->prepare_for_coarsening_and_refinement(y);\\n return true;\\n        }\\n else\\n return false;\\n    };\\n \\n    petsc_ts.transfer_solution_vectors_to_new_mesh =\\n      [&](const double                                   time,\\n const std::vector<PETScWrappers::MPI::Vector> &all_in,\\n          std::vector<PETScWrappers::MPI::Vector>       &all_out) {\\n        this->transfer_solution_vectors_to_new_mesh(time, all_in, all_out);\\n      };\\n \\n    petsc_ts.monitor = [&](const double                      time,\\n const PETScWrappers::MPI::Vector &y,\\n const unsigned int                step_number) {\\n      pcout << \\\"Time step \\\" << step_number << \\\" at t=\\\" << time << std::endl;\\n      this->output_results(time, step_number, y);\\n    };\\n \\n \\n PETScWrappers::MPI::Vector solution(locally_owned_dofs, mpi_communicator);\\n VectorTools::interpolate(dof_handler, initial_value_function, solution);\\n \\n    petsc_ts.solve(solution);\\n  }\\n} // namespace Step86\\n \\n \\nint main(int argc, char **argv)\\n{\\n try\\n    {\\n using namespace Step86;\\n \\n Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n      HeatEquation<2>                  heat_equation_solver(MPI_COMM_WORLD);\\n \\n const std::string input_filename =\\n        (argc > 1 ? argv[1] : \\\"heat_equation.prm\\\");\\n ParameterAcceptor::initialize(input_filename, \\\"heat_equation_used.prm\\\");\\n      heat_equation_solver.run();\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n \\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n return 1;\\n    }\\n \\n return 0;\\n}\\naffine_constraints.h\\nDataOutInterface::write_vtu_in_parallelvoid write_vtu_in_parallel(const std::string &filename, const MPI_Comm comm) constDefinition data_out_base.cc:7715\\nDataOutInterface::set_flagsvoid set_flags(const FlagType &flags)Definition data_out_base.cc:8863\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\nPETScWrappers::MPI::Vector::reinitvoid reinit(const MPI_Comm communicator, const size_type N, const size_type locally_owned_size, const bool omit_zeroing_entries=false)Definition petsc_parallel_vector.cc:155\\nPETScWrappers::VectorBase::compressvoid compress(const VectorOperation::values operation)Definition petsc_vector_base.cc:540\\nconditional_ostream.h\\ngrid_refinement.h\\nsolution_transfer.h\\ntria.h\\ndof_handler.h\\ndof_tools.h\\ndynamic_sparsity_pattern.h\\nerror_estimator.h\\nfe_values.h\\nfe_q.h\\nfull_matrix.h\\nfunction.h\\ngrid_generator.h\\ngrid_out.h\\nLinearAlgebra::CUDAWrappers::kernel::set__global__ void set(Number *val, const Number s, const size_type N)\\nutilities.h\\nindex_set.h\\nlogstream.h\\nmatrix_tools.h\\nGridGenerator::implicit_functionvoid implicit_function(Triangulation< dim, 3 > &tria, const Function< 3 > &implicit_function, const CGALWrappers::AdditionalData< dim > &data=CGALWrappers::AdditionalData< dim >{}, const Point< 3 > &interior_point=Point< 3 >(), const double &outer_ball_radius=1.0)\\nLocalIntegrators::Advection::cell_matrixvoid cell_matrix(FullMatrix< double > &M, const FEValuesBase< dim > &fe, const FEValuesBase< dim > &fetest, const ArrayView< const std::vector< double > > &velocity, const double factor=1.)Definition advection.h:74\\nLocalIntegrators::Advection::cell_residualvoid cell_residual(Vector< double > &result, const FEValuesBase< dim > &fe, const std::vector< Tensor< 1, dim > > &input, const ArrayView< const std::vector< double > > &velocity, double factor=1.)Definition advection.h:130\\ninternal::VectorizationTypes::index@ index\\ndata_out.h\\nsolution_transfer.h\\nparameter_acceptor.h\\nparsed_function.h\\npetsc_precondition.h\\npetsc_solver.h\\npetsc_sparse_matrix.h\\npetsc_ts.h\\npetsc_vector.h\\nquadrature_lib.h\\nsparsity_tools.h\\ntimer.h\\nvector.h\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"