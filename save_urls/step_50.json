"[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://dealii.org/current/doxygen/deal.II/step_50.html\", \"content_type\": \"text/html\", \"title\": \"The deal.II Library: The step-50 tutorial program\", \"language\": \"en-US\"}, \"page_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\nThe deal.II Library: The step-50 tutorial program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\u00a0Reference documentation for deal.II version 9.6.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\(\\\\newcommand{\\\\dealvcentcolon}{\\\\mathrel{\\\\mathop{:}}}\\\\)\\n\\\\(\\\\newcommand{\\\\dealcoloneq}{\\\\dealvcentcolon\\\\mathrel{\\\\mkern-1.2mu}=}\\\\)\\n\\\\(\\\\newcommand{\\\\jump}[1]{\\\\left[\\\\!\\\\left[ #1 \\\\right]\\\\!\\\\right]}\\\\)\\n\\\\(\\\\newcommand{\\\\average}[1]{\\\\left\\\\{\\\\!\\\\left\\\\{ #1 \\\\right\\\\}\\\\!\\\\right\\\\}}\\\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoading...\\nSearching...\\nNo Matches\\n\\n\\n\\n\\n\\n\\n\\nThe step-50 tutorial program\\n\\n\\nThis tutorial depends on step-37.\\n\\n\\nTable of contents\\n\\n\\n Introduction\\n\\nThe testcase\\nWorkload imbalance for geometric multigrid methods\\nWorkload imbalance for algebraic multigrid methods\\nRunning the program\\n\\n The commented program\\n\\nInclude files\\nCoefficients and helper classes\\nRun time parameters\\nLaplaceProblem class\\n\\nLaplaceProblem::setup_system()\\nLaplaceProblem::setup_multigrid()\\nLaplaceProblem::assemble_system()\\nLaplaceProblem::assemble_multigrid()\\nLaplaceProblem::assemble_rhs()\\nLaplaceProblem::solve()\\n\\nThe error estimator\\n\\nLaplaceProblem::refine_grid()\\nLaplaceProblem::output_results()\\nLaplaceProblem::run()\\n\\nThe main() function\\n\\n\\n Results\\n\\n Possibilities for extensions \\n\\n Testing convergence and higher order elements \\n Coarse solver \\n\\n\\n The plain program\\n   \\n\\n\\n This program was contributed by Thomas C. Clevenger and Timo Heister. \\n This material is based upon work partly supported by the National Science Foundation Award DMS-2028346, OAC-2015848, EAR-1925575, by the Computational Infrastructure in Geodynamics initiative (CIG), through the NSF under Award EAR-0949446 and EAR-1550901 and The University of California \\u2013 Davis. \\nNoteIf you use this program as a basis for your own work, please consider citing it in your list of references. The initial version of this work was contributed to the deal.II project by the authors listed in the following citation:   \\n\\nAs a prerequisite of this program, you need to have both p4est and either the PETSc or Trilinos library installed. The installation of deal.II together with these additional libraries is described in the README file.\\n Introduction\\nThis example shows the usage of the multilevel functions in deal.II on parallel, distributed meshes and gives a comparison between geometric and algebraic multigrid methods. The algebraic multigrid (AMG) preconditioner is the same used in step-40. Two geometric multigrid (GMG) preconditioners are considered: a matrix-based version similar to that in step-16 (but for parallel computations) and a matrix-free version discussed in step-37. The goal is to find out which approach leads to the best solver for large parallel computations.\\nThis tutorial is based on one of the numerical examples in [61]. Please see that publication for a detailed background on the multigrid implementation in deal.II. We will summarize some of the results in the following text.\\nAlgebraic multigrid methods are obviously the easiest to implement with deal.II since classes such as TrilinosWrappers::PreconditionAMG and PETScWrappers::PreconditionBoomerAMG are, in essence, black box preconditioners that require only a couple of lines to set up even for parallel computations. On the other hand, geometric multigrid methods require changes throughout a code base \\u2013 not very many, but one has to know what one is doing.\\nWhat the results of this program will show is that algebraic and geometric multigrid methods are roughly comparable in performance when using matrix-based formulations, and that matrix-free geometric multigrid methods are vastly better for the problem under consideration here. A secondary conclusion will be that matrix-based geometric multigrid methods really don't scale well strongly when the number of unknowns per processor becomes smaller than 20,000 or so.\\nThe testcase\\nWe consider the variable-coefficient Laplacian weak formulation   \\n\\\\begin{align*}\\n (\\\\epsilon \\\\nabla u, \\\\nabla v) = (f,v) \\\\quad \\\\forall v \\\\in V_h\\n\\\\end{align*}\\n\\n on the domain \\\\(\\\\Omega = [-1,1]^\\\\text{dim} \\\\setminus [0,1]^\\\\text{dim}\\\\) (an L-shaped domain for 2D and a Fichera corner for 3D) with \\\\(\\\\epsilon = 1\\\\) if \\\\(\\\\min(x,y,z)>-\\\\frac{1}{2}\\\\) and \\\\(\\\\epsilon = 100\\\\) otherwise. In other words, \\\\(\\\\epsilon\\\\) is small along the edges or faces of the domain that run into the reentrant corner, as will be visible in the figure below.\\nThe boundary conditions are \\\\(u=0\\\\) on the whole boundary and the right-hand side is \\\\(f=1\\\\). We use continuous \\\\(Q_2\\\\) elements for the discrete finite element space \\\\(V_h\\\\), and use a residual-based, cell-wise a posteriori error estimator \\\\(e(K) = e_{\\\\text{cell}}(K) + e_{\\\\text{face}}(K)\\\\) from [126] with    \\n\\\\begin{align*}\\n e_{\\\\text{cell}}(K) &= h^2 \\\\| f + \\\\epsilon \\\\triangle u \\\\|_K^2, \\\\\\\\\\n e_{\\\\text{face}}(K) &= \\\\sum_F h_F \\\\| \\\\jump{ \\\\epsilon \\\\nabla u \\\\cdot n } \\\\|_F^2,\\n\\\\end{align*}\\n\\n to adaptively refine the mesh. (This is a generalization of the Kelly error estimator used in the KellyErrorEstimator class that drives mesh refinement in most of the other tutorial programs.) The following figure visualizes the solution and refinement for 2D:  In 3D, the solution looks similar (see below). On the left you can see the solution and on the right we show a slice for \\\\(x\\\\) close to the center of the domain showing the adaptively refined mesh. \\n\\n  \\n\\nBoth in 2D and 3D you can see the adaptive refinement picking up the corner singularity and the inner singularity where the viscosity jumps, while the interface along the line that separates the two viscosities is (correctly) not refined as it is resolved adequately. This is because the kink in the solution that results from the jump in the coefficient is aligned with cell interfaces.\\nWorkload imbalance for geometric multigrid methods\\nAs mentioned above, the purpose of this program is to demonstrate the use of algebraic and geometric multigrid methods for this problem, and to do so for parallel computations. An important component of making algorithms scale to large parallel machines is ensuring that every processor has the same amount of work to do. (More precisely, what matters is that there are no small fraction of processors that have substantially more work than the rest since, if that were so, a large fraction of processors will sit idle waiting for the small fraction to finish. Conversely, a small fraction of processors having substantially less work is not a problem because the majority of processors continues to be productive and only the small fraction sits idle once finished with their work.)\\nFor the active mesh, we use the parallel::distributed::Triangulation class as done in step-40 which uses functionality in the external library p4est for the distribution of the active cells among processors. For the non-active cells in the multilevel hierarchy, deal.II implements what we will refer to as the \\\"first-child rule\\\" where, for each cell in the hierarchy, we recursively assign the parent of a cell to the owner of the first child cell. The following figures give an example of such a distribution. Here the left image represents the active cells for a sample 2D mesh partitioned using a space-filling curve (which is what p4est uses to partition cells); the center image gives the tree representation of the active mesh; and the right image gives the multilevel hierarchy of cells. The colors and numbers represent the different processors. The circular nodes in the tree are the non-active cells which are distributed using the \\\"first-child rule\\\".\\n\\nIncluded among the output to screen in this example is a value \\\"Partition efficiency\\\" given by one over MGTools::workload_imbalance(). This value, which will be denoted by \\\\(\\\\mathbb{E}\\\\), quantifies the overhead produced by not having a perfect work balance on each level of the multigrid hierarchy. This imbalance is evident from the example above: while level \\\\(\\\\ell=2\\\\) is about as well balanced as is possible with four cells among three processors, the coarse level \\\\(\\\\ell=0\\\\) has work for only one processor, and level \\\\(\\\\ell=1\\\\) has work for only two processors of which one has three times as much work as the other.\\nFor defining \\\\(\\\\mathbb{E}\\\\), it is important to note that, as we are using local smoothing to define the multigrid hierarchy (see the multigrid paper for a description of local smoothing), the refinement level of a cell corresponds to that cell's multigrid level. Now, let \\\\(N_{\\\\ell}\\\\) be the number of cells on level \\\\(\\\\ell\\\\) (both active and non-active cells) and \\\\(N_{\\\\ell,p}\\\\) be the subset owned by process \\\\(p\\\\). We will also denote by \\\\(P\\\\) the total number of processors. Assuming that the workload for any one processor is proportional to the number of cells owned by that processor, the optimal workload per processor is given by   \\n\\\\begin{align*}\\nW_{\\\\text{opt}} = \\\\frac1{P}\\\\sum_{\\\\ell} N_{\\\\ell} = \\\\sum_{\\\\ell}\\\\left(\\\\frac1{P}\\\\sum_{p}N_{\\\\ell,p}\\\\right).\\n\\\\end{align*}\\n\\n Next, assuming a synchronization of work on each level (i.e., on each level of a V-cycle, work must be completed by all processors before moving on to the next level), the limiting effort on each level is given by   \\n\\\\begin{align*}\\nW_\\\\ell = \\\\max_{p} N_{\\\\ell,p},\\n\\\\end{align*}\\n\\n and the total parallel complexity   \\n\\\\begin{align*}\\nW = \\\\sum_{\\\\ell} W_\\\\ell.\\n\\\\end{align*}\\n\\n Then we define \\\\(\\\\mathbb{E}\\\\) as a ratio of the optimal partition to the parallel complexity of the current partition   \\n\\\\begin{align*}\\n  \\\\mathbb{E} = \\\\frac{W_{\\\\text{opt}}}{W}.\\n\\\\end{align*}\\n\\n For the example distribution above, we have       \\n\\\\begin{align*}\\nW_{\\\\text{opt}}&=\\\\frac{1}{P}\\\\sum_{\\\\ell} N_{\\\\ell} = \\\\frac{1}{3} \\\\left(1+4+4\\\\right)= 3 \\\\qquad\\n\\\\\\\\\\nW &= \\\\sum_\\\\ell W_\\\\ell = 1 + 2 + 3 = 6\\n\\\\\\\\\\n\\\\mathbb{E} &= \\\\frac{W_{\\\\text{opt}}}{W} = \\\\frac12.\\n\\\\end{align*}\\n\\n The value MGTools::workload_imbalance() \\\\(= 1/\\\\mathbb{E}\\\\) then represents the factor increase in timings we expect for GMG methods (vmults, assembly, etc.) due to the imbalance of the mesh partition compared to a perfectly load-balanced workload. We will report on these in the results section below for a sequence of meshes, and compare with the observed slow-downs as we go to larger and larger processor numbers (where, typically, the load imbalance becomes larger as well).\\nThese sorts of considerations are considered in much greater detail in [61], which contains a full discussion of the partition efficiency model and the effect the imbalance has on the GMG V-cycle timing. In summary, the value of \\\\(\\\\mathbb{E}\\\\) is highly dependent on the degree of local mesh refinement used and has an optimal value \\\\(\\\\mathbb{E} \\\\approx 1\\\\) for globally refined meshes. Typically for adaptively refined meshes, the number of processors used to distribute a single mesh has a negative impact on \\\\(\\\\mathbb{E}\\\\) but only up to a leveling off point, where the imbalance remains relatively constant for an increasing number of processors, and further refinement has very little impact on \\\\(\\\\mathbb{E}\\\\). Finally, \\\\(1/\\\\mathbb{E}\\\\) was shown to give an accurate representation of the slowdown in parallel scaling expected for the timing of a V-cycle.\\nIt should be noted that there is potential for some asynchronous work between multigrid levels, specifically with purely nearest neighbor MPI communication, and an adaptive mesh could be constructed such that the efficiency model would far overestimate the V-cycle slowdown due to the asynchronous work \\\"covering up\\\" the imbalance (which assumes synchronization over levels). However, for most realistic adaptive meshes the expectation is that this asynchronous work will only cover up a very small portion of the imbalance and the efficiency model will describe the slowdown very well.\\nWorkload imbalance for algebraic multigrid methods\\nThe considerations above show that one has to expect certain limits on the scalability of the geometric multigrid algorithm as it is implemented in deal.II because even in cases where the finest levels of a mesh are perfectly load balanced, the coarser levels may not be. At the same time, the coarser levels are weighted less (the contributions of \\\\(W_\\\\ell\\\\) to \\\\(W\\\\) are small) because coarser levels have fewer cells and, consequently, do not contribute to the overall run time as much as finer levels. In other words, imbalances in the coarser levels may not lead to large effects in the big picture.\\nAlgebraic multigrid methods are of course based on an entirely different approach to creating a hierarchy of levels. In particular, they create these purely based on analyzing the system matrix, and very sophisticated algorithms for ensuring that the problem is well load-balanced on every level are implemented in both the hypre and ML/MueLu packages that underly the TrilinosWrappers::PreconditionAMG and PETScWrappers::PreconditionBoomerAMG classes. In some sense, these algorithms are simpler than for geometric multigrid methods because they only deal with the matrix itself, rather than all of the connotations of meshes, neighbors, parents, and other geometric entities. At the same time, much work has also been put into making algebraic multigrid methods scale to very large problems, including questions such as reducing the number of processors that work on a given level of the hierarchy to a subset of all processors, if otherwise processors would spend less time on computations than on communication. (One might note that it is of course possible to implement these same kinds of ideas also in geometric multigrid algorithms where one purposefully idles some processors on coarser levels to reduce the amount of communication. deal.II just doesn't do this at this time.)\\nThese are not considerations we typically have to worry about here, however: For most purposes, we use algebraic multigrid methods as black-box methods.\\nRunning the program\\nAs mentioned above, this program can use three different ways of solving the linear system: matrix-based geometric multigrid (\\\"MB\\\"), matrix-free geometric multigrid (\\\"MF\\\"), and algebraic multigrid (\\\"AMG\\\"). The directory in which this program resides has input files with suffix .prm for all three of these options, and for both 2d and 3d.\\nYou can execute the program as in ./step-50 gmg_mb_2d.prm\\n and this will take the run-time parameters from the given input file (here, gmg_mb_2d.prm).\\nThe program is intended to be run in parallel, and you can achieve this using a command such as mpirun -np 4 ./step-50 gmg_mb_2d.prm\\n if you want to, for example, run on four processors. (That said, the program is also ready to run with, say, -np 28672 if that's how many processors you have available.)\\n The commented program\\n Include files\\nThe include files are a combination of step-40, step-16, and step-37:\\n\\u00a0 #include <deal.II/base/conditional_ostream.h>\\n\\u00a0 #include <deal.II/base/data_out_base.h>\\n\\u00a0 #include <deal.II/base/index_set.h>\\n\\u00a0 #include <deal.II/base/quadrature_lib.h>\\n\\u00a0 #include <deal.II/base/timer.h>\\n\\u00a0 #include <deal.II/base/parameter_handler.h>\\n\\u00a0 #include <deal.II/distributed/grid_refinement.h>\\n\\u00a0 #include <deal.II/distributed/tria.h>\\n\\u00a0 #include <deal.II/dofs/dof_tools.h>\\n\\u00a0 #include <deal.II/fe/fe_q.h>\\n\\u00a0 #include <deal.II/fe/fe_values.h>\\n\\u00a0 #include <deal.II/fe/mapping_q1.h>\\n\\u00a0 #include <deal.II/grid/grid_generator.h>\\n\\u00a0 #include <deal.II/grid/grid_refinement.h>\\n\\u00a0 #include <deal.II/grid/tria.h>\\n\\u00a0 #include <deal.II/lac/affine_constraints.h>\\n\\u00a0 #include <deal.II/lac/dynamic_sparsity_pattern.h>\\n\\u00a0 #include <deal.II/lac/sparsity_tools.h>\\n\\u00a0 #include <deal.II/lac/solver_cg.h>\\n\\u00a0 \\nWe use the same strategy as in step-40 to switch between PETSc and Trilinos:\\n\\u00a0 #include <deal.II/lac/generic_linear_algebra.h>\\n\\u00a0 \\nComment the following preprocessor definition in or out if you have PETSc and Trilinos installed and you prefer using PETSc in this example:\\n\\u00a0 #define FORCE_USE_OF_TRILINOS\\n\\u00a0 \\n\\u00a0 namespace LA\\n\\u00a0 {\\n\\u00a0 #if defined(DEAL_II_WITH_PETSC) && !defined(DEAL_II_PETSC_WITH_COMPLEX) && \\\\\\n\\u00a0   !(defined(DEAL_II_WITH_TRILINOS) && defined(FORCE_USE_OF_TRILINOS))\\n\\u00a0   using namespace dealii::LinearAlgebraPETSc;\\n\\u00a0 #  define USE_PETSC_LA\\n\\u00a0 #elif defined(DEAL_II_WITH_TRILINOS)\\n\\u00a0   using namespace dealii::LinearAlgebraTrilinos;\\n\\u00a0 #else\\n\\u00a0 #  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required\\n\\u00a0 #endif\\n\\u00a0 } // namespace LA\\n\\u00a0 \\n\\u00a0 #include <deal.II/matrix_free/matrix_free.h>\\n\\u00a0 #include <deal.II/matrix_free/operators.h>\\n\\u00a0 #include <deal.II/matrix_free/fe_evaluation.h>\\n\\u00a0 #include <deal.II/multigrid/mg_coarse.h>\\n\\u00a0 #include <deal.II/multigrid/mg_constrained_dofs.h>\\n\\u00a0 #include <deal.II/multigrid/mg_matrix.h>\\n\\u00a0 #include <deal.II/multigrid/mg_smoother.h>\\n\\u00a0 #include <deal.II/multigrid/mg_tools.h>\\n\\u00a0 #include <deal.II/multigrid/mg_transfer.h>\\n\\u00a0 #include <deal.II/multigrid/multigrid.h>\\n\\u00a0 #include <deal.II/multigrid/mg_transfer_matrix_free.h>\\n\\u00a0 #include <deal.II/numerics/data_out.h>\\n\\u00a0 #include <deal.II/numerics/vector_tools.h>\\n\\u00a0 \\nThe following files are used to assemble the error estimator like in step-12:\\n\\u00a0 #include <deal.II/fe/fe_interface_values.h>\\n\\u00a0 #include <deal.II/meshworker/mesh_loop.h>\\n\\u00a0 \\n\\u00a0 using namespace dealii;\\n\\u00a0 \\n\\u00a0 \\ndealiiDefinition namespace_dealii.h:25\\n Coefficients and helper classes\\nMatrixFree operators must use the LinearAlgebra::distributed::Vector vector type. Here we define operations which copy to and from Trilinos vectors for compatibility with the matrix-based code. Note that this functionality does not currently exist for PETSc vector types, so Trilinos must be installed to use the MatrixFree solver in this tutorial.\\n\\u00a0 namespace ChangeVectorTypes\\n\\u00a0 {\\n\\u00a0   template <typename number>\\n\\u00a0   void copy(LA::MPI::Vector                                  &out,\\n\\u00a0             const LinearAlgebra::distributed::Vector<number> &in)\\n\\u00a0   {\\n\\u00a0     LinearAlgebra::ReadWriteVector<double> rwv(out.locally_owned_elements());\\n\\u00a0     rwv.import_elements(in, VectorOperation::insert);\\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0     AssertThrow(false,\\n\\u00a0                 ExcMessage(\\\"ChangeVectorTypes::copy() not implemented for \\\"\\n\\u00a0                            \\\"PETSc vector types.\\\"));\\n\\u00a0 #else\\n\\u00a0     out.import_elements(rwv, VectorOperation::insert);\\n\\u00a0 #endif\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <typename number>\\n\\u00a0   void copy(LinearAlgebra::distributed::Vector<number> &out,\\n\\u00a0             const LA::MPI::Vector                      &in)\\n\\u00a0   {\\n\\u00a0     LinearAlgebra::ReadWriteVector<double> rwv;\\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0     (void)in;\\n\\u00a0     AssertThrow(false,\\n\\u00a0                 ExcMessage(\\\"ChangeVectorTypes::copy() not implemented for \\\"\\n\\u00a0                            \\\"PETSc vector types.\\\"));\\n\\u00a0 #else\\n\\u00a0     rwv.reinit(in);\\n\\u00a0 #endif\\n\\u00a0     out.import_elements(rwv, VectorOperation::insert);\\n\\u00a0   }\\n\\u00a0 } // namespace ChangeVectorTypes\\n\\u00a0 \\n\\u00a0 \\nLinearAlgebra::ReadWriteVectorDefinition trilinos_epetra_vector.h:42\\nLinearAlgebra::distributed::VectorDefinition la_parallel_vector.h:250\\nLinearAlgebra::distributed::Vector::import_elementsvoid import_elements(const Vector< Number, MemorySpace2 > &src, VectorOperation::values operation)\\nAssertThrow#define AssertThrow(cond, exc)Definition exceptions.h:1739\\nVectorOperation::insert@ insertDefinition vector_operation.h:49\\nLet's move on to the description of the problem we want to solve. We set the right-hand side function to 1.0. The value function returning a VectorizedArray is used by the matrix-free code path.\\n\\u00a0 template <int dim>\\n\\u00a0 class RightHandSide : public Function<dim>\\n\\u00a0 {\\n\\u00a0 public:\\n\\u00a0   virtual double value(const Point<dim> & /*p*/,\\n\\u00a0                        const unsigned int /*component*/ = 0) const override\\n\\u00a0   {\\n\\u00a0     return 1.0;\\n\\u00a0   }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   template <typename number>\\n\\u00a0   VectorizedArray<number>\\n\\u00a0   value(const Point<dim, VectorizedArray<number>> & /*p*/,\\n\\u00a0         const unsigned int /*component*/ = 0) const\\n\\u00a0   {\\n\\u00a0     return VectorizedArray<number>(1.0);\\n\\u00a0   }\\n\\u00a0 };\\n\\u00a0 \\n\\u00a0 \\nFunctionDefinition function.h:152\\nFunction::valuevirtual RangeNumberType value(const Point< dim > &p, const unsigned int component=0) const\\nPointDefinition point.h:111\\nVectorizedArrayDefinition vectorization.h:445\\nThis next class represents the diffusion coefficient. We use a variable coefficient which is 100.0 at any point where at least one coordinate is less than -0.5, and 1.0 at all other points. As above, a separate value() returning a VectorizedArray is used for the matrix-free code. An average() function computes the arithmetic average for a set of points.\\n\\u00a0 template <int dim>\\n\\u00a0 class Coefficient : public Function<dim>\\n\\u00a0 {\\n\\u00a0 public:\\n\\u00a0   virtual double value(const Point<dim> &p,\\n\\u00a0                        const unsigned int /*component*/ = 0) const override;\\n\\u00a0 \\n\\u00a0   template <typename number>\\n\\u00a0   VectorizedArray<number> value(const Point<dim, VectorizedArray<number>> &p,\\n\\u00a0                                 const unsigned int /*component*/ = 0) const;\\n\\u00a0 \\n\\u00a0   template <typename number>\\n\\u00a0   number average_value(const std::vector<Point<dim, number>> &points) const;\\n\\u00a0 \\nWhen using a coefficient in the MatrixFree framework, we also need a function that creates a Table of coefficient values for a set of cells provided by the MatrixFree operator argument here.\\n\\u00a0   template <typename number>\\n\\u00a0   std::shared_ptr<Table<2, VectorizedArray<number>>> make_coefficient_table(\\n\\u00a0     const MatrixFree<dim, number, VectorizedArray<number>> &mf_storage) const;\\n\\u00a0 };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0 template <int dim>\\n\\u00a0 double Coefficient<dim>::value(const Point<dim> &p, const unsigned int) const\\n\\u00a0 {\\n\\u00a0   for (int d = 0; d < dim; ++d)\\n\\u00a0     {\\n\\u00a0       if (p[d] < -0.5)\\n\\u00a0         return 100.0;\\n\\u00a0     }\\n\\u00a0   return 1.0;\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0 template <int dim>\\n\\u00a0 template <typename number>\\n\\u00a0 VectorizedArray<number>\\n\\u00a0 Coefficient<dim>::value(const Point<dim, VectorizedArray<number>> &p,\\n\\u00a0                         const unsigned int) const\\n\\u00a0 {\\n\\u00a0   VectorizedArray<number> return_value = VectorizedArray<number>(1.0);\\n\\u00a0   for (unsigned int i = 0; i < VectorizedArray<number>::size(); ++i)\\n\\u00a0     {\\n\\u00a0       for (int d = 0; d < dim; ++d)\\n\\u00a0         if (p[d][i] < -0.5)\\n\\u00a0           {\\n\\u00a0             return_value[i] = 100.0;\\n\\u00a0             break;\\n\\u00a0           }\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return return_value;\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0 template <int dim>\\n\\u00a0 template <typename number>\\n\\u00a0 number Coefficient<dim>::average_value(\\n\\u00a0   const std::vector<Point<dim, number>> &points) const\\n\\u00a0 {\\n\\u00a0   number average(0);\\n\\u00a0   for (unsigned int i = 0; i < points.size(); ++i)\\n\\u00a0     average += value(points[i]);\\n\\u00a0   average /= points.size();\\n\\u00a0 \\n\\u00a0   return average;\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0 template <int dim>\\n\\u00a0 template <typename number>\\n\\u00a0 std::shared_ptr<Table<2, VectorizedArray<number>>>\\n\\u00a0 Coefficient<dim>::make_coefficient_table(\\n\\u00a0   const MatrixFree<dim, number, VectorizedArray<number>> &mf_storage) const\\n\\u00a0 {\\n\\u00a0   auto coefficient_table =\\n\\u00a0     std::make_shared<Table<2, VectorizedArray<number>>>();\\n\\u00a0 \\n\\u00a0   FEEvaluation<dim, -1, 0, 1, number> fe_eval(mf_storage);\\n\\u00a0 \\n\\u00a0   const unsigned int n_cells = mf_storage.n_cell_batches();\\n\\u00a0 \\n\\u00a0   coefficient_table->reinit(n_cells, 1);\\n\\u00a0 \\n\\u00a0   for (unsigned int cell = 0; cell < n_cells; ++cell)\\n\\u00a0     {\\n\\u00a0       fe_eval.reinit(cell);\\n\\u00a0 \\n\\u00a0       VectorizedArray<number> average_value = 0.;\\n\\u00a0       for (const unsigned int q : fe_eval.quadrature_point_indices())\\n\\u00a0         average_value += value(fe_eval.quadrature_point(q));\\n\\u00a0       average_value /= fe_eval.n_q_points;\\n\\u00a0 \\n\\u00a0       (*coefficient_table)(cell, 0) = average_value;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return coefficient_table;\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nFEEvaluationDefinition fe_evaluation.h:1355\\nMatrixFreeDefinition matrix_free.h:113\\nPhysics::Elasticity::Kinematics::dSymmetricTensor< 2, dim, Number > d(const Tensor< 2, dim, Number > &F, const Tensor< 2, dim, Number > &dF_dt)\\ninternal::TriangulationImplementation::n_cellsunsigned int n_cells(const internal::TriangulationImplementation::NumberCache< 1 > &c)Definition tria.cc:14883\\ninternal::EvaluatorQuantity::value@ value\\n Run time parameters\\nWe will use ParameterHandler to pass in parameters at runtime. The structure Settings parses and stores these parameters to be queried throughout the program.\\n\\u00a0 struct Settings\\n\\u00a0 {\\n\\u00a0   bool try_parse(const std::string &prm_filename);\\n\\u00a0 \\n\\u00a0   enum SolverType\\n\\u00a0   {\\n\\u00a0     gmg_mb,\\n\\u00a0     gmg_mf,\\n\\u00a0     amg\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0   SolverType solver;\\n\\u00a0 \\n\\u00a0   int          dimension;\\n\\u00a0   double       smoother_dampen;\\n\\u00a0   unsigned int smoother_steps;\\n\\u00a0   unsigned int n_steps;\\n\\u00a0   bool         output;\\n\\u00a0 };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0 bool Settings::try_parse(const std::string &prm_filename)\\n\\u00a0 {\\n\\u00a0   ParameterHandler prm;\\n\\u00a0   prm.declare_entry(\\\"dim\\\", \\\"2\\\", Patterns::Integer(), \\\"The problem dimension.\\\");\\n\\u00a0   prm.declare_entry(\\\"n_steps\\\",\\n\\u00a0                     \\\"10\\\",\\n\\u00a0                     Patterns::Integer(0),\\n\\u00a0                     \\\"Number of adaptive refinement steps.\\\");\\n\\u00a0   prm.declare_entry(\\\"smoother dampen\\\",\\n\\u00a0                     \\\"1.0\\\",\\n\\u00a0                     Patterns::Double(0.0),\\n\\u00a0                     \\\"Dampen factor for the smoother.\\\");\\n\\u00a0   prm.declare_entry(\\\"smoother steps\\\",\\n\\u00a0                     \\\"1\\\",\\n\\u00a0                     Patterns::Integer(1),\\n\\u00a0                     \\\"Number of smoother steps.\\\");\\n\\u00a0   prm.declare_entry(\\\"solver\\\",\\n\\u00a0                     \\\"MF\\\",\\n\\u00a0                     Patterns::Selection(\\\"MF|MB|AMG\\\"),\\n\\u00a0                     \\\"Switch between matrix-free GMG, \\\"\\n\\u00a0                     \\\"matrix-based GMG, and AMG.\\\");\\n\\u00a0   prm.declare_entry(\\\"output\\\",\\n\\u00a0                     \\\"false\\\",\\n\\u00a0                     Patterns::Bool(),\\n\\u00a0                     \\\"Output graphical results.\\\");\\n\\u00a0 \\n\\u00a0   if (prm_filename.empty())\\n\\u00a0     {\\n\\u00a0       std::cout << \\\"****  Error: No input file provided!\\\\n\\\"\\n\\u00a0                 << \\\"****  Error: Call this program as './step-50 input.prm\\\\n\\\"\\n\\u00a0                 << '\\\\n'\\n\\u00a0                 << \\\"****  You may want to use one of the input files in this\\\\n\\\"\\n\\u00a0                 << \\\"****  directory, or use the following default values\\\\n\\\"\\n\\u00a0                 << \\\"****  to create an input file:\\\\n\\\";\\n\\u00a0       if (Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)\\n\\u00a0         prm.print_parameters(std::cout, ParameterHandler::Text);\\n\\u00a0       return false;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       prm.parse_input(prm_filename);\\n\\u00a0     }\\n\\u00a0   catch (std::exception &e)\\n\\u00a0     {\\n\\u00a0       if (Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)\\n\\u00a0         std::cerr << e.what() << std::endl;\\n\\u00a0       return false;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   if (prm.get(\\\"solver\\\") == \\\"MF\\\")\\n\\u00a0     this->solver = gmg_mf;\\n\\u00a0   else if (prm.get(\\\"solver\\\") == \\\"MB\\\")\\n\\u00a0     this->solver = gmg_mb;\\n\\u00a0   else if (prm.get(\\\"solver\\\") == \\\"AMG\\\")\\n\\u00a0     this->solver = amg;\\n\\u00a0   else\\n\\u00a0     AssertThrow(false, ExcNotImplemented());\\n\\u00a0 \\n\\u00a0   this->dimension       = prm.get_integer(\\\"dim\\\");\\n\\u00a0   this->n_steps         = prm.get_integer(\\\"n_steps\\\");\\n\\u00a0   this->smoother_dampen = prm.get_double(\\\"smoother dampen\\\");\\n\\u00a0   this->smoother_steps  = prm.get_integer(\\\"smoother steps\\\");\\n\\u00a0   this->output          = prm.get_bool(\\\"output\\\");\\n\\u00a0 \\n\\u00a0   return true;\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nParameterHandlerDefinition parameter_handler.h:855\\nParameterHandler::declare_entryvoid declare_entry(const std::string &entry, const std::string &default_value, const Patterns::PatternBase &pattern=Patterns::Anything(), const std::string &documentation=\\\"\\\", const bool has_to_be_set=false)Definition parameter_handler.cc:846\\nParameterHandler::Text@ TextDefinition parameter_handler.h:903\\nPatterns::BoolDefinition patterns.h:980\\nPatterns::DoubleDefinition patterns.h:291\\nPatterns::IntegerDefinition patterns.h:188\\nPatterns::SelectionDefinition patterns.h:381\\nPhysics::Elasticity::Kinematics::eSymmetricTensor< 2, dim, Number > e(const Tensor< 2, dim, Number > &F)\\nUtilities::MPI::this_mpi_processunsigned int this_mpi_process(const MPI_Comm mpi_communicator)Definition mpi.cc:107\\n LaplaceProblem class\\nThis is the main class of the program. It looks very similar to step-16, step-37, and step-40. For the MatrixFree setup, we use the MatrixFreeOperators::LaplaceOperator class which defines local_apply(), compute_diagonal(), and set_coefficient() functions internally. Note that the polynomial degree is a template parameter of this class. This is necessary for the matrix-free code.\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 class LaplaceProblem\\n\\u00a0 {\\n\\u00a0 public:\\n\\u00a0   LaplaceProblem(const Settings &settings);\\n\\u00a0   void run();\\n\\u00a0 \\n\\u00a0 private:\\nWe will use the following types throughout the program. First the matrix-based types, after that the matrix-free classes. For the matrix-free implementation, we use float for the level operators.\\n\\u00a0   using MatrixType      = LA::MPI::SparseMatrix;\\n\\u00a0   using VectorType      = LA::MPI::Vector;\\n\\u00a0   using PreconditionAMG = LA::MPI::PreconditionAMG;\\n\\u00a0 \\n\\u00a0   using MatrixFreeLevelMatrix = MatrixFreeOperators::LaplaceOperator<\\n\\u00a0     dim,\\n\\u00a0     degree,\\n\\u00a0     degree + 1,\\n\\u00a0     1,\\n\\u00a0     LinearAlgebra::distributed::Vector<float>>;\\n\\u00a0   using MatrixFreeActiveMatrix = MatrixFreeOperators::LaplaceOperator<\\n\\u00a0     dim,\\n\\u00a0     degree,\\n\\u00a0     degree + 1,\\n\\u00a0     1,\\n\\u00a0     LinearAlgebra::distributed::Vector<double>>;\\n\\u00a0 \\n\\u00a0   using MatrixFreeLevelVector  = LinearAlgebra::distributed::Vector<float>;\\n\\u00a0   using MatrixFreeActiveVector = LinearAlgebra::distributed::Vector<double>;\\n\\u00a0 \\n\\u00a0   void setup_system();\\n\\u00a0   void setup_multigrid();\\n\\u00a0   void assemble_system();\\n\\u00a0   void assemble_multigrid();\\n\\u00a0   void assemble_rhs();\\n\\u00a0   void solve();\\n\\u00a0   void estimate();\\n\\u00a0   void refine_grid();\\n\\u00a0   void output_results(const unsigned int cycle);\\n\\u00a0 \\n\\u00a0   Settings settings;\\n\\u00a0 \\n\\u00a0   MPI_Comm           mpi_communicator;\\n\\u00a0   ConditionalOStream pcout;\\n\\u00a0 \\n\\u00a0   parallel::distributed::Triangulation<dim> triangulation;\\n\\u00a0   const MappingQ1<dim>                      mapping;\\n\\u00a0   const FE_Q<dim>                           fe;\\n\\u00a0 \\n\\u00a0   DoFHandler<dim> dof_handler;\\n\\u00a0 \\n\\u00a0   IndexSet                  locally_owned_dofs;\\n\\u00a0   IndexSet                  locally_relevant_dofs;\\n\\u00a0   AffineConstraints<double> constraints;\\n\\u00a0 \\n\\u00a0   MatrixType             system_matrix;\\n\\u00a0   MatrixFreeActiveMatrix mf_system_matrix;\\n\\u00a0   VectorType             solution;\\n\\u00a0   VectorType             right_hand_side;\\n\\u00a0   Vector<double>         estimated_error_square_per_cell;\\n\\u00a0 \\n\\u00a0   MGLevelObject<MatrixType> mg_matrix;\\n\\u00a0   MGLevelObject<MatrixType> mg_interface_in;\\n\\u00a0   MGConstrainedDoFs         mg_constrained_dofs;\\n\\u00a0 \\n\\u00a0   MGLevelObject<MatrixFreeLevelMatrix> mf_mg_matrix;\\n\\u00a0 \\n\\u00a0   TimerOutput computing_timer;\\n\\u00a0 };\\n\\u00a0 \\n\\u00a0 \\nAffineConstraintsDefinition affine_constraints.h:507\\nConditionalOStreamDefinition conditional_ostream.h:80\\nDoFHandlerDefinition dof_handler.h:317\\nFE_QDefinition fe_q.h:554\\nIndexSetDefinition index_set.h:70\\nMGConstrainedDoFsDefinition mg_constrained_dofs.h:45\\nMGLevelObjectDefinition mg_level_object.h:49\\nMPI_Comm\\nMappingQ1Definition mapping_q1.h:55\\nMatrixFreeOperators::LaplaceOperatorDefinition operators.h:870\\nTimerOutputDefinition timer.h:549\\nVectorDefinition vector.h:120\\nparallel::distributed::TriangulationDefinition tria.h:268\\ntriangulationconst ::parallel::distributed::Triangulation< dim, spacedim > * triangulationDefinition p4est_wrappers.cc:68\\nThe only interesting part about the constructor is that we construct the multigrid hierarchy unless we use AMG. For that, we need to parse the run time parameters before this constructor completes.\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 LaplaceProblem<dim, degree>::LaplaceProblem(const Settings &settings)\\n\\u00a0   : settings(settings)\\n\\u00a0   , mpi_communicator(MPI_COMM_WORLD)\\n\\u00a0   , pcout(std::cout, (Utilities::MPI::this_mpi_process(mpi_communicator) == 0))\\n\\u00a0   , triangulation(mpi_communicator,\\n\\u00a0                   Triangulation<dim>::limit_level_difference_at_vertices,\\n\\u00a0                   (settings.solver == Settings::amg) ?\\n\\u00a0                     parallel::distributed::Triangulation<dim>::default_setting :\\n\\u00a0                     parallel::distributed::Triangulation<\\n\\u00a0                       dim>::construct_multigrid_hierarchy)\\n\\u00a0   , mapping()\\n\\u00a0   , fe(degree)\\n\\u00a0   , dof_handler(triangulation)\\n\\u00a0   , computing_timer(pcout, TimerOutput::never, TimerOutput::wall_times)\\n\\u00a0 {\\n\\u00a0   GridGenerator::hyper_L(triangulation, -1., 1., /*colorize*/ false);\\n\\u00a0   triangulation.refine_global(1);\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTriangulationDefinition tria.h:1323\\nTriangulation::refine_globalvoid refine_global(const unsigned int times=1)\\nInitializeLibrary::MPI@ MPI\\nGridGeneratorDefinition grid_generator.h:64\\nUtilitiesDefinition communication_pattern_base.h:30\\nparallelDefinition distributed.h:424\\nstdSTL namespace.\\n LaplaceProblem::setup_system()\\nUnlike step-16 and step-37, we split the set up into two parts, setup_system() and setup_multigrid(). Here is the typical setup_system() function for the active mesh found in most tutorials. For matrix-free, the active mesh set up is similar to step-37; for matrix-based (GMG and AMG solvers), the setup is similar to step-40.\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::setup_system()\\n\\u00a0 {\\n\\u00a0   TimerOutput::Scope timing(computing_timer, \\\"Setup\\\");\\n\\u00a0 \\n\\u00a0   dof_handler.distribute_dofs(fe);\\n\\u00a0 \\n\\u00a0   locally_relevant_dofs = DoFTools::extract_locally_relevant_dofs(dof_handler);\\n\\u00a0   locally_owned_dofs    = dof_handler.locally_owned_dofs();\\n\\u00a0 \\n\\u00a0   solution.reinit(locally_owned_dofs, mpi_communicator);\\n\\u00a0   right_hand_side.reinit(locally_owned_dofs, mpi_communicator);\\n\\u00a0   constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n\\u00a0   DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n\\u00a0 \\n\\u00a0   VectorTools::interpolate_boundary_values(\\n\\u00a0     mapping, dof_handler, 0, Functions::ZeroFunction<dim>(), constraints);\\n\\u00a0   constraints.close();\\n\\u00a0 \\n\\u00a0   switch (settings.solver)\\n\\u00a0     {\\n\\u00a0       case Settings::gmg_mf:\\n\\u00a0         {\\n\\u00a0           typename MatrixFree<dim, double>::AdditionalData additional_data;\\n\\u00a0           additional_data.tasks_parallel_scheme =\\n\\u00a0             MatrixFree<dim, double>::AdditionalData::none;\\n\\u00a0           additional_data.mapping_update_flags =\\n\\u00a0             (update_gradients | update_JxW_values | update_quadrature_points);\\n\\u00a0           std::shared_ptr<MatrixFree<dim, double>> mf_storage =\\n\\u00a0             std::make_shared<MatrixFree<dim, double>>();\\n\\u00a0           mf_storage->reinit(mapping,\\n\\u00a0                              dof_handler,\\n\\u00a0                              constraints,\\n\\u00a0                              QGauss<1>(degree + 1),\\n\\u00a0                              additional_data);\\n\\u00a0 \\n\\u00a0           mf_system_matrix.initialize(mf_storage);\\n\\u00a0 \\n\\u00a0           const Coefficient<dim> coefficient;\\n\\u00a0           mf_system_matrix.set_coefficient(\\n\\u00a0             coefficient.make_coefficient_table(*mf_storage));\\n\\u00a0 \\n\\u00a0           break;\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       case Settings::gmg_mb:\\n\\u00a0       case Settings::amg:\\n\\u00a0         {\\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0           DynamicSparsityPattern dsp(locally_relevant_dofs);\\n\\u00a0           DoFTools::make_sparsity_pattern(dof_handler, dsp, constraints);\\n\\u00a0 \\n\\u00a0           SparsityTools::distribute_sparsity_pattern(dsp,\\n\\u00a0                                                      locally_owned_dofs,\\n\\u00a0                                                      mpi_communicator,\\n\\u00a0                                                      locally_relevant_dofs);\\n\\u00a0 \\n\\u00a0           system_matrix.reinit(locally_owned_dofs,\\n\\u00a0                                locally_owned_dofs,\\n\\u00a0                                dsp,\\n\\u00a0                                mpi_communicator);\\n\\u00a0 #else\\n\\u00a0           TrilinosWrappers::SparsityPattern dsp(locally_owned_dofs,\\n\\u00a0                                                 locally_owned_dofs,\\n\\u00a0                                                 locally_relevant_dofs,\\n\\u00a0                                                 mpi_communicator);\\n\\u00a0           DoFTools::make_sparsity_pattern(dof_handler, dsp, constraints);\\n\\u00a0           dsp.compress();\\n\\u00a0           system_matrix.reinit(dsp);\\n\\u00a0 #endif\\n\\u00a0 \\n\\u00a0           break;\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       default:\\n\\u00a0         DEAL_II_NOT_IMPLEMENTED();\\n\\u00a0     }\\n\\u00a0 }\\n\\u00a0 \\nDynamicSparsityPatternDefinition dynamic_sparsity_pattern.h:322\\nFunctions::ZeroFunctionDefinition function.h:510\\nQGaussDefinition quadrature_lib.h:40\\nTimerOutput::ScopeDefinition timer.h:557\\nTrilinosWrappers::SparsityPatternDefinition trilinos_sparsity_pattern.h:275\\nDoFTools::make_hanging_node_constraintsvoid make_hanging_node_constraints(const DoFHandler< dim, spacedim > &dof_handler, AffineConstraints< number > &constraints)Definition dof_tools_constraints.cc:3073\\nDoFTools::make_sparsity_patternvoid make_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, SparsityPatternBase &sparsity_pattern, const AffineConstraints< number > &constraints={}, const bool keep_constrained_dofs=true, const types::subdomain_id subdomain_id=numbers::invalid_subdomain_id)Definition dof_tools_sparsity.cc:56\\nupdate_JxW_values@ update_JxW_valuesTransformed quadrature weights.Definition fe_update_flags.h:134\\nupdate_gradients@ update_gradientsShape function gradients.Definition fe_update_flags.h:81\\nupdate_quadrature_points@ update_quadrature_pointsTransformed quadrature points.Definition fe_update_flags.h:127\\nDEAL_II_NOT_IMPLEMENTED#define DEAL_II_NOT_IMPLEMENTED()Definition exceptions.h:1814\\nDoFTools::extract_locally_relevant_dofsIndexSet extract_locally_relevant_dofs(const DoFHandler< dim, spacedim > &dof_handler)Definition dof_tools.cc:1164\\nSparsityTools::distribute_sparsity_patternvoid distribute_sparsity_pattern(DynamicSparsityPattern &dsp, const IndexSet &locally_owned_rows, const MPI_Comm mpi_comm, const IndexSet &locally_relevant_rows)Definition sparsity_tools.cc:1020\\nVectorTools::interpolate_boundary_valuesvoid interpolate_boundary_values(const Mapping< dim, spacedim > &mapping, const DoFHandler< dim, spacedim > &dof, const std::map< types::boundary_id, const Function< spacedim, number > * > &function_map, std::map< types::global_dof_index, number > &boundary_values, const ComponentMask &component_mask={})\\nMatrixFree::AdditionalDataDefinition matrix_free.h:184\\nMatrixFree::AdditionalData::tasks_parallel_schemeTasksParallelScheme tasks_parallel_schemeDefinition matrix_free.h:347\\n LaplaceProblem::setup_multigrid()\\nThis function does the multilevel setup for both matrix-free and matrix-based GMG. The matrix-free setup is similar to that of step-37, and the matrix-based is similar to step-16, except we must use appropriate distributed sparsity patterns.\\nThe function is not called for the AMG approach, but to err on the safe side, the main switch statement of this function nevertheless makes sure that the function only operates on known multigrid settings by throwing an assertion if the function were called for anything other than the two geometric multigrid methods.\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::setup_multigrid()\\n\\u00a0 {\\n\\u00a0   TimerOutput::Scope timing(computing_timer, \\\"Setup multigrid\\\");\\n\\u00a0 \\n\\u00a0   dof_handler.distribute_mg_dofs();\\n\\u00a0 \\n\\u00a0   mg_constrained_dofs.clear();\\n\\u00a0   mg_constrained_dofs.initialize(dof_handler);\\n\\u00a0 \\n\\u00a0   const std::set<types::boundary_id> boundary_ids = {types::boundary_id(0)};\\n\\u00a0   mg_constrained_dofs.make_zero_boundary_constraints(dof_handler, boundary_ids);\\n\\u00a0 \\n\\u00a0   const unsigned int n_levels = triangulation.n_global_levels();\\n\\u00a0 \\n\\u00a0   switch (settings.solver)\\n\\u00a0     {\\n\\u00a0       case Settings::gmg_mf:\\n\\u00a0         {\\n\\u00a0           mf_mg_matrix.resize(0, n_levels - 1);\\n\\u00a0 \\n\\u00a0           for (unsigned int level = 0; level < n_levels; ++level)\\n\\u00a0             {\\n\\u00a0               AffineConstraints<double> level_constraints(\\n\\u00a0                 dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                 DoFTools::extract_locally_relevant_level_dofs(dof_handler,\\n\\u00a0                                                               level));\\n\\u00a0               for (const types::global_dof_index dof_index :\\n\\u00a0                    mg_constrained_dofs.get_boundary_indices(level))\\n\\u00a0                 level_constraints.constrain_dof_to_zero(dof_index);\\n\\u00a0               level_constraints.close();\\n\\u00a0 \\n\\u00a0               typename MatrixFree<dim, float>::AdditionalData additional_data;\\n\\u00a0               additional_data.tasks_parallel_scheme =\\n\\u00a0                 MatrixFree<dim, float>::AdditionalData::none;\\n\\u00a0               additional_data.mapping_update_flags =\\n\\u00a0                 (update_gradients | update_JxW_values |\\n\\u00a0                  update_quadrature_points);\\n\\u00a0               additional_data.mg_level = level;\\n\\u00a0               std::shared_ptr<MatrixFree<dim, float>> mf_storage_level(\\n\\u00a0                 new MatrixFree<dim, float>());\\n\\u00a0               mf_storage_level->reinit(mapping,\\n\\u00a0                                        dof_handler,\\n\\u00a0                                        level_constraints,\\n\\u00a0                                        QGauss<1>(degree + 1),\\n\\u00a0                                        additional_data);\\n\\u00a0 \\n\\u00a0               mf_mg_matrix[level].initialize(mf_storage_level,\\n\\u00a0                                              mg_constrained_dofs,\\n\\u00a0                                              level);\\n\\u00a0 \\n\\u00a0               const Coefficient<dim> coefficient;\\n\\u00a0               mf_mg_matrix[level].set_coefficient(\\n\\u00a0                 coefficient.make_coefficient_table(*mf_storage_level));\\n\\u00a0 \\n\\u00a0               mf_mg_matrix[level].compute_diagonal();\\n\\u00a0             }\\n\\u00a0 \\n\\u00a0           break;\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       case Settings::gmg_mb:\\n\\u00a0         {\\n\\u00a0           mg_matrix.resize(0, n_levels - 1);\\n\\u00a0           mg_matrix.clear_elements();\\n\\u00a0           mg_interface_in.resize(0, n_levels - 1);\\n\\u00a0           mg_interface_in.clear_elements();\\n\\u00a0 \\n\\u00a0           for (unsigned int level = 0; level < n_levels; ++level)\\n\\u00a0             {\\n\\u00a0               const IndexSet dof_set =\\n\\u00a0                 DoFTools::extract_locally_relevant_level_dofs(dof_handler,\\n\\u00a0                                                               level);\\n\\u00a0 \\n\\u00a0               {\\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0                 DynamicSparsityPattern dsp(dof_set);\\n\\u00a0                 MGTools::make_sparsity_pattern(dof_handler, dsp, level);\\n\\u00a0                 dsp.compress();\\n\\u00a0                 SparsityTools::distribute_sparsity_pattern(\\n\\u00a0                   dsp,\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   mpi_communicator,\\n\\u00a0                   dof_set);\\n\\u00a0 \\n\\u00a0                 mg_matrix[level].reinit(\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   dsp,\\n\\u00a0                   mpi_communicator);\\n\\u00a0 #else\\n\\u00a0                 TrilinosWrappers::SparsityPattern dsp(\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   dof_set,\\n\\u00a0                   mpi_communicator);\\n\\u00a0                 MGTools::make_sparsity_pattern(dof_handler, dsp, level);\\n\\u00a0 \\n\\u00a0                 dsp.compress();\\n\\u00a0                 mg_matrix[level].reinit(dsp);\\n\\u00a0 #endif\\n\\u00a0               }\\n\\u00a0 \\n\\u00a0               {\\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0                 DynamicSparsityPattern dsp(dof_set);\\n\\u00a0                 MGTools::make_interface_sparsity_pattern(dof_handler,\\n\\u00a0                                                          mg_constrained_dofs,\\n\\u00a0                                                          dsp,\\n\\u00a0                                                          level);\\n\\u00a0                 dsp.compress();\\n\\u00a0                 SparsityTools::distribute_sparsity_pattern(\\n\\u00a0                   dsp,\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   mpi_communicator,\\n\\u00a0                   dof_set);\\n\\u00a0 \\n\\u00a0                 mg_interface_in[level].reinit(\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   dsp,\\n\\u00a0                   mpi_communicator);\\n\\u00a0 #else\\n\\u00a0                 TrilinosWrappers::SparsityPattern dsp(\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0                   dof_set,\\n\\u00a0                   mpi_communicator);\\n\\u00a0 \\n\\u00a0                 MGTools::make_interface_sparsity_pattern(dof_handler,\\n\\u00a0                                                          mg_constrained_dofs,\\n\\u00a0                                                          dsp,\\n\\u00a0                                                          level);\\n\\u00a0                 dsp.compress();\\n\\u00a0                 mg_interface_in[level].reinit(dsp);\\n\\u00a0 #endif\\n\\u00a0               }\\n\\u00a0             }\\n\\u00a0           break;\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       default:\\n\\u00a0         DEAL_II_NOT_IMPLEMENTED();\\n\\u00a0     }\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\nparallel::TriangulationBase::n_global_levelsvirtual unsigned int n_global_levels() const overrideDefinition tria_base.cc:141\\nunsigned int\\nlevelunsigned int levelDefinition grid_out.cc:4626\\nDoFTools::extract_locally_relevant_level_dofsIndexSet extract_locally_relevant_level_dofs(const DoFHandler< dim, spacedim > &dof_handler, const unsigned int level)Definition dof_tools.cc:1212\\nMGTools::make_interface_sparsity_patternvoid make_interface_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, const MGConstrainedDoFs &mg_constrained_dofs, SparsityPatternBase &sparsity, const unsigned int level)Definition mg_tools.cc:1013\\nMGTools::make_sparsity_patternvoid make_sparsity_pattern(const DoFHandler< dim, spacedim > &dof_handler, SparsityPatternBase &sparsity, const unsigned int level, const AffineConstraints< number > &constraints={}, const bool keep_constrained_dofs=true)Definition mg_tools.cc:575\\ntypes::boundary_idunsigned int boundary_idDefinition types.h:144\\n LaplaceProblem::assemble_system()\\nThe assembly is split into three parts: assemble_system(), assemble_multigrid(), and assemble_rhs(). The assemble_system() function here assembles and stores the (global) system matrix and the right-hand side for the matrix-based methods. It is similar to the assembly in step-40.\\nNote that the matrix-free method does not execute this function as it does not need to assemble a matrix, and it will instead assemble the right-hand side in assemble_rhs().\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::assemble_system()\\n\\u00a0 {\\n\\u00a0   TimerOutput::Scope timing(computing_timer, \\\"Assemble\\\");\\n\\u00a0 \\n\\u00a0   const QGauss<dim> quadrature_formula(degree + 1);\\n\\u00a0 \\n\\u00a0   FEValues<dim> fe_values(fe,\\n\\u00a0                           quadrature_formula,\\n\\u00a0                           update_values | update_gradients |\\n\\u00a0                             update_quadrature_points | update_JxW_values);\\n\\u00a0 \\n\\u00a0   const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0   const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0   FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n\\u00a0   Vector<double>     cell_rhs(dofs_per_cell);\\n\\u00a0 \\n\\u00a0   std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0 \\n\\u00a0   const Coefficient<dim> coefficient;\\n\\u00a0   RightHandSide<dim>     rhs;\\n\\u00a0   std::vector<double>    rhs_values(n_q_points);\\n\\u00a0 \\n\\u00a0   for (const auto &cell : dof_handler.active_cell_iterators())\\n\\u00a0     if (cell->is_locally_owned())\\n\\u00a0       {\\n\\u00a0         cell_matrix = 0;\\n\\u00a0         cell_rhs    = 0;\\n\\u00a0 \\n\\u00a0         fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0         const double coefficient_value =\\n\\u00a0           coefficient.average_value(fe_values.get_quadrature_points());\\n\\u00a0         rhs.value_list(fe_values.get_quadrature_points(), rhs_values);\\n\\u00a0 \\n\\u00a0         for (unsigned int q_point = 0; q_point < n_q_points; ++q_point)\\n\\u00a0           for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0             {\\n\\u00a0               for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0                 cell_matrix(i, j) +=\\n\\u00a0                   coefficient_value *                // epsilon(x)\\n\\u00a0                   fe_values.shape_grad(i, q_point) * // * grad phi_i(x)\\n\\u00a0                   fe_values.shape_grad(j, q_point) * // * grad phi_j(x)\\n\\u00a0                   fe_values.JxW(q_point);            // * dx\\n\\u00a0 \\n\\u00a0               cell_rhs(i) +=\\n\\u00a0                 fe_values.shape_value(i, q_point) * // grad phi_i(x)\\n\\u00a0                 rhs_values[q_point] *               // * f(x)\\n\\u00a0                 fe_values.JxW(q_point);             // * dx\\n\\u00a0             }\\n\\u00a0 \\n\\u00a0         cell->get_dof_indices(local_dof_indices);\\n\\u00a0         constraints.distribute_local_to_global(cell_matrix,\\n\\u00a0                                                cell_rhs,\\n\\u00a0                                                local_dof_indices,\\n\\u00a0                                                system_matrix,\\n\\u00a0                                                right_hand_side);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0   system_matrix.compress(VectorOperation::add);\\n\\u00a0   right_hand_side.compress(VectorOperation::add);\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\nFEValuesDefinition fe_values.h:63\\nFullMatrixDefinition full_matrix.h:79\\nupdate_values@ update_valuesShape function values.Definition fe_update_flags.h:75\\nVectorOperation::add@ addDefinition vector_operation.h:53\\n LaplaceProblem::assemble_multigrid()\\nThe following function assembles and stores the multilevel matrices for the matrix-based GMG method. This function is similar to the one found in step-16, only here it works for distributed meshes. This difference amounts to adding a condition that we only assemble on locally owned level cells and a call to compress() for each matrix that is built.\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::assemble_multigrid()\\n\\u00a0 {\\n\\u00a0   TimerOutput::Scope timing(computing_timer, \\\"Assemble multigrid\\\");\\n\\u00a0 \\n\\u00a0   const QGauss<dim> quadrature_formula(degree + 1);\\n\\u00a0 \\n\\u00a0   FEValues<dim> fe_values(fe,\\n\\u00a0                           quadrature_formula,\\n\\u00a0                           update_values | update_gradients |\\n\\u00a0                             update_quadrature_points | update_JxW_values);\\n\\u00a0 \\n\\u00a0   const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n\\u00a0   const unsigned int n_q_points    = quadrature_formula.size();\\n\\u00a0 \\n\\u00a0   FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n\\u00a0 \\n\\u00a0   std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n\\u00a0 \\n\\u00a0   const Coefficient<dim> coefficient;\\n\\u00a0 \\n\\u00a0   std::vector<AffineConstraints<double>> boundary_constraints(\\n\\u00a0     triangulation.n_global_levels());\\n\\u00a0   for (unsigned int level = 0; level < triangulation.n_global_levels(); ++level)\\n\\u00a0     {\\n\\u00a0       boundary_constraints[level].reinit(\\n\\u00a0         dof_handler.locally_owned_mg_dofs(level),\\n\\u00a0         DoFTools::extract_locally_relevant_level_dofs(dof_handler, level));\\n\\u00a0 \\n\\u00a0       for (const types::global_dof_index dof_index :\\n\\u00a0            mg_constrained_dofs.get_refinement_edge_indices(level))\\n\\u00a0         boundary_constraints[level].constrain_dof_to_zero(dof_index);\\n\\u00a0       for (const types::global_dof_index dof_index :\\n\\u00a0            mg_constrained_dofs.get_boundary_indices(level))\\n\\u00a0         boundary_constraints[level].constrain_dof_to_zero(dof_index);\\n\\u00a0       boundary_constraints[level].close();\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   for (const auto &cell : dof_handler.cell_iterators())\\n\\u00a0     if (cell->level_subdomain_id() == triangulation.locally_owned_subdomain())\\n\\u00a0       {\\n\\u00a0         cell_matrix = 0;\\n\\u00a0         fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0         const double coefficient_value =\\n\\u00a0           coefficient.average_value(fe_values.get_quadrature_points());\\n\\u00a0 \\n\\u00a0         for (unsigned int q_point = 0; q_point < n_q_points; ++q_point)\\n\\u00a0           for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0             for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0               cell_matrix(i, j) +=\\n\\u00a0                 coefficient_value * fe_values.shape_grad(i, q_point) *\\n\\u00a0                 fe_values.shape_grad(j, q_point) * fe_values.JxW(q_point);\\n\\u00a0 \\n\\u00a0         cell->get_mg_dof_indices(local_dof_indices);\\n\\u00a0 \\n\\u00a0         boundary_constraints[cell->level()].distribute_local_to_global(\\n\\u00a0           cell_matrix, local_dof_indices, mg_matrix[cell->level()]);\\n\\u00a0 \\n\\u00a0         for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n\\u00a0           for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n\\u00a0             if (mg_constrained_dofs.is_interface_matrix_entry(\\n\\u00a0                   cell->level(), local_dof_indices[i], local_dof_indices[j]))\\n\\u00a0               mg_interface_in[cell->level()].add(local_dof_indices[i],\\n\\u00a0                                                  local_dof_indices[j],\\n\\u00a0                                                  cell_matrix(i, j));\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0   for (unsigned int i = 0; i < triangulation.n_global_levels(); ++i)\\n\\u00a0     {\\n\\u00a0       mg_matrix[i].compress(VectorOperation::add);\\n\\u00a0       mg_interface_in[i].compress(VectorOperation::add);\\n\\u00a0     }\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n LaplaceProblem::assemble_rhs()\\nThe final function in this triptych assembles the right-hand side vector for the matrix-free method \\u2013 because in the matrix-free framework, we don't have to assemble the matrix and can get away with only assembling the right hand side. We could do this by extracting the code from the assemble_system() function above that deals with the right hand side, but we decide instead to go all in on the matrix-free approach and do the assembly using that way as well.\\nThe result is a function that is similar to the one found in the \\\"Use FEEvaluation::read_dof_values_plain()\\n   to avoid resolving constraints\\\" subsection in the \\\"Possibilities\\n   for extensions\\\" section of step-37.\\nThe reason for this function is that the MatrixFree operators do not take into account non-homogeneous Dirichlet constraints, instead treating all Dirichlet constraints as homogeneous. To account for this, the right-hand side here is assembled as the residual \\\\(r_0 = f-Au_0\\\\), where \\\\(u_0\\\\) is a zero vector except in the Dirichlet values. Then when solving, we have that the solution is \\\\(u = u_0 + A^{-1}r_0\\\\). This can be seen as a Newton iteration on a linear system with initial guess \\\\(u_0\\\\). The CG solve in the solve() function below computes \\\\(A^{-1}r_0\\\\) and the call to constraints.distribute() (which directly follows) adds the \\\\(u_0\\\\).\\nObviously, since we are considering a problem with zero Dirichlet boundary, we could have taken a similar approach to step-37 assemble_rhs(), but this additional work allows us to change the problem declaration if we so choose.\\nThis function has two parts in the integration loop: applying the negative of matrix \\\\(A\\\\) to \\\\(u_0\\\\) by submitting the negative of the gradient, and adding the right-hand side contribution by submitting the value \\\\(f\\\\). We must be sure to use read_dof_values_plain() for evaluating \\\\(u_0\\\\) as read_dof_values() would set all Dirichlet values to zero.\\nFinally, the system_rhs vector is of type LA::MPI::Vector, but the MatrixFree class only work for LinearAlgebra::distributed::Vector. Therefore we must compute the right-hand side using MatrixFree functionality and then use the functions in the ChangeVectorType namespace to copy it to the correct type.\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::assemble_rhs()\\n\\u00a0 {\\n\\u00a0   TimerOutput::Scope timing(computing_timer, \\\"Assemble right-hand side\\\");\\n\\u00a0 \\n\\u00a0   MatrixFreeActiveVector solution_copy;\\n\\u00a0   MatrixFreeActiveVector right_hand_side_copy;\\n\\u00a0   mf_system_matrix.initialize_dof_vector(solution_copy);\\n\\u00a0   mf_system_matrix.initialize_dof_vector(right_hand_side_copy);\\n\\u00a0 \\n\\u00a0   solution_copy = 0.;\\n\\u00a0   constraints.distribute(solution_copy);\\n\\u00a0   solution_copy.update_ghost_values();\\n\\u00a0   right_hand_side_copy = 0;\\n\\u00a0   const Table<2, VectorizedArray<double>> &coefficient =\\n\\u00a0     *(mf_system_matrix.get_coefficient());\\n\\u00a0 \\n\\u00a0   RightHandSide<dim> right_hand_side_function;\\n\\u00a0 \\n\\u00a0   FEEvaluation<dim, degree, degree + 1, 1, double> phi(\\n\\u00a0     *mf_system_matrix.get_matrix_free());\\n\\u00a0 \\n\\u00a0   for (unsigned int cell = 0;\\n\\u00a0        cell < mf_system_matrix.get_matrix_free()->n_cell_batches();\\n\\u00a0        ++cell)\\n\\u00a0     {\\n\\u00a0       phi.reinit(cell);\\n\\u00a0       phi.read_dof_values_plain(solution_copy);\\n\\u00a0       phi.evaluate(EvaluationFlags::gradients);\\n\\u00a0 \\n\\u00a0       for (const unsigned int q : phi.quadrature_point_indices())\\n\\u00a0         {\\n\\u00a0           phi.submit_gradient(-1.0 *\\n\\u00a0                                 (coefficient(cell, 0) * phi.get_gradient(q)),\\n\\u00a0                               q);\\n\\u00a0           phi.submit_value(\\n\\u00a0             right_hand_side_function.value(phi.quadrature_point(q)), q);\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       phi.integrate_scatter(EvaluationFlags::values |\\n\\u00a0                               EvaluationFlags::gradients,\\n\\u00a0                             right_hand_side_copy);\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   right_hand_side_copy.compress(VectorOperation::add);\\n\\u00a0 \\n\\u00a0   ChangeVectorTypes::copy(right_hand_side, right_hand_side_copy);\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\nTableDefinition array_view.h:39\\nEvaluationFlags::gradients@ gradientsDefinition evaluation_flags.h:54\\nEvaluationFlags::values@ valuesDefinition evaluation_flags.h:50\\n LaplaceProblem::solve()\\nHere we set up the multigrid preconditioner, test the timing of a single V-cycle, and solve the linear system. Unsurprisingly, this is one of the places where the three methods differ the most.\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::solve()\\n\\u00a0 {\\n\\u00a0   TimerOutput::Scope timing(computing_timer, \\\"Solve\\\");\\n\\u00a0 \\n\\u00a0   SolverControl solver_control(1000, 1.e-10 * right_hand_side.l2_norm());\\n\\u00a0   solver_control.enable_history_data();\\n\\u00a0 \\n\\u00a0   solution = 0.;\\n\\u00a0 \\nSolverControlDefinition solver_control.h:67\\nThe solver for the matrix-free GMG method is similar to step-37, apart from adding some interface matrices in complete analogy to step-16.\\n\\u00a0   switch (settings.solver)\\n\\u00a0     {\\n\\u00a0       case Settings::gmg_mf:\\n\\u00a0         {\\n\\u00a0           computing_timer.enter_subsection(\\\"Solve: Preconditioner setup\\\");\\n\\u00a0 \\n\\u00a0           MGTransferMatrixFree<dim, float> mg_transfer(mg_constrained_dofs);\\n\\u00a0           mg_transfer.build(dof_handler);\\n\\u00a0 \\n\\u00a0           SolverControl coarse_solver_control(1000, 1e-12, false, false);\\n\\u00a0           SolverCG<MatrixFreeLevelVector> coarse_solver(coarse_solver_control);\\n\\u00a0           PreconditionIdentity identity;\\n\\u00a0           MGCoarseGridIterativeSolver<MatrixFreeLevelVector,\\n\\u00a0                                       SolverCG<MatrixFreeLevelVector>,\\n\\u00a0                                       MatrixFreeLevelMatrix,\\n\\u00a0                                       PreconditionIdentity>\\n\\u00a0             coarse_grid_solver(coarse_solver, mf_mg_matrix[0], identity);\\n\\u00a0 \\n\\u00a0           using Smoother = PreconditionJacobi<MatrixFreeLevelMatrix>;\\n\\u00a0           MGSmootherPrecondition<MatrixFreeLevelMatrix,\\n\\u00a0                                  Smoother,\\n\\u00a0                                  MatrixFreeLevelVector>\\n\\u00a0             smoother;\\n\\u00a0           smoother.initialize(mf_mg_matrix,\\n\\u00a0                               typename Smoother::AdditionalData(\\n\\u00a0                                 settings.smoother_dampen));\\n\\u00a0           smoother.set_steps(settings.smoother_steps);\\n\\u00a0 \\n\\u00a0           mg::Matrix<MatrixFreeLevelVector> mg_m(mf_mg_matrix);\\n\\u00a0 \\n\\u00a0           MGLevelObject<\\n\\u00a0             MatrixFreeOperators::MGInterfaceOperator<MatrixFreeLevelMatrix>>\\n\\u00a0             mg_interface_matrices;\\n\\u00a0           mg_interface_matrices.resize(0, triangulation.n_global_levels() - 1);\\n\\u00a0           for (unsigned int level = 0; level < triangulation.n_global_levels();\\n\\u00a0                ++level)\\n\\u00a0             mg_interface_matrices[level].initialize(mf_mg_matrix[level]);\\n\\u00a0           mg::Matrix<MatrixFreeLevelVector> mg_interface(mg_interface_matrices);\\n\\u00a0 \\n\\u00a0           Multigrid<MatrixFreeLevelVector> mg(\\n\\u00a0             mg_m, coarse_grid_solver, mg_transfer, smoother, smoother);\\n\\u00a0           mg.set_edge_matrices(mg_interface, mg_interface);\\n\\u00a0 \\n\\u00a0           PreconditionMG<dim,\\n\\u00a0                          MatrixFreeLevelVector,\\n\\u00a0                          MGTransferMatrixFree<dim, float>>\\n\\u00a0             preconditioner(dof_handler, mg, mg_transfer);\\n\\u00a0 \\nMGCoarseGridIterativeSolverDefinition mg_coarse.h:94\\nMGSmootherPreconditionDefinition mg_smoother.h:446\\nMGSmootherPrecondition::initializevoid initialize(const MGLevelObject< MatrixType2 > &matrices, const typename PreconditionerType::AdditionalData &additional_data=typename PreconditionerType::AdditionalData())\\nMGTransferMatrixFreeDefinition mg_transfer_matrix_free.h:57\\nMatrixFreeOperators::MGInterfaceOperatorDefinition operators.h:538\\nMultigridDefinition multigrid.h:163\\nPreconditionIdentityDefinition precondition.h:220\\nPreconditionJacobiDefinition precondition.h:1656\\nPreconditionMGDefinition multigrid.h:501\\nSolverCGDefinition solver_cg.h:179\\nmg::MatrixDefinition mg_matrix.h:46\\nmgDefinition mg.h:81\\nstd_cxx20::type_identityDefinition type_traits.h:90\\nidentitystd_cxx20::type_identity< T > identityDefinition template_constraints.h:322\\nCopy the solution vector and right-hand side from LA::MPI::Vector to LinearAlgebra::distributed::Vector so that we can solve.\\n\\u00a0           MatrixFreeActiveVector solution_copy;\\n\\u00a0           MatrixFreeActiveVector right_hand_side_copy;\\n\\u00a0           mf_system_matrix.initialize_dof_vector(solution_copy);\\n\\u00a0           mf_system_matrix.initialize_dof_vector(right_hand_side_copy);\\n\\u00a0 \\n\\u00a0           ChangeVectorTypes::copy(solution_copy, solution);\\n\\u00a0           ChangeVectorTypes::copy(right_hand_side_copy, right_hand_side);\\n\\u00a0           computing_timer.leave_subsection(\\\"Solve: Preconditioner setup\\\");\\n\\u00a0 \\nTiming for 1 V-cycle.\\n\\u00a0           {\\n\\u00a0             TimerOutput::Scope timing(computing_timer,\\n\\u00a0                                       \\\"Solve: 1 multigrid V-cycle\\\");\\n\\u00a0             preconditioner.vmult(solution_copy, right_hand_side_copy);\\n\\u00a0           }\\n\\u00a0           solution_copy = 0.;\\n\\u00a0 \\nSolve the linear system, update the ghost values of the solution, copy back to LA::MPI::Vector and distribute constraints.\\n\\u00a0           {\\n\\u00a0             SolverCG<MatrixFreeActiveVector> solver(solver_control);\\n\\u00a0 \\n\\u00a0             TimerOutput::Scope timing(computing_timer, \\\"Solve: CG\\\");\\n\\u00a0             solver.solve(mf_system_matrix,\\n\\u00a0                          solution_copy,\\n\\u00a0                          right_hand_side_copy,\\n\\u00a0                          preconditioner);\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0           solution_copy.update_ghost_values();\\n\\u00a0           ChangeVectorTypes::copy(solution, solution_copy);\\n\\u00a0           constraints.distribute(solution);\\n\\u00a0 \\n\\u00a0           break;\\n\\u00a0         }\\n\\u00a0 \\nSolver for the matrix-based GMG method, similar to step-16, only using a Jacobi smoother instead of a SOR smoother (which is not implemented in parallel).\\n\\u00a0       case Settings::gmg_mb:\\n\\u00a0         {\\n\\u00a0           computing_timer.enter_subsection(\\\"Solve: Preconditioner setup\\\");\\n\\u00a0 \\n\\u00a0           MGTransferPrebuilt<VectorType> mg_transfer(mg_constrained_dofs);\\n\\u00a0           mg_transfer.build(dof_handler);\\n\\u00a0 \\n\\u00a0           SolverControl        coarse_solver_control(1000, 1e-12, false, false);\\n\\u00a0           SolverCG<VectorType> coarse_solver(coarse_solver_control);\\n\\u00a0           PreconditionIdentity identity;\\n\\u00a0           MGCoarseGridIterativeSolver<VectorType,\\n\\u00a0                                       SolverCG<VectorType>,\\n\\u00a0                                       MatrixType,\\n\\u00a0                                       PreconditionIdentity>\\n\\u00a0             coarse_grid_solver(coarse_solver, mg_matrix[0], identity);\\n\\u00a0 \\n\\u00a0           using Smoother = LA::MPI::PreconditionJacobi;\\n\\u00a0           MGSmootherPrecondition<MatrixType, Smoother, VectorType> smoother;\\n\\u00a0 \\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0           smoother.initialize(mg_matrix);\\n\\u00a0           Assert(\\n\\u00a0             settings.smoother_dampen == 1.0,\\n\\u00a0             ExcNotImplemented(\\n\\u00a0               \\\"PETSc's PreconditionJacobi has no support for a damping parameter.\\\"));\\n\\u00a0 #else\\n\\u00a0           smoother.initialize(mg_matrix, settings.smoother_dampen);\\n\\u00a0 #endif\\n\\u00a0 \\n\\u00a0           smoother.set_steps(settings.smoother_steps);\\n\\u00a0 \\n\\u00a0           mg::Matrix<VectorType> mg_m(mg_matrix);\\n\\u00a0           mg::Matrix<VectorType> mg_in(mg_interface_in);\\n\\u00a0           mg::Matrix<VectorType> mg_out(mg_interface_in);\\n\\u00a0 \\n\\u00a0           Multigrid<VectorType> mg(\\n\\u00a0             mg_m, coarse_grid_solver, mg_transfer, smoother, smoother);\\n\\u00a0           mg.set_edge_matrices(mg_out, mg_in);\\n\\u00a0 \\n\\u00a0 \\n\\u00a0           PreconditionMG<dim, VectorType, MGTransferPrebuilt<VectorType>>\\n\\u00a0             preconditioner(dof_handler, mg, mg_transfer);\\n\\u00a0 \\n\\u00a0           computing_timer.leave_subsection(\\\"Solve: Preconditioner setup\\\");\\n\\u00a0 \\nMGTransferPrebuiltDefinition mg_transfer.h:628\\nAssert#define Assert(cond, exc)Definition exceptions.h:1638\\nTiming for 1 V-cycle.\\n\\u00a0           {\\n\\u00a0             TimerOutput::Scope timing(computing_timer,\\n\\u00a0                                       \\\"Solve: 1 multigrid V-cycle\\\");\\n\\u00a0             preconditioner.vmult(solution, right_hand_side);\\n\\u00a0           }\\n\\u00a0           solution = 0.;\\n\\u00a0 \\nSolve the linear system and distribute constraints.\\n\\u00a0           {\\n\\u00a0             SolverCG<VectorType> solver(solver_control);\\n\\u00a0 \\n\\u00a0             TimerOutput::Scope timing(computing_timer, \\\"Solve: CG\\\");\\n\\u00a0             solver.solve(system_matrix,\\n\\u00a0                          solution,\\n\\u00a0                          right_hand_side,\\n\\u00a0                          preconditioner);\\n\\u00a0           }\\n\\u00a0 \\n\\u00a0           constraints.distribute(solution);\\n\\u00a0 \\n\\u00a0           break;\\n\\u00a0         }\\n\\u00a0 \\nSolver for the AMG method, similar to step-40.\\n\\u00a0       case Settings::amg:\\n\\u00a0         {\\n\\u00a0           computing_timer.enter_subsection(\\\"Solve: Preconditioner setup\\\");\\n\\u00a0 \\n\\u00a0           PreconditionAMG                 preconditioner;\\n\\u00a0           PreconditionAMG::AdditionalData Amg_data;\\n\\u00a0 \\n\\u00a0 #ifdef USE_PETSC_LA\\n\\u00a0           Amg_data.symmetric_operator = true;\\n\\u00a0 #else\\n\\u00a0           Amg_data.elliptic              = true;\\n\\u00a0           Amg_data.smoother_type         = \\\"Jacobi\\\";\\n\\u00a0           Amg_data.higher_order_elements = true;\\n\\u00a0           Amg_data.smoother_sweeps       = settings.smoother_steps;\\n\\u00a0           Amg_data.aggregation_threshold = 0.02;\\n\\u00a0 #endif\\n\\u00a0 \\n\\u00a0           Amg_data.output_details = false;\\n\\u00a0 \\n\\u00a0           preconditioner.initialize(system_matrix, Amg_data);\\n\\u00a0           computing_timer.leave_subsection(\\\"Solve: Preconditioner setup\\\");\\n\\u00a0 \\nTiming for 1 V-cycle.\\n\\u00a0           {\\n\\u00a0             TimerOutput::Scope timing(computing_timer,\\n\\u00a0                                       \\\"Solve: 1 multigrid V-cycle\\\");\\n\\u00a0             preconditioner.vmult(solution, right_hand_side);\\n\\u00a0           }\\n\\u00a0           solution = 0.;\\n\\u00a0 \\nSolve the linear system and distribute constraints.\\n\\u00a0           {\\n\\u00a0             SolverCG<VectorType> solver(solver_control);\\n\\u00a0 \\n\\u00a0             TimerOutput::Scope timing(computing_timer, \\\"Solve: CG\\\");\\n\\u00a0             solver.solve(system_matrix,\\n\\u00a0                          solution,\\n\\u00a0                          right_hand_side,\\n\\u00a0                          preconditioner);\\n\\u00a0           }\\n\\u00a0           constraints.distribute(solution);\\n\\u00a0 \\n\\u00a0           break;\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       default:\\n\\u00a0         DEAL_II_ASSERT_UNREACHABLE();\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   pcout << \\\"   Number of CG iterations:      \\\" << solver_control.last_step()\\n\\u00a0         << std::endl;\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\nDEAL_II_ASSERT_UNREACHABLE#define DEAL_II_ASSERT_UNREACHABLE()Definition exceptions.h:1897\\n The error estimator\\nWe use the FEInterfaceValues class to assemble an error estimator to decide which cells to refine. See the exact definition of the cell and face integrals in the introduction. To use the method, we define Scratch and Copy objects for the MeshWorker::mesh_loop() with much of the following code being in essence as was set up in step-12 already (or at least similar in spirit).\\n\\u00a0 template <int dim>\\n\\u00a0 struct ScratchData\\n\\u00a0 {\\n\\u00a0   ScratchData(const Mapping<dim>       &mapping,\\n\\u00a0               const FiniteElement<dim> &fe,\\n\\u00a0               const unsigned int        quadrature_degree,\\n\\u00a0               const UpdateFlags         update_flags,\\n\\u00a0               const UpdateFlags         interface_update_flags)\\n\\u00a0     : fe_values(mapping, fe, QGauss<dim>(quadrature_degree), update_flags)\\n\\u00a0     , fe_interface_values(mapping,\\n\\u00a0                           fe,\\n\\u00a0                           QGauss<dim - 1>(quadrature_degree),\\n\\u00a0                           interface_update_flags)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0 \\n\\u00a0   ScratchData(const ScratchData<dim> &scratch_data)\\n\\u00a0     : fe_values(scratch_data.fe_values.get_mapping(),\\n\\u00a0                 scratch_data.fe_values.get_fe(),\\n\\u00a0                 scratch_data.fe_values.get_quadrature(),\\n\\u00a0                 scratch_data.fe_values.get_update_flags())\\n\\u00a0     , fe_interface_values(scratch_data.fe_values.get_mapping(),\\n\\u00a0                           scratch_data.fe_values.get_fe(),\\n\\u00a0                           scratch_data.fe_interface_values.get_quadrature(),\\n\\u00a0                           scratch_data.fe_interface_values.get_update_flags())\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0   FEValues<dim>          fe_values;\\n\\u00a0   FEInterfaceValues<dim> fe_interface_values;\\n\\u00a0 };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 \\n\\u00a0 struct CopyData\\n\\u00a0 {\\n\\u00a0   CopyData()\\n\\u00a0     : cell_index(numbers::invalid_unsigned_int)\\n\\u00a0     , value(0.)\\n\\u00a0   {}\\n\\u00a0 \\n\\u00a0   struct FaceData\\n\\u00a0   {\\n\\u00a0     unsigned int cell_indices[2];\\n\\u00a0     double       values[2];\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0   unsigned int cell_index;\\n\\u00a0   double value;\\n\\u00a0   std::vector<FaceData> face_data;\\n\\u00a0 };\\n\\u00a0 \\n\\u00a0 \\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::estimate()\\n\\u00a0 {\\n\\u00a0   TimerOutput::Scope timing(computing_timer, \\\"Estimate\\\");\\n\\u00a0 \\n\\u00a0   VectorType temp_solution;\\n\\u00a0   temp_solution.reinit(locally_owned_dofs,\\n\\u00a0                        locally_relevant_dofs,\\n\\u00a0                        mpi_communicator);\\n\\u00a0   temp_solution = solution;\\n\\u00a0 \\n\\u00a0   const Coefficient<dim> coefficient;\\n\\u00a0 \\n\\u00a0   estimated_error_square_per_cell.reinit(triangulation.n_active_cells());\\n\\u00a0 \\n\\u00a0   using Iterator = typename DoFHandler<dim>::active_cell_iterator;\\n\\u00a0 \\nFEInterfaceValuesDefinition fe_interface_values.h:1277\\nFiniteElementDefinition fe.h:655\\nMappingAbstract base class for mapping classes.Definition mapping.h:318\\nTriangulation::n_active_cellsunsigned int n_active_cells() const\\ncell_indexunsigned int cell_indexDefinition grid_tools_topology.cc:783\\nDoFHandler::active_cell_iteratortypename ActiveSelector::active_cell_iterator active_cell_iteratorDefinition dof_handler.h:440\\nUpdateFlagsUpdateFlagsDefinition fe_update_flags.h:64\\nnumbersDefinition numbers.h:230\\nAssembler for cell residual \\\\(h^2 \\\\| f + \\\\epsilon \\\\triangle u \\\\|_K^2\\\\)\\n\\u00a0   auto cell_worker = [&](const Iterator   &cell,\\n\\u00a0                          ScratchData<dim> &scratch_data,\\n\\u00a0                          CopyData         &copy_data) {\\n\\u00a0     FEValues<dim> &fe_values = scratch_data.fe_values;\\n\\u00a0     fe_values.reinit(cell);\\n\\u00a0 \\n\\u00a0     RightHandSide<dim> rhs;\\n\\u00a0     const double       rhs_value = rhs.value(cell->center());\\n\\u00a0 \\n\\u00a0     const double nu = coefficient.value(cell->center());\\n\\u00a0 \\n\\u00a0     std::vector<Tensor<2, dim>> hessians(fe_values.n_quadrature_points);\\n\\u00a0     fe_values.get_function_hessians(temp_solution, hessians);\\n\\u00a0 \\n\\u00a0     copy_data.cell_index = cell->active_cell_index();\\n\\u00a0 \\n\\u00a0     double residual_norm_square = 0.;\\n\\u00a0     for (unsigned k = 0; k < fe_values.n_quadrature_points; ++k)\\n\\u00a0       {\\n\\u00a0         const double residual = (rhs_value + nu * trace(hessians[k]));\\n\\u00a0         residual_norm_square += residual * residual * fe_values.JxW(k);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     copy_data.value =\\n\\u00a0       cell->diameter() * cell->diameter() * residual_norm_square;\\n\\u00a0   };\\n\\u00a0 \\nFEValues::reinitvoid reinit(const TriaIterator< DoFCellAccessor< dim, spacedim, level_dof_access > > &cell)\\ntraceDEAL_II_HOST constexpr Number trace(const SymmetricTensor< 2, dim2, Number > &)\\nAssembler for face term  \\\\(\\\\sum_F h_F \\\\| \\\\jump{\\\\epsilon \\\\nabla u \\\\cdot n}\\n   \\\\|_F^2\\\\)\\n\\u00a0   auto face_worker = [&](const Iterator     &cell,\\n\\u00a0                          const unsigned int &f,\\n\\u00a0                          const unsigned int &sf,\\n\\u00a0                          const Iterator     &ncell,\\n\\u00a0                          const unsigned int &nf,\\n\\u00a0                          const unsigned int &nsf,\\n\\u00a0                          ScratchData<dim>   &scratch_data,\\n\\u00a0                          CopyData           &copy_data) {\\n\\u00a0     FEInterfaceValues<dim> &fe_interface_values =\\n\\u00a0       scratch_data.fe_interface_values;\\n\\u00a0     fe_interface_values.reinit(cell, f, sf, ncell, nf, nsf);\\n\\u00a0 \\n\\u00a0     copy_data.face_data.emplace_back();\\n\\u00a0     CopyData::FaceData &copy_data_face = copy_data.face_data.back();\\n\\u00a0 \\n\\u00a0     copy_data_face.cell_indices[0] = cell->active_cell_index();\\n\\u00a0     copy_data_face.cell_indices[1] = ncell->active_cell_index();\\n\\u00a0 \\n\\u00a0     const double coeff1 = coefficient.value(cell->center());\\n\\u00a0     const double coeff2 = coefficient.value(ncell->center());\\n\\u00a0 \\n\\u00a0     std::vector<Tensor<1, dim>> grad_u[2];\\n\\u00a0 \\n\\u00a0     for (unsigned int i = 0; i < 2; ++i)\\n\\u00a0       {\\n\\u00a0         grad_u[i].resize(fe_interface_values.n_quadrature_points);\\n\\u00a0         fe_interface_values.get_fe_face_values(i).get_function_gradients(\\n\\u00a0           temp_solution, grad_u[i]);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     double jump_norm_square = 0.;\\n\\u00a0 \\n\\u00a0     for (unsigned int qpoint = 0;\\n\\u00a0          qpoint < fe_interface_values.n_quadrature_points;\\n\\u00a0          ++qpoint)\\n\\u00a0       {\\n\\u00a0         const double jump =\\n\\u00a0           coeff1 * grad_u[0][qpoint] * fe_interface_values.normal(qpoint) -\\n\\u00a0           coeff2 * grad_u[1][qpoint] * fe_interface_values.normal(qpoint);\\n\\u00a0 \\n\\u00a0         jump_norm_square += jump * jump * fe_interface_values.JxW(qpoint);\\n\\u00a0       }\\n\\u00a0 \\n\\u00a0     const double h           = cell->face(f)->measure();\\n\\u00a0     copy_data_face.values[0] = 0.5 * h * jump_norm_square;\\n\\u00a0     copy_data_face.values[1] = copy_data_face.values[0];\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0   auto copier = [&](const CopyData &copy_data) {\\n\\u00a0     if (copy_data.cell_index != numbers::invalid_unsigned_int)\\n\\u00a0       estimated_error_square_per_cell[copy_data.cell_index] += copy_data.value;\\n\\u00a0 \\n\\u00a0     for (const auto &cdf : copy_data.face_data)\\n\\u00a0       for (unsigned int j = 0; j < 2; ++j)\\n\\u00a0         estimated_error_square_per_cell[cdf.cell_indices[j]] += cdf.values[j];\\n\\u00a0   };\\n\\u00a0 \\n\\u00a0   const unsigned int n_gauss_points = degree + 1;\\n\\u00a0   ScratchData<dim>   scratch_data(mapping,\\n\\u00a0                                 fe,\\n\\u00a0                                 n_gauss_points,\\n\\u00a0                                 update_hessians | update_quadrature_points |\\n\\u00a0                                   update_JxW_values,\\n\\u00a0                                 update_values | update_gradients |\\n\\u00a0                                   update_JxW_values | update_normal_vectors);\\n\\u00a0   CopyData           copy_data;\\n\\u00a0 \\nFEInterfaceValues::reinitvoid reinit(const CellIteratorType &cell, const unsigned int face_no, const unsigned int sub_face_no, const CellNeighborIteratorType &cell_neighbor, const unsigned int face_no_neighbor, const unsigned int sub_face_no_neighbor, const unsigned int q_index=numbers::invalid_unsigned_int, const unsigned int mapping_index=numbers::invalid_unsigned_int, const unsigned int fe_index=numbers::invalid_unsigned_int, const unsigned int fe_index_neighbor=numbers::invalid_unsigned_int)\\nint\\nupdate_hessians@ update_hessiansSecond derivatives of shape functions.Definition fe_update_flags.h:87\\nupdate_normal_vectors@ update_normal_vectorsNormal vectors.Definition fe_update_flags.h:141\\nnumbers::invalid_unsigned_intstatic const unsigned int invalid_unsigned_intDefinition types.h:220\\nWe need to assemble each interior face once but we need to make sure that both processes assemble the face term between a locally owned and a ghost cell. This is achieved by setting the MeshWorker::assemble_ghost_faces_both flag. We need to do this, because we do not communicate the error estimator contributions here.\\n\\u00a0   MeshWorker::mesh_loop(dof_handler.begin_active(),\\n\\u00a0                         dof_handler.end(),\\n\\u00a0                         cell_worker,\\n\\u00a0                         copier,\\n\\u00a0                         scratch_data,\\n\\u00a0                         copy_data,\\n\\u00a0                         MeshWorker::assemble_own_cells |\\n\\u00a0                           MeshWorker::assemble_ghost_faces_both |\\n\\u00a0                           MeshWorker::assemble_own_interior_faces_once,\\n\\u00a0                         /*boundary_worker=*/nullptr,\\n\\u00a0                         face_worker);\\n\\u00a0 \\n\\u00a0   const double global_error_estimate =\\n\\u00a0     std::sqrt(Utilities::MPI::sum(estimated_error_square_per_cell.l1_norm(),\\n\\u00a0                                   mpi_communicator));\\n\\u00a0   pcout << \\\"   Global error estimate:        \\\" << global_error_estimate\\n\\u00a0         << std::endl;\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\nMeshWorker::mesh_loopvoid mesh_loop(const CellIteratorType &begin, const CellIteratorType &end, const CellWorkerFunctionType &cell_worker, const CopierType &copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const AssembleFlags flags=assemble_own_cells, const BoundaryWorkerFunctionType &boundary_worker=BoundaryWorkerFunctionType(), const FaceWorkerFunctionType &face_worker=FaceWorkerFunctionType(), const unsigned int queue_length=2 *MultithreadInfo::n_threads(), const unsigned int chunk_size=8)Definition mesh_loop.h:281\\nMeshWorker::assemble_own_cells@ assemble_own_cellsDefinition assemble_flags.h:49\\nMeshWorker::assemble_own_interior_faces_once@ assemble_own_interior_faces_onceDefinition assemble_flags.h:58\\nMeshWorker::assemble_ghost_faces_both@ assemble_ghost_faces_bothDefinition assemble_flags.h:76\\nUtilities::MPI::sumT sum(const T &t, const MPI_Comm mpi_communicator)\\nstd::sqrt::VectorizedArray< Number, width > sqrt(const ::VectorizedArray< Number, width > &)Definition vectorization.h:6869\\n LaplaceProblem::refine_grid()\\nWe use the cell-wise estimator stored in the vector estimate_vector and refine a fixed number of cells (chosen here to roughly double the number of DoFs in each step).\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::refine_grid()\\n\\u00a0 {\\n\\u00a0   TimerOutput::Scope timing(computing_timer, \\\"Refine grid\\\");\\n\\u00a0 \\n\\u00a0   const double refinement_fraction = 1. / (std::pow(2.0, dim) - 1.);\\n\\u00a0   parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number(\\n\\u00a0     triangulation, estimated_error_square_per_cell, refinement_fraction, 0.0);\\n\\u00a0 \\n\\u00a0   triangulation.execute_coarsening_and_refinement();\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\nparallel::distributed::Triangulation::execute_coarsening_and_refinementvirtual void execute_coarsening_and_refinement() overrideDefinition tria.cc:3320\\nparallel::distributed::GridRefinement::refine_and_coarsen_fixed_numbervoid refine_and_coarsen_fixed_number(::Triangulation< dim, spacedim > &tria, const ::Vector< Number > &criteria, const double top_fraction_of_cells, const double bottom_fraction_of_cells, const types::global_cell_index max_n_cells=std::numeric_limits< types::global_cell_index >::max())Definition grid_refinement.cc:503\\nstd::pow::VectorizedArray< Number, width > pow(const ::VectorizedArray< Number, width > &, const Number p)Definition vectorization.h:6885\\n LaplaceProblem::output_results()\\nThe output_results() function is similar to the ones found in many of the tutorials (see step-40 for example).\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::output_results(const unsigned int cycle)\\n\\u00a0 {\\n\\u00a0   TimerOutput::Scope timing(computing_timer, \\\"Output results\\\");\\n\\u00a0 \\n\\u00a0   VectorType temp_solution;\\n\\u00a0   temp_solution.reinit(locally_owned_dofs,\\n\\u00a0                        locally_relevant_dofs,\\n\\u00a0                        mpi_communicator);\\n\\u00a0   temp_solution = solution;\\n\\u00a0 \\n\\u00a0   DataOut<dim> data_out;\\n\\u00a0   data_out.attach_dof_handler(dof_handler);\\n\\u00a0   data_out.add_data_vector(temp_solution, \\\"solution\\\");\\n\\u00a0 \\n\\u00a0   Vector<float> subdomain(triangulation.n_active_cells());\\n\\u00a0   for (unsigned int i = 0; i < subdomain.size(); ++i)\\n\\u00a0     subdomain(i) = triangulation.locally_owned_subdomain();\\n\\u00a0   data_out.add_data_vector(subdomain, \\\"subdomain\\\");\\n\\u00a0 \\n\\u00a0   Vector<float> level(triangulation.n_active_cells());\\n\\u00a0   for (const auto &cell : triangulation.active_cell_iterators())\\n\\u00a0     level(cell->active_cell_index()) = cell->level();\\n\\u00a0   data_out.add_data_vector(level, \\\"level\\\");\\n\\u00a0 \\n\\u00a0   if (estimated_error_square_per_cell.size() > 0)\\n\\u00a0     data_out.add_data_vector(estimated_error_square_per_cell,\\n\\u00a0                              \\\"estimated_error_square_per_cell\\\");\\n\\u00a0 \\n\\u00a0   data_out.build_patches();\\n\\u00a0 \\n\\u00a0   const std::string pvtu_filename = data_out.write_vtu_with_pvtu_record(\\n\\u00a0     \\\"\\\", \\\"solution\\\", cycle, mpi_communicator, 2 /*n_digits*/, 1 /*n_groups*/);\\n\\u00a0 \\n\\u00a0   pcout << \\\"   Wrote \\\" << pvtu_filename << std::endl;\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\nDataOut_DoFData::attach_dof_handlervoid attach_dof_handler(const DoFHandler< dim, spacedim > &)\\nDataOutDefinition data_out.h:147\\nparallel::TriangulationBase::locally_owned_subdomaintypes::subdomain_id locally_owned_subdomain() const overrideDefinition tria_base.cc:345\\n LaplaceProblem::run()\\nAs in most tutorials, this function calls the various functions defined above to set up, assemble, solve, and output the results.\\n\\u00a0 template <int dim, int degree>\\n\\u00a0 void LaplaceProblem<dim, degree>::run()\\n\\u00a0 {\\n\\u00a0   for (unsigned int cycle = 0; cycle < settings.n_steps; ++cycle)\\n\\u00a0     {\\n\\u00a0       pcout << \\\"Cycle \\\" << cycle << ':' << std::endl;\\n\\u00a0       if (cycle > 0)\\n\\u00a0         refine_grid();\\n\\u00a0 \\n\\u00a0       pcout << \\\"   Number of active cells:       \\\"\\n\\u00a0             << triangulation.n_global_active_cells();\\n\\u00a0 \\nparallel::TriangulationBase::n_global_active_cellsvirtual types::global_cell_index n_global_active_cells() const overrideDefinition tria_base.cc:151\\nWe only output level cell data for the GMG methods (same with DoF data below). Note that the partition efficiency is irrelevant for AMG since the level hierarchy is not distributed or used during the computation.\\n\\u00a0       if (settings.solver == Settings::gmg_mf ||\\n\\u00a0           settings.solver == Settings::gmg_mb)\\n\\u00a0         pcout << \\\" (\\\" << triangulation.n_global_levels() << \\\" global levels)\\\"\\n\\u00a0               << std::endl\\n\\u00a0               << \\\"   Partition efficiency:         \\\"\\n\\u00a0               << 1.0 / MGTools::workload_imbalance(triangulation);\\n\\u00a0       pcout << std::endl;\\n\\u00a0 \\n\\u00a0       setup_system();\\n\\u00a0 \\nMGTools::workload_imbalancedouble workload_imbalance(const Triangulation< dim, spacedim > &tria)Definition mg_tools.cc:1674\\nOnly set up the multilevel hierarchy for GMG.\\n\\u00a0       if (settings.solver == Settings::gmg_mf ||\\n\\u00a0           settings.solver == Settings::gmg_mb)\\n\\u00a0         setup_multigrid();\\n\\u00a0 \\n\\u00a0       pcout << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs();\\n\\u00a0       if (settings.solver == Settings::gmg_mf ||\\n\\u00a0           settings.solver == Settings::gmg_mb)\\n\\u00a0         {\\n\\u00a0           pcout << \\\" (by level: \\\";\\n\\u00a0           for (unsigned int level = 0; level < triangulation.n_global_levels();\\n\\u00a0                ++level)\\n\\u00a0             pcout << dof_handler.n_dofs(level)\\n\\u00a0                   << (level == triangulation.n_global_levels() - 1 ? \\\")\\\" :\\n\\u00a0                                                                      \\\", \\\");\\n\\u00a0         }\\n\\u00a0       pcout << std::endl;\\n\\u00a0 \\nFor the matrix-free method, we only assemble the right-hand side. For both matrix-based methods, we assemble both active matrix and right-hand side, and only assemble the multigrid matrices for matrix-based GMG.\\n\\u00a0       if (settings.solver == Settings::gmg_mf)\\n\\u00a0         assemble_rhs();\\n\\u00a0       else /*gmg_mb or amg*/\\n\\u00a0         {\\n\\u00a0           assemble_system();\\n\\u00a0           if (settings.solver == Settings::gmg_mb)\\n\\u00a0             assemble_multigrid();\\n\\u00a0         }\\n\\u00a0 \\n\\u00a0       solve();\\n\\u00a0       estimate();\\n\\u00a0 \\n\\u00a0       if (settings.output)\\n\\u00a0         output_results(cycle);\\n\\u00a0 \\n\\u00a0       computing_timer.print_summary();\\n\\u00a0       computing_timer.reset();\\n\\u00a0     }\\n\\u00a0 }\\n\\u00a0 \\n\\u00a0 \\n The main() function\\nThis is a similar main function to step-40, with the exception that we require the user to pass a .prm file as a sole command line argument (see step-29 and the documentation of the ParameterHandler class for a complete discussion of parameter files).\\n\\u00a0 int main(int argc, char *argv[])\\n\\u00a0 {\\n\\u00a0   using namespace dealii;\\n\\u00a0   Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n\\u00a0 \\n\\u00a0   Settings settings;\\n\\u00a0   if (!settings.try_parse((argc > 1) ? (argv[1]) : \\\"\\\"))\\n\\u00a0     return 0;\\n\\u00a0 \\n\\u00a0   try\\n\\u00a0     {\\n\\u00a0       constexpr unsigned int fe_degree = 2;\\n\\u00a0 \\n\\u00a0       switch (settings.dimension)\\n\\u00a0         {\\n\\u00a0           case 2:\\n\\u00a0             {\\n\\u00a0               LaplaceProblem<2, fe_degree> test(settings);\\n\\u00a0               test.run();\\n\\u00a0 \\n\\u00a0               break;\\n\\u00a0             }\\n\\u00a0 \\n\\u00a0           case 3:\\n\\u00a0             {\\n\\u00a0               LaplaceProblem<3, fe_degree> test(settings);\\n\\u00a0               test.run();\\n\\u00a0 \\n\\u00a0               break;\\n\\u00a0             }\\n\\u00a0 \\n\\u00a0           default:\\n\\u00a0             DEAL_II_NOT_IMPLEMENTED();\\n\\u00a0         }\\n\\u00a0     }\\n\\u00a0   catch (std::exception &exc)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Exception on processing: \\\" << std::endl\\n\\u00a0                 << exc.what() << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       MPI_Abort(MPI_COMM_WORLD, 1);\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0   catch (...)\\n\\u00a0     {\\n\\u00a0       std::cerr << std::endl\\n\\u00a0                 << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       std::cerr << \\\"Unknown exception!\\\" << std::endl\\n\\u00a0                 << \\\"Aborting!\\\" << std::endl\\n\\u00a0                 << \\\"----------------------------------------------------\\\"\\n\\u00a0                 << std::endl;\\n\\u00a0       MPI_Abort(MPI_COMM_WORLD, 2);\\n\\u00a0       return 1;\\n\\u00a0     }\\n\\u00a0 \\n\\u00a0   return 0;\\n\\u00a0 }\\nUtilities::MPI::MPI_InitFinalizeDefinition mpi.h:1081\\n Results\\nWhen you run the program using the following command mpirun -np 16 ./step-50  gmg_mf_2d.prm\\n the screen output should look like the following: Cycle 0:\\n   Number of active cells:       12 (2 global levels)\\n   Partition efficiency:         0.1875\\n   Number of degrees of freedom: 65 (by level: 21, 65)\\n   Number of CG iterations:      10\\n   Global error estimate:        0.355373\\n   Wrote solution_00.pvtu\\n \\n \\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |    0.0163s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| Assemble right-hand side        |         1 |  0.000374s |       2.3% |\\n| Estimate                        |         1 |  0.000724s |       4.4% |\\n| Output results                  |         1 |   0.00277s |        17% |\\n| Setup                           |         1 |   0.00225s |        14% |\\n| Setup multigrid                 |         1 |   0.00181s |        11% |\\n| Solve                           |         1 |   0.00364s |        22% |\\n| Solve: 1 multigrid V-cycle      |         1 |  0.000354s |       2.2% |\\n| Solve: CG                       |         1 |   0.00151s |       9.3% |\\n| Solve: Preconditioner setup     |         1 |   0.00125s |       7.7% |\\n+---------------------------------+-----------+------------+------------+\\n \\nCycle 1:\\n   Number of active cells:       24 (3 global levels)\\n   Partition efficiency:         0.276786\\n   Number of degrees of freedom: 139 (by level: 21, 65, 99)\\n   Number of CG iterations:      10\\n   Global error estimate:        0.216726\\n   Wrote solution_01.pvtu\\n \\n \\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |    0.0169s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| Assemble right-hand side        |         1 |  0.000309s |       1.8% |\\n| Estimate                        |         1 |   0.00156s |       9.2% |\\n| Output results                  |         1 |   0.00222s |        13% |\\n| Refine grid                     |         1 |   0.00278s |        16% |\\n| Setup                           |         1 |   0.00196s |        12% |\\n| Setup multigrid                 |         1 |    0.0023s |        14% |\\n| Solve                           |         1 |   0.00565s |        33% |\\n| Solve: 1 multigrid V-cycle      |         1 |  0.000349s |       2.1% |\\n| Solve: CG                       |         1 |   0.00285s |        17% |\\n| Solve: Preconditioner setup     |         1 |   0.00195s |        12% |\\n+---------------------------------+-----------+------------+------------+\\n \\nCycle 2:\\n   Number of active cells:       51 (4 global levels)\\n   Partition efficiency:         0.41875\\n   Number of degrees of freedom: 245 (by level: 21, 65, 225, 25)\\n   Number of CG iterations:      11\\n   Global error estimate:        0.112098\\n   Wrote solution_02.pvtu\\n \\n \\n+---------------------------------------------+------------+------------+\\n| Total wallclock time elapsed since start    |    0.0183s |            |\\n|                                             |            |            |\\n| Section                         | no. calls |  wall time | % of total |\\n+---------------------------------+-----------+------------+------------+\\n| Assemble right-hand side        |         1 |  0.000274s |       1.5% |\\n| Estimate                        |         1 |   0.00127s |       6.9% |\\n| Output results                  |         1 |   0.00227s |        12% |\\n| Refine grid                     |         1 |    0.0024s |        13% |\\n| Setup                           |         1 |   0.00191s |        10% |\\n| Setup multigrid                 |         1 |   0.00295s |        16% |\\n| Solve                           |         1 |   0.00702s |        38% |\\n| Solve: 1 multigrid V-cycle      |         1 |  0.000398s |       2.2% |\\n| Solve: CG                       |         1 |   0.00376s |        21% |\\n| Solve: Preconditioner setup     |         1 |   0.00238s |        13% |\\n+---------------------------------+-----------+------------+------------+\\n.\\n.\\n.\\n Here, the timing of the solve() function is split up in 3 parts: setting up the multigrid preconditioner, execution of a single multigrid V-cycle, and the CG solver. The V-cycle that is timed is unnecessary for the overall solve and only meant to give an insight at the different costs for AMG and GMG. Also it should be noted that when using the AMG solver, \\\"Workload imbalance\\\" is not included in the output since the hierarchy of coarse meshes is not required.\\nAll results in this section are gathered on Intel Xeon Platinum 8280 (Cascade Lake) nodes which have 56 cores and 192GB per node and support AVX-512 instructions, allowing for vectorization over 8 doubles (vectorization used only in the matrix-free computations). The code is compiled using gcc 7.1.0 with intel-mpi 17.0.3. Trilinos 12.10.1 is used for the matrix-based GMG/AMG computations.\\nWe can then gather a variety of information by calling the program with the input files that are provided in the directory in which step-50 is located. Using these, and adjusting the number of mesh refinement steps, we can produce information about how well the program scales.\\nThe following table gives weak scaling timings for this program on up to 256M DoFs and 7,168 processors. (Recall that weak scaling keeps the number of degrees of freedom per processor constant while increasing the number of processors; i.e., it considers larger and larger problems.) Here, \\\\(\\\\mathbb{E}\\\\) is the partition efficiency from the introduction (also equal to 1.0/workload imbalance), \\\"Setup\\\" is a combination of setup, setup multigrid, assemble, and assemble multigrid from the timing blocks, and \\\"Prec\\\" is the preconditioner setup. Ideally all times would stay constant over each problem size for the individual solvers, but since the partition efficiency decreases from 0.371 to 0.161 from largest to smallest problem size, we expect to see an approximately \\\\(0.371/0.161=2.3\\\\) times increase in timings for GMG. This is, in fact, pretty close to what we really get:\\n\\n\\nMF-GMG MB-GMG AMG  \\n\\nProcs Cycle DoFs \\\\(\\\\mathbb{E}\\\\) Setup Prec Solve Total Setup Prec Solve Total Setup Prec Solve Total  \\n\\n112 13 4M 0.37 0.742 0.393 0.200 1.335 1.714 2.934 0.716 5.364 1.544 0.456 1.150 3.150  \\n\\n448 15 16M 0.29 0.884 0.535 0.253 1.672 1.927 3.776 1.190 6.893 1.544 0.456 1.150 3.150  \\n\\n1,792 17 65M 0.22 1.122 0.686 0.309 2.117 2.171 4.862 1.660 8.693 1.654 0.546 1.460 3.660  \\n\\n7,168 19 256M 0.16 1.214 0.893 0.521 2.628 2.386 7.260 2.560 12.206 1.844 1.010 1.890 4.744  \\n\\nOn the other hand, the algebraic multigrid in the last set of columns is relatively unaffected by the increasing imbalance of the mesh hierarchy (because it doesn't use the mesh hierarchy) and the growth in time is rather driven by other factors that are well documented in the literature (most notably that the algorithmic complexity of some parts of algebraic multigrid methods appears to be  \\\\({\\\\cal O}(N\\n\\\\log N)\\\\) instead of \\\\({\\\\cal O}(N)\\\\) for geometric multigrid).\\nThe upshort of the table above is that the matrix-free geometric multigrid method appears to be the fastest approach to solving this equation if not by a huge margin. Matrix-based methods, on the other hand, are consistently the worst.\\nThe following figure provides strong scaling results for each method, i.e., we solve the same problem on more and more processors. Specifically, we consider the problems after 16 mesh refinement cycles (32M DoFs) and 19 cycles (256M DoFs), on between 56 to 28,672 processors:\\n\\nWhile the matrix-based GMG solver and AMG scale similarly and have a similar time to solution (at least as long as there is a substantial number of unknowns per processor \\u2013 say, several 10,000), the matrix-free GMG solver scales much better and solves the finer problem in roughly the same time as the AMG solver for the coarser mesh with only an eighth of the number of processors. Conversely, it can solve the same problem on the same number of processors in about one eighth the time.\\nPossibilities for extensions \\nTesting convergence and higher order elements \\nThe finite element degree is currently hard-coded as 2, see the template arguments of the main class. It is easy to change. To test, it would be interesting to switch to a test problem with a reference solution. This way, you can compare error rates.\\nCoarse solver \\nA more interesting example would involve a more complicated coarse mesh (see step-49 for inspiration). The issue in that case is that the coarsest level of the mesh hierarchy is actually quite large, and one would have to think about ways to solve the coarse level problem efficiently. (This is not an issue for algebraic multigrid methods because they would just continue to build coarser and coarser levels of the matrix, regardless of their geometric origin.)\\nIn the program here, we simply solve the coarse level problem with a Conjugate Gradient method without any preconditioner. That is acceptable if the coarse problem is really small \\u2013 for example, if the coarse mesh had a single cell, then the coarse mesh problems has a \\\\(9\\\\times 9\\\\) matrix in 2d, and a \\\\(27\\\\times 27\\\\) matrix in 3d; for the coarse mesh we use on the \\\\(L\\\\)-shaped domain of the current program, these sizes are \\\\(21\\\\times 21\\\\) in 2d and \\\\(117\\\\times 117\\\\) in 3d. But if the coarse mesh consists of hundreds or thousands of cells, this approach will no longer work and might start to dominate the overall run-time of each V-cycle. A common approach is then to solve the coarse mesh problem using an algebraic multigrid preconditioner; this would then, however, require assembling the coarse matrix (even for the matrix-free version) as input to the AMG implementation.\\n The plain program\\n/* ------------------------------------------------------------------------\\n *\\n * SPDX-License-Identifier: LGPL-2.1-or-later\\n * Copyright (C) 2014 - 2024 by the deal.II authors\\n *\\n * This file is part of the deal.II library.\\n *\\n * Part of the source code is dual licensed under Apache-2.0 WITH\\n * LLVM-exception OR LGPL-2.1-or-later. Detailed license information\\n * governing the source code and code contributions can be found in\\n * LICENSE.md and CONTRIBUTING.md at the top level directory of deal.II.\\n *\\n * ------------------------------------------------------------------------\\n *\\n * Authors: Thomas C. Clevenger, Clemson University\\n *          Timo Heister, Clemson University\\n *          Guido Kanschat, Heidelberg University\\n *          Martin Kronbichler, Technical University of Munich\\n */\\n \\n \\n \\n \\n#include <deal.II/base/conditional_ostream.h>\\n#include <deal.II/base/data_out_base.h>\\n#include <deal.II/base/index_set.h>\\n#include <deal.II/base/quadrature_lib.h>\\n#include <deal.II/base/timer.h>\\n#include <deal.II/base/parameter_handler.h>\\n#include <deal.II/distributed/grid_refinement.h>\\n#include <deal.II/distributed/tria.h>\\n#include <deal.II/dofs/dof_tools.h>\\n#include <deal.II/fe/fe_q.h>\\n#include <deal.II/fe/fe_values.h>\\n#include <deal.II/fe/mapping_q1.h>\\n#include <deal.II/grid/grid_generator.h>\\n#include <deal.II/grid/grid_refinement.h>\\n#include <deal.II/grid/tria.h>\\n#include <deal.II/lac/affine_constraints.h>\\n#include <deal.II/lac/dynamic_sparsity_pattern.h>\\n#include <deal.II/lac/sparsity_tools.h>\\n#include <deal.II/lac/solver_cg.h>\\n \\n \\n#include <deal.II/lac/generic_linear_algebra.h>\\n \\n#define FORCE_USE_OF_TRILINOS\\n \\nnamespace LA\\n{\\n#if defined(DEAL_II_WITH_PETSC) && !defined(DEAL_II_PETSC_WITH_COMPLEX) && \\\\\\n  !(defined(DEAL_II_WITH_TRILINOS) && defined(FORCE_USE_OF_TRILINOS))\\n using namespace dealii::LinearAlgebraPETSc;\\n#  define USE_PETSC_LA\\n#elif defined(DEAL_II_WITH_TRILINOS)\\n using namespace dealii::LinearAlgebraTrilinos;\\n#else\\n#  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required\\n#endif\\n} // namespace LA\\n \\n#include <deal.II/matrix_free/matrix_free.h>\\n#include <deal.II/matrix_free/operators.h>\\n#include <deal.II/matrix_free/fe_evaluation.h>\\n#include <deal.II/multigrid/mg_coarse.h>\\n#include <deal.II/multigrid/mg_constrained_dofs.h>\\n#include <deal.II/multigrid/mg_matrix.h>\\n#include <deal.II/multigrid/mg_smoother.h>\\n#include <deal.II/multigrid/mg_tools.h>\\n#include <deal.II/multigrid/mg_transfer.h>\\n#include <deal.II/multigrid/multigrid.h>\\n#include <deal.II/multigrid/mg_transfer_matrix_free.h>\\n#include <deal.II/numerics/data_out.h>\\n#include <deal.II/numerics/vector_tools.h>\\n \\n#include <deal.II/fe/fe_interface_values.h>\\n#include <deal.II/meshworker/mesh_loop.h>\\n \\nusing namespace dealii;\\n \\n \\n \\nnamespace ChangeVectorTypes\\n{\\n template <typename number>\\n void copy(LA::MPI::Vector                                  &out,\\n const LinearAlgebra::distributed::Vector<number> &in)\\n  {\\n LinearAlgebra::ReadWriteVector<double> rwv(out.locally_owned_elements());\\n    rwv.import_elements(in, VectorOperation::insert);\\n#ifdef USE_PETSC_LA\\n AssertThrow(false,\\n                ExcMessage(\\\"ChangeVectorTypes::copy() not implemented for \\\"\\n \\\"PETSc vector types.\\\"));\\n#else\\n    out.import_elements(rwv, VectorOperation::insert);\\n#endif\\n  }\\n \\n \\n \\n template <typename number>\\n void copy(LinearAlgebra::distributed::Vector<number> &out,\\n const LA::MPI::Vector                      &in)\\n  {\\n LinearAlgebra::ReadWriteVector<double> rwv;\\n#ifdef USE_PETSC_LA\\n    (void)in;\\n AssertThrow(false,\\n                ExcMessage(\\\"ChangeVectorTypes::copy() not implemented for \\\"\\n \\\"PETSc vector types.\\\"));\\n#else\\n    rwv.reinit(in);\\n#endif\\n    out.import_elements(rwv, VectorOperation::insert);\\n  }\\n} // namespace ChangeVectorTypes\\n \\n \\ntemplate <int dim>\\nclass RightHandSide : public Function<dim>\\n{\\npublic:\\n virtual double value(const Point<dim> & /*p*/,\\n const unsigned int /*component*/ = 0) const override\\n {\\n return 1.0;\\n  }\\n \\n \\n template <typename number>\\n VectorizedArray<number>\\n value(const Point<dim, VectorizedArray<number>> & /*p*/,\\n const unsigned int /*component*/ = 0) const\\n {\\n return VectorizedArray<number>(1.0);\\n  }\\n};\\n \\n \\ntemplate <int dim>\\nclass Coefficient : public Function<dim>\\n{\\npublic:\\n virtual double value(const Point<dim> &p,\\n const unsigned int /*component*/ = 0) const override;\\n \\n template <typename number>\\n VectorizedArray<number> value(const Point<dim, VectorizedArray<number>> &p,\\n const unsigned int /*component*/ = 0) const;\\n \\n template <typename number>\\n  number average_value(const std::vector<Point<dim, number>> &points) const;\\n \\n template <typename number>\\n  std::shared_ptr<Table<2, VectorizedArray<number>>> make_coefficient_table(\\n const MatrixFree<dim, number, VectorizedArray<number>> &mf_storage) const;\\n};\\n \\n \\n \\ntemplate <int dim>\\ndouble Coefficient<dim>::value(const Point<dim> &p, const unsigned int) const\\n{\\n for (int d = 0; d < dim; ++d)\\n    {\\n if (p[d] < -0.5)\\n return 100.0;\\n    }\\n return 1.0;\\n}\\n \\n \\n \\ntemplate <int dim>\\ntemplate <typename number>\\nVectorizedArray<number>\\nCoefficient<dim>::value(const Point<dim, VectorizedArray<number>> &p,\\n const unsigned int) const\\n{\\n VectorizedArray<number> return_value = VectorizedArray<number>(1.0);\\n for (unsigned int i = 0; i < VectorizedArray<number>::size(); ++i)\\n    {\\n for (int d = 0; d < dim; ++d)\\n if (p[d][i] < -0.5)\\n          {\\n            return_value[i] = 100.0;\\n break;\\n          }\\n    }\\n \\n return return_value;\\n}\\n \\n \\n \\ntemplate <int dim>\\ntemplate <typename number>\\nnumber Coefficient<dim>::average_value(\\n const std::vector<Point<dim, number>> &points) const\\n{\\n  number average(0);\\n for (unsigned int i = 0; i < points.size(); ++i)\\n    average += value(points[i]);\\n  average /= points.size();\\n \\n return average;\\n}\\n \\n \\n \\ntemplate <int dim>\\ntemplate <typename number>\\nstd::shared_ptr<Table<2, VectorizedArray<number>>>\\nCoefficient<dim>::make_coefficient_table(\\n const MatrixFree<dim, number, VectorizedArray<number>> &mf_storage) const\\n{\\n auto coefficient_table =\\n    std::make_shared<Table<2, VectorizedArray<number>>>();\\n \\n FEEvaluation<dim, -1, 0, 1, number> fe_eval(mf_storage);\\n \\n const unsigned int n_cells = mf_storage.n_cell_batches();\\n \\n  coefficient_table->reinit(n_cells, 1);\\n \\n for (unsigned int cell = 0; cell < n_cells; ++cell)\\n    {\\n      fe_eval.reinit(cell);\\n \\n VectorizedArray<number> average_value = 0.;\\n for (const unsigned int q : fe_eval.quadrature_point_indices())\\n        average_value += value(fe_eval.quadrature_point(q));\\n      average_value /= fe_eval.n_q_points;\\n \\n      (*coefficient_table)(cell, 0) = average_value;\\n    }\\n \\n return coefficient_table;\\n}\\n \\n \\n \\n \\nstruct Settings\\n{\\n bool try_parse(const std::string &prm_filename);\\n \\n enum SolverType\\n  {\\n    gmg_mb,\\n    gmg_mf,\\n    amg\\n  };\\n \\n  SolverType solver;\\n \\n int          dimension;\\n double       smoother_dampen;\\n unsigned int smoother_steps;\\n unsigned int n_steps;\\n bool         output;\\n};\\n \\n \\n \\nbool Settings::try_parse(const std::string &prm_filename)\\n{\\n ParameterHandler prm;\\n  prm.declare_entry(\\\"dim\\\", \\\"2\\\", Patterns::Integer(), \\\"The problem dimension.\\\");\\n  prm.declare_entry(\\\"n_steps\\\",\\n \\\"10\\\",\\n Patterns::Integer(0),\\n \\\"Number of adaptive refinement steps.\\\");\\n  prm.declare_entry(\\\"smoother dampen\\\",\\n \\\"1.0\\\",\\n Patterns::Double(0.0),\\n \\\"Dampen factor for the smoother.\\\");\\n  prm.declare_entry(\\\"smoother steps\\\",\\n \\\"1\\\",\\n Patterns::Integer(1),\\n \\\"Number of smoother steps.\\\");\\n  prm.declare_entry(\\\"solver\\\",\\n \\\"MF\\\",\\n Patterns::Selection(\\\"MF|MB|AMG\\\"),\\n \\\"Switch between matrix-free GMG, \\\"\\n \\\"matrix-based GMG, and AMG.\\\");\\n  prm.declare_entry(\\\"output\\\",\\n \\\"false\\\",\\n Patterns::Bool(),\\n \\\"Output graphical results.\\\");\\n \\n if (prm_filename.empty())\\n    {\\n      std::cout << \\\"****  Error: No input file provided!\\\\n\\\"\\n                << \\\"****  Error: Call this program as './step-50 input.prm\\\\n\\\"\\n                << '\\\\n'\\n                << \\\"****  You may want to use one of the input files in this\\\\n\\\"\\n                << \\\"****  directory, or use the following default values\\\\n\\\"\\n                << \\\"****  to create an input file:\\\\n\\\";\\n if (Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)\\n        prm.print_parameters(std::cout, ParameterHandler::Text);\\n return false;\\n    }\\n \\n try\\n    {\\n      prm.parse_input(prm_filename);\\n    }\\n catch (std::exception &e)\\n    {\\n if (Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)\\n        std::cerr << e.what() << std::endl;\\n return false;\\n    }\\n \\n if (prm.get(\\\"solver\\\") == \\\"MF\\\")\\n    this->solver = gmg_mf;\\n else if (prm.get(\\\"solver\\\") == \\\"MB\\\")\\n    this->solver = gmg_mb;\\n else if (prm.get(\\\"solver\\\") == \\\"AMG\\\")\\n    this->solver = amg;\\n else\\n AssertThrow(false, ExcNotImplemented());\\n \\n  this->dimension       = prm.get_integer(\\\"dim\\\");\\n  this->n_steps         = prm.get_integer(\\\"n_steps\\\");\\n  this->smoother_dampen = prm.get_double(\\\"smoother dampen\\\");\\n  this->smoother_steps  = prm.get_integer(\\\"smoother steps\\\");\\n  this->output          = prm.get_bool(\\\"output\\\");\\n \\n return true;\\n}\\n \\n \\n \\n \\ntemplate <int dim, int degree>\\nclass LaplaceProblem\\n{\\npublic:\\n  LaplaceProblem(const Settings &settings);\\n void run();\\n \\nprivate:\\n using MatrixType      = LA::MPI::SparseMatrix;\\n using VectorType      = LA::MPI::Vector;\\n using PreconditionAMG = LA::MPI::PreconditionAMG;\\n \\n using MatrixFreeLevelMatrix = MatrixFreeOperators::LaplaceOperator<\\n    dim,\\n    degree,\\n    degree + 1,\\n    1,\\n LinearAlgebra::distributed::Vector<float>>;\\n using MatrixFreeActiveMatrix = MatrixFreeOperators::LaplaceOperator<\\n    dim,\\n    degree,\\n    degree + 1,\\n    1,\\n LinearAlgebra::distributed::Vector<double>>;\\n \\n using MatrixFreeLevelVector  = LinearAlgebra::distributed::Vector<float>;\\n using MatrixFreeActiveVector = LinearAlgebra::distributed::Vector<double>;\\n \\n void setup_system();\\n void setup_multigrid();\\n void assemble_system();\\n void assemble_multigrid();\\n void assemble_rhs();\\n void solve();\\n void estimate();\\n void refine_grid();\\n void output_results(const unsigned int cycle);\\n \\n  Settings settings;\\n \\n MPI_Comm           mpi_communicator;\\n ConditionalOStream pcout;\\n \\n parallel::distributed::Triangulation<dim> triangulation;\\n const MappingQ1<dim>                      mapping;\\n const FE_Q<dim>                           fe;\\n \\n DoFHandler<dim> dof_handler;\\n \\n IndexSet                  locally_owned_dofs;\\n IndexSet                  locally_relevant_dofs;\\n AffineConstraints<double> constraints;\\n \\n  MatrixType             system_matrix;\\n  MatrixFreeActiveMatrix mf_system_matrix;\\n  VectorType             solution;\\n  VectorType             right_hand_side;\\n Vector<double>         estimated_error_square_per_cell;\\n \\n MGLevelObject<MatrixType> mg_matrix;\\n MGLevelObject<MatrixType> mg_interface_in;\\n MGConstrainedDoFs         mg_constrained_dofs;\\n \\n MGLevelObject<MatrixFreeLevelMatrix> mf_mg_matrix;\\n \\n TimerOutput computing_timer;\\n};\\n \\n \\ntemplate <int dim, int degree>\\nLaplaceProblem<dim, degree>::LaplaceProblem(const Settings &settings)\\n  : settings(settings)\\n  , mpi_communicator(MPI_COMM_WORLD)\\n  , pcout(std::cout, (Utilities::MPI::this_mpi_process(mpi_communicator) == 0))\\n  , triangulation(mpi_communicator,\\n Triangulation<dim>::limit_level_difference_at_vertices,\\n                  (settings.solver == Settings::amg) ?\\n parallel::distributed::Triangulation<dim>::default_setting :\\n parallel::distributed::Triangulation<\\n                      dim>::construct_multigrid_hierarchy)\\n  , mapping()\\n  , fe(degree)\\n  , dof_handler(triangulation)\\n  , computing_timer(pcout, TimerOutput::never, TimerOutput::wall_times)\\n{\\n GridGenerator::hyper_L(triangulation, -1., 1., /*colorize*/ false);\\n triangulation.refine_global(1);\\n}\\n \\n \\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::setup_system()\\n{\\n TimerOutput::Scope timing(computing_timer, \\\"Setup\\\");\\n \\n  dof_handler.distribute_dofs(fe);\\n \\n  locally_relevant_dofs = DoFTools::extract_locally_relevant_dofs(dof_handler);\\n  locally_owned_dofs    = dof_handler.locally_owned_dofs();\\n \\n  solution.reinit(locally_owned_dofs, mpi_communicator);\\n  right_hand_side.reinit(locally_owned_dofs, mpi_communicator);\\n  constraints.reinit(locally_owned_dofs, locally_relevant_dofs);\\n DoFTools::make_hanging_node_constraints(dof_handler, constraints);\\n \\n VectorTools::interpolate_boundary_values(\\n    mapping, dof_handler, 0, Functions::ZeroFunction<dim>(), constraints);\\n  constraints.close();\\n \\n switch (settings.solver)\\n    {\\n case Settings::gmg_mf:\\n        {\\n typename MatrixFree<dim, double>::AdditionalData additional_data;\\n          additional_data.tasks_parallel_scheme =\\n MatrixFree<dim, double>::AdditionalData::none;\\n          additional_data.mapping_update_flags =\\n            (update_gradients | update_JxW_values | update_quadrature_points);\\n          std::shared_ptr<MatrixFree<dim, double>> mf_storage =\\n            std::make_shared<MatrixFree<dim, double>>();\\n          mf_storage->reinit(mapping,\\n                             dof_handler,\\n                             constraints,\\n QGauss<1>(degree + 1),\\n                             additional_data);\\n \\n          mf_system_matrix.initialize(mf_storage);\\n \\n const Coefficient<dim> coefficient;\\n          mf_system_matrix.set_coefficient(\\n            coefficient.make_coefficient_table(*mf_storage));\\n \\n break;\\n        }\\n \\n case Settings::gmg_mb:\\n case Settings::amg:\\n        {\\n#ifdef USE_PETSC_LA\\n DynamicSparsityPattern dsp(locally_relevant_dofs);\\n DoFTools::make_sparsity_pattern(dof_handler, dsp, constraints);\\n \\n SparsityTools::distribute_sparsity_pattern(dsp,\\n                                                     locally_owned_dofs,\\n                                                     mpi_communicator,\\n                                                     locally_relevant_dofs);\\n \\n          system_matrix.reinit(locally_owned_dofs,\\n                               locally_owned_dofs,\\n                               dsp,\\n                               mpi_communicator);\\n#else\\n TrilinosWrappers::SparsityPattern dsp(locally_owned_dofs,\\n                                                locally_owned_dofs,\\n                                                locally_relevant_dofs,\\n                                                mpi_communicator);\\n DoFTools::make_sparsity_pattern(dof_handler, dsp, constraints);\\n          dsp.compress();\\n          system_matrix.reinit(dsp);\\n#endif\\n \\n break;\\n        }\\n \\n default:\\n DEAL_II_NOT_IMPLEMENTED();\\n    }\\n}\\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::setup_multigrid()\\n{\\n TimerOutput::Scope timing(computing_timer, \\\"Setup multigrid\\\");\\n \\n  dof_handler.distribute_mg_dofs();\\n \\n  mg_constrained_dofs.clear();\\n  mg_constrained_dofs.initialize(dof_handler);\\n \\n const std::set<types::boundary_id> boundary_ids = {types::boundary_id(0)};\\n  mg_constrained_dofs.make_zero_boundary_constraints(dof_handler, boundary_ids);\\n \\n const unsigned int n_levels = triangulation.n_global_levels();\\n \\n switch (settings.solver)\\n    {\\n case Settings::gmg_mf:\\n        {\\n          mf_mg_matrix.resize(0, n_levels - 1);\\n \\n for (unsigned int level = 0; level < n_levels; ++level)\\n            {\\n AffineConstraints<double> level_constraints(\\n                dof_handler.locally_owned_mg_dofs(level),\\n DoFTools::extract_locally_relevant_level_dofs(dof_handler,\\n level));\\n for (const types::global_dof_index dof_index :\\n                   mg_constrained_dofs.get_boundary_indices(level))\\n                level_constraints.constrain_dof_to_zero(dof_index);\\n              level_constraints.close();\\n \\n typename MatrixFree<dim, float>::AdditionalData additional_data;\\n              additional_data.tasks_parallel_scheme =\\n MatrixFree<dim, float>::AdditionalData::none;\\n              additional_data.mapping_update_flags =\\n                (update_gradients | update_JxW_values |\\n update_quadrature_points);\\n              additional_data.mg_level = level;\\n              std::shared_ptr<MatrixFree<dim, float>> mf_storage_level(\\n new MatrixFree<dim, float>());\\n              mf_storage_level->reinit(mapping,\\n                                       dof_handler,\\n                                       level_constraints,\\n QGauss<1>(degree + 1),\\n                                       additional_data);\\n \\n              mf_mg_matrix[level].initialize(mf_storage_level,\\n                                             mg_constrained_dofs,\\n level);\\n \\n const Coefficient<dim> coefficient;\\n              mf_mg_matrix[level].set_coefficient(\\n                coefficient.make_coefficient_table(*mf_storage_level));\\n \\n              mf_mg_matrix[level].compute_diagonal();\\n            }\\n \\n break;\\n        }\\n \\n case Settings::gmg_mb:\\n        {\\n          mg_matrix.resize(0, n_levels - 1);\\n          mg_matrix.clear_elements();\\n          mg_interface_in.resize(0, n_levels - 1);\\n          mg_interface_in.clear_elements();\\n \\n for (unsigned int level = 0; level < n_levels; ++level)\\n            {\\n const IndexSet dof_set =\\n DoFTools::extract_locally_relevant_level_dofs(dof_handler,\\n level);\\n \\n              {\\n#ifdef USE_PETSC_LA\\n DynamicSparsityPattern dsp(dof_set);\\n MGTools::make_sparsity_pattern(dof_handler, dsp, level);\\n                dsp.compress();\\n SparsityTools::distribute_sparsity_pattern(\\n                  dsp,\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  mpi_communicator,\\n                  dof_set);\\n \\n                mg_matrix[level].reinit(\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  dsp,\\n                  mpi_communicator);\\n#else\\n TrilinosWrappers::SparsityPattern dsp(\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  dof_set,\\n                  mpi_communicator);\\n MGTools::make_sparsity_pattern(dof_handler, dsp, level);\\n \\n                dsp.compress();\\n                mg_matrix[level].reinit(dsp);\\n#endif\\n              }\\n \\n              {\\n#ifdef USE_PETSC_LA\\n DynamicSparsityPattern dsp(dof_set);\\n MGTools::make_interface_sparsity_pattern(dof_handler,\\n                                                         mg_constrained_dofs,\\n                                                         dsp,\\n level);\\n                dsp.compress();\\n SparsityTools::distribute_sparsity_pattern(\\n                  dsp,\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  mpi_communicator,\\n                  dof_set);\\n \\n                mg_interface_in[level].reinit(\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  dsp,\\n                  mpi_communicator);\\n#else\\n TrilinosWrappers::SparsityPattern dsp(\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  dof_handler.locally_owned_mg_dofs(level),\\n                  dof_set,\\n                  mpi_communicator);\\n \\n MGTools::make_interface_sparsity_pattern(dof_handler,\\n                                                         mg_constrained_dofs,\\n                                                         dsp,\\n level);\\n                dsp.compress();\\n                mg_interface_in[level].reinit(dsp);\\n#endif\\n              }\\n            }\\n break;\\n        }\\n \\n default:\\n DEAL_II_NOT_IMPLEMENTED();\\n    }\\n}\\n \\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::assemble_system()\\n{\\n TimerOutput::Scope timing(computing_timer, \\\"Assemble\\\");\\n \\n const QGauss<dim> quadrature_formula(degree + 1);\\n \\n FEValues<dim> fe_values(fe,\\n                          quadrature_formula,\\n update_values | update_gradients |\\n update_quadrature_points | update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n Vector<double>     cell_rhs(dofs_per_cell);\\n \\n  std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n \\n const Coefficient<dim> coefficient;\\n  RightHandSide<dim>     rhs;\\n  std::vector<double>    rhs_values(n_q_points);\\n \\n for (const auto &cell : dof_handler.active_cell_iterators())\\n    if (cell->is_locally_owned())\\n      {\\n cell_matrix = 0;\\n        cell_rhs    = 0;\\n \\n        fe_values.reinit(cell);\\n \\n const double coefficient_value =\\n          coefficient.average_value(fe_values.get_quadrature_points());\\n        rhs.value_list(fe_values.get_quadrature_points(), rhs_values);\\n \\n for (unsigned int q_point = 0; q_point < n_q_points; ++q_point)\\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n            {\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n cell_matrix(i, j) +=\\n                  coefficient_value *                // epsilon(x)\\n                  fe_values.shape_grad(i, q_point) * // * grad phi_i(x)\\n                  fe_values.shape_grad(j, q_point) * // * grad phi_j(x)\\n                  fe_values.JxW(q_point);            // * dx\\n \\n              cell_rhs(i) +=\\n                fe_values.shape_value(i, q_point) * // grad phi_i(x)\\n                rhs_values[q_point] *               // * f(x)\\n                fe_values.JxW(q_point);             // * dx\\n            }\\n \\n        cell->get_dof_indices(local_dof_indices);\\n        constraints.distribute_local_to_global(cell_matrix,\\n                                               cell_rhs,\\n                                               local_dof_indices,\\n                                               system_matrix,\\n                                               right_hand_side);\\n      }\\n \\n  system_matrix.compress(VectorOperation::add);\\n  right_hand_side.compress(VectorOperation::add);\\n}\\n \\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::assemble_multigrid()\\n{\\n TimerOutput::Scope timing(computing_timer, \\\"Assemble multigrid\\\");\\n \\n const QGauss<dim> quadrature_formula(degree + 1);\\n \\n FEValues<dim> fe_values(fe,\\n                          quadrature_formula,\\n update_values | update_gradients |\\n update_quadrature_points | update_JxW_values);\\n \\n const unsigned int dofs_per_cell = fe.n_dofs_per_cell();\\n const unsigned int n_q_points    = quadrature_formula.size();\\n \\n FullMatrix<double> cell_matrix(dofs_per_cell, dofs_per_cell);\\n \\n  std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);\\n \\n const Coefficient<dim> coefficient;\\n \\n  std::vector<AffineConstraints<double>> boundary_constraints(\\n triangulation.n_global_levels());\\n for (unsigned int level = 0; level < triangulation.n_global_levels(); ++level)\\n    {\\n      boundary_constraints[level].reinit(\\n        dof_handler.locally_owned_mg_dofs(level),\\n DoFTools::extract_locally_relevant_level_dofs(dof_handler, level));\\n \\n for (const types::global_dof_index dof_index :\\n           mg_constrained_dofs.get_refinement_edge_indices(level))\\n        boundary_constraints[level].constrain_dof_to_zero(dof_index);\\n for (const types::global_dof_index dof_index :\\n           mg_constrained_dofs.get_boundary_indices(level))\\n        boundary_constraints[level].constrain_dof_to_zero(dof_index);\\n      boundary_constraints[level].close();\\n    }\\n \\n for (const auto &cell : dof_handler.cell_iterators())\\n    if (cell->level_subdomain_id() == triangulation.locally_owned_subdomain())\\n      {\\n cell_matrix = 0;\\n        fe_values.reinit(cell);\\n \\n const double coefficient_value =\\n          coefficient.average_value(fe_values.get_quadrature_points());\\n \\n for (unsigned int q_point = 0; q_point < n_q_points; ++q_point)\\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n cell_matrix(i, j) +=\\n                coefficient_value * fe_values.shape_grad(i, q_point) *\\n                fe_values.shape_grad(j, q_point) * fe_values.JxW(q_point);\\n \\n        cell->get_mg_dof_indices(local_dof_indices);\\n \\n        boundary_constraints[cell->level()].distribute_local_to_global(\\n          cell_matrix, local_dof_indices, mg_matrix[cell->level()]);\\n \\n for (unsigned int i = 0; i < dofs_per_cell; ++i)\\n for (unsigned int j = 0; j < dofs_per_cell; ++j)\\n if (mg_constrained_dofs.is_interface_matrix_entry(\\n                  cell->level(), local_dof_indices[i], local_dof_indices[j]))\\n              mg_interface_in[cell->level()].add(local_dof_indices[i],\\n                                                 local_dof_indices[j],\\n cell_matrix(i, j));\\n      }\\n \\n for (unsigned int i = 0; i < triangulation.n_global_levels(); ++i)\\n    {\\n      mg_matrix[i].compress(VectorOperation::add);\\n      mg_interface_in[i].compress(VectorOperation::add);\\n    }\\n}\\n \\n \\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::assemble_rhs()\\n{\\n TimerOutput::Scope timing(computing_timer, \\\"Assemble right-hand side\\\");\\n \\n  MatrixFreeActiveVector solution_copy;\\n  MatrixFreeActiveVector right_hand_side_copy;\\n  mf_system_matrix.initialize_dof_vector(solution_copy);\\n  mf_system_matrix.initialize_dof_vector(right_hand_side_copy);\\n \\n  solution_copy = 0.;\\n  constraints.distribute(solution_copy);\\n  solution_copy.update_ghost_values();\\n  right_hand_side_copy = 0;\\n const Table<2, VectorizedArray<double>> &coefficient =\\n    *(mf_system_matrix.get_coefficient());\\n \\n  RightHandSide<dim> right_hand_side_function;\\n \\n FEEvaluation<dim, degree, degree + 1, 1, double> phi(\\n    *mf_system_matrix.get_matrix_free());\\n \\n for (unsigned int cell = 0;\\n       cell < mf_system_matrix.get_matrix_free()->n_cell_batches();\\n       ++cell)\\n    {\\n      phi.reinit(cell);\\n      phi.read_dof_values_plain(solution_copy);\\n      phi.evaluate(EvaluationFlags::gradients);\\n \\n for (const unsigned int q : phi.quadrature_point_indices())\\n        {\\n          phi.submit_gradient(-1.0 *\\n                                (coefficient(cell, 0) * phi.get_gradient(q)),\\n                              q);\\n          phi.submit_value(\\n            right_hand_side_function.value(phi.quadrature_point(q)), q);\\n        }\\n \\n      phi.integrate_scatter(EvaluationFlags::values |\\n EvaluationFlags::gradients,\\n                            right_hand_side_copy);\\n    }\\n \\n  right_hand_side_copy.compress(VectorOperation::add);\\n \\n  ChangeVectorTypes::copy(right_hand_side, right_hand_side_copy);\\n}\\n \\n \\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::solve()\\n{\\n TimerOutput::Scope timing(computing_timer, \\\"Solve\\\");\\n \\n SolverControl solver_control(1000, 1.e-10 * right_hand_side.l2_norm());\\n  solver_control.enable_history_data();\\n \\n  solution = 0.;\\n \\n switch (settings.solver)\\n    {\\n case Settings::gmg_mf:\\n        {\\n          computing_timer.enter_subsection(\\\"Solve: Preconditioner setup\\\");\\n \\n MGTransferMatrixFree<dim, float> mg_transfer(mg_constrained_dofs);\\n          mg_transfer.build(dof_handler);\\n \\n SolverControl coarse_solver_control(1000, 1e-12, false, false);\\n SolverCG<MatrixFreeLevelVector> coarse_solver(coarse_solver_control);\\n PreconditionIdentity identity;\\n MGCoarseGridIterativeSolver<MatrixFreeLevelVector,\\n SolverCG<MatrixFreeLevelVector>,\\n                                      MatrixFreeLevelMatrix,\\n PreconditionIdentity>\\n            coarse_grid_solver(coarse_solver, mf_mg_matrix[0], identity);\\n \\n using Smoother = PreconditionJacobi<MatrixFreeLevelMatrix>;\\n MGSmootherPrecondition<MatrixFreeLevelMatrix,\\n                                 Smoother,\\n                                 MatrixFreeLevelVector>\\n            smoother;\\n          smoother.initialize(mf_mg_matrix,\\n typename Smoother::AdditionalData(\\n                                settings.smoother_dampen));\\n          smoother.set_steps(settings.smoother_steps);\\n \\n mg::Matrix<MatrixFreeLevelVector> mg_m(mf_mg_matrix);\\n \\n MGLevelObject<\\n MatrixFreeOperators::MGInterfaceOperator<MatrixFreeLevelMatrix>>\\n            mg_interface_matrices;\\n          mg_interface_matrices.resize(0, triangulation.n_global_levels() - 1);\\n for (unsigned int level = 0; level < triangulation.n_global_levels();\\n               ++level)\\n            mg_interface_matrices[level].initialize(mf_mg_matrix[level]);\\n mg::Matrix<MatrixFreeLevelVector> mg_interface(mg_interface_matrices);\\n \\n Multigrid<MatrixFreeLevelVector> mg(\\n            mg_m, coarse_grid_solver, mg_transfer, smoother, smoother);\\n mg.set_edge_matrices(mg_interface, mg_interface);\\n \\n PreconditionMG<dim,\\n                         MatrixFreeLevelVector,\\n MGTransferMatrixFree<dim, float>>\\n            preconditioner(dof_handler, mg, mg_transfer);\\n \\n          MatrixFreeActiveVector solution_copy;\\n          MatrixFreeActiveVector right_hand_side_copy;\\n          mf_system_matrix.initialize_dof_vector(solution_copy);\\n          mf_system_matrix.initialize_dof_vector(right_hand_side_copy);\\n \\n          ChangeVectorTypes::copy(solution_copy, solution);\\n          ChangeVectorTypes::copy(right_hand_side_copy, right_hand_side);\\n          computing_timer.leave_subsection(\\\"Solve: Preconditioner setup\\\");\\n \\n          {\\n TimerOutput::Scope timing(computing_timer,\\n \\\"Solve: 1 multigrid V-cycle\\\");\\n            preconditioner.vmult(solution_copy, right_hand_side_copy);\\n          }\\n          solution_copy = 0.;\\n \\n          {\\n SolverCG<MatrixFreeActiveVector> solver(solver_control);\\n \\n TimerOutput::Scope timing(computing_timer, \\\"Solve: CG\\\");\\n            solver.solve(mf_system_matrix,\\n                         solution_copy,\\n                         right_hand_side_copy,\\n                         preconditioner);\\n          }\\n \\n          solution_copy.update_ghost_values();\\n          ChangeVectorTypes::copy(solution, solution_copy);\\n          constraints.distribute(solution);\\n \\n break;\\n        }\\n \\n case Settings::gmg_mb:\\n        {\\n          computing_timer.enter_subsection(\\\"Solve: Preconditioner setup\\\");\\n \\n MGTransferPrebuilt<VectorType> mg_transfer(mg_constrained_dofs);\\n          mg_transfer.build(dof_handler);\\n \\n SolverControl        coarse_solver_control(1000, 1e-12, false, false);\\n SolverCG<VectorType> coarse_solver(coarse_solver_control);\\n PreconditionIdentity identity;\\n MGCoarseGridIterativeSolver<VectorType,\\n SolverCG<VectorType>,\\n                                      MatrixType,\\n PreconditionIdentity>\\n            coarse_grid_solver(coarse_solver, mg_matrix[0], identity);\\n \\n using Smoother = LA::MPI::PreconditionJacobi;\\n MGSmootherPrecondition<MatrixType, Smoother, VectorType> smoother;\\n \\n#ifdef USE_PETSC_LA\\n          smoother.initialize(mg_matrix);\\n Assert(\\n            settings.smoother_dampen == 1.0,\\n            ExcNotImplemented(\\n \\\"PETSc's PreconditionJacobi has no support for a damping parameter.\\\"));\\n#else\\n          smoother.initialize(mg_matrix, settings.smoother_dampen);\\n#endif\\n \\n          smoother.set_steps(settings.smoother_steps);\\n \\n mg::Matrix<VectorType> mg_m(mg_matrix);\\n mg::Matrix<VectorType> mg_in(mg_interface_in);\\n mg::Matrix<VectorType> mg_out(mg_interface_in);\\n \\n Multigrid<VectorType> mg(\\n            mg_m, coarse_grid_solver, mg_transfer, smoother, smoother);\\n mg.set_edge_matrices(mg_out, mg_in);\\n \\n \\n PreconditionMG<dim, VectorType, MGTransferPrebuilt<VectorType>>\\n            preconditioner(dof_handler, mg, mg_transfer);\\n \\n          computing_timer.leave_subsection(\\\"Solve: Preconditioner setup\\\");\\n \\n          {\\n TimerOutput::Scope timing(computing_timer,\\n \\\"Solve: 1 multigrid V-cycle\\\");\\n            preconditioner.vmult(solution, right_hand_side);\\n          }\\n          solution = 0.;\\n \\n          {\\n SolverCG<VectorType> solver(solver_control);\\n \\n TimerOutput::Scope timing(computing_timer, \\\"Solve: CG\\\");\\n            solver.solve(system_matrix,\\n                         solution,\\n                         right_hand_side,\\n                         preconditioner);\\n          }\\n \\n          constraints.distribute(solution);\\n \\n break;\\n        }\\n \\n case Settings::amg:\\n        {\\n          computing_timer.enter_subsection(\\\"Solve: Preconditioner setup\\\");\\n \\n PreconditionAMG                 preconditioner;\\n          PreconditionAMG::AdditionalData Amg_data;\\n \\n#ifdef USE_PETSC_LA\\n          Amg_data.symmetric_operator = true;\\n#else\\n          Amg_data.elliptic              = true;\\n          Amg_data.smoother_type         = \\\"Jacobi\\\";\\n          Amg_data.higher_order_elements = true;\\n          Amg_data.smoother_sweeps       = settings.smoother_steps;\\n          Amg_data.aggregation_threshold = 0.02;\\n#endif\\n \\n          Amg_data.output_details = false;\\n \\n          preconditioner.initialize(system_matrix, Amg_data);\\n          computing_timer.leave_subsection(\\\"Solve: Preconditioner setup\\\");\\n \\n          {\\n TimerOutput::Scope timing(computing_timer,\\n \\\"Solve: 1 multigrid V-cycle\\\");\\n            preconditioner.vmult(solution, right_hand_side);\\n          }\\n          solution = 0.;\\n \\n          {\\n SolverCG<VectorType> solver(solver_control);\\n \\n TimerOutput::Scope timing(computing_timer, \\\"Solve: CG\\\");\\n            solver.solve(system_matrix,\\n                         solution,\\n                         right_hand_side,\\n                         preconditioner);\\n          }\\n          constraints.distribute(solution);\\n \\n break;\\n        }\\n \\n default:\\n DEAL_II_ASSERT_UNREACHABLE();\\n    }\\n \\n  pcout << \\\"   Number of CG iterations:      \\\" << solver_control.last_step()\\n        << std::endl;\\n}\\n \\n \\n \\ntemplate <int dim>\\nstruct ScratchData\\n{\\n  ScratchData(const Mapping<dim>       &mapping,\\n const FiniteElement<dim> &fe,\\n const unsigned int        quadrature_degree,\\n const UpdateFlags         update_flags,\\n const UpdateFlags         interface_update_flags)\\n    : fe_values(mapping, fe, QGauss<dim>(quadrature_degree), update_flags)\\n    , fe_interface_values(mapping,\\n                          fe,\\n QGauss<dim - 1>(quadrature_degree),\\n                          interface_update_flags)\\n  {}\\n \\n \\n  ScratchData(const ScratchData<dim> &scratch_data)\\n    : fe_values(scratch_data.fe_values.get_mapping(),\\n                scratch_data.fe_values.get_fe(),\\n                scratch_data.fe_values.get_quadrature(),\\n                scratch_data.fe_values.get_update_flags())\\n    , fe_interface_values(scratch_data.fe_values.get_mapping(),\\n                          scratch_data.fe_values.get_fe(),\\n                          scratch_data.fe_interface_values.get_quadrature(),\\n                          scratch_data.fe_interface_values.get_update_flags())\\n  {}\\n \\n FEValues<dim>          fe_values;\\n FEInterfaceValues<dim> fe_interface_values;\\n};\\n \\n \\n \\nstruct CopyData\\n{\\n  CopyData()\\n    : cell_index(numbers::invalid_unsigned_int)\\n    , value(0.)\\n  {}\\n \\n struct FaceData\\n  {\\n unsigned int cell_indices[2];\\n double       values[2];\\n  };\\n \\n unsigned int cell_index;\\n double value;\\n  std::vector<FaceData> face_data;\\n};\\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::estimate()\\n{\\n TimerOutput::Scope timing(computing_timer, \\\"Estimate\\\");\\n \\n  VectorType temp_solution;\\n  temp_solution.reinit(locally_owned_dofs,\\n                       locally_relevant_dofs,\\n                       mpi_communicator);\\n  temp_solution = solution;\\n \\n const Coefficient<dim> coefficient;\\n \\n  estimated_error_square_per_cell.reinit(triangulation.n_active_cells());\\n \\n using Iterator = typename DoFHandler<dim>::active_cell_iterator;\\n \\n auto cell_worker = [&](const Iterator   &cell,\\n                         ScratchData<dim> &scratch_data,\\n                         CopyData         &copy_data) {\\n FEValues<dim> &fe_values = scratch_data.fe_values;\\n    fe_values.reinit(cell);\\n \\n    RightHandSide<dim> rhs;\\n const double       rhs_value = rhs.value(cell->center());\\n \\n const double nu = coefficient.value(cell->center());\\n \\n    std::vector<Tensor<2, dim>> hessians(fe_values.n_quadrature_points);\\n    fe_values.get_function_hessians(temp_solution, hessians);\\n \\n    copy_data.cell_index = cell->active_cell_index();\\n \\n double residual_norm_square = 0.;\\n for (unsigned k = 0; k < fe_values.n_quadrature_points; ++k)\\n      {\\n const double residual = (rhs_value + nu * trace(hessians[k]));\\n        residual_norm_square += residual * residual * fe_values.JxW(k);\\n      }\\n \\n    copy_data.value =\\n      cell->diameter() * cell->diameter() * residual_norm_square;\\n  };\\n \\n auto face_worker = [&](const Iterator     &cell,\\n const unsigned int &f,\\n const unsigned int &sf,\\n const Iterator     &ncell,\\n const unsigned int &nf,\\n const unsigned int &nsf,\\n                         ScratchData<dim>   &scratch_data,\\n                         CopyData           &copy_data) {\\n FEInterfaceValues<dim> &fe_interface_values =\\n      scratch_data.fe_interface_values;\\n    fe_interface_values.reinit(cell, f, sf, ncell, nf, nsf);\\n \\n    copy_data.face_data.emplace_back();\\n    CopyData::FaceData &copy_data_face = copy_data.face_data.back();\\n \\n    copy_data_face.cell_indices[0] = cell->active_cell_index();\\n    copy_data_face.cell_indices[1] = ncell->active_cell_index();\\n \\n const double coeff1 = coefficient.value(cell->center());\\n const double coeff2 = coefficient.value(ncell->center());\\n \\n    std::vector<Tensor<1, dim>> grad_u[2];\\n \\n for (unsigned int i = 0; i < 2; ++i)\\n      {\\n        grad_u[i].resize(fe_interface_values.n_quadrature_points);\\n        fe_interface_values.get_fe_face_values(i).get_function_gradients(\\n          temp_solution, grad_u[i]);\\n      }\\n \\n double jump_norm_square = 0.;\\n \\n for (unsigned int qpoint = 0;\\n         qpoint < fe_interface_values.n_quadrature_points;\\n         ++qpoint)\\n      {\\n const double jump =\\n          coeff1 * grad_u[0][qpoint] * fe_interface_values.normal(qpoint) -\\n          coeff2 * grad_u[1][qpoint] * fe_interface_values.normal(qpoint);\\n \\n        jump_norm_square += jump * jump * fe_interface_values.JxW(qpoint);\\n      }\\n \\n const double h           = cell->face(f)->measure();\\n    copy_data_face.values[0] = 0.5 * h * jump_norm_square;\\n    copy_data_face.values[1] = copy_data_face.values[0];\\n  };\\n \\n auto copier = [&](const CopyData &copy_data) {\\n if (copy_data.cell_index != numbers::invalid_unsigned_int)\\n      estimated_error_square_per_cell[copy_data.cell_index] += copy_data.value;\\n \\n for (const auto &cdf : copy_data.face_data)\\n      for (unsigned int j = 0; j < 2; ++j)\\n        estimated_error_square_per_cell[cdf.cell_indices[j]] += cdf.values[j];\\n  };\\n \\n const unsigned int n_gauss_points = degree + 1;\\n  ScratchData<dim>   scratch_data(mapping,\\n                                fe,\\n                                n_gauss_points,\\n update_hessians | update_quadrature_points |\\n update_JxW_values,\\n update_values | update_gradients |\\n update_JxW_values | update_normal_vectors);\\n  CopyData           copy_data;\\n \\n MeshWorker::mesh_loop(dof_handler.begin_active(),\\n                        dof_handler.end(),\\n                        cell_worker,\\n                        copier,\\n                        scratch_data,\\n                        copy_data,\\n MeshWorker::assemble_own_cells |\\n MeshWorker::assemble_ghost_faces_both |\\n MeshWorker::assemble_own_interior_faces_once,\\n /*boundary_worker=*/nullptr,\\n                        face_worker);\\n \\n const double global_error_estimate =\\n std::sqrt(Utilities::MPI::sum(estimated_error_square_per_cell.l1_norm(),\\n                                  mpi_communicator));\\n  pcout << \\\"   Global error estimate:        \\\" << global_error_estimate\\n        << std::endl;\\n}\\n \\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::refine_grid()\\n{\\n TimerOutput::Scope timing(computing_timer, \\\"Refine grid\\\");\\n \\n const double refinement_fraction = 1. / (std::pow(2.0, dim) - 1.);\\n parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number(\\n triangulation, estimated_error_square_per_cell, refinement_fraction, 0.0);\\n \\n triangulation.execute_coarsening_and_refinement();\\n}\\n \\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::output_results(const unsigned int cycle)\\n{\\n TimerOutput::Scope timing(computing_timer, \\\"Output results\\\");\\n \\n  VectorType temp_solution;\\n  temp_solution.reinit(locally_owned_dofs,\\n                       locally_relevant_dofs,\\n                       mpi_communicator);\\n  temp_solution = solution;\\n \\n DataOut<dim> data_out;\\n  data_out.attach_dof_handler(dof_handler);\\n  data_out.add_data_vector(temp_solution, \\\"solution\\\");\\n \\n Vector<float> subdomain(triangulation.n_active_cells());\\n for (unsigned int i = 0; i < subdomain.size(); ++i)\\n    subdomain(i) = triangulation.locally_owned_subdomain();\\n  data_out.add_data_vector(subdomain, \\\"subdomain\\\");\\n \\n Vector<float> level(triangulation.n_active_cells());\\n for (const auto &cell : triangulation.active_cell_iterators())\\n level(cell->active_cell_index()) = cell->level();\\n  data_out.add_data_vector(level, \\\"level\\\");\\n \\n if (estimated_error_square_per_cell.size() > 0)\\n    data_out.add_data_vector(estimated_error_square_per_cell,\\n \\\"estimated_error_square_per_cell\\\");\\n \\n  data_out.build_patches();\\n \\n const std::string pvtu_filename = data_out.write_vtu_with_pvtu_record(\\n \\\"\\\", \\\"solution\\\", cycle, mpi_communicator, 2 /*n_digits*/, 1 /*n_groups*/);\\n \\n  pcout << \\\"   Wrote \\\" << pvtu_filename << std::endl;\\n}\\n \\n \\n \\ntemplate <int dim, int degree>\\nvoid LaplaceProblem<dim, degree>::run()\\n{\\n for (unsigned int cycle = 0; cycle < settings.n_steps; ++cycle)\\n    {\\n      pcout << \\\"Cycle \\\" << cycle << ':' << std::endl;\\n if (cycle > 0)\\n        refine_grid();\\n \\n      pcout << \\\"   Number of active cells:       \\\"\\n            << triangulation.n_global_active_cells();\\n \\n if (settings.solver == Settings::gmg_mf ||\\n          settings.solver == Settings::gmg_mb)\\n        pcout << \\\" (\\\" << triangulation.n_global_levels() << \\\" global levels)\\\"\\n              << std::endl\\n              << \\\"   Partition efficiency:         \\\"\\n              << 1.0 / MGTools::workload_imbalance(triangulation);\\n      pcout << std::endl;\\n \\n      setup_system();\\n \\n if (settings.solver == Settings::gmg_mf ||\\n          settings.solver == Settings::gmg_mb)\\n        setup_multigrid();\\n \\n      pcout << \\\"   Number of degrees of freedom: \\\" << dof_handler.n_dofs();\\n if (settings.solver == Settings::gmg_mf ||\\n          settings.solver == Settings::gmg_mb)\\n        {\\n          pcout << \\\" (by level: \\\";\\n for (unsigned int level = 0; level < triangulation.n_global_levels();\\n               ++level)\\n            pcout << dof_handler.n_dofs(level)\\n                  << (level == triangulation.n_global_levels() - 1 ? \\\")\\\" :\\n \\\", \\\");\\n        }\\n      pcout << std::endl;\\n \\n if (settings.solver == Settings::gmg_mf)\\n        assemble_rhs();\\n else /*gmg_mb or amg*/\\n        {\\n          assemble_system();\\n if (settings.solver == Settings::gmg_mb)\\n            assemble_multigrid();\\n        }\\n \\n      solve();\\n      estimate();\\n \\n if (settings.output)\\n        output_results(cycle);\\n \\n      computing_timer.print_summary();\\n      computing_timer.reset();\\n    }\\n}\\n \\n \\n \\nint main(int argc, char *argv[])\\n{\\n using namespace dealii;\\n Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);\\n \\n  Settings settings;\\n if (!settings.try_parse((argc > 1) ? (argv[1]) : \\\"\\\"))\\n    return 0;\\n \\n try\\n    {\\n constexpr unsigned int fe_degree = 2;\\n \\n switch (settings.dimension)\\n        {\\n case 2:\\n            {\\n              LaplaceProblem<2, fe_degree> test(settings);\\n              test.run();\\n \\n break;\\n            }\\n \\n case 3:\\n            {\\n              LaplaceProblem<3, fe_degree> test(settings);\\n              test.run();\\n \\n break;\\n            }\\n \\n default:\\n DEAL_II_NOT_IMPLEMENTED();\\n        }\\n    }\\n catch (std::exception &exc)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Exception on processing: \\\" << std::endl\\n                << exc.what() << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      MPI_Abort(MPI_COMM_WORLD, 1);\\n return 1;\\n    }\\n catch (...)\\n    {\\n      std::cerr << std::endl\\n                << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      std::cerr << \\\"Unknown exception!\\\" << std::endl\\n                << \\\"Aborting!\\\" << std::endl\\n                << \\\"----------------------------------------------------\\\"\\n                << std::endl;\\n      MPI_Abort(MPI_COMM_WORLD, 2);\\n return 1;\\n    }\\n \\n return 0;\\n}\\naffine_constraints.h\\nDataOutInterface::write_vtu_with_pvtu_recordstd::string write_vtu_with_pvtu_record(const std::string &directory, const std::string &filename_without_extension, const unsigned int counter, const MPI_Comm mpi_communicator, const unsigned int n_digits_for_counter=numbers::invalid_unsigned_int, const unsigned int n_groups=0) constDefinition data_out_base.cc:7854\\nDataOut_DoFData::add_data_vectorvoid add_data_vector(const VectorType &data, const std::vector< std::string > &names, const DataVectorType type=type_automatic, const std::vector< DataComponentInterpretation::DataComponentInterpretation > &data_component_interpretation={})Definition data_out_dof_data.h:1069\\nDataOut::build_patchesvirtual void build_patches(const unsigned int n_subdivisions=0)Definition data_out.cc:1062\\nFEInterfaceValues::n_quadrature_pointsconst unsigned int n_quadrature_pointsDefinition fe_interface_values.h:1282\\nFEInterfaceValues::get_fe_face_valuesconst FEFaceValuesBase< dim, spacedim > & get_fe_face_values(const unsigned int cell_index) const\\nFEInterfaceValues::JxWdouble JxW(const unsigned int quadrature_point) const\\nFEInterfaceValues::normalTensor< 1, spacedim > normal(const unsigned int q_point_index) const\\nFEValuesBase::get_function_hessiansvoid get_function_hessians(const ReadVector< Number > &fe_function, std::vector< Tensor< 2, spacedim, Number > > &hessians) constDefinition fe_values_base.cc:954\\nFEValuesBase::n_quadrature_pointsconst unsigned int n_quadrature_pointsDefinition fe_values_base.h:174\\nFEValuesBase::get_function_gradientsvoid get_function_gradients(const ReadVector< Number > &fe_function, std::vector< Tensor< 1, spacedim, Number > > &gradients) constDefinition fe_values_base.cc:851\\nFEValuesBase::JxWdouble JxW(const unsigned int q_point) const\\nLinearAlgebra::ReadWriteVector::reinitvirtual void reinit(const size_type size, const bool omit_zeroing_entries=false)\\nMGCoarseGridIterativeSolver::initializevoid initialize(SolverType &solver, const MatrixType &matrix, const PreconditionerType &precondition)\\nMGSmoother::set_stepsvoid set_steps(const unsigned int)\\nParameterHandler::parse_inputvirtual void parse_input(std::istream &input, const std::string &filename=\\\"input file\\\", const std::string &last_line=\\\"\\\", const bool skip_undefined=false)Definition parameter_handler.cc:433\\nParameterHandler::print_parametersstd::ostream & print_parameters(std::ostream &out, const OutputStyle style) constDefinition parameter_handler.cc:1312\\nParameterHandler::get_integerlong int get_integer(const std::string &entry_string) constDefinition parameter_handler.cc:1087\\nParameterHandler::get_boolbool get_bool(const std::string &entry_name) constDefinition parameter_handler.cc:1175\\nParameterHandler::getstd::string get(const std::string &entry_string) constDefinition parameter_handler.cc:1049\\nParameterHandler::get_doubledouble get_double(const std::string &entry_name) constDefinition parameter_handler.cc:1130\\nconditional_ostream.h\\ndata_out_base.h\\ngrid_refinement.h\\ntria.h\\ndof_tools.h\\ndynamic_sparsity_pattern.h\\nfe_values.h\\nfe_evaluation.h\\nfe_interface_values.h\\nfe_q.h\\ngeneric_linear_algebra.h\\ngrid_refinement.h\\ntria.h\\ngrid_generator.h\\nindex_set.h\\nmapping_q1.h\\nmatrix_free.h\\nmesh_loop.h\\nmg_coarse.h\\nmg_constrained_dofs.h\\nmg_matrix.h\\nmg_smoother.h\\nmg_tools.h\\nmg_transfer.h\\nmg_transfer_matrix_free.h\\nmultigrid.h\\nEvaluationFlags::hessians@ hessiansDefinition evaluation_flags.h:58\\nGridGenerator::hyper_Lvoid hyper_L(Triangulation< dim > &tria, const double left=-1., const double right=1., const bool colorize=false)\\nLinearAlgebraPETSc::MPI::PreconditionAMGPETScWrappers::PreconditionBoomerAMG PreconditionAMGDefinition generic_linear_algebra.h:132\\nLocalIntegrators::Advection::cell_matrixvoid cell_matrix(FullMatrix< double > &M, const FEValuesBase< dim > &fe, const FEValuesBase< dim > &fetest, const ArrayView< const std::vector< double > > &velocity, const double factor=1.)Definition advection.h:74\\nTriangulationDescription::default_setting@ default_settingDefinition tria_description.h:313\\nTriangulationDescription::construct_multigrid_hierarchy@ construct_multigrid_hierarchyDefinition tria_description.h:319\\nWorkStream::internal::tbb_no_coloring::runvoid run(const Iterator &begin, const std_cxx20::type_identity_t< Iterator > &end, Worker worker, Copier copier, const ScratchData &sample_scratch_data, const CopyData &sample_copy_data, const unsigned int queue_length, const unsigned int chunk_size)Definition work_stream.h:471\\ninternal::VectorOperations::copyvoid copy(const T *begin, const T *end, U *dest)Definition vector_operations_internal.h:65\\ndata_out.h\\noperators.h\\nparameter_handler.h\\nquadrature_lib.h\\nsolver_cg.h\\nsparsity_tools.h\\nMatrixFree::AdditionalData::mg_levelunsigned int mg_levelDefinition matrix_free.h:451\\nMatrixFree::AdditionalData::mapping_update_flagsUpdateFlags mapping_update_flagsDefinition matrix_free.h:373\\ntimer.h\\nvector_tools.h\\n \\n\\n\\n\\n\\nGenerated by\\u00a0 1.11.0\\n\\n\\n\\n\\n\", \"type\": \"Document\"}}]"